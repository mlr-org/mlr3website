[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Entry points to learn about mlr3.\n\n\n\n\n\n\nCentral entry point to the mlr3verse.\n\n\n\n\n\n\n\n\nCollection of case studies and demos.\n\n\n\n\n\n\n\n\nThe mlr3 ecosystem is build on R6 classes. The link gives an introduction to R6.\n\n\n\n\n\n\n\n\nLink to the future framework that is used to parallelize functions in mlr3.\n\n\n\n\n\n\n\n\nLink to mlr3 developer wiki."
  },
  {
    "objectID": "resources.html#documentation",
    "href": "resources.html#documentation",
    "title": "Resources",
    "section": "",
    "text": "Entry points to learn about mlr3.\n\n\n\n\n\n\nCentral entry point to the mlr3verse.\n\n\n\n\n\n\n\n\nCollection of case studies and demos.\n\n\n\n\n\n\n\n\nThe mlr3 ecosystem is build on R6 classes. The link gives an introduction to R6.\n\n\n\n\n\n\n\n\nLink to the future framework that is used to parallelize functions in mlr3.\n\n\n\n\n\n\n\n\nLink to mlr3 developer wiki."
  },
  {
    "objectID": "resources.html#cheat-sheets",
    "href": "resources.html#cheat-sheets",
    "title": "Resources",
    "section": "Cheat Sheets",
    "text": "Cheat Sheets\nThe essential things neatly summarized. Perfectly printed out next to the keyboard or on a second monitor.\n\n\n\n\n\n\n \nmlr3\n\n\nCore package cheat sheet.\n\n\n\n\n\n\n\n\n \nmlr3tuning\n\n\nTuning cheat sheet.\n\n\n\n\n\n\n\n\n \nmlr3fselect\n\n\nFeature selection cheat sheet.\n\n\n\n\n\n\n\n\n \nmlr3pipelines\n\n\nPipelines cheat sheet."
  },
  {
    "objectID": "resources.html#videos",
    "href": "resources.html#videos",
    "title": "Resources",
    "section": "Videos",
    "text": "Videos\nRecorded tutorials and lectures we have given.\n\n\n\n\n\n\n \nuseR2019 talk\n\n\nShort intro to mlr3.\n\n\n\n\n\n\n\n\n \nuseR2019 talk\n\n\nShort intro to mlr3pipelines and mlr3tuning.\n\n\n\n\n\n\n\n\n \nuseR2020 tutorial\n\n\nTutorial on mlr3, mlr3tuning and mlr3pipelines.\n\n\n\n\n\n\n\n\n \nODSC talk 2021\n\n\nInto to mlr3spatiotempcv and mlr3spatial."
  },
  {
    "objectID": "resources.html#courseslectures",
    "href": "resources.html#courseslectures",
    "title": "Resources",
    "section": "Courses/Lectures",
    "text": "Courses/Lectures\nMaterial from teaching at our universities.\n\n\n\n\n\n\n \nI2ML course\n\n\nIntroduction to ML course. Free video lectures, slides, quizzes. Exercises use mlr3.\n\n\n\n\n\n\n\n\n \nmlr-outreach\n\n\nSlides and other material for teaching mlr3."
  },
  {
    "objectID": "resources.html#peer-reviewed-articles",
    "href": "resources.html#peer-reviewed-articles",
    "title": "Resources",
    "section": "Peer-reviewed Articles",
    "text": "Peer-reviewed Articles\nA more scientific view on our packages and the packages we depend on.\n\nLang et al. (2019): about the base package mlr3\nBinder et al. (2021): building machine learning pipelines with mlr3pipelines\nSonabend et al. (2021): probabilistic regression with mlr3proba (including survival analysis)\nBengtsson (2021): the parallelization framework package future we build upon\nLang (2017): package checkmate for argument checking and defensive programming\nLang, Bischl, and Surmann (2017): parallelization framework batchtools for high-performance computing clusters, used via future or mlr3batchmark"
  },
  {
    "objectID": "resources.html#external-tutorials",
    "href": "resources.html#external-tutorials",
    "title": "Resources",
    "section": "External Tutorials",
    "text": "External Tutorials\n\nPargent, Schoedel, and Stachl (2023): An Introduction to Machine Learning for Psychologists in R\nZhao et al. (2024): Tutorial on survival modeling with applications to omics data. Tutorial Website.\nToby Hocking has written various tutorials on mlr3, including a comparison with other ML frameworks.\nLouis J. M. Aslett has also written a brief tutorial on mlr3."
  },
  {
    "objectID": "measures.html",
    "href": "measures.html",
    "title": "Measures",
    "section": "",
    "text": "Measures\nMeasures operate on Prediction objects generated by learners. They quantify the prediction by comparing prediction with ground truth. The Measure objects provide an abstraction for a plethora of performance measures."
  },
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Gallery",
    "section": "",
    "text": "In the gallery, you find case studies and demos. The posts are mostly about specific features and cover advanced topics. If you are completely new to mlr3 or machine learning, you should start with the book. Pick a post from one of the four categories or browse all posts sorted by date.\n\n\n\n  \n       \n  \n  \n\n\n  \n    \n      Wrapper-based Ensemble Feature Selection\n      Find the most stable and predictive features using multiple learners and resampling techniques.\n\n    \n  \n    \n      Time constraints in the mlr3 ecosystem\n      Set time limits for learners, tuning and nested resampling.\n\n    \n  \n    \n      Analyzing the Runtime Performance of tidymodels and mlr3\n      Compare the runtime performance of tidymodels and mlr3.\n\n    \n  \n    \n      Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)\n      Demonstrate use of survival BART on the lung dataset via mlr3proba and distr6.\n\n    \n  \n    \n      Spatial Data in the mlr3 Ecosystem\n      Run a land cover classification of the city of Leipzig.\n\n    \n  \n\nNo matching items\n\n  \n\n\n  \n    See all posts  \n  \n  \n\n\n\n\n\n  \n       \n  \n  \n\n\n  \n    \n      Imbalanced Data Handling\n      Handle imbalanced data with oversampling, undersampling, and SMOTE imbalance correction.\n\n    \n  \n    \n      Resampling - Stratified, Blocked and Predefined\n      Apply stratified, block and custom resampling.\n\n    \n  \n    \n      Factor Encoding\n      Encode factor variables in a task.\n\n    \n  \n    \n      German Credit Series\n      Train, tune and pipeline different machine learning algorithms.\n\n    \n  \n\nNo matching items\n\n  \n    See all posts  \n  \n  \n\n\n\n\n\n  \n       \n  \n  \n\n\n  \n    \n      Recursive Feature Elimination on the Sonar Data Set\n      Utilize the built-in feature importance of models.\n\n    \n  \n    \n      Hyperband Series\n      Use the Hyperband optimizer with different budget parameters.\n\n    \n  \n    \n      Practical Tuning Series\n      Start with a tuned SVM and finish with a AutoML model.\n\n    \n  \n    \n      Default Hyperparameter Configuration\n      Run the default hyperparameter configuration of learners as a baseline.\n\n    \n  \n\nNo matching items\n\n  \n    See all posts  \n  \n  \n\n\n\n\n\n  \n       \n  \n  \n\n\n  \n    \n      A Pipeline for the Titanic Data Set\n      Create new features and impute missing values with a pipeline.\n\n    \n  \n    \n      Pipelines, Selectors, Branches\n      Build a preprocessing pipeline with branching.\n\n    \n  \n    \n      Target Transformations via Pipelines\n      Transform the target variable.\n\n    \n  \n    \n      Tuning a Complex Graph\n      Tune a preprocessing pipeline and multiple tuners at once.\n\n    \n  \n\nNo matching items\n\n  \n    See all posts  \n  \n  \n\n\n\n\n\n  \n       \n  \n  \n\n\n  \n    \n      Time constraints in the mlr3 ecosystem\n      Set time limits for learners, tuning and nested resampling.\n\n    \n  \n    \n      Production Example Using Plumber and Docker\n      Write a REST API using plumber and deploy it using Docker.\n\n    \n  \n    \n      Visualization in mlr3\n      Quickly plot objects of the mlr3 ecosystem.\n\n    \n  \n    \n      Spatial Data in the mlr3 Ecosystem\n      Run a land cover classification of the city of Leipzig.\n\n    \n  \n\nNo matching items\n\n  \n    See all posts  \n  \n  \n\n\n\n\n\n  \n       \n  \n  \n\n\n\nThis curated exercise collection offers hands-on, solvable exercises focused on core concepts, from basic modeling and resampling to tuning and advanced techniques. Each exercise follows a consistent structure: clear learning goal, prerequisites, structured task list with hints how to solve them, collapsible password-protected solutions, and key take-aways designed to support systematic practice and self-paced learning.\n  \n    See all exercises"
  },
  {
    "objectID": "gallery.html#latest",
    "href": "gallery.html#latest",
    "title": "Gallery",
    "section": "",
    "text": "Wrapper-based Ensemble Feature Selection\n      Find the most stable and predictive features using multiple learners and resampling techniques.\n\n    \n  \n    \n      Time constraints in the mlr3 ecosystem\n      Set time limits for learners, tuning and nested resampling.\n\n    \n  \n    \n      Analyzing the Runtime Performance of tidymodels and mlr3\n      Compare the runtime performance of tidymodels and mlr3.\n\n    \n  \n    \n      Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)\n      Demonstrate use of survival BART on the lung dataset via mlr3proba and distr6.\n\n    \n  \n    \n      Spatial Data in the mlr3 Ecosystem\n      Run a land cover classification of the city of Leipzig.\n\n    \n  \n\nNo matching items\n\n  \n\n\n  \n    See all posts"
  },
  {
    "objectID": "gallery.html#train-and-evaluate-models",
    "href": "gallery.html#train-and-evaluate-models",
    "title": "Gallery",
    "section": "",
    "text": "Imbalanced Data Handling\n      Handle imbalanced data with oversampling, undersampling, and SMOTE imbalance correction.\n\n    \n  \n    \n      Resampling - Stratified, Blocked and Predefined\n      Apply stratified, block and custom resampling.\n\n    \n  \n    \n      Factor Encoding\n      Encode factor variables in a task.\n\n    \n  \n    \n      German Credit Series\n      Train, tune and pipeline different machine learning algorithms.\n\n    \n  \n\nNo matching items\n\n  \n    See all posts"
  },
  {
    "objectID": "gallery.html#optimize-models",
    "href": "gallery.html#optimize-models",
    "title": "Gallery",
    "section": "",
    "text": "Recursive Feature Elimination on the Sonar Data Set\n      Utilize the built-in feature importance of models.\n\n    \n  \n    \n      Hyperband Series\n      Use the Hyperband optimizer with different budget parameters.\n\n    \n  \n    \n      Practical Tuning Series\n      Start with a tuned SVM and finish with a AutoML model.\n\n    \n  \n    \n      Default Hyperparameter Configuration\n      Run the default hyperparameter configuration of learners as a baseline.\n\n    \n  \n\nNo matching items\n\n  \n    See all posts"
  },
  {
    "objectID": "gallery.html#build-pipelines",
    "href": "gallery.html#build-pipelines",
    "title": "Gallery",
    "section": "",
    "text": "A Pipeline for the Titanic Data Set\n      Create new features and impute missing values with a pipeline.\n\n    \n  \n    \n      Pipelines, Selectors, Branches\n      Build a preprocessing pipeline with branching.\n\n    \n  \n    \n      Target Transformations via Pipelines\n      Transform the target variable.\n\n    \n  \n    \n      Tuning a Complex Graph\n      Tune a preprocessing pipeline and multiple tuners at once.\n\n    \n  \n\nNo matching items\n\n  \n    See all posts"
  },
  {
    "objectID": "gallery.html#apply-technical-tools-and-run-special-tasks",
    "href": "gallery.html#apply-technical-tools-and-run-special-tasks",
    "title": "Gallery",
    "section": "",
    "text": "Time constraints in the mlr3 ecosystem\n      Set time limits for learners, tuning and nested resampling.\n\n    \n  \n    \n      Production Example Using Plumber and Docker\n      Write a REST API using plumber and deploy it using Docker.\n\n    \n  \n    \n      Visualization in mlr3\n      Quickly plot objects of the mlr3 ecosystem.\n\n    \n  \n    \n      Spatial Data in the mlr3 Ecosystem\n      Run a land cover classification of the city of Leipzig.\n\n    \n  \n\nNo matching items\n\n  \n    See all posts"
  },
  {
    "objectID": "gallery.html#exercise-collection-for-practice-and-learning",
    "href": "gallery.html#exercise-collection-for-practice-and-learning",
    "title": "Gallery",
    "section": "",
    "text": "This curated exercise collection offers hands-on, solvable exercises focused on core concepts, from basic modeling and resampling to tuning and advanced techniques. Each exercise follows a consistent structure: clear learning goal, prerequisites, structured task list with hints how to solve them, collapsible password-protected solutions, and key take-aways designed to support systematic practice and self-paced learning.\n  \n    See all exercises"
  },
  {
    "objectID": "gallery-all-basic.html",
    "href": "gallery-all-basic.html",
    "title": "Train and Evaluate Models",
    "section": "",
    "text": "Train and Evaluate Models\n\n\n  \n    \n      Comparison of Decision Boundaries of Classification Learners\n      Visualize the decision boundaries of multiple classification learners on some artificial data sets.\n\n      2020-08-14 - Michel Lang\n    \n  \n    \n      mlr3 and OpenML - Moneyball Use Case\n      Download data from OpenML data and impute missing values.\n\n      2020-05-04 - Philipp Kopper\n    \n  \n    \n      Feature Engineering of Date-Time Variables\n      Engineer features using date-time variables.\n\n      2020-05-02 - Lennart Schneider\n    \n  \n    \n      Imbalanced Data Handling with mlr3\n      Handle imbalanced data with oversampling, undersampling, and SMOTE imbalance correction.\n\n      2020-03-30 - Giuseppe Casalicchio\n    \n  \n    \n      Resampling - Stratified, Blocked and Predefined\n      Apply stratified, block and custom resampling.\n\n      2020-03-30 - Milan Dragicevic, Giuseppe Casalicchio\n    \n  \n    \n      German Credit Series - Pipelines\n      Impute missing values, filter features and stack Learners.\n\n      2020-03-11 - Martin Binder, Florian Pfisterer\n    \n  \n    \n      German Credit Series - Tuning\n      Optimize Hyperparameters and apply nested resampling.\n\n      2020-03-11 - Martin Binder, Florian Pfisterer\n    \n  \n    \n      German Credit Series - Basics\n      Train different models.\n\n      2020-03-11 - Martin Binder, Florian Pfisterer, Michel Lang\n    \n  \n    \n      Select Uncorrelated Features\n      Remove correlated features with a pipeline.\n\n      2020-02-25 - Martin Binder, Florian Pfisterer\n    \n  \n    \n      Impute Missing Variables\n      Augment a Random Forest with automatic imputation.\n\n      2020-01-31 - Florian Pfisterer\n    \n  \n    \n      Encode Factor Levels for xgboost\n      Encode factor variables in a task.\n\n      2020-01-31 - Michel Lang\n    \n  \n    \n      House Prices in King County\n      Apply multiple preprocessing steps, fit a model and visualize the results.\n\n      2020-01-30 - Florian Pfisterer\n    \n  \n\nNo matching items"
  },
  {
    "objectID": "fselectors.html",
    "href": "fselectors.html",
    "title": "Feature Selection Wrapper",
    "section": "",
    "text": "Feature selection wrappers can be found in the mlr3fselect packages. The goal is to find the best subset of features with respect to a performance measure in an iterative fashion.\n\n\n\n\n\n\n\n\nRun a sequential feature selection on the Pima Indian Diabetes data set.\n\nlibrary(mlr3verse)\n\n# retrieve task\ntask = tsk(\"pima\")\n\n# load learner\nlearner = lrn(\"classif.rpart\")\n\n# feature selection on the pima indians diabetes data set\ninstance = fselect(\n  fselector = fs(\"sequential\"),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\")\n)\n\n# best performing feature subset\ninstance$result\n\n      age glucose insulin   mass pedigree pregnant pressure triceps\n   &lt;lgcl&gt;  &lt;lgcl&gt;  &lt;lgcl&gt; &lt;lgcl&gt;   &lt;lgcl&gt;   &lt;lgcl&gt;   &lt;lgcl&gt;  &lt;lgcl&gt;\n1:   TRUE    TRUE    TRUE   TRUE     TRUE     TRUE     TRUE   FALSE\n                                         features n_features classif.ce\n                                           &lt;list&gt;      &lt;int&gt;      &lt;num&gt;\n1: age,glucose,insulin,mass,pedigree,pregnant,...          7   0.234375\n\n# subset the task and fit the final model\ntask$select(instance$result_feature_set)\nlearner$train(task)\n\nprint(learner)\n\n\n── &lt;LearnerClassifRpart&gt; (classif.rpart): Classification Tree ──────────────────\n• Model: rpart\n• Parameters: xval=0\n• Packages: mlr3 and rpart\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, factor, and ordered\n• Encapsulation: none (fallback: -)\n• Properties: importance, missings, multiclass, selected_features, twoclass,\nand weights\n• Other settings: use_weights = 'use'"
  },
  {
    "objectID": "fselectors.html#example-usage",
    "href": "fselectors.html#example-usage",
    "title": "Feature Selection Wrapper",
    "section": "",
    "text": "Run a sequential feature selection on the Pima Indian Diabetes data set.\n\nlibrary(mlr3verse)\n\n# retrieve task\ntask = tsk(\"pima\")\n\n# load learner\nlearner = lrn(\"classif.rpart\")\n\n# feature selection on the pima indians diabetes data set\ninstance = fselect(\n  fselector = fs(\"sequential\"),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\")\n)\n\n# best performing feature subset\ninstance$result\n\n      age glucose insulin   mass pedigree pregnant pressure triceps\n   &lt;lgcl&gt;  &lt;lgcl&gt;  &lt;lgcl&gt; &lt;lgcl&gt;   &lt;lgcl&gt;   &lt;lgcl&gt;   &lt;lgcl&gt;  &lt;lgcl&gt;\n1:   TRUE    TRUE    TRUE   TRUE     TRUE     TRUE     TRUE   FALSE\n                                         features n_features classif.ce\n                                           &lt;list&gt;      &lt;int&gt;      &lt;num&gt;\n1: age,glucose,insulin,mass,pedigree,pregnant,...          7   0.234375\n\n# subset the task and fit the final model\ntask$select(instance$result_feature_set)\nlearner$train(task)\n\nprint(learner)\n\n\n── &lt;LearnerClassifRpart&gt; (classif.rpart): Classification Tree ──────────────────\n• Model: rpart\n• Parameters: xval=0\n• Packages: mlr3 and rpart\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, factor, and ordered\n• Encapsulation: none (fallback: -)\n• Properties: importance, missings, multiclass, selected_features, twoclass,\nand weights\n• Other settings: use_weights = 'use'"
  },
  {
    "objectID": "dependencies.html",
    "href": "dependencies.html",
    "title": "Dependencies",
    "section": "",
    "text": "mlr3 tries to be light on dependencies. The following packages are at runtime:\n\nparallelly: Helper functions for parallelization. No extra recursive dependencies.\nfuture.apply: Resampling and benchmarking is parallelized with the future abstraction interfacing many parallel backends.\nbackports: Ensures backward compatibility with older R releases. Developed by members of the mlr team. No recursive dependencies.\ncheckmate: Fast argument checks. Developed by members of the mlr team. No extra recursive dependencies.\nmlr3misc: Miscellaneous functions used in multiple mlr3 extension packages. Developed by the mlr team.\nparadox: Descriptions for parameters and parameter sets. Developed by the mlr team. No extra recursive dependencies.\nR6: Reference class objects. No recursive dependencies.\ndata.table: Extension of R’s data.frame. No recursive dependencies.\ndigest (via mlr3misc): Hash digests. No recursive dependencies.\nuuid: Create unique string identifiers. No recursive dependencies.\nlgr: Logging facility. No extra recursive dependencies.\nmlr3measures: Performance measures. No extra recursive dependencies.\nmlbench: A collection of machine learning data sets. No dependencies.\npalmerpenguins: A classification data set about penguins, used on examples and provided as a toy task. No dependencies."
  },
  {
    "objectID": "gallery-all-technical.html",
    "href": "gallery-all-technical.html",
    "title": "Customize and Apply Technical Tools",
    "section": "",
    "text": "Customize and Apply Technical Tools\n\n\n  \n    \n      Wrapper-based Ensemble Feature Selection\n      Find the most stable and predictive features using multiple learners and resampling techniques.\n\n      2025-01-12 - John Zobolas\n    \n  \n    \n      Time constraints in the mlr3 ecosystem\n      Set time limits for learners, tuning and nested resampling.\n\n      2023-12-21 - Marc Becker\n    \n  \n    \n      Analyzing the Runtime Performance of tidymodels and mlr3\n      Compare the runtime performance of tidymodels and mlr3.\n\n      2023-10-30 - Marc Becker\n    \n  \n    \n      Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)\n      Demonstrate use of survival BART on the lung dataset via mlr3proba and distr6.\n\n      2023-10-25 - John Zobolas\n    \n  \n    \n      Spatial Data in the mlr3 Ecosystem\n      Run a land cover classification of the city of Leipzig.\n\n      2023-02-27 - Marc Becker\n    \n  \n    \n      Visualization in mlr3\n      Quickly plot the mlr3 ecosystem.\n\n      2022-12-22 - Marc Becker\n    \n  \n    \n      A Production Example Using Plumber and Docker\n      Write a REST API using plumber and deploy it using Docker.\n\n      2020-08-13 - Lennart Schneider\n    \n  \n\nNo matching items"
  },
  {
    "objectID": "ecosystem.html",
    "href": "ecosystem.html",
    "title": "Ecosystem",
    "section": "",
    "text": "The mlr3 ecosystem is a collection of R packages for machine learning. The base package mlr3 only provides the basic building blocks for machine learning. The extensions packages extent mlr3 with functionality for additional task types, learning algorithms, tuning algorithms, feature selection strategies, visualizations or preprocessing capabilities. The packages are listed bellow with a short description. For more information about the packages, check out their respective homepages.\nThe dot next to the package name indicates the lifecycle stage.\nIf you use our packages in your research, please cite our articles on mlr3 (Lang et al. 2019), mlr3proba (Sonabend et al. 2021) or mlr3pipelines (Binder et al. 2021). To get the citation information of other packages, call\ncitation(\"[package]\")"
  },
  {
    "objectID": "ecosystem.html#core",
    "href": "ecosystem.html#core",
    "title": "Ecosystem",
    "section": " Core",
    "text": "Core\n\n\n\n\n\nmlr3\n\n\nBasic building blocks for machine learning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3verse\n\n\nMeta-package intended to simplify both installation and loading of packages from the mlr3 ecosystem.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3pipelines\n\n\nDataflow programming toolkit."
  },
  {
    "objectID": "ecosystem.html#learners",
    "href": "ecosystem.html#learners",
    "title": "Ecosystem",
    "section": " Learners",
    "text": "Learners\n\n\n\n\n\nmlr3learners\n\n\nEssential learners for mlr3, maintained by the mlr-org team.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3extralearners\n\n\nExtra learners for mlr3, implemented by the community.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3torch\n\n\nDeep learning with torch."
  },
  {
    "objectID": "ecosystem.html#tuning-optimization",
    "href": "ecosystem.html#tuning-optimization",
    "title": "Ecosystem",
    "section": " Tuning & Optimization",
    "text": "Tuning & Optimization\n\n\n\n\n\nmlr3tuning\n\n\nHyperparameter tuning for mlr3 learners.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3tuningspaces\n\n\nCollection of search spaces for hyperparameter tuning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3hyperband\n\n\nSuccessive halving and hyperband tuner for mlr3tuning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3mbo\n\n\nModel-based optimization for mlr3tuning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmiesmuschel\n\n\nFlexible mixed integer evolutionary strategies.\n\n\n\n\n\n\n\n\n\n\n\nbbotk\n\n\nBlack-box optimization toolkit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3automl\n\n\nAutomated machine learning."
  },
  {
    "objectID": "ecosystem.html#tasks-and-datatypes",
    "href": "ecosystem.html#tasks-and-datatypes",
    "title": "Ecosystem",
    "section": " Tasks and Datatypes",
    "text": "Tasks and Datatypes\n\n\n\n\n\nmlr3spatiotempcv\n\n\nSpatiotemporal resampling and visualization methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3cluster\n\n\nCluster analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3proba\n\n\nProbabilistic predictions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3spatial\n\n\nSpatial data backends and prediction functions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3fda\n\n\nFunctional Data Analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3fairness\n\n\nFairness in Machine Learning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3forecast\n\n\nTime series analysis."
  },
  {
    "objectID": "ecosystem.html#feature-selection",
    "href": "ecosystem.html#feature-selection",
    "title": "Ecosystem",
    "section": " Feature Selection",
    "text": "Feature Selection\n\n\n\n\n\nmlr3filters\n\n\nFilter Feature Selection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3fselect\n\n\nWrapper Feature Selection."
  },
  {
    "objectID": "ecosystem.html#data",
    "href": "ecosystem.html#data",
    "title": "Ecosystem",
    "section": " Data",
    "text": "Data\n\n\n\n\n\nmlr3db\n\n\nData backend to transparently work with databases.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3oml\n\n\nConnector to OpenML.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3data\n\n\nData sets and tasks."
  },
  {
    "objectID": "ecosystem.html#analysis",
    "href": "ecosystem.html#analysis",
    "title": "Ecosystem",
    "section": " Analysis",
    "text": "Analysis\n\n\n\n\n\nmlr3viz\n\n\nVisualizations for tasks, predictions, resample results and benchmarks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3benchmark\n\n\nAnalysis and tools for benchmarking.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3inferr\n\n\nStatistical methods for inference on the generalization error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3summary\n\n\nSummary methods for mlr3."
  },
  {
    "objectID": "ecosystem.html#other",
    "href": "ecosystem.html#other",
    "title": "Ecosystem",
    "section": " Other",
    "text": "Other\n\n\n\n\n\nparadox\n\n\nUniversal parameter space description and tools.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3misc\n\n\nMiscellaneous helper functions for mlr3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmlr3measures\n\n\nPerformance measures for supervised learning."
  },
  {
    "objectID": "ecosystem.html#parallelization",
    "href": "ecosystem.html#parallelization",
    "title": "Ecosystem",
    "section": " Parallelization",
    "text": "Parallelization\n\n\n\n\n\nmlr3batchmark\n\n\nConnector between mlr3 and batchtools.\n\n\n\n\n\n\n\n\n\n\n\nrush\n\n\nAsynchronous parallelization."
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Team",
    "section": "",
    "text": "Team\n\n\n\n\n\n\n\n\n\nBernd Bischl\n\nProfessor of Statistical Learning and Data Science at the LMU Munich. I created mlr a long time ago at the beginning of my PhD. Nowadays, I spend most of my time in project supervision, code reviews and helping to design new parts of the framework. I was part of the design process of nearly all parts of the new mlr3, but nearly all code has been written by the other developers.\n\n\n\n\n\n\n\n\nMichel Lang\n\nPostdoc at the TU Dortmund and one of the main developers of mlr. I've worked on many internal parts of mlr and started to implement support for survival analysis. Now main developer of mlr3.\n\n\n\n\n\n\n\n\nMarc Becker\n\nResearch engineer at the LMU Munich and main developer of the mlr3 optimization packages.\n\n\n\n\n\n\n\n\nRaphael Sonabend\n\nPostdoc at Imperial College London. I was the main developer of mlr3proba and also the previous maintainer of mlr3extralearners.\n\n\n\n\n\n\n\n\nSebastian Fischer\n\nResearch Engineer at LMU Munich. Is working on mlr3torch, mlr3oml and maintains mlr3extralearners.\n\n\n\n\n\n\n\n\nLars Kotthoff\n\nComputer Science Professor at University of Wyoming, contributes small pieces here and there.\n\n\n\n\n\n\n\n\nFlorian Pfisterer\n\nPhD Student at LMU Munich. I am interested in projects on the intersection of Meta-Learning, AutoML and Algorithmic Fairness. Mainly working on mlr3pipelines and mlr3keras/mlr3torch\n\n\n\n\n\n\n\n\nLennart Schneider\n\nPhD Student at LMU Munich. Interested in black box optimization, HPO and AutoML. Mainly working on mlr3mbo.\n\n\n\n\n\n\n\n\nJohn Zobolas\n\nPostdoc Researcher in Clinical AI at the Institute for Cancer Research in Oslo. I am the main developer and maintainer of mlr3proba."
  },
  {
    "objectID": "documentation-listings/gallery.html",
    "href": "documentation-listings/gallery.html",
    "title": "Gallery",
    "section": "",
    "text": "foo"
  },
  {
    "objectID": "graphs.html",
    "href": "graphs.html",
    "title": "Graphs",
    "section": "",
    "text": "Graphs\nGraphs are predefined arrangements of PipeOp objects from the mlr3pipelines package. The goal is to simplify some popular operations which usually consist of multiple steps."
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html",
    "href": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html",
    "title": "A Pipeline for the Titanic Data Set - Advanced",
    "section": "",
    "text": "This is the second post of the titanic use case series. You can find the first use case here.\nIn this section we will focus on more advanced usage of mlr3pipelines . Specifically, this section illustrates the different options when it comes to data imputation and feature engineering. Furthermore, the section shows how to benchmark, feature engineer and compare our results.\nWe load the mlr3verse package which pulls in the most important packages for this example. The mlr3learners package loads additional learners. The data is part of the mlr3data package.\nlibrary(mlr3verse)\nlibrary(mlr3learners)\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\nfuture::plan(\"multicore\")\nAs in the basics chapter, we use the titanic data set. To recap we have undertaken the following steps:\ndata(\"titanic\", package = \"mlr3data\")\n\n# setting up the task\ntask = as_task_classif(titanic, target = \"survived\", positive = \"yes\")\ntask$set_row_roles(892:1309, \"holdout\")\ntask$select(cols = setdiff(task$feature_names, c(\"cabin\", \"name\", \"ticket\")))\n\n# setting up the learner\nlearner = lrn(\"classif.rpart\")\n\n#setting up our resampling method\nresampling = rsmp(\"cv\", folds = 3L)$instantiate(task)\n\nres = resample(task, learner, resampling, store_models = TRUE)"
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html#imputation",
    "href": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html#imputation",
    "title": "A Pipeline for the Titanic Data Set - Advanced",
    "section": "Imputation",
    "text": "Imputation\nA very simple way to do this to just impute a constant value for each feature. We could i.e. impute every character or factor column with missing and every numeric column with -999. And depending on the model, this might actually be fine. This approach has a few drawbacks though:\n\n-999 could be a real value in the data.\nimputing -999 skews the distribution of the data, which might result in bad models.\n\nAs a result, instead of imputing a constant value, we will do two things: * Draw samples from each numeric features’ histogram using PipeOpImputeHist * Add an additional column for each variable that indicates whether a value was missing or not. If the information that a value was missing is important, this column contains this information.\nThis imputation scheme is called ‘imputation with constants’ and is already implemented in mlr3pipelines . It can be done using PipeOpImputeConstant.\nRemember that we are trying to optimize our predictive power by using a random forest model (mlr_learners_classif.ranger). Now, random forest models do not naturally handle missing values which is the reason why we need imputation. Before imputation, our data looks as follows:\n\ntask$missings()\n\nsurvived      age embarked     fare    parch   pclass      sex   sib_sp \n       0      177        2        0        0        0        0        0 \n\n\nLet’s first deal with the categorical variables:\n\npo_newlvl = po(\"imputeoor\")\ntask_newlvl = po_newlvl$train(list(task))[[1]]\n\nNote that we use the PipeOp in an unusual way, which is why the syntax does not look very clean. We’ll learn how to use a full graph below.\nFirst, let’s look at the result:\n\ntask_newlvl$missings()\n\nsurvived     fare    parch   pclass      sex   sib_sp      age embarked \n       0        0        0        0        0        0        0        0 \n\n\nCool! embarked does not have missing values anymore. Note that PipeOpImputeOOR by default affects character, factor and ordered columns.\nFor the numeric features we want to do two things, impute values and add an indicator column. In order to do this, we need a more complicated structure, a Graph.\nOur po_indicator creates the indicator column. We tell it to only do this for numeric and integer columns via its param_vals, and additionally tell it to create a numeric column (0 = “not missing”, 1 = “missing”).\n\npo_indicator = po(\"missind\",\n  affect_columns = selector_type(c(\"numeric\", \"integer\")), type = \"numeric\")\n\nNow we can simultaneously impute features from the histogram and create indicator columns. This can be achieved using the gunion function, which puts two operations in parallel:\n\ngraph = gunion(list(po_indicator, po(\"imputehist\")))\ngraph = graph %&gt;&gt;% po(\"featureunion\")\n\nAfterwards, we cbind the resulting data using po(\"featureunion\"), connecting the different operations using our graph connector: %&gt;&gt;%. We can now also connect the newlvl imputation:\n\ngraph = graph %&gt;&gt;% po(\"imputeoor\")\n\nand see what happens when we now train the whole Graph:\n\ntask_imputed = graph$clone()$train(task)[[1]]\ntask_imputed$missings()\n\n   survived missing_age      pclass         sex        fare       parch      sib_sp         age    embarked \n          0           0           0           0           0           0           0           0           0 \n\n\nAwesome, now we do not have any missing values!\n\nautoplot(task_imputed)\n\n\n\n\n\n\n\n\nWe could now use task_imputed for resampling and see whether a ranger model does better. But this is dangerous! If we preprocess all training data at once, data could leak through the different cross-validation folds. In order to do this properly, we have to process the training data in every fold separately. Luckily, this is automatically handled in our Graph, if we use it through a GraphLearner.\nWe can simply append a ranger learner to the Graph and create a GraphLearner from this.\n\ngraph_learner = as_learner(graph$clone() %&gt;&gt;%\n  po(\"imputesample\") %&gt;&gt;%\n  po(\"fixfactors\") %&gt;&gt;%\n  po(learner))\n\nWe needed to use the following commands for the Graph: * PipeOpFixFactors: Removes empty factor levels and removes factor levels that do not exist during training. * PipeOpImputeSample: In some cases, if missing factor levels do not occur during training but only while predicting, PipeOpImputeOOR does not create a new level. For those, we sample a random value.\n\nrr = resample(task, graph_learner, resampling, store_models = TRUE)\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n  0.7934905 \n\n\nSo our model has not improved heavily, currently it has an accuracy of 0.79."
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html#feature-engineering",
    "href": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html#feature-engineering",
    "title": "A Pipeline for the Titanic Data Set - Advanced",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nWe will do this using PipeOpMutate in order to showcase the power of mlr3pipelines . Additionally, we will make use of the character columns. Hence, we will re-select them:\n\ntask$col_roles$feature = c(task$feature_names, c(\"cabin\", \"name\", \"ticket\"))\n\n\nlibrary(\"stringi\")\npo_ftextract = po(\"mutate\", mutation = list(\n  fare_per_person = ~ fare / (parch + sib_sp + 1),\n  deck = ~ factor(stri_sub(cabin, 1, 1)),\n  title = ~ factor(stri_match(name, regex = \", (.*)\\\\.\")[, 2]),\n  surname = ~ factor(stri_match(name, regex = \"(.*),\")[, 2]),\n  ticket_prefix = ~ factor(stri_replace_all_fixed(stri_trim(stri_match(ticket, regex = \"(.*) \")[, 2]), \".\", \"\"))\n))\n\nQuickly checking what happens:\n\ntask_eng = po_ftextract$clone()$train(list(task))[[1]]\ntask_eng$data()\n\n     survived age embarked    fare parch pclass    sex sib_sp cabin                                                name\n  1:       no  22        S  7.2500     0      3   male      1  &lt;NA&gt;                             Braund, Mr. Owen Harris\n  2:      yes  38        C 71.2833     0      1 female      1   C85 Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n  3:      yes  26        S  7.9250     0      3 female      0  &lt;NA&gt;                              Heikkinen, Miss. Laina\n  4:      yes  35        S 53.1000     0      1 female      1  C123        Futrelle, Mrs. Jacques Heath (Lily May Peel)\n  5:       no  35        S  8.0500     0      3   male      0  &lt;NA&gt;                            Allen, Mr. William Henry\n ---                                                                                                                   \n887:       no  27        S 13.0000     0      2   male      0  &lt;NA&gt;                               Montvila, Rev. Juozas\n888:      yes  19        S 30.0000     0      1 female      0   B42                        Graham, Miss. Margaret Edith\n889:       no  NA        S 23.4500     2      3 female      1  &lt;NA&gt;            Johnston, Miss. Catherine Helen \"Carrie\"\n890:      yes  26        C 30.0000     0      1   male      0  C148                               Behr, Mr. Karl Howell\n891:       no  32        Q  7.7500     0      3   male      0  &lt;NA&gt;                                 Dooley, Mr. Patrick\n               ticket fare_per_person deck title   surname ticket_prefix\n  1:        A/5 21171         3.62500 &lt;NA&gt;    Mr    Braund           A/5\n  2:         PC 17599        35.64165    C   Mrs   Cumings            PC\n  3: STON/O2. 3101282         7.92500 &lt;NA&gt;  Miss Heikkinen       STON/O2\n  4:           113803        26.55000    C   Mrs  Futrelle          &lt;NA&gt;\n  5:           373450         8.05000 &lt;NA&gt;    Mr     Allen          &lt;NA&gt;\n ---                                                                    \n887:           211536        13.00000 &lt;NA&gt;   Rev  Montvila          &lt;NA&gt;\n888:           112053        30.00000    B  Miss    Graham          &lt;NA&gt;\n889:       W./C. 6607         5.86250 &lt;NA&gt;  Miss  Johnston           W/C\n890:           111369        30.00000    C    Mr      Behr          &lt;NA&gt;\n891:           370376         7.75000 &lt;NA&gt;    Mr    Dooley          &lt;NA&gt;\n\n\n\nautoplot(task_eng$clone()$select(c(\"sex\", \"age\")), type = \"pairs\")\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nWarning: Removed 177 rows containing non-finite values (`stat_boxplot()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 177 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 177 rows containing non-finite values (`stat_boxplot()`).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\nNow we can put everything together again, we concatenate our new PipeOp with the Graph created above and use PipeOpSelect in order to de-select the character features we used for feature extraction. Additionally, we collapse the ‘surname’, so only surnames that make up more than 0.6 % of the data are kept.\nIn summary, we do the following:\n\nmutate: The po_ftextract we defined above extracts additional features from the data.\ncollapsefactors: Removes factor levels that make up less then 3 % of the data.\nselect: Drops character columns.\ngunion: Puts two PipeOps in parallel.\n\nmissind: po_indicator adds a column for each numeric with the info whether the value is NA or not.\nimputehist: Imputes numeric and integer columns by sampling from the histogram.\n\nfeatureunion: Cbind’s parallel data streams.\nimputeoor: Imputes factor and ordered columns.\nfixfactors: Removes empty factor levels and removes factor levels that do not exist during training.\nimputesample: In some cases, if missing factor levels do not occur during training but only while predicting, imputeoor does not create a new level. For those, we sample a random value.\nLearner: Appends a learner to the Graph.\n\nThe full graph we created is the following:\n\nlearner = lrn(\"classif.ranger\", num.trees = 500, min.node.size = 4)\n\n\ngraph_final = po_ftextract %&gt;&gt;%\n  po(\"collapsefactors\", param_vals = list(no_collapse_above_prevalence = 0.03)) %&gt;&gt;%\n  po(\"select\", param_vals = list(selector = selector_invert(selector_type(\"character\")))) %&gt;&gt;%\n  gunion(list(po_indicator, po(\"imputehist\"))) %&gt;&gt;%\n  po(\"featureunion\") %&gt;&gt;%\n  po(\"imputeoor\") %&gt;&gt;%\n  po(\"fixfactors\") %&gt;&gt;%\n  po(\"imputesample\") %&gt;&gt;%\n  po(learner)"
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html#evaluation",
    "href": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html#evaluation",
    "title": "A Pipeline for the Titanic Data Set - Advanced",
    "section": "Evaluation",
    "text": "Evaluation\nLet us see if things have improved:\n\ngraph_learner = as_learner(graph_final)\n\nrr = resample(task, graph_learner, resampling, store_models = TRUE)\n\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n  0.8249158 \n\n\nWe have improved even more!"
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html#benchmarking",
    "href": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html#benchmarking",
    "title": "A Pipeline for the Titanic Data Set - Advanced",
    "section": "Benchmarking",
    "text": "Benchmarking\nTo undertake benchmarking, we need to set up a benchmarking design. The first step is creating a list with the learners we used, namely the learners form the first and second part of this use case.\n\nlearners = list(\n  lrn(\"classif.rpart\", predict_type = \"prob\"),\n  lrn(\"classif.ranger\", predict_type = \"prob\")\n)\n\nNow we can define our benchmark design. This is done to ensure exhaustive and consistent resampling for all learners. This step is needed to execute over the same train/test split for each task.\n\nbm_design = benchmark_grid(task_imputed, learners, rsmp(\"cv\", folds = 10))\nbmr = benchmark(bm_design, store_models = TRUE)\nprint(bmr)\n\n&lt;BenchmarkResult&gt; of 20 rows with 2 resampling runs\n nr task_id     learner_id resampling_id iters warnings errors\n  1 titanic  classif.rpart            cv    10        0      0\n  2 titanic classif.ranger            cv    10        0      0\n\n\nSo, where do we go from here? We could for instance use a boxplot:\n\nautoplot(bmr)\n\n\n\n\n\n\n\n\nFurther we are able to compare sensitivity and specificity. Here we need to ensure that the benchmark results only contain a single Task:\n\nautoplot(bmr$clone()$filter(task_id = \"titanic\"), type = \"roc\")\n\n\n\n\n\n\n\n\nMoreover, one can compare the precision-recall:\n\n# Precision vs Recall\nggplot2::autoplot(bmr, type = \"prc\")\n\n\n\n\n\n\n\n\nAs one can see, there are various options when it comes to benchmarking and visualizing. You could have a look at some other use cases in our gallery for inspiration."
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html#future",
    "href": "gallery/pipelines/2020-04-27-mlr3pipelines-Imputation-titanic/index.html#future",
    "title": "A Pipeline for the Titanic Data Set - Advanced",
    "section": "Future",
    "text": "Future\nIn this case we have examined a number of different features, but there are many more things to explore! We could extract even more information from the different features and see what happens. But now you are left to yourself! There are many kaggle kernels that treat the Titanic Dataset available. This can be a great starter to find even better models."
  },
  {
    "objectID": "gallery/pipelines/2020-06-15-target-transformations-via-pipelines/index.html",
    "href": "gallery/pipelines/2020-06-15-target-transformations-via-pipelines/index.html",
    "title": "Target Transformations via Pipelines",
    "section": "",
    "text": "Transforming the target variable often can lead to predictive improvement and is a widely used tool. Typical transformations are for example the \\(\\log\\) transformation of the target aiming at minimizing (right) skewness, or the Box Cox and Yeo-Johnson transformations being more flexible but having a similar goal.\nOne option to perform, e.g., a \\(\\log\\) transformation would be to manually transform the target prior to training a Learner (and also predicting from it) and then manually invert this transformation via \\(\\exp\\) after predicting from the Learner. This is quite cumbersome, especially if a transformation and inverse transformation require information about both the training and prediction data.\nIn this post, we show how to do various kinds of target transformations using mlr3pipelines and explain the design of the target transformation and inversion PipeOps.\nYou will:\n\nlearn how to do simple target transformations using PipeOpTargetMutate\nbe introduced to the abstract base class to implement custom target transformations, PipeOpTargetTrafo\nimplement a custom target transformation PipeOp, PipeOpTargetTrafoBoxCox\n\nAs a prerequisite, you should be quite familiar with mlr3pipelines, i.e, know about the $state field of PipeOps, input and output channels, as well as Graphs. We will start with a PipeOp for simple target transformations, PipeOpTargetMutate.\nWe load the most important packages for this example.\n\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(paradox)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\nIn all sections we will use the mtcars regression task with mpg being a numerical, positive target:\n\ntask = tsk(\"mtcars\")\nsummary(task$data(cols = task$target_names))\n\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90  \n\n\nMoreover, as a Learner we will use an ordinary linear regression learner:\n\nlearner_lm = lrn(\"regr.lm\")\n\n\nSimple Target Transformations\nThe term simple refers to transformations that are given by a function of the target, relying on no other arguments (constants are of course allowed). The most prominent example is given by the \\(\\log\\) transformation which we can later invert by applying the \\(\\exp\\) transformation.\nIf you are only interested in doing such a transformation and you do not have the time to read more of this post, simply use the following syntactic sugar:\n\ng_ppl = ppl(\"targettrafo\", graph = learner_lm)\ng_ppl$param_set$values$targetmutate.trafo = function(x) log(x)\ng_ppl$param_set$values$targetmutate.inverter = function(x) list(response = exp(x$response))\n\nThis constructs a Graph that will \\(\\log\\) transform the target prior to training the linear regression learner (or predicting from it) and \\(\\exp\\) transform the target after predicting from it. Note that you can supply any other Learner or even a whole Graph as the graph argument.\nNow, we will go into more detail about how this actually works:\nWe can perform a \\(\\log\\) transformation of our numerical, positive target, mpg, using PipeOpTargetMutate (by default, ppl(\"targettrafo\") uses this target transformation PipeOp):\n\ntrafo = po(\"targetmutate\", param_vals = list(trafo = function(x) log(x)))\n\nWe have to specify the trafo parameter as a function of x (which will internally be evaluated to be the target of the Task): trafo = function(x) log(x)). In principle, this is all that is needed to transform the target prior to training a Learner (or predicting from it), i.e., if we now train this PipeOp, we see that the target is transformed as specified:\n\ntrafo$train(list(task))$output$data(cols = task$target_names)\n\n         mpg\n 1: 3.044522\n 2: 3.044522\n 3: 3.126761\n 4: 3.063391\n 5: 2.928524\n 6: 2.895912\n 7: 2.660260\n 8: 3.194583\n 9: 3.126761\n10: 2.954910\n11: 2.879198\n12: 2.797281\n13: 2.850707\n14: 2.721295\n15: 2.341806\n16: 2.341806\n17: 2.687847\n18: 3.478158\n19: 3.414443\n20: 3.523415\n21: 3.068053\n22: 2.740840\n23: 2.721295\n24: 2.587764\n25: 2.954910\n26: 3.306887\n27: 3.258097\n28: 3.414443\n29: 2.760010\n30: 2.980619\n31: 2.708050\n32: 3.063391\n         mpg\n\n\nAfter having predicted from the Learner we could then proceed to apply the inverse transformation function in a similar manner. However, in mlr3pipelines, we decided to go with a more unified design of handling target transformations. In all target transformation PipeOps also the inverse transformation function of the target has to be specified. Therefore, in PipeOpTargetMutate, the parameter inverter also has to be correctly specified:\n\ntrafo$param_set$values$inverter = function(x) list(response = exp(x$response))\n\nInternally, this function will be applied to the data.table downstream of a Prediction object without the $row_id and $truth columns, and we specify that the $response column should be transformed. Note that applying the inverse transformation will typically only be done to the $response column, because transforming standard errors or probabilities is often not straightforward.\nTo actually carry out the inverse transformation function after predicting from the Learner, we then rely on PipeOpTargetInvert. PipeOpTargetInvert has an empty ParamSet and its sole purpose is to apply the inverse transformation function after having predicted from a Learner (note that this whole design of target transformations may seem somewhat over-engineered at first glance, however, we will learn of its advantages when we later move to the advanced section).\nPipeOpTargetInvert has two input channels named \"fun\" and \"prediction\". During training, both take NULL as input (because this is what a Learner’s \"output\" output and PipeOpTargetMutate’s \"fun\" output will return during training). During prediction, the \"prediction\" input takes a Prediction, and the \"fun\" input takes the \"fun\" output from PipeOpTargetMutate (you may have noticed already, that PipeOpTargetMutate has actually two outputs, \"fun\" and \"output\", with \"fun\" returning NULL during training and a function during prediction, while \"output\" always returns the transformed input Task). We can see this, if we look at:\n\ntrafo$output\n\n     name train  predict\n1:    fun  NULL function\n2: output  Task     Task\n\ntrafo$predict(list(task))\n\n$fun\nfunction(inputs) {\n        assert_list(inputs, len = 1L, types = \"Prediction\")\n        list(private$.invert(inputs[[1L]], predict_phase_state))\n      }\n&lt;bytecode: 0x555be761be00&gt;\n&lt;environment: 0x555be76215b0&gt;\n\n$output\n&lt;TaskRegr:mtcars&gt; (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\n\nWe will talk more about such technical details in the advanced section. For now, to finally construct our target transformation pipeline, we build a Graph:\n\ng = Graph$new()\ng$add_pipeop(trafo)\ng$add_pipeop(learner_lm)\ng$add_pipeop(po(\"targetinvert\"))\n\nManually connecting the edges is quite cumbersome. First we connect the \"output\" output of \"targetmutate\" to the \"input\" input of \"regr.lm\":\n\ng$add_edge(src_id = \"targetmutate\", dst_id = \"regr.lm\",\n  src_channel = 2, dst_channel = 1)\n\nThen we connect the \"output\" output of \"regr.lm\" to the \"prediction\" input of \"targetinvert\":\n\ng$add_edge(src_id = \"regr.lm\", dst_id = \"targetinvert\",\n  src_channel = 1, dst_channel = 2)\n\nFinally, we connect the \"fun\" output of \"targetmutate\" to the \"fun\" input of \"targetinvert\":\n\ng$add_edge(src_id = \"targetmutate\", dst_id = \"targetinvert\",\n  src_channel = 1, dst_channel = 1)\n\nThis graph (which is conceptually the same graph as constructed via the ppl(\"targettrafo\") syntactic sugar above) looks like the following:\n\ng$plot(html = FALSE)\n\n\n\n\n\n\n\n\nWe can then finally call $train() and $predict() (prior to this we wrap the Graph in a GraphLearner):\n\ngl = GraphLearner$new(g)\ngl$train(task)\ngl$state\n\n$model\n$model$targetmutate\nlist()\n\n$model$regr.lm\n$model$regr.lm$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n(Intercept)           am         carb          cyl         disp         drat         gear           hp         qsec  \n  2.776e+00    4.738e-02   -2.012e-02    7.657e-03    4.989e-05    2.220e-02    5.925e-02   -8.964e-04    3.077e-02  \n         vs           wt  \n -2.874e-03   -1.723e-01  \n\n\n$model$regr.lm$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$model$regr.lm$train_time\n[1] 0.012\n\n$model$regr.lm$param_vals\nnamed list()\n\n$model$regr.lm$task_hash\n[1] \"6ca8c90cdf732078\"\n\n$model$regr.lm$data_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$model$regr.lm$task_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$model$regr.lm$mlr3_version\n[1] '0.16.1'\n\n$model$regr.lm$train_task\n&lt;TaskRegr:mtcars&gt; (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\n\n$model$targetinvert\nlist()\n\n\n$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$train_time\n[1] 0.098\n\n$param_vals\n$param_vals$targetmutate.trafo\nfunction(x) log(x)\n&lt;bytecode: 0x555be6fb60f0&gt;\n\n$param_vals$targetmutate.inverter\nfunction(x) list(response = exp(x$response))\n\n\n$task_hash\n[1] \"58a137d2055e8406\"\n\n$data_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$task_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$mlr3_version\n[1] '0.16.1'\n\n$train_task\n&lt;TaskRegr:mtcars&gt; (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\ngl$predict(task)\n\n&lt;PredictionRegr&gt; for 32 observations:\n    row_ids truth response\n          1  21.0 21.67976\n          2  21.0 21.10831\n          3  22.8 25.73690\n---                       \n         30  19.7 19.58533\n         31  15.0 14.11015\n         32  21.4 23.11105\n\n\nand contrast this with $train() and $predict() of the naive linear regression learner (also look at the estimated coefficients of the linear regression contained in $state$model):\n\nlearner_lm$train(task)\nlearner_lm$state\n\n$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n(Intercept)           am         carb          cyl         disp         drat         gear           hp         qsec  \n   12.30337      2.52023     -0.19942     -0.11144      0.01334      0.78711      0.65541     -0.02148      0.82104  \n         vs           wt  \n    0.31776     -3.71530  \n\n\n$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$train_time\n[1] 0.004\n\n$param_vals\nnamed list()\n\n$task_hash\n[1] \"58a137d2055e8406\"\n\n$data_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$task_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$mlr3_version\n[1] '0.16.1'\n\n$train_task\n&lt;TaskRegr:mtcars&gt; (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\nlearner_lm$predict(task)\n\n&lt;PredictionRegr&gt; for 32 observations:\n    row_ids truth response\n          1  21.0 22.59951\n          2  21.0 22.11189\n          3  22.8 26.25064\n---                       \n         30  19.7 19.69383\n         31  15.0 13.94112\n         32  21.4 24.36827\n\n\nYou should continue reading, if you are interested in more advanced target transformations, i.e., where the transformation and inverse transformation require information about both the training and prediction data.\nFirst we will introduce the abstract base class for doing target transformations, PipeOpTargetTrafo, from which PipeOpTargetMutate inherits.\n\n\nAbstract Base Class: PipeOpTargetTrafo\nNo matter how “complicated” the actual target transformation and inverse transformation may be, applying the inverse transformation function after having predicted from a Learner will always be done via PipeOpTargetInvert (as already outlined above, PipeOpTargetInvert has an empty ParamSet and its sole purpose is to apply the inverse transformation function after having predicted from a Learner). All Graphs for doing target transformations will therefore look similar like the simple one above, i.e., a target transformation PipeOp followed by some Learner or a whole Graph, followed by PipeOpTargetInvert. Therefore, using ppl(\"targettrafo\") to construct such Graphs is highly recommended.\nTo allow for more advanced target transformations, we now have a closer look at the abstract base class, PipeOpTargetTrafo:\nPipeOpTargetTrafo has one input channel, named \"input\" taking a Task both during training and prediction. It’s two output channels are named \"fun\" and \"output\". During training \"fun\" returns NULL and during prediction \"fun\" returns a function that will be used by PipeOpTargetInvert to perform the inverse target transformation on PipeOpTargetInvert’s \"prediction\" input. \"output\" returns the modified input Task both during training and prediction.\nSubclasses can overload up to four functions:\n\n.get_state() takes the input Task and returns a list() which will internally be used to set the $state. Typically it is sensible to make use of the $state during .transform() and .train_invert(). The base implementation returns list() and should be overloaded if setting the state is desired.\n.transform() takes the input Task and returns a modified Task (i.e., the Task with the transformed target). This is the main function for doing the actual target transformation. Note that .get_state() is evaluated a single time during training right before .transform() and therefore, you can rely on the $state that has been set. To update the input Task with respect to the transformed target, subclasses should make use of the convert_task() function and drop the original target from the Task. .transform() also accepts a phase argument that will receive \"train\" during training and \"predict\" during prediction. This can be used to enable different behavior during training and prediction. .transform() should always be overloaded by subclasses.\n.train_invert() takes the input Task and returns a predict_phase_state object. This can be anything. Note that .train_invert() should not modify the input Task. The base implementation returns a list with a single argument, the $truth column of the input Task and should be overloaded if a more training-phase-dependent state is desired.\n.invert() takes a Prediction and a predict_phase_state object as inputs and returns a Prediction. This is the main function for specifying the actual inverse target transformation that will later be carried out by PipeOpTargetInvert. Internally a private helper function , .invert_help() will construct the function that will be returned by the \"fun\" output of PipeOpTargetTrafo so that PipeOpTargetInvert can later simply dispatch this inverse target transformation on its \"prediction\" input.\n\nThe supposed workflow of a class inherited from PipeOpTargetTrafo is given in the following figure:\n\n\n\n\n\n\n\n\n\nTo solidify our understanding we will design a new target transformation PipeOp in the next section: PipeOpTargetTrafoBoxCox\n\n\nHands on: PipeOpTargetTrafoBoxCox\n\nlibrary(R6)\n\nThe Box-Cox transformation of a target \\(y_{i}\\) is given as:\n\\[y_{i}(\\lambda) = \\begin{cases}\n\\frac{y_{i}^{\\lambda} - 1}{\\lambda} & \\text{if}~\\lambda \\neq 0; \\\\\n\\log(y_{i}) & \\text{if}~\\lambda = 0\n\\end{cases}\\]\nmlr3pipelines already supports the Box-Cox transformation for numerical, positive features, see ?PipeOpBoxCox.\nHere we will design a PipeOp to apply the Box-Cox transformation as a target transformation. The \\(\\lambda\\) parameter of the transformation is estimated during training and used for both the training and prediction transformation. After predicting from a Learner we will as always apply the inverse transformation function. To do the actual transformation we will use bestNormalize::boxcox().\nFirst, we inherit from PipeOpTargetTrafo and overload the initialize() function:\n\nPipeOpTargetTrafoBoxCox = R6Class(\"PipeOpTargetTrafoBoxCox\",\n  inherit = PipeOpTargetTrafo,\n  public = list(\n    initialize = function(id = \"targettrafoboxcox\", param_vals = list()) {\n      param_set = ps(\n        standardize = p_lgl(default = TRUE, tags = c(\"train\", \"boxcox\")),\n        eps = p_dbl(default = 0.001, lower = 0, tags = c(\"train\", \"boxcox\")),\n        lower = p_dbl(default = -1L, tags = c(\"train\", \"boxcox\")),\n        upper = p_dbl(default = 2L, tags = c(\"train\", \"boxcox\"))\n      )\n      super$initialize(id = id, param_set = param_set, param_vals = param_vals,\n        packages = \"bestNormalize\", task_type_in = \"TaskRegr\",\n        task_type_out = \"TaskRegr\")\n    }\n  ),\n  private = list(\n\n    .get_state = function(task) {\n      ...\n    },\n\n    .transform = function(task, phase) {\n      ...\n    },\n\n    .train_invert = function(task) {\n      ...\n    },\n\n    .invert = function(prediction, predict_phase_state) {\n      ...\n    }\n  )\n)\n\nAs parameters, we allow \"standardize\" (whether to center and scale the transformed values to attempt a standard normal distribution), \"eps\" (tolerance parameter to identify if the \\(\\lambda\\) parameter is equal to zero), \"lower\" (lower value for the estimation of the \\(\\lambda\\) parameter) and \"upper\" (upper value for the estimation of the \\(\\lambda\\) parameter). Note that we set task_type_in = \"TaskRegr\" and task_type_out = \"TaskRegr\" to specify that this PipeOp only works for regression Tasks.\nSecond, we overload the four functions as mentioned above.\nWe start with .get_state(). We extract the target and apply the Box-Cox transformation to the target. This yields an object of class \"boxcox\" which we will wrap in a list() and set as the $state (bc$x.t = NULL and bc$x = NULL is done to save some memory because we do not need the transformed original data and original data later):\n\n    .get_state = function(task) {\n      target = task$data(cols = task$target_names)[[1L]]\n      bc = mlr3misc::invoke(bestNormalize::boxcox, target,\n        .args = self$param_set$get_values(tags = \"boxcox\"))\n      bc$x.t = NULL\n      bc$x = NULL\n      list(bc = bc)\n    },\n\nNext, we tackle .transform(). This is quite straightforward, because objects of class \"boxcox\" have their own predict method which we can use here to carry out the actual Box-Cox transformation based on the learned \\(\\lambda\\) parameter as stored in the \"boxcox\" object in the $state (both during training and prediction). We then rename the target, add it to the task and finally update the task with respect to this new target:\n\n    .transform = function(task, phase) {\n      target = task$data(cols = task$target_names)[[1L]]\n      new_target = as.data.table(predict(self$state$bc, newdata = target))\n      colnames(new_target) = paste0(task$target_names, \".bc\")\n      task$cbind(new_target)\n      convert_task(task, target = colnames(new_target),\n        drop_original_target = TRUE)\n    },\n\nTime to overload .train_invert(). This is even more straightforward, because the prediction method for objects of class \"boxcox\" directly allows for inverting the transformation via setting the argument inverse = TRUE. Therefore, we only need the \"boxcox\" object stored in the $state along the $truth column of the input Task (remember that this list will later be available as the predict_phase_state object):\n\n    .train_invert = function(task) {\n      list(truth = task$truth(), bc = self$state$bc)\n    },\n\nFinally, we overload .invert(). We extract the truth from the predict_phase_state and the response from the Prediction. We then apply the inverse Box-Cox transformation to the response based on the \\(\\lambda\\) parameter and the mean and standard deviation learned during training, relying on the predict_phase_state object. Finally, we construct a new Prediction object:\n\n    .invert = function(prediction, predict_phase_state) {\n      truth = predict_phase_state$truth\n      response = predict(predict_phase_state$bc, newdata = prediction$response,\n        inverse = TRUE)\n      PredictionRegr$new(row_ids = prediction$row_ids, truth = truth,\n        response = response)\n    }\n\nNote that this PipeOp is ill-equipped to handle the case of predict_type = \"se\", i.e., we always only return a response prediction (as outlined above, this is the case for most target transformations, because transforming standard errors or probabilities of a prediction is often not straightforward). We could of course check whether the predict_type is set to \"se\" and if this is the case, return NA as the standard errors.\nTo construct our final target transformation Graph with our linear regression learner, we again simply make use of ppl(\"targettrafo\"):\n\ng_bc = ppl(\"targettrafo\", graph = learner_lm,\n  trafo_pipeop = PipeOpTargetTrafoBoxCox$new())\n\nThe following plot should already look quite familiar:\n\ng_bc$plot(html = FALSE)\n\n\n\n\n\n\n\n\nFinally we $train() and $predict() on the task (again, we wrap the Graph in a GraphLearner):\n\ngl_bc = GraphLearner$new(g_bc)\ngl_bc$train(task)\ngl_bc$state\n\n$model\n$model$regr.lm\n$model$regr.lm$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n(Intercept)           am         carb          cyl         disp         drat         gear           hp         qsec  \n -0.6272999    0.1670950   -0.0663126    0.0237529    0.0002376    0.0759944    0.1963335   -0.0030367    0.1043210  \n         vs           wt  \n -0.0080166   -0.5800635  \n\n\n$model$regr.lm$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$model$regr.lm$train_time\n[1] 0.007\n\n$model$regr.lm$param_vals\nnamed list()\n\n$model$regr.lm$task_hash\n[1] \"612ab4e0ad596159\"\n\n$model$regr.lm$data_prototype\nEmpty data.table (0 rows and 11 cols): mpg.bc,am,carb,cyl,disp,drat...\n\n$model$regr.lm$task_prototype\nEmpty data.table (0 rows and 11 cols): mpg.bc,am,carb,cyl,disp,drat...\n\n$model$regr.lm$mlr3_version\n[1] '0.16.1'\n\n$model$regr.lm$train_task\n&lt;TaskRegr:mtcars&gt; (32 x 11): Motor Trends\n* Target: mpg.bc\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\n\n$model$targettrafoboxcox\n$model$targettrafoboxcox$bc\nStandardized Box Cox Transformation with 32 nonmissing obs.:\n Estimated statistics:\n - lambda = 0.02955701 \n - mean (before standardization) = 3.092016 \n - sd (before standardization) = 0.324959 \n\n\n$model$targetinvert\nlist()\n\n\n$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$train_time\n[1] 0.075\n\n$param_vals\nnamed list()\n\n$task_hash\n[1] \"58a137d2055e8406\"\n\n$data_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$task_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$mlr3_version\n[1] '0.16.1'\n\n$train_task\n&lt;TaskRegr:mtcars&gt; (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\ngl_bc$predict(task)\n\n&lt;PredictionRegr&gt; for 32 observations:\n    row_ids truth response\n          1  21.0 21.70854\n          2  21.0 21.13946\n          3  22.8 25.75242\n---                       \n         30  19.7 19.58934\n         31  15.0 14.10658\n         32  21.4 23.15263\n\n\nWe could now proceed to benchmark our different target transformations:\n\nbg = benchmark_grid(list(task), learners = list(learner_lm, gl, gl_bc),\n  resamplings = list(rsmp(\"cv\", folds = 10)))\nbmr = benchmark(bg)\n\n\nbmr$aggregate(msr(\"regr.mse\"))\n\n   nr task_id                             learner_id resampling_id iters  regr.mse\n1:  1  mtcars                                regr.lm            cv    10 11.866071\n2:  2  mtcars      targetmutate.regr.lm.targetinvert            cv    10  7.793303\n3:  3  mtcars targettrafoboxcox.regr.lm.targetinvert            cv    10  8.230192\nHidden columns: resample_result"
  },
  {
    "objectID": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html",
    "href": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html",
    "title": "Tuning a Complex Graph",
    "section": "",
    "text": "# include: false\nrequireNamespace(\"bst\")\n\nLoading required namespace: bst\n\nrequireNamespace(\"fastICA\")\n\nLoading required namespace: fastICA\nIn this use case we show how to tune a rather complex graph consisting of different preprocessing steps and different learners where each preprocessing step and learner itself has parameters that can be tuned. You will learn the following:\nIdeally you already had a look at how to tune over multiple learners.\nFirst, we load the packages we will need:\nlibrary(mlr3verse)\nlibrary(mlr3learners)\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented. The lgr package is used for logging in all mlr3 packages. The mlr3 logger prints the logging messages from the base package, whereas the bbotk logger is responsible for logging messages from the optimization packages (e.g. mlr3tuning ).\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#data-and-task",
    "href": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#data-and-task",
    "title": "Tuning a Complex Graph",
    "section": "Data and Task",
    "text": "Data and Task\nWe are going to work with some gene expression data included as a supplement in the bst package. The data consists of 2308 gene profiles in 63 training and 20 test samples. The following data preprocessing steps are done analogously as in vignette(\"khan\", package = \"bst\"):\n\ndatafile = system.file(\"extdata\", \"supplemental_data\", package = \"bst\")\ndat0 = read.delim(datafile, header = TRUE, skip = 1)[, -(1:2)]\ndat0 = t(dat0)\ndat = data.frame(dat0[!(rownames(dat0) %in%\n  c(\"TEST.9\", \"TEST.13\", \"TEST.5\", \"TEST.3\", \"TEST.11\")), ])\ndat$class = as.factor(\n  c(substr(rownames(dat)[1:63], start = 1, stop = 2),\n    c(\"NB\", \"RM\", \"NB\", \"EW\", \"RM\", \"BL\", \"EW\", \"RM\", \"EW\", \"EW\", \"EW\", \"RM\",\n      \"BL\", \"RM\", \"NB\", \"NB\", \"NB\", \"NB\", \"BL\", \"EW\")\n  )\n)\n\nWe then construct our training and test Task :\n\ntask = as_task_classif(dat, target = \"class\", id = \"SRBCT\")\ntask_train = task$clone(deep = TRUE)\ntask_train$filter(1:63)\ntask_test = task$clone(deep = TRUE)\ntask_test$filter(64:83)"
  },
  {
    "objectID": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#workflow",
    "href": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#workflow",
    "title": "Tuning a Complex Graph",
    "section": "Workflow",
    "text": "Workflow\nOur graph will start with log transforming the features, followed by scaling them. Then, either a PCA or ICA is applied to extract principal / independent components followed by fitting a LDA or a ranger random forest is fitted without any preprocessing (the log transformation and scaling should most likely affect the LDA more than the ranger random forest). Regarding the PCA and ICA, both the number of principal / independent components are tuning parameters. Regarding the LDA, we can further choose different methods for estimating the mean and variance and regarding the ranger, we want to tune the mtry and num.tree parameters. Note that the PCA-LDA combination has already been successfully applied in different cancer diagnostic contexts when the feature space is of high dimensionality (Morais and Lima 2018).\nTo allow for switching between the PCA / ICA-LDA and ranger we can either use branching or proxy pipelines, i.e., PipeOpBranch and PipeOpUnbranch or PipeOpProxy. We will first cover branching in detail and later show how the same can be done using PipeOpProxy."
  },
  {
    "objectID": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#baseline",
    "href": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#baseline",
    "title": "Tuning a Complex Graph",
    "section": "Baseline",
    "text": "Baseline\nFirst, we have a look at the baseline classification accuracy of the LDA and ranger on the training task:\n\nbase = benchmark(benchmark_grid(\n  task_train,\n  learners = list(lrn(\"classif.lda\"), lrn(\"classif.ranger\")),\n  resamplings = rsmp(\"cv\", folds = 3)))\n\nWarning in lda.default(x, grouping, ...): variables are collinear\n\nWarning in lda.default(x, grouping, ...): variables are collinear\n\nWarning in lda.default(x, grouping, ...): variables are collinear\n\nbase$aggregate(measures = msr(\"classif.acc\"))\n\n   nr task_id     learner_id resampling_id iters classif.acc\n1:  1   SRBCT    classif.lda            cv     3   0.6666667\n2:  2   SRBCT classif.ranger            cv     3   0.9206349\nHidden columns: resample_result\n\n\nThe out-of-the-box ranger appears to already have good performance on the training task. Regarding the LDA, we do get a warning message that some features are colinear. This strongly suggests to reduce the dimensionality of the feature space. Let’s see if we can get some better performance, at least for the LDA."
  },
  {
    "objectID": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#branching",
    "href": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#branching",
    "title": "Tuning a Complex Graph",
    "section": "Branching",
    "text": "Branching\nOur graph starts with log transforming the features (we explicitly use base 10 only for better interpretability when inspecting the model later), using PipeOpColApply, followed by scaling the features using PipeOpScale. Then, the first branch allows for switching between the PCA / ICA-LDA and ranger, and within PCA / ICA-LDA, the second branch allows for switching between PCA and ICA:\n\ngraph1 =\n  po(\"colapply\", applicator = function(x) log(x, base = 10)) %&gt;&gt;%\n  po(\"scale\") %&gt;&gt;%\n  # pca / ica followed by lda vs. ranger\n  po(\"branch\", id = \"branch_learner\", options = c(\"pca_ica_lda\", \"ranger\")) %&gt;&gt;%\n  gunion(list(\n    po(\"branch\", id = \"branch_preproc_lda\", options = c(\"pca\", \"ica\")) %&gt;&gt;%\n      gunion(list(\n        po(\"pca\"), po(\"ica\")\n      )) %&gt;&gt;%\n      po(\"unbranch\", id = \"unbranch_preproc_lda\") %&gt;&gt;%\n      lrn(\"classif.lda\"),\n    lrn(\"classif.ranger\")\n  )) %&gt;&gt;%\n  po(\"unbranch\", id = \"unbranch_learner\")\n\nNote that the names of the options within each branch are arbitrary, but ideally they describe what is happening. Therefore we go with \"pca_ica_lda\" / \"ranger” and \"pca\" / \"ica\". Finally, we also could have used the branch ppl to make branching easier (we will come back to this in the Proxy section). The graph looks like the following:\n\ngraph1$plot(html = FALSE)\n\n\n\n\n\n\n\n\nWe can inspect the parameters of the ParamSet of the graph to see which parameters can be set:\n\ngraph1$param_set$ids()\n\n [1] \"colapply.applicator\"                         \"colapply.affect_columns\"                    \n [3] \"scale.center\"                                \"scale.scale\"                                \n [5] \"scale.robust\"                                \"scale.affect_columns\"                       \n [7] \"branch_learner.selection\"                    \"branch_preproc_lda.selection\"               \n [9] \"pca.center\"                                  \"pca.scale.\"                                 \n[11] \"pca.rank.\"                                   \"pca.affect_columns\"                         \n[13] \"ica.n.comp\"                                  \"ica.alg.typ\"                                \n[15] \"ica.fun\"                                     \"ica.alpha\"                                  \n[17] \"ica.method\"                                  \"ica.row.norm\"                               \n[19] \"ica.maxit\"                                   \"ica.tol\"                                    \n[21] \"ica.verbose\"                                 \"ica.w.init\"                                 \n[23] \"ica.affect_columns\"                          \"classif.lda.dimen\"                          \n[25] \"classif.lda.method\"                          \"classif.lda.nu\"                             \n[27] \"classif.lda.predict.method\"                  \"classif.lda.predict.prior\"                  \n[29] \"classif.lda.prior\"                           \"classif.lda.tol\"                            \n[31] \"classif.ranger.alpha\"                        \"classif.ranger.always.split.variables\"      \n[33] \"classif.ranger.class.weights\"                \"classif.ranger.holdout\"                     \n[35] \"classif.ranger.importance\"                   \"classif.ranger.keep.inbag\"                  \n[37] \"classif.ranger.max.depth\"                    \"classif.ranger.min.node.size\"               \n[39] \"classif.ranger.min.prop\"                     \"classif.ranger.minprop\"                     \n[41] \"classif.ranger.mtry\"                         \"classif.ranger.mtry.ratio\"                  \n[43] \"classif.ranger.num.random.splits\"            \"classif.ranger.num.threads\"                 \n[45] \"classif.ranger.num.trees\"                    \"classif.ranger.oob.error\"                   \n[47] \"classif.ranger.regularization.factor\"        \"classif.ranger.regularization.usedepth\"     \n[49] \"classif.ranger.replace\"                      \"classif.ranger.respect.unordered.factors\"   \n[51] \"classif.ranger.sample.fraction\"              \"classif.ranger.save.memory\"                 \n[53] \"classif.ranger.scale.permutation.importance\" \"classif.ranger.se.method\"                   \n[55] \"classif.ranger.seed\"                         \"classif.ranger.split.select.weights\"        \n[57] \"classif.ranger.splitrule\"                    \"classif.ranger.verbose\"                     \n[59] \"classif.ranger.write.forest\"                \n\n\nThe id’s are prefixed by the respective PipeOp they belong to, e.g., pca.rank. refers to the rank. parameter of PipeOpPCA."
  },
  {
    "objectID": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#search-space",
    "href": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#search-space",
    "title": "Tuning a Complex Graph",
    "section": "Search Space",
    "text": "Search Space\nOur graph either fits a LDA after applying PCA or ICA, or alternatively a ranger with no preprocessing. These two options each define selection parameters that we can tune. Moreover, within the respective PipeOp’s we want to tune the following parameters: pca.rank., ica.n.comp, classif.lda.method, classif.ranger.mtry, and classif.ranger.num.trees. The first two parameters are integers that in-principal could range from 1 to the number of features. However, for ICA, the upper bound must not exceed the number of observations and as we will later use 3-fold cross-validation as the resampling method for the tuning, we just set the upper bound to 30 (and do the same for PCA). Regarding the classif.lda.method we will only be interested in \"moment\" estimation vs. minimum volume ellipsoid covariance estimation (\"mve\"). Moreover, we set the lower bound of classif.ranger.mtry to 200 (which is around the number of features divided by 10) and the upper bound to 1000.\n\ntune_ps1 = ps(\n  branch_learner.selection =\n    p_fct(c(\"pca_ica_lda\", \"ranger\")),\n  branch_preproc_lda.selection =\n    p_fct(c(\"pca\", \"ica\"), depends = branch_learner.selection == \"pca_ica_lda\"),\n  pca.rank. =\n    p_int(1, 30, depends = branch_preproc_lda.selection == \"pca\"),\n  ica.n.comp =\n    p_int(1, 30, depends = branch_preproc_lda.selection == \"ica\"),\n  classif.lda.method =\n    p_fct(c(\"moment\", \"mve\"), depends = branch_preproc_lda.selection == \"ica\"),\n  classif.ranger.mtry =\n    p_int(200, 1000, depends = branch_learner.selection == \"ranger\"),\n  classif.ranger.num.trees =\n    p_int(500, 2000, depends = branch_learner.selection == \"ranger\"))\n\nThe parameter branch_learner.selection defines whether we go down the left (PCA / ICA followed by LDA) or the right branch (ranger). The parameter branch_preproc_lda.selection defines whether a PCA or ICA will be applied prior to the LDA. The other parameters directly belong to the ParamSet of the PCA / ICA / LDA / ranger. Note that it only makes sense to switch between PCA / ICA if the \"pca_ica_lda\" branch was selected beforehand. We have to specify this via the depends parameter.\nFinally, we also could have proceeded to tune the numeric parameters on a log scale. I.e., looking at pca.rank. the performance difference between rank 1 and 2 is probably much larger than between rank 29 and rank 30. The mlr3tuning Tutorial covers such transformations."
  },
  {
    "objectID": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#tuning",
    "href": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#tuning",
    "title": "Tuning a Complex Graph",
    "section": "Tuning",
    "text": "Tuning\nWe can now tune the parameters of our graph as defined in the search space with respect to a measure. We will use the classification accuracy. As a resampling method we use 3-fold cross-validation. We will use the TerminatorNone (i.e., no early termination) for terminating the tuning because we will apply a grid search (we use a grid search because it gives nicely plottable and understandable results but if there were much more parameters, random search or more intelligent optimization methods would be preferred to a grid search:\n\ntune1 = TuningInstanceSingleCrit$new(\n  task_train,\n  learner = graph1,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\"),\n  search_space = tune_ps1,\n  terminator = trm(\"none\")\n)\n\nWe then perform a grid search using a resolution of 4 for the numeric parameters. The grid being used will look like the following (note that the dependencies we specified above are handled automatically):\n\ngenerate_design_grid(tune_ps1, resolution = 4)\n\n\n\n\n\n\n\nWe trigger the tuning.\n\ntuner_gs = tnr(\"grid_search\", resolution = 4, batch_size = 10)\ntuner_gs$optimize(tune1)\n\n   branch_learner.selection branch_preproc_lda.selection pca.rank. ica.n.comp classif.lda.method classif.ranger.mtry\n1:              pca_ica_lda                          ica        NA         10                mve                  NA\n   classif.ranger.num.trees learner_param_vals  x_domain classif.acc\n1:                       NA          &lt;list[8]&gt; &lt;list[4]&gt;    0.984127\n\n\nNow, we can inspect the results ordered by the classification accuracy:\n\nas.data.table(tune1$archive)[order(classif.acc), ]\n\n\n\n\n\n\n\nWe achieve very good accuracy using ranger, more or less regardless how mtry and num.trees are set. However, the LDA also shows very good accuracy when combined with PCA or ICA retaining 30 components.\nFor now, we decide to use ranger with mtry set to 200 and num.trees set to 1000.\nSetting these parameters manually in our graph, then training on the training task and predicting on the test task yields an accuracy of:\n\ngraph1$param_set$values$branch_learner.selection = \"ranger\"\ngraph1$param_set$values$classif.ranger.mtry = 200\ngraph1$param_set$values$classif.ranger.num.trees = 1000\ngraph1$train(task_train)\n\n$unbranch_learner.output\nNULL\n\ngraph1$predict(task_test)[[1L]]$score(msr(\"classif.acc\"))\n\nclassif.acc \n          1 \n\n\nNote that we also could have wrapped our graph in a GraphLearner and proceeded to use this as a learner in an AutoTuner."
  },
  {
    "objectID": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#proxy",
    "href": "gallery/pipelines/2021-02-03-tuning-a-complex-graph/tuning-a-complex-graph.html#proxy",
    "title": "Tuning a Complex Graph",
    "section": "Proxy",
    "text": "Proxy\nInstead of using branches to split our graph with respect to the learner and preprocessing options, we can also use PipeOpProxy. PipeOpProxy accepts a single content parameter that can contain any other PipeOp or Graph. This is extremely flexible in the sense that we do not have to specify our options during construction. However, the parameters of the contained PipeOp or Graph are no longer directly contained in the ParamSet of the resulting graph. Therefore, when tuning the graph, we do have to make use of a trafo function.\n\ngraph2 =\n  po(\"colapply\", applicator = function(x) log(x, base = 10)) %&gt;&gt;%\n  po(\"scale\") %&gt;&gt;%\n  po(\"proxy\")\n\nThis graph now looks like the following:\n\ngraph2$plot(html = FALSE)\n\n\n\n\n\n\n\n\nAt first, this may look like a linear graph. However, as the content parameter of PipeOpProxy can be tuned and set to contain any other PipeOp or Graph, this will allow for a similar non-linear graph as when doing branching.\n\ngraph2$param_set$ids()\n\n[1] \"colapply.applicator\"     \"colapply.affect_columns\" \"scale.center\"            \"scale.scale\"            \n[5] \"scale.robust\"            \"scale.affect_columns\"    \"proxy.content\"          \n\n\nWe can tune the graph by using the same search space as before. However, here the trafo function is of central importance to actually set our options and parameters:\n\ntune_ps2 = tune_ps1$clone(deep = TRUE)\n\nThe trafo function does all the work, i.e., selecting either the PCA / ICA-LDA or ranger as the proxy.content as well as setting the parameters of the respective preprocessing PipeOps and Learners.\n\nproxy_options = list(\n  pca_ica_lda =\n    ppl(\"branch\", graphs = list(pca = po(\"pca\"), ica = po(\"ica\"))) %&gt;&gt;%\n      lrn(\"classif.lda\"),\n  ranger = lrn(\"classif.ranger\")\n)\n\nAbove, we made use of the branch ppl allowing us to easily construct a branching graph. Of course we also could have use another nested PipeOpProxy to specify the preprocessing options (\"pca\" vs. \"ica\") within proxy_options if for some reason we do not want to do branching at all. The trafo function below selects one of the proxy_options from above and sets the respective parameters for the PCA, ICA, LDA and ranger. Here, the argument x is a list which will contain sampled / selected parameters from our ParamSet (in our case, tune_ps2). The return value is a list only including the appropriate proxy.content parameter. In each tuning iteration, the proxy.content parameter of our graph will be set to this value.\n\ntune_ps2$trafo = function(x, param_set) {\n  proxy.content = proxy_options[[x$branch_learner.selection]]\n  if (x$branch_learner.selection == \"pca_ica_lda\") {\n    # pca_ica_lda\n    proxy.content$param_set$values$branch.selection = x$branch_preproc_lda.selection\n    if (x$branch_preproc_lda.selection == \"pca\") {\n      proxy.content$param_set$values$pca.rank. = x$pca.rank.\n    } else {\n      proxy.content$param_set$values$ica.n.comp = x$ica.n.comp\n    }\n    proxy.content$param_set$values$classif.lda.method = x$classif.lda.method\n  } else {\n    # ranger\n    proxy.content$param_set$values$mtry = x$classif.ranger.mtry\n    proxy.content$param_set$values$num.trees = x$classif.ranger.num.trees\n  }\n  list(proxy.content = proxy.content)\n}\n\nI.e., suppose that the following parameters will be selected from our ParamSet:\n\nx = list(\n  branch_learner.selection = \"ranger\",\n  classif.ranger.mtry = 200,\n  classif.ranger.num.trees = 500)\n\nThe trafo function will then return:\n\ntune_ps2$trafo(x)\n\n$proxy.content\n&lt;LearnerClassifRanger:classif.ranger&gt;\n* Model: -\n* Parameters: num.threads=1, mtry=200, num.trees=500\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error, twoclass, weights\n\n\nTuning can be carried out analogously as done above:\n\ntune2 = TuningInstanceSingleCrit$new(\n  task_train,\n  learner = graph2,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\"),\n  search_space = tune_ps2,\n  terminator = trm(\"none\")\n)\ntuner_gs$optimize(tune2)\n\n\nas.data.table(tune2$archive)[order(classif.acc), ]"
  },
  {
    "objectID": "gallery/pipelines/2020-02-01-tuning-multiplexer/index.html",
    "href": "gallery/pipelines/2020-02-01-tuning-multiplexer/index.html",
    "title": "Tuning Over Multiple Learners",
    "section": "",
    "text": "This use case shows how to tune over multiple learners for a single task. You will learn the following:\n\nBuild a pipeline that can switch between multiple learners\nDefine the hyperparameter search space for the pipeline\nRun a random or grid search (or any other tuner, always works the same)\nRun nested resampling for unbiased performance estimates\n\nThis is an advanced use case. What should you know before:\n\nmlr3 basics\nmlr3tuning basics, especially AutoTuner\nmlr3pipelines, especially branching\n\n\nThe Setup\nAssume, you are given some ML task and what to compare a couple of learners, probably because you want to select the best of them at the end of the analysis. That’s a super standard scenario, it actually sounds so common that you might wonder: Why an (advanced) blog post about this? With pipelines? We will consider 2 cases: (a) Running the learners in their default, so without tuning, and (b) with tuning.\nWe load the mlr3verse package which pulls in the most important packages for this example. The mlr3learners package loads additional learners.\n\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3learners)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nLet’s define our learners.\n\nlearners = list(\n  lrn(\"classif.xgboost\", id = \"xgb\", eval_metric = \"logloss\"),\n  lrn(\"classif.ranger\", id = \"rf\")\n)\nlearners_ids = sapply(learners, function(x) x$id)\n\ntask = tsk(\"sonar\") # some random data for this demo\ninner_cv2 = rsmp(\"cv\", folds = 2) # inner loop for nested CV\nouter_cv5 = rsmp(\"cv\", folds = 5) # outer loop for nested CV\n\n\n\nDefault Parameters\n\n\nThe Benchmark-Table Approach\nAssume we don’t want to perform tuning and or with running all learner in their respective defaults. Simply run benchmark on the learners and the tasks. That tabulates our results nicely and shows us what works best.\n\ngrid = benchmark_grid(task, learners, outer_cv5)\nbmr = benchmark(grid)\nbmr$aggregate(measures = msr(\"classif.ce\"))\n\n   nr task_id learner_id resampling_id iters classif.ce\n1:  1   sonar        xgb            cv     5  0.2736353\n2:  2   sonar         rf            cv     5  0.1973287\nHidden columns: resample_result\n\n\n\n\nThe Pipelines Approach\nOk, why would we ever want to change the simple approach above - and use pipelines / tuning for this? Three reasons:\n\nWhat we are doing with benchmark() is actually statistically flawed, insofar if we report the error of the numerically best method from the benchmark table as its estimated future performance. If we do that we have “optimized on the CV” (we basically ran a grid search over our learners!) and we know that this is will produce optimistically biased results. NB: This is a somewhat ridiculous criticism if we are going over only a handful of options, and the bias will be very small. But it will be noticeable if we do this over hundreds of learners, so it is important to understand the underlying problem. This is a somewhat subtle point, and this gallery post is more about technical hints for mlr3, so we will stop this discussion here.\nFor some tuning algorithms, you might have a chance to more efficiently select from the set of algorithms than running the full benchmark. Because of the categorical nature of the problem, you will not be able to learn stuff like “If learner A works bad, I don’t have to try learner B”, but you can potentially save some resampling iterations. Assume you have so select from 100 candidates, experiments are expensive, and you use a 20-fold CV. If learner A has super-bad results in the first 5 folds of the CV, you might already want to stop here. “Racing” would be such a tuning algorithm.\nIt helps us to foreshadow what comes later in this post where we tune the learners.\n\nThe pipeline just has a single purpose in this example: It should allow us to switch between different learners, depending on a hyperparameter. The pipe consists of three elements:\n\nbranch pipes incoming data to one of the following elements, on different data channels. We can name these channel on construction with options.\nour learners (combined with gunion())\nunbranch combines the forked paths at the end.\n\n\ngraph =\n  po(\"branch\", options = learners_ids) %&gt;&gt;%\n  gunion(lapply(learners, po)) %&gt;&gt;%\n  po(\"unbranch\")\nplot(graph, html = FALSE)\n\n\n\n\n\n\n\n\nThe pipeline has now quite a lot of available hyperparameters. It includes all hyperparameters from all contained learners. But as we don’t tune them here (yet), we don’t care (yet). But the first hyperparameter is special. branch.selection controls over which (named) branching channel our data flows.\n\ngraph$param_set$ids()\n\n [1] \"branch.selection\"                \"xgb.alpha\"                       \"xgb.approxcontrib\"              \n [4] \"xgb.base_score\"                  \"xgb.booster\"                     \"xgb.callbacks\"                  \n [7] \"xgb.colsample_bylevel\"           \"xgb.colsample_bynode\"            \"xgb.colsample_bytree\"           \n[10] \"xgb.disable_default_eval_metric\" \"xgb.early_stopping_rounds\"       \"xgb.early_stopping_set\"         \n[13] \"xgb.eta\"                         \"xgb.eval_metric\"                 \"xgb.feature_selector\"           \n[16] \"xgb.feval\"                       \"xgb.gamma\"                       \"xgb.grow_policy\"                \n[19] \"xgb.interaction_constraints\"     \"xgb.iterationrange\"              \"xgb.lambda\"                     \n[22] \"xgb.lambda_bias\"                 \"xgb.max_bin\"                     \"xgb.max_delta_step\"             \n[25] \"xgb.max_depth\"                   \"xgb.max_leaves\"                  \"xgb.maximize\"                   \n[28] \"xgb.min_child_weight\"            \"xgb.missing\"                     \"xgb.monotone_constraints\"       \n[31] \"xgb.normalize_type\"              \"xgb.nrounds\"                     \"xgb.nthread\"                    \n[34] \"xgb.ntreelimit\"                  \"xgb.num_parallel_tree\"           \"xgb.objective\"                  \n[37] \"xgb.one_drop\"                    \"xgb.outputmargin\"                \"xgb.predcontrib\"                \n[40] \"xgb.predictor\"                   \"xgb.predinteraction\"             \"xgb.predleaf\"                   \n[43] \"xgb.print_every_n\"               \"xgb.process_type\"                \"xgb.rate_drop\"                  \n[46] \"xgb.refresh_leaf\"                \"xgb.reshape\"                     \"xgb.seed_per_iteration\"         \n[49] \"xgb.sampling_method\"             \"xgb.sample_type\"                 \"xgb.save_name\"                  \n[52] \"xgb.save_period\"                 \"xgb.scale_pos_weight\"            \"xgb.skip_drop\"                  \n[55] \"xgb.strict_shape\"                \"xgb.subsample\"                   \"xgb.top_k\"                      \n[58] \"xgb.training\"                    \"xgb.tree_method\"                 \"xgb.tweedie_variance_power\"     \n[61] \"xgb.updater\"                     \"xgb.verbose\"                     \"xgb.watchlist\"                  \n[64] \"xgb.xgb_model\"                   \"rf.alpha\"                        \"rf.always.split.variables\"      \n[67] \"rf.class.weights\"                \"rf.holdout\"                      \"rf.importance\"                  \n[70] \"rf.keep.inbag\"                   \"rf.max.depth\"                    \"rf.min.node.size\"               \n[73] \"rf.min.prop\"                     \"rf.minprop\"                      \"rf.mtry\"                        \n[76] \"rf.mtry.ratio\"                   \"rf.num.random.splits\"            \"rf.num.threads\"                 \n[79] \"rf.num.trees\"                    \"rf.oob.error\"                    \"rf.regularization.factor\"       \n[82] \"rf.regularization.usedepth\"      \"rf.replace\"                      \"rf.respect.unordered.factors\"   \n[85] \"rf.sample.fraction\"              \"rf.save.memory\"                  \"rf.scale.permutation.importance\"\n[88] \"rf.se.method\"                    \"rf.seed\"                         \"rf.split.select.weights\"        \n[91] \"rf.splitrule\"                    \"rf.verbose\"                      \"rf.write.forest\"                \n\ngraph$param_set$params$branch.selection\n\n                 id    class lower upper levels        default\n1: branch.selection ParamFct    NA    NA xgb,rf &lt;NoDefault[3]&gt;\n\n\nWe can now tune over this pipeline, and probably running grid search seems a good idea to “touch” every available learner. NB: We have now written down in (much more complicated code) what we did before with benchmark.\n\ngraph_learner = as_learner(graph)\ngraph_learner$id = \"g\"\n\nsearch_space = ps(\n  branch.selection = p_fct(c(\"rf\", \"xgb\"))\n)\n\ninstance = tune(\n  tuner = tnr(\"grid_search\"),\n  task = task,\n  learner = graph_learner,\n  resampling = inner_cv2,\n  measure = msr(\"classif.ce\"),\n  search_space = search_space\n)\n\nas.data.table(instance$archive)[, list(branch.selection, classif.ce)]\n\n   branch.selection classif.ce\n1:               rf  0.1778846\n2:              xgb  0.3269231\n\n\nBut: Via this approach we can now get unbiased performance results via nested resampling and using the AutoTuner (which would make much more sense if we would select from 100 models and not 2).\n\nat = auto_tuner(\n  tuner = tnr(\"grid_search\"),\n  learner = graph_learner,\n  resampling = inner_cv2,\n  measure = msr(\"classif.ce\"),\n  search_space = search_space\n)\n\nrr = resample(task, at, outer_cv5, store_models = TRUE)\n\nAccess inner tuning result.\n\nextract_inner_tuning_results(rr)[, list(iteration, branch.selection, classif.ce)]\n\n   iteration branch.selection classif.ce\n1:         1               rf  0.2349398\n2:         2               rf  0.1626506\n3:         3               rf  0.3012048\n4:         4               rf  0.2813396\n5:         5               rf  0.2932444\n\n\nAccess inner tuning archives.\n\nextract_inner_tuning_archives(rr)[, list(iteration, branch.selection, classif.ce, resample_result)]\n\n    iteration branch.selection classif.ce      resample_result\n 1:         1               rf  0.2349398 &lt;ResampleResult[21]&gt;\n 2:         1              xgb  0.2469880 &lt;ResampleResult[21]&gt;\n 3:         2               rf  0.1626506 &lt;ResampleResult[21]&gt;\n 4:         2              xgb  0.2530120 &lt;ResampleResult[21]&gt;\n 5:         3              xgb  0.3795181 &lt;ResampleResult[21]&gt;\n 6:         3               rf  0.3012048 &lt;ResampleResult[21]&gt;\n 7:         4              xgb  0.3714859 &lt;ResampleResult[21]&gt;\n 8:         4               rf  0.2813396 &lt;ResampleResult[21]&gt;\n 9:         5              xgb  0.3353414 &lt;ResampleResult[21]&gt;\n10:         5               rf  0.2932444 &lt;ResampleResult[21]&gt;\n\n\n\n\nModel-Selection and Tuning with Pipelines\nNow let’s select from our given set of models and tune their hyperparameters. One way to do this is to define a search space for each individual learner, wrap them all with the AutoTuner, then call benchmark() on them. As this is pretty standard, we will skip this here, and show an even neater option, where you can tune over models and hyperparameters in one go. If you have quite a large space of potential learners and combine this with an efficient tuning algorithm, this can save quite some time in tuning as you can learn during optimization which options work best and focus on them. NB: Many AutoML systems work in a very similar way.\n\n\nDefine the Search Space\nRemember, that the pipeline contains a joint set of all contained hyperparameters. Prefixed with the respective PipeOp ID, to make names unique.\n\nas.data.table(graph$param_set)[, list(id, class, lower, upper, nlevels)]\n\n                                 id    class lower upper nlevels\n 1:                branch.selection ParamFct    NA    NA       2\n 2:                       xgb.alpha ParamDbl     0   Inf     Inf\n 3:               xgb.approxcontrib ParamLgl    NA    NA       2\n 4:                  xgb.base_score ParamDbl  -Inf   Inf     Inf\n 5:                     xgb.booster ParamFct    NA    NA       3\n 6:                   xgb.callbacks ParamUty    NA    NA     Inf\n 7:           xgb.colsample_bylevel ParamDbl     0     1     Inf\n 8:            xgb.colsample_bynode ParamDbl     0     1     Inf\n 9:            xgb.colsample_bytree ParamDbl     0     1     Inf\n10: xgb.disable_default_eval_metric ParamLgl    NA    NA       2\n11:       xgb.early_stopping_rounds ParamInt     1   Inf     Inf\n12:          xgb.early_stopping_set ParamFct    NA    NA       3\n13:                         xgb.eta ParamDbl     0     1     Inf\n14:                 xgb.eval_metric ParamUty    NA    NA     Inf\n15:            xgb.feature_selector ParamFct    NA    NA       5\n16:                       xgb.feval ParamUty    NA    NA     Inf\n17:                       xgb.gamma ParamDbl     0   Inf     Inf\n18:                 xgb.grow_policy ParamFct    NA    NA       2\n19:     xgb.interaction_constraints ParamUty    NA    NA     Inf\n20:              xgb.iterationrange ParamUty    NA    NA     Inf\n21:                      xgb.lambda ParamDbl     0   Inf     Inf\n22:                 xgb.lambda_bias ParamDbl     0   Inf     Inf\n23:                     xgb.max_bin ParamInt     2   Inf     Inf\n24:              xgb.max_delta_step ParamDbl     0   Inf     Inf\n25:                   xgb.max_depth ParamInt     0   Inf     Inf\n26:                  xgb.max_leaves ParamInt     0   Inf     Inf\n27:                    xgb.maximize ParamLgl    NA    NA       2\n28:            xgb.min_child_weight ParamDbl     0   Inf     Inf\n29:                     xgb.missing ParamDbl  -Inf   Inf     Inf\n30:        xgb.monotone_constraints ParamUty    NA    NA     Inf\n31:              xgb.normalize_type ParamFct    NA    NA       2\n32:                     xgb.nrounds ParamInt     1   Inf     Inf\n33:                     xgb.nthread ParamInt     1   Inf     Inf\n34:                  xgb.ntreelimit ParamInt     1   Inf     Inf\n35:           xgb.num_parallel_tree ParamInt     1   Inf     Inf\n36:                   xgb.objective ParamUty    NA    NA     Inf\n37:                    xgb.one_drop ParamLgl    NA    NA       2\n38:                xgb.outputmargin ParamLgl    NA    NA       2\n39:                 xgb.predcontrib ParamLgl    NA    NA       2\n40:                   xgb.predictor ParamFct    NA    NA       2\n41:             xgb.predinteraction ParamLgl    NA    NA       2\n42:                    xgb.predleaf ParamLgl    NA    NA       2\n43:               xgb.print_every_n ParamInt     1   Inf     Inf\n44:                xgb.process_type ParamFct    NA    NA       2\n45:                   xgb.rate_drop ParamDbl     0     1     Inf\n46:                xgb.refresh_leaf ParamLgl    NA    NA       2\n47:                     xgb.reshape ParamLgl    NA    NA       2\n48:          xgb.seed_per_iteration ParamLgl    NA    NA       2\n49:             xgb.sampling_method ParamFct    NA    NA       2\n50:                 xgb.sample_type ParamFct    NA    NA       2\n51:                   xgb.save_name ParamUty    NA    NA     Inf\n52:                 xgb.save_period ParamInt     0   Inf     Inf\n53:            xgb.scale_pos_weight ParamDbl  -Inf   Inf     Inf\n54:                   xgb.skip_drop ParamDbl     0     1     Inf\n55:                xgb.strict_shape ParamLgl    NA    NA       2\n56:                   xgb.subsample ParamDbl     0     1     Inf\n57:                       xgb.top_k ParamInt     0   Inf     Inf\n58:                    xgb.training ParamLgl    NA    NA       2\n59:                 xgb.tree_method ParamFct    NA    NA       5\n60:      xgb.tweedie_variance_power ParamDbl     1     2     Inf\n61:                     xgb.updater ParamUty    NA    NA     Inf\n62:                     xgb.verbose ParamInt     0     2       3\n63:                   xgb.watchlist ParamUty    NA    NA     Inf\n64:                   xgb.xgb_model ParamUty    NA    NA     Inf\n65:                        rf.alpha ParamDbl  -Inf   Inf     Inf\n66:       rf.always.split.variables ParamUty    NA    NA     Inf\n67:                rf.class.weights ParamUty    NA    NA     Inf\n68:                      rf.holdout ParamLgl    NA    NA       2\n69:                   rf.importance ParamFct    NA    NA       4\n70:                   rf.keep.inbag ParamLgl    NA    NA       2\n71:                    rf.max.depth ParamInt     0   Inf     Inf\n72:                rf.min.node.size ParamInt     1   Inf     Inf\n73:                     rf.min.prop ParamDbl  -Inf   Inf     Inf\n74:                      rf.minprop ParamDbl  -Inf   Inf     Inf\n75:                         rf.mtry ParamInt     1   Inf     Inf\n76:                   rf.mtry.ratio ParamDbl     0     1     Inf\n77:            rf.num.random.splits ParamInt     1   Inf     Inf\n78:                  rf.num.threads ParamInt     1   Inf     Inf\n79:                    rf.num.trees ParamInt     1   Inf     Inf\n80:                    rf.oob.error ParamLgl    NA    NA       2\n81:        rf.regularization.factor ParamUty    NA    NA     Inf\n82:      rf.regularization.usedepth ParamLgl    NA    NA       2\n83:                      rf.replace ParamLgl    NA    NA       2\n84:    rf.respect.unordered.factors ParamFct    NA    NA       3\n85:              rf.sample.fraction ParamDbl     0     1     Inf\n86:                  rf.save.memory ParamLgl    NA    NA       2\n87: rf.scale.permutation.importance ParamLgl    NA    NA       2\n88:                    rf.se.method ParamFct    NA    NA       2\n89:                         rf.seed ParamInt  -Inf   Inf     Inf\n90:         rf.split.select.weights ParamUty    NA    NA     Inf\n91:                    rf.splitrule ParamFct    NA    NA       3\n92:                      rf.verbose ParamLgl    NA    NA       2\n93:                 rf.write.forest ParamLgl    NA    NA       2\n                                 id    class lower upper nlevels\n\n\nWe decide to tune the mtry parameter of the random forest and the nrounds parameter of xgboost. Additionally, we tune branching parameter that selects our learner.\nWe also have to reflect the hierarchical order of the parameter sets (admittedly, this is somewhat inconvenient). We can only set the mtry value if the pipe is configured to use the random forest (ranger). The same applies for the xgboost parameter.\n\nsearch_space = ps(\n  branch.selection = p_fct(c(\"rf\", \"xgb\")),\n  rf.mtry = p_int(1L, 20L, depends = branch.selection == \"rf\"),\n  xgb.nrounds = p_int(1, 500, depends = branch.selection == \"xgb\"))\n\n\n\nTune the Pipeline with a Random Search\nVery similar code as before, we just swap out the search space. And now use random search.\n\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = task,\n  learner = graph_learner,\n  resampling = inner_cv2,\n  measure = msr(\"classif.ce\"),\n  term_evals = 10,\n  search_space = search_space\n)\n\nas.data.table(instance$archive)[, list(branch.selection, xgb.nrounds, rf.mtry, classif.ce)]\n\n    branch.selection xgb.nrounds rf.mtry classif.ce\n 1:              xgb         292      NA  0.1875000\n 2:               rf          NA      19  0.2692308\n 3:               rf          NA       5  0.2307692\n 4:              xgb         229      NA  0.1875000\n 5:              xgb         301      NA  0.1875000\n 6:               rf          NA      20  0.2596154\n 7:               rf          NA       8  0.2355769\n 8:               rf          NA       2  0.2355769\n 9:               rf          NA       5  0.2500000\n10:               rf          NA      18  0.2451923\n\n\nThe following shows a quick way to visualize the tuning results.\n\nautoplot(instance, cols_x = c(\"xgb.nrounds\",\"rf.mtry\"))\n\n\n\n\n\n\n\n\nNested resampling, now really needed:\n\nrr = tune_nested(\n  tuner = tnr(\"grid_search\"),\n  task = task,\n  learner = graph_learner,\n  inner_resampling = inner_cv2,\n  outer_resampling = outer_cv5,\n  measure = msr(\"classif.ce\"),\n  term_evals = 10L,\n  search_space = search_space)\n\nAccess inner tuning result.\n\nextract_inner_tuning_results(rr)[, list(iteration, branch.selection, classif.ce)]\n\n   iteration branch.selection classif.ce\n1:         1               rf  0.2108434\n2:         2               rf  0.1807229\n3:         3               rf  0.2289157\n4:         4              xgb  0.1915519\n5:         5               rf  0.1858147"
  },
  {
    "objectID": "gallery/pipelines/2020-03-12-intro-pipelines-titanic/index.html",
    "href": "gallery/pipelines/2020-03-12-intro-pipelines-titanic/index.html",
    "title": "A Pipeline for the Titanic Data Set - Basics",
    "section": "",
    "text": "We load the mlr3verse package which pulls in the most important packages for this example. The mlr3learners package loads additional learners. The data is part of the mlr3data package.\n\nlibrary(mlr3verse)\nlibrary(mlr3learners)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\nThe titanic data is very interesting to analyze, even though it is part of many tutorials and showcases. This is because it requires many steps often required in real-world applications of machine learning techniques, such as missing value imputation, handling factors and others.\nThe following features are illustrated in this use case section:\n\nSummarizing the data set\nVisualizing data\nSplitting data into train and test data sets\nDefining a task and a learner"
  },
  {
    "objectID": "gallery/pipelines/2020-03-12-intro-pipelines-titanic/index.html#intro",
    "href": "gallery/pipelines/2020-03-12-intro-pipelines-titanic/index.html#intro",
    "title": "A Pipeline for the Titanic Data Set - Basics",
    "section": "",
    "text": "We load the mlr3verse package which pulls in the most important packages for this example. The mlr3learners package loads additional learners. The data is part of the mlr3data package.\n\nlibrary(mlr3verse)\nlibrary(mlr3learners)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\nThe titanic data is very interesting to analyze, even though it is part of many tutorials and showcases. This is because it requires many steps often required in real-world applications of machine learning techniques, such as missing value imputation, handling factors and others.\nThe following features are illustrated in this use case section:\n\nSummarizing the data set\nVisualizing data\nSplitting data into train and test data sets\nDefining a task and a learner"
  },
  {
    "objectID": "gallery/pipelines/2020-03-12-intro-pipelines-titanic/index.html#exploratory-data-analysis",
    "href": "gallery/pipelines/2020-03-12-intro-pipelines-titanic/index.html#exploratory-data-analysis",
    "title": "A Pipeline for the Titanic Data Set - Basics",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nWith the dataset, we get an explanation of the meanings of the different variables:\n\n\n\nVariables\nDescription\n\n\n\n\nsurvived\nSurvival\n\n\nname\nName\n\n\nage\nAge\n\n\nsex\nSex\n\n\nsib_sp\nNumber of siblings / spouses aboard\n\n\nparch\nNumber of parents / children aboard\n\n\nfare\nAmount paid for the ticket\n\n\npc_class\nPassenger class\n\n\nembarked\nPort of embarkation\n\n\nticket\nTicket number\n\n\ncabin\nCabin\n\n\n\nWe can use the skimr package in order to get a first overview of the data:\n\ndata(\"titanic\", package = \"mlr3data\")\n\nskimr::skim(titanic)\n\n\nData summary\n\n\nName\ntitanic\n\n\nNumber of rows\n1309\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nfactor\n4\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nname\n0\n1.00\n12\n82\n0\n1307\n0\n\n\nticket\n0\n1.00\n3\n18\n0\n929\n0\n\n\ncabin\n1014\n0.23\n1\n15\n0\n186\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsurvived\n418\n0.68\nFALSE\n2\nno: 549, yes: 342\n\n\npclass\n0\n1.00\nTRUE\n3\n3: 709, 1: 323, 2: 277\n\n\nsex\n0\n1.00\nFALSE\n2\nmal: 843, fem: 466\n\n\nembarked\n2\n1.00\nFALSE\n3\nS: 914, C: 270, Q: 123\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nage\n263\n0.8\n29.88\n14.41\n0.17\n21.0\n28.00\n39.00\n80.00\n▂▇▅▂▁\n\n\nsib_sp\n0\n1.0\n0.50\n1.04\n0.00\n0.0\n0.00\n1.00\n8.00\n▇▁▁▁▁\n\n\nparch\n0\n1.0\n0.39\n0.87\n0.00\n0.0\n0.00\n0.00\n9.00\n▇▁▁▁▁\n\n\nfare\n1\n1.0\n33.30\n51.76\n0.00\n7.9\n14.45\n31.27\n512.33\n▇▁▁▁▁\n\n\n\n\n\nWe can now create a Task from our data. As we want to classify whether the person survived or not, we will create a TaskClassif. We’ll ignore the ‘titanic_test’ data for now and come back to it later."
  },
  {
    "objectID": "gallery/pipelines/2020-03-12-intro-pipelines-titanic/index.html#a-first-model",
    "href": "gallery/pipelines/2020-03-12-intro-pipelines-titanic/index.html#a-first-model",
    "title": "A Pipeline for the Titanic Data Set - Basics",
    "section": "A first model",
    "text": "A first model\nIn order to obtain solutions comparable to official leaderboards, such as the ones available from kaggle, we split the data into train and validation set before doing any further analysis. Here we are using the predefined split used by Kaggle.\n\ntask = as_task_classif(titanic, target = \"survived\", positive = \"yes\")\ntask$set_row_roles(892:1309, \"holdout\")\ntask\n\n&lt;TaskClassif:titanic&gt; (891 x 11)\n* Target: survived\n* Properties: twoclass\n* Features (10):\n  - chr (3): cabin, name, ticket\n  - dbl (2): age, fare\n  - fct (2): embarked, sex\n  - int (2): parch, sib_sp\n  - ord (1): pclass\n\n\nOur Task currently has \\(3\\) features of type character, which we don’t really know how to handle: “Cabin”, “Name”, “Ticket” and “PassengerId”. Additionally, from our skimr::skim() of the data, we have seen, that they have many unique values (up to 891).\nWe’ll drop them for now and see how we can deal with them later on.\n\ntask$select(cols = setdiff(task$feature_names, c(\"cabin\", \"name\", \"ticket\")))\n\nAdditionally, we create a resampling instance that allows to compare data.\n\ncv3 = rsmp(\"cv\", folds = 3L)$instantiate(task)\n\nTo get a first impression of what performance we can fit a simple decision tree:\n\nlearner = mlr_learners$get(\"classif.rpart\")\n# or shorter:\nlearner = lrn(\"classif.rpart\")\n\nrr = resample(task, learner, cv3, store_models = TRUE)\n\nrr$aggregate(msr(\"classif.acc\"))\n\nclassif.acc \n  0.8013468 \n\n\nSo our model should have a minimal accuracy of 0.80 in order to improve over the simple decision tree. In order to improve more, we might need to do some feature engineering."
  },
  {
    "objectID": "gallery/basic/2020-01-30-impute-missing-levels/index.html",
    "href": "gallery/basic/2020-01-30-impute-missing-levels/index.html",
    "title": "Impute Missing Variables",
    "section": "",
    "text": "This tutorial assumes familiarity with the basics of mlr3pipelines. Consult the mlr3book if some aspects are not fully understandable. It deals with the problem of missing data.\nThe random forest implementation in the package ranger unfortunately does not support missing values. Therefore, it is required to impute missing features before passing the data to the learner.\nWe show how to use mlr3pipelines to augment the ranger learner with automatic imputation.\nWe load the mlr3verse package which pulls in the most important packages for this example.\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/basic/2020-01-30-impute-missing-levels/index.html#prerequisites",
    "href": "gallery/basic/2020-01-30-impute-missing-levels/index.html#prerequisites",
    "title": "Impute Missing Variables",
    "section": "",
    "text": "This tutorial assumes familiarity with the basics of mlr3pipelines. Consult the mlr3book if some aspects are not fully understandable. It deals with the problem of missing data.\nThe random forest implementation in the package ranger unfortunately does not support missing values. Therefore, it is required to impute missing features before passing the data to the learner.\nWe show how to use mlr3pipelines to augment the ranger learner with automatic imputation.\nWe load the mlr3verse package which pulls in the most important packages for this example.\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/basic/2020-01-30-impute-missing-levels/index.html#construct-the-base-objects",
    "href": "gallery/basic/2020-01-30-impute-missing-levels/index.html#construct-the-base-objects",
    "title": "Impute Missing Variables",
    "section": "Construct the Base Objects",
    "text": "Construct the Base Objects\nFirst, we take an example task with missing values (pima) and create the ranger learner:\n\nlibrary(mlr3learners)\n\ntask = tsk(\"pima\")\nprint(task)\n\n&lt;TaskClassif:pima&gt; (768 x 9): Pima Indian Diabetes\n* Target: diabetes\n* Properties: twoclass\n* Features (8):\n  - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure, triceps\n\nlearner = lrn(\"classif.ranger\")\nprint(learner)\n\n&lt;LearnerClassifRanger:classif.ranger&gt;\n* Model: -\n* Parameters: num.threads=1\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error, twoclass, weights\n\n\nWe can now inspect the task for missing values. task$missings() returns the count of missing values for each variable.\n\ntask$missings()\n\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure  triceps \n       0        0        5      374       11        0        0       35      227 \n\n\nAdditionally, we can see that the ranger learner can not handle missing values:\n\nlearner$properties\n\n[1] \"hotstart_backward\" \"importance\"        \"multiclass\"        \"oob_error\"         \"twoclass\"         \n[6] \"weights\"          \n\n\nFor comparison, other learners, e.g. the rpart learner can handle missing values internally.\n\nlrn(\"classif.rpart\")$properties\n\n[1] \"importance\"        \"missings\"          \"multiclass\"        \"selected_features\" \"twoclass\"         \n[6] \"weights\"          \n\n\nBefore we dive deeper, we quickly try to visualize the columns with many missing values:\n\nautoplot(task$clone()$select(c(\"insulin\", \"triceps\")), type = \"pairs\")"
  },
  {
    "objectID": "gallery/basic/2020-01-30-impute-missing-levels/index.html#operators-overview",
    "href": "gallery/basic/2020-01-30-impute-missing-levels/index.html#operators-overview",
    "title": "Impute Missing Variables",
    "section": "Operators overview",
    "text": "Operators overview\nAn overview over implemented PipeOps for imputation can be obtained like so:\n\nas.data.table(mlr_pipeops)[tags %in% \"missings\", list(key)]\n\n              key\n1: imputeconstant\n2:     imputehist\n3:  imputelearner\n4:     imputemean\n5:   imputemedian\n6:     imputemode\n7:      imputeoor\n8:   imputesample"
  },
  {
    "objectID": "gallery/basic/2020-01-30-impute-missing-levels/index.html#construct-operators",
    "href": "gallery/basic/2020-01-30-impute-missing-levels/index.html#construct-operators",
    "title": "Impute Missing Variables",
    "section": "Construct Operators",
    "text": "Construct Operators\nmlr3pipelines contains several imputation methods. We focus on rather simple ones, and show how to impute missing values for factor features and numeric features respectively.\nSince our task only has numeric features, we do not need to deal with imputing factor levels, and can instead concentrate on imputing numeric values:\nWe do this in a two-step process: * We create new indicator columns, that tells us whether the value of a feature is “missing” or “present”. We achieve this using the missind PipeOp.\n\nAfterwards, we impute every missing value by sampling from the histogram of the respective column. We achieve this using the imputehist PipeOp.\n\nWe also have to make sure to apply the pipe operators in the correct order!\n\nimp_missind = po(\"missind\")\nimp_num = po(\"imputehist\", affect_columns = selector_type(\"numeric\"))\n\nIn order to better understand we can look at the results of every PipeOp separately.\nWe can manually trigger the PipeOp to test the operator on our task:\n\ntask_ext = imp_missind$train(list(task))[[1]]\ntask_ext$data()\n\n     diabetes missing_glucose missing_insulin missing_mass missing_pressure missing_triceps\n  1:      pos         present         missing      present          present         present\n  2:      neg         present         missing      present          present         present\n  3:      pos         present         missing      present          present         missing\n  4:      neg         present         present      present          present         present\n  5:      pos         present         present      present          present         present\n ---                                                                                       \n764:      neg         present         present      present          present         present\n765:      neg         present         missing      present          present         present\n766:      neg         present         present      present          present         present\n767:      pos         present         missing      present          present         missing\n768:      neg         present         missing      present          present         present\n\n\nFor imputehist, we can do the same:\n\ntask_ext = imp_num$train(list(task))[[1]]\ntask_ext$data()\n\n     diabetes age pedigree pregnant glucose   insulin mass pressure   triceps\n  1:      pos  50    0.627        6     148 163.11747 33.6       72 35.000000\n  2:      neg  31    0.351        1      85 160.63628 26.6       66 29.000000\n  3:      pos  32    0.672        8     183 297.18282 23.3       64  8.204983\n  4:      neg  21    0.167        1      89  94.00000 28.1       66 23.000000\n  5:      pos  33    2.288        0     137 168.00000 43.1       40 35.000000\n ---                                                                         \n764:      neg  63    0.171       10     101 180.00000 32.9       76 48.000000\n765:      neg  27    0.340        2     122  83.69836 36.8       70 27.000000\n766:      neg  30    0.245        5     121 112.00000 26.2       72 23.000000\n767:      pos  47    0.349        1     126  68.49318 30.1       60 24.460702\n768:      neg  23    0.315        1      93  17.80534 30.4       70 31.000000\n\n\nThis time we obtain the imputed data set without missing values.\n\ntask_ext$missings()\n\ndiabetes      age pedigree pregnant  glucose  insulin     mass pressure  triceps \n       0        0        0        0        0        0        0        0        0"
  },
  {
    "objectID": "gallery/basic/2020-01-30-impute-missing-levels/index.html#putting-everything-together",
    "href": "gallery/basic/2020-01-30-impute-missing-levels/index.html#putting-everything-together",
    "title": "Impute Missing Variables",
    "section": "Putting everything together",
    "text": "Putting everything together\nNow we have to put all PipeOps together in order to form a graph that handles imputation automatically.\nWe do this by creating a Graph that copies the data twice, processes each copy using the respective imputation method and afterwards unions the features. For this we need the following two PipeOps : * copy: Creates copies of the data. * featureunion Merges the two tasks together.\n\ngraph = po(\"copy\", 2) %&gt;&gt;%\n  gunion(list(imp_missind, imp_num)) %&gt;&gt;%\n  po(\"featureunion\")\n\nas a last step we append the learner we planned on using:\n\ngraph = graph %&gt;&gt;% po(learner)\n\nWe can now visualize the resulting graph:\n\ngraph$plot()"
  },
  {
    "objectID": "gallery/basic/2020-01-30-impute-missing-levels/index.html#resampling",
    "href": "gallery/basic/2020-01-30-impute-missing-levels/index.html#resampling",
    "title": "Impute Missing Variables",
    "section": "Resampling",
    "text": "Resampling\nCorrect imputation is especially important when applying imputation to held-out data during the predict step. If applied incorrectly, imputation could leak info from the test set, which potentially skews our performance estimates. mlr3pipelines takes this complexity away from the user and handles correct imputation internally.\nBy wrapping this graph into a GraphLearner, we can now train resample the full graph, here with a 3-fold cross validation:\n\ngraph_learner = as_learner(graph)\nrr = resample(task, graph_learner, rsmp(\"cv\", folds = 3))\nrr$aggregate()\n\nclassif.ce \n 0.2330729"
  },
  {
    "objectID": "gallery/basic/2020-01-30-impute-missing-levels/index.html#missing-values-during-prediction",
    "href": "gallery/basic/2020-01-30-impute-missing-levels/index.html#missing-values-during-prediction",
    "title": "Impute Missing Variables",
    "section": "Missing values during prediction",
    "text": "Missing values during prediction\nIn some cases, we have missing values only in the data we want to predict on. In order to showcase this, we create a copy of the task with several more missing columns.\n\ndt = task$data()\ndt[1:10, \"age\"] = NA\ndt[30:70, \"pedigree\"] = NA\ntask_2 = as_task_classif(dt, id = \"pima2\", target = \"diabetes\")\n\nAnd now we learn on task, while trying to predict on task_2.\n\ngraph_learner$train(task)\ngraph_learner$predict(task_2)\n\n&lt;PredictionClassif&gt; for 768 observations:\n    row_ids truth response\n          1   pos      pos\n          2   neg      neg\n          3   pos      pos\n---                       \n        766   neg      neg\n        767   pos      pos\n        768   neg      neg"
  },
  {
    "objectID": "gallery/basic/2020-01-30-impute-missing-levels/index.html#missing-factor-features",
    "href": "gallery/basic/2020-01-30-impute-missing-levels/index.html#missing-factor-features",
    "title": "Impute Missing Variables",
    "section": "Missing factor features",
    "text": "Missing factor features\nFor factor features, the process works analogously. Instead of using imputehist, we can for example use imputeoor. This will simply replace every NA in each factor variable with a new value missing.\nA full graph might the look like this:\n\nimp_missind = po(\"missind\", affect_columns = NULL, which = \"all\")\nimp_fct = po(\"imputeoor\", affect_columns = selector_type(\"factor\"))\ngraph = po(\"copy\", 2) %&gt;&gt;%\n  gunion(list(imp_missind, imp_num %&gt;&gt;% imp_fct)) %&gt;&gt;%\n  po(\"featureunion\")\n\nNote that we specify the parameter affect_columns = NULL when initializing missind, because we also want indicator columns for our factor features. By default, affect_columns would be set to selector_invert(selector_type(c(\"factor\", \"ordered\", \"character\"))). We also set the parameter which to \"all\" to add indicator columns for all features, regardless whether values were missing during training or not.\nIn order to test out our new graph, we again create a situation where our task has missing factor levels. As the (pima) task does not have any factor levels, we use the famous (boston_housing) task.\n\n# task_bh_1 is the training data without missings\ntask_bh_1 = tsk(\"boston_housing\")\n\n# task_bh_2 is the prediction data with missings\ndt = task_bh_1$data()\ndt[1:10, chas := NA][20:30, rm := NA]\ntask_bh_2 = as_task_regr(dt, id = \"bh\", target = \"medv\")\n\nNow we train on task_bh_1 and predict on task_bh_2:\n\ngraph_learner = as_learner(graph %&gt;&gt;% po(lrn(\"regr.ranger\")))\ngraph_learner$train(task_bh_1)\ngraph_learner$predict(task_bh_2)\n\n&lt;PredictionRegr&gt; for 506 observations:\n    row_ids truth response\n          1  24.0 25.16204\n          2  21.6 22.21102\n          3  34.7 33.84124\n---                       \n        504  23.9 23.68916\n        505  22.0 22.20551\n        506  11.9 16.14491\n\n\nSuccess! We learned how to deal with missing values in less than 10 minutes."
  },
  {
    "objectID": "gallery/basic/2020-01-30-house-prices-in-king-county/index.html",
    "href": "gallery/basic/2020-01-30-house-prices-in-king-county/index.html",
    "title": "House Prices in King County",
    "section": "",
    "text": "The use-case illustrated below touches on the following concepts:\nThe relevant sections in the mlr3book are linked to for the reader’s convenience.\nThis use case shows how to model housing price data in King County. Following features are illustrated:\nWe load the mlr3verse package which pulls in the most important packages for this example.\nlibrary(mlr3verse)\n\nLoading required package: mlr3\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/basic/2020-01-30-house-prices-in-king-county/index.html#use-case-regr-houses",
    "href": "gallery/basic/2020-01-30-house-prices-in-king-county/index.html#use-case-regr-houses",
    "title": "House Prices in King County",
    "section": "House Price Prediction in King County",
    "text": "House Price Prediction in King County\nWe use the kc_housing dataset contained in the package mlr3data in order to provide a use-case for the application of mlr3 on real-world data.\n\ndata(\"kc_housing\", package = \"mlr3data\")\n\n\nExploratory Data Analysis\nIn order to get a quick impression of our data, we perform some initial Exploratory Data Analysis. This helps us to get a first impression of our data and might help us arrive at additional features that can help with the prediction of the house prices.\nWe can get a quick overview using R’s summary function:\n\nsummary(kc_housing)\n\n      date                           price            bedrooms        bathrooms      sqft_living       sqft_lot      \n Min.   :2014-05-02 00:00:00.0   Min.   :  75000   Min.   : 0.000   Min.   :0.000   Min.   :  290   Min.   :    520  \n 1st Qu.:2014-07-22 00:00:00.0   1st Qu.: 321950   1st Qu.: 3.000   1st Qu.:1.750   1st Qu.: 1427   1st Qu.:   5040  \n Median :2014-10-16 00:00:00.0   Median : 450000   Median : 3.000   Median :2.250   Median : 1910   Median :   7618  \n Mean   :2014-10-29 03:58:09.9   Mean   : 540088   Mean   : 3.371   Mean   :2.115   Mean   : 2080   Mean   :  15107  \n 3rd Qu.:2015-02-17 00:00:00.0   3rd Qu.: 645000   3rd Qu.: 4.000   3rd Qu.:2.500   3rd Qu.: 2550   3rd Qu.:  10688  \n Max.   :2015-05-27 00:00:00.0   Max.   :7700000   Max.   :33.000   Max.   :8.000   Max.   :13540   Max.   :1651359  \n                                                                                                                     \n     floors      waterfront           view          condition         grade          sqft_above   sqft_basement   \n Min.   :1.000   Mode :logical   Min.   :0.0000   Min.   :1.000   Min.   : 1.000   Min.   : 290   Min.   :  10.0  \n 1st Qu.:1.000   FALSE:21450     1st Qu.:0.0000   1st Qu.:3.000   1st Qu.: 7.000   1st Qu.:1190   1st Qu.: 450.0  \n Median :1.500   TRUE :163       Median :0.0000   Median :3.000   Median : 7.000   Median :1560   Median : 700.0  \n Mean   :1.494                   Mean   :0.2343   Mean   :3.409   Mean   : 7.657   Mean   :1788   Mean   : 742.4  \n 3rd Qu.:2.000                   3rd Qu.:0.0000   3rd Qu.:4.000   3rd Qu.: 8.000   3rd Qu.:2210   3rd Qu.: 980.0  \n Max.   :3.500                   Max.   :4.0000   Max.   :5.000   Max.   :13.000   Max.   :9410   Max.   :4820.0  \n                                                                                                  NA's   :13126   \n    yr_built     yr_renovated      zipcode           lat             long        sqft_living15    sqft_lot15    \n Min.   :1900   Min.   :1934    Min.   :98001   Min.   :47.16   Min.   :-122.5   Min.   : 399   Min.   :   651  \n 1st Qu.:1951   1st Qu.:1987    1st Qu.:98033   1st Qu.:47.47   1st Qu.:-122.3   1st Qu.:1490   1st Qu.:  5100  \n Median :1975   Median :2000    Median :98065   Median :47.57   Median :-122.2   Median :1840   Median :  7620  \n Mean   :1971   Mean   :1996    Mean   :98078   Mean   :47.56   Mean   :-122.2   Mean   :1987   Mean   : 12768  \n 3rd Qu.:1997   3rd Qu.:2007    3rd Qu.:98118   3rd Qu.:47.68   3rd Qu.:-122.1   3rd Qu.:2360   3rd Qu.: 10083  \n Max.   :2015   Max.   :2015    Max.   :98199   Max.   :47.78   Max.   :-121.3   Max.   :6210   Max.   :871200  \n                NA's   :20699                                                                                   \n\ndim(kc_housing)\n\n[1] 21613    20\n\n\nOur dataset has 21613 observations and 20 columns. The variable we want to predict is price. In addition to the price column, we have several other columns:\n\nid: A unique identifier for every house.\ndate: A date column, indicating when the house was sold. This column is currently not encoded as a date and requires some preprocessing.\nzipcode: A column indicating the ZIP code. This is a categorical variable with many factor levels.\nlong, lat The longitude and latitude of the house\n... several other numeric columns providing information about the house, such as number of rooms, square feet etc.\n\nBefore we continue with the analysis, we preprocess some features so that they are stored in the correct format.\nFirst we convert the date column to numeric. To do so, we convert the date to the POSIXct date/time class with the anytime package. Next, use difftime() to convert to days since the first day recorded in the data set:\n\nlibrary(anytime)\ndates = anytime(kc_housing$date)\nkc_housing$date = as.numeric(difftime(dates, min(dates), units = \"days\"))\n\nAfterwards, we convert the zip code to a factor:\n\nkc_housing$zipcode = as.factor(kc_housing$zipcode)\n\nAnd add a new column renovated indicating whether a house was renovated at some point.\n\nkc_housing$renovated = as.numeric(!is.na(kc_housing$yr_renovated))\nkc_housing$has_basement = as.numeric(!is.na(kc_housing$sqft_basement))\n\nWe drop the id column which provides no information about the house prices:\n\nkc_housing$id = NULL\n\nAdditionally, we convert the price from Dollar to units of 1000 Dollar to improve readability.\n\nkc_housing$price = kc_housing$price / 1000\n\nAdditionally, for now we simply drop the columns that have missing values, as some of our learners can not deal with them. A better option to deal with missing values would be imputation, i.e. replacing missing values with valid ones. We will deal with this in a separate article.\n\nkc_housing$yr_renovated = NULL\nkc_housing$sqft_basement = NULL\n\nWe can now plot the density of the price to get a first impression on its distribution.\n\nlibrary(ggplot2)\nggplot(kc_housing, aes(x = price)) + geom_density()\n\n\n\n\n\n\n\n\nWe can see that the prices for most houses lie between 75.000 and 1.5 million dollars. There are few extreme values of up to 7.7 million dollars.\nFeature engineering often allows us to incorporate additional knowledge about the data and underlying processes. This can often greatly enhance predictive performance. A simple example: A house which has yr_renovated == 0 means that is has not been renovated yet. Additionally, we want to drop features which should not have any influence (id column).\nAfter those initial manipulations, we load all required packages and create a TaskRegr containing our data.\n\ntsk = as_task_regr(kc_housing, target = \"price\")\n\nWe can inspect associations between variables using mlr3viz’s autoplot function in order to get some good first impressions for our data. Note, that this does in no way prevent us from using other powerful plot functions of our choice on the original data.\n\nDistribution of the price:\nThe outcome we want to predict is the price variable. The autoplot function provides a good first glimpse on our data. As the resulting object is a ggplot2 object, we can use faceting and other functions from ggplot2 in order to enhance plots.\n\nautoplot(tsk) + facet_wrap(~renovated)\n\n\n\n\n\n\n\n\nWe can observe that renovated flats seem to achieve higher sales values, and this might thus be a relevant feature.\nAdditionally, we can for example look at the condition of the house. Again, we clearly can see that the price rises with increasing condition.\n\nautoplot(tsk) + facet_wrap(~condition)\n\n\n\n\n\n\n\n\n\n\nAssociation between variables\nIn addition to the association with the target variable, the association between the features can also lead to interesting insights. We investigate using variables associated with the quality and size of the house. Note that we use $clone() and $select() to clone the task and select only a subset of the features for the autoplot function, as autoplot per default uses all features. The task is cloned before we select features in order to keep the original task intact.\n\n# Variables associated with quality\nautoplot(tsk$clone()$select(tsk$feature_names[c(3, 17)]), type = \"pairs\")\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nautoplot(tsk$clone()$select(tsk$feature_names[c(9:12)]), type = \"pairs\")\n\n\n\n\n\n\n\n\n\n\n\nSplitting into train and test data\nIn mlr3, we do not create train and test data sets, but instead keep only a vector of train and test indices.\n\ntrain.idx = sample(seq_len(tsk$nrow), 0.7 * tsk$nrow)\ntest.idx = setdiff(seq_len(tsk$nrow), train.idx)\n\nWe can do the same for our task:\n\ntask_train = tsk$clone()$filter(train.idx)\ntask_test = tsk$clone()$filter(test.idx)\n\n\n\nA first model: Decision Tree\nDecision trees cannot only be used as a powerful tool for predictive models but also for exploratory data analysis. In order to fit a decision tree, we first get the regr.rpart learner from the mlr_learners dictionary by using the sugar function lrn.\nFor now, we leave out the zipcode variable, as we also have the latitude and longitude of each house. Again, we use $clone(), so we do not change the original task.\n\ntsk_nozip = task_train$clone()$select(setdiff(tsk$feature_names, \"zipcode\"))\n\n# Get the learner\nlrn = lrn(\"regr.rpart\")\n\n# And train on the task\nlrn$train(tsk_nozip, row_ids = train.idx)\n\n\nplot(lrn$model)\ntext(lrn$model)\n\n\n\n\n\n\n\n\nThe learned tree relies on several variables in order to distinguish between cheaper and pricier houses. The features we split along are grade, sqft_living, but also some features related to the area (longitude and latitude). We can visualize the price across different regions in order to get more info:\n\n# Load the ggmap package in order to visualize on a map\nlibrary(ggmap)\n\n# And create a quick plot for the price\nqmplot(long, lat, maptype = \"watercolor\", color = log(price),\n  data = kc_housing[train.idx[1:3000], ]) +\n  scale_colour_viridis_c()\n\n\n\n\n\n\n\n# And the zipcode\nqmplot(long, lat, maptype = \"watercolor\", color = zipcode,\n  data = kc_housing[train.idx[1:3000], ]) + guides(color = FALSE)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as of ggplot2 3.3.4.\n\n\n\n\n\n\n\n\n\nWe can see that the price is clearly associated with the zipcode when comparing then two plots. As a result, we might want to indeed use the zipcode column in our future endeavors.\n\n\nA first baseline: Decision Tree\nAfter getting an initial idea for our data, we might want to construct a first baseline, in order to see what a simple model already can achieve.\nWe use resample() with 3-fold cross-validation on our training data in order to get a reliable estimate of the algorithm’s performance on future data. Before we start with defining and training learners, we create a Resampling in order to make sure that we always compare on exactly the same data.\n\ncv3 = rsmp(\"cv\", folds = 3)\n\nFor the cross-validation we only use the training data by cloning the task and selecting only observations from the training set.\n\nlrn_rpart = lrn(\"regr.rpart\")\nres = resample(task = task_train, lrn_rpart, cv3)\nres$score(msr(\"regr.rmse\"))\n\n      task_id learner_id resampling_id iteration regr.rmse\n1: kc_housing regr.rpart            cv         1  205.7541\n2: kc_housing regr.rpart            cv         2  205.6597\n3: kc_housing regr.rpart            cv         3  213.3846\nHidden columns: task, learner, resampling, prediction\n\nsprintf(\"RMSE of the simple rpart: %s\", round(sqrt(res$aggregate()), 2))\n\n[1] \"RMSE of the simple rpart: 208.3\"\n\n\n\n\nMany Trees: Random Forest\nWe might be able to improve upon the RMSE using more powerful learners. We first load the mlr3learners package, which contains the ranger learner (a package which implements the “Random Forest” algorithm).\n\nlibrary(mlr3learners)\nlrn_ranger = lrn(\"regr.ranger\", num.trees = 15L)\nres = resample(task = task_train, lrn_ranger, cv3)\nres$score(msr(\"regr.rmse\"))\n\n      task_id  learner_id resampling_id iteration regr.rmse\n1: kc_housing regr.ranger            cv         1  142.2424\n2: kc_housing regr.ranger            cv         2  161.6867\n3: kc_housing regr.ranger            cv         3  138.2549\nHidden columns: task, learner, resampling, prediction\n\nsprintf(\"RMSE of the simple ranger: %s\", round(sqrt(res$aggregate()), 2))\n\n[1] \"RMSE of the simple ranger: 147.75\"\n\n\nOften tuning RandomForest methods does not increase predictive performances substantially. If time permits, it can nonetheless lead to improvements and should thus be performed. In this case, we resort to tune a different kind of model: Gradient Boosted Decision Trees from the package xgboost.\n\n\nA better baseline: AutoTuner\nTuning can often further improve the performance. In this case, we tune the xgboost learner in order to see whether this can improve performance. For the AutoTuner we have to specify a Termination Criterion (how long the tuning should run) a Tuner (which tuning method to use) and a ParamSet (which space we might want to search through). For now, we do not use the zipcode column, as xgboost cannot naturally deal with categorical features. The AutoTuner automatically performs nested cross-validation.\n\nlrn_xgb = lrn(\"regr.xgboost\")\n\n# Define the search space\nsearch_space = ps(\n  eta = p_dbl(lower = 0.2, upper = .4),\n  min_child_weight = p_dbl(lower = 1, upper = 20),\n  subsample = p_dbl(lower = .7, upper = .8),\n  colsample_bytree = p_dbl(lower = .9, upper = 1),\n  colsample_bylevel = p_dbl(lower = .5, upper = .7),\n  nrounds = p_int(lower = 1L, upper = 25))\n\nat = auto_tuner(\n  tuner = tnr(\"random_search\", batch_size = 40),\n  learner = lrn_xgb,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"regr.rmse\"),\n  search_space = search_space,\n  term_evals = 10)\n\n\n# And resample the AutoTuner\nres = resample(tsk_nozip, at, cv3, store_models = TRUE)\n\n\nres$score(msr(\"regr.rmse\"))\n\n      task_id         learner_id resampling_id iteration regr.rmse\n1: kc_housing regr.xgboost.tuned            cv         1  147.0554\n2: kc_housing regr.xgboost.tuned            cv         2  136.4282\n3: kc_housing regr.xgboost.tuned            cv         3  132.4484\nHidden columns: task, learner, resampling, prediction\n\nsprintf(\"RMSE of the tuned xgboost: %s\", round(sqrt(res$aggregate()), 2))\n\n[1] \"RMSE of the tuned xgboost: 138.78\"\n\n\nWe can obtain the resulting parameters in the respective splits by accessing the ResampleResult.\n\nsapply(res$learners, function(x) x$learner$param_set$values)[-2, ]\n\n                   [,1]      [,2]      [,3]     \nnrounds            25        23        24       \nverbose            0         0         0        \nearly_stopping_set \"none\"    \"none\"    \"none\"   \neta                0.220869  0.3809271 0.2115116\nmin_child_weight   1.885109  7.472919  1.910491 \nsubsample          0.7542127 0.7884631 0.7000357\ncolsample_bytree   0.9032271 0.9675298 0.9120812\ncolsample_bylevel  0.5259713 0.6817893 0.630818 \n\n\nNOTE: To keep runtime low, we only tune parts of the hyperparameter space of xgboost in this example. Additionally, we only allow for \\(10\\) random search iterations, which is usually too little for real-world applications. Nonetheless, we are able to obtain an improved performance when comparing to the ranger model.\nIn order to further improve our results we have several options:\n\nFind or engineer better features\nRemove Features to avoid overfitting\nObtain additional data (often prohibitive)\nTry more models\nImprove the tuning\n\nIncrease the tuning budget\nEnlarge the tuning search space\nUse a more efficient tuning algorithm\n\nStacking and Ensembles\n\nBelow we will investigate some of those possibilities and investigate whether this improves performance.\n\n\nAdvanced: Engineering Features: Mutating ZIP-Codes\nIn order to better cluster the zip codes, we compute a new feature: med_price: It computes the median price in each zip-code. This might help our model to improve the prediction. This is equivalent to impact encoding more information:\nWe can equip a learner with impact encoding using mlr3pipelines. More information on mlr3pipelines can be obtained from other posts.\n\nlrn_impact = po(\"encodeimpact\", affect_columns = selector_name(\"zipcode\")) %&gt;&gt;% lrn(\"regr.ranger\")\n\nAgain, we run resample() and compute the RMSE.\n\nres = resample(task = task_train, lrn_impact, cv3)\n\n\nres$score(msr(\"regr.rmse\"))\n\n      task_id               learner_id resampling_id iteration regr.rmse\n1: kc_housing encodeimpact.regr.ranger            cv         1  119.4597\n2: kc_housing encodeimpact.regr.ranger            cv         2  146.3315\n3: kc_housing encodeimpact.regr.ranger            cv         3  125.4193\nHidden columns: task, learner, resampling, prediction\n\nsprintf(\"RMSE of ranger with med_price: %s\", round(sqrt(res$aggregate()), 2))\n\n[1] \"RMSE of ranger with med_price: 130.91\"\n\n\n\n\nAdvanced: Obtaining a sparser model\nIn many cases, we might want to have a sparse model. For this purpose we can use a mlr3filters::Filter implemented in mlr3filters. This can prevent our learner from overfitting make it easier for humans to interpret models as fewer variables influence the resulting prediction.\nIn this example, we use PipeOpFilter (via po(\"filter\", ...)) to add a feature-filter before training the model. For a more in-depth insight, refer to the sections on mlr3pipelines and mlr3filters in the mlr3 book: Feature Selection and Pipelines.\n\nfilter = flt(\"mrmr\")\n\nThe resulting RMSE is slightly higher, and at the same time we only use \\(12\\) features.\n\ngraph = po(\"filter\", filter, param_vals = list(filter.nfeat = 12)) %&gt;&gt;% po(\"learner\", lrn(\"regr.ranger\"))\nlrn_filter = as_learner(graph)\nres = resample(task = task_train, lrn_filter, cv3)\n\n\nres$score(msr(\"regr.rmse\"))\n\n      task_id       learner_id resampling_id iteration regr.rmse\n1: kc_housing mrmr.regr.ranger            cv         1  152.6009\n2: kc_housing mrmr.regr.ranger            cv         2  156.7883\n3: kc_housing mrmr.regr.ranger            cv         3  149.0143\nHidden columns: task, learner, resampling, prediction\n\nsprintf(\"RMSE of ranger with filtering: %s\", round(sqrt(res$aggregate()), 2))\n\n[1] \"RMSE of ranger with filtering: 152.83\""
  },
  {
    "objectID": "gallery/basic/2020-01-30-house-prices-in-king-county/index.html#summary",
    "href": "gallery/basic/2020-01-30-house-prices-in-king-county/index.html#summary",
    "title": "House Prices in King County",
    "section": "Summary:",
    "text": "Summary:\nWe have seen different ways to improve models with respect to our criteria by:\n\nChoosing a suitable algorithm\nChoosing good hyperparameters (tuning)\nFiltering features\nEngineering new features\n\nA combination of all the above would most likely yield an even better model. This is left as an exercise to the reader.\nThe best model we found in this example is the ranger model with the added med_price feature. In a final step, we now want to assess the model’s quality on the held-out data we stored in our task_test. In order to do so, and to prevent data leakage, we can only add the median price from the training data.\n\nlibrary(data.table)\n\ndata = task_train$data(cols = c(\"price\", \"zipcode\"))\ndata[, med_price := median(price), by = \"zipcode\"]\ntest_data = task_test$data(cols = \"zipcode\")\ntest = merge(test_data, unique(data[, .(zipcode, med_price)]), all.x = TRUE)\ntask_test$cbind(test)\n\nNow we can use the augmented task_test to predict on new data.\n\nlrn_ranger$train(task_train)\npred = lrn_ranger$predict(task_test)\npred$score(msr(\"regr.rmse\"))\n\nregr.rmse \n   145.01"
  },
  {
    "objectID": "gallery/basic/2020-03-11-basics-german-credit/index.html",
    "href": "gallery/basic/2020-03-11-basics-german-credit/index.html",
    "title": "German Credit Series - Basics",
    "section": "",
    "text": "This is the first part in a serial of tutorials. The other parts of this series can be found here:\n\nPart II - Tuning\nPart III - Pipelines\n\nWe will walk through this tutorial interactively. The text is kept short to be followed in real time."
  },
  {
    "objectID": "gallery/basic/2020-03-11-basics-german-credit/index.html#intro",
    "href": "gallery/basic/2020-03-11-basics-german-credit/index.html#intro",
    "title": "German Credit Series - Basics",
    "section": "",
    "text": "This is the first part in a serial of tutorials. The other parts of this series can be found here:\n\nPart II - Tuning\nPart III - Pipelines\n\nWe will walk through this tutorial interactively. The text is kept short to be followed in real time."
  },
  {
    "objectID": "gallery/basic/2020-03-11-basics-german-credit/index.html#prerequisites",
    "href": "gallery/basic/2020-03-11-basics-german-credit/index.html#prerequisites",
    "title": "German Credit Series - Basics",
    "section": "Prerequisites",
    "text": "Prerequisites\nEnsure all packages used in this tutorial are installed. This includes the mlr3verse package, as well as other packages for data handling, cleaning and visualization which we are going to use (data.table, ggplot2, rchallenge, and skimr).\nThen, load the main packages we are going to use:\n\nlibrary(\"mlr3verse\")\nlibrary(\"mlr3learners\")\nlibrary(\"mlr3tuning\")\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/basic/2020-03-11-basics-german-credit/index.html#machine-learning-use-case-german-credit-data",
    "href": "gallery/basic/2020-03-11-basics-german-credit/index.html#machine-learning-use-case-german-credit-data",
    "title": "German Credit Series - Basics",
    "section": "Machine Learning Use Case: German Credit Data",
    "text": "Machine Learning Use Case: German Credit Data\nThe German credit data was originally donated in 1994 by Prof. Dr. Hans Hoffman of the University of Hamburg. A description can be found at the UCI repository. The goal is to classify people by their credit risk (good or bad) using 20 personal, demographic and financial features:\n\n\n\n\n\n\n\nFeature Name\nDescription\n\n\n\n\nage\nage in years\n\n\namount\namount asked by applicant\n\n\ncredit_history\npast credit history of applicant at this bank\n\n\nduration\nduration of the credit in months\n\n\nemployment_duration\npresent employment since\n\n\nforeign_worker\nis applicant foreign worker?\n\n\nhousing\ntype of apartment rented, owned, for free / no payment\n\n\ninstallment_rate\ninstallment rate in percentage of disposable income\n\n\njob\ncurrent job information\n\n\nnumber_credits\nnumber of existing credits at this bank\n\n\nother_debtors\nother debtors/guarantors present?\n\n\nother_installment_plans\nother installment plans the applicant is paying\n\n\npeople_liable\nnumber of people being liable to provide maintenance\n\n\npersonal_status_sex\ncombination of sex and personal status of applicant\n\n\npresent_residence\npresent residence since\n\n\nproperty\nproperties that applicant has\n\n\npurpose\nreason customer is applying for a loan\n\n\nsavings\nsavings accounts/bonds at this bank\n\n\nstatus\nstatus/balance of checking account at this bank\n\n\ntelephone\nis there any telephone registered for this customer?\n\n\n\n\nImporting the Data\nThe dataset we are going to use is a transformed version of this German credit dataset, as provided by the rchallenge package (this transformed dataset was proposed by Ulrike Grömping, with factors instead of dummy variables and corrected features):\n\ndata(\"german\", package = \"rchallenge\")\n\nFirst, we’ll do a thorough investigation of the dataset.\n\n\nExploring the Data\nWe can get a quick overview of our dataset using R’s summary function:\n\ndim(german)\n\n[1] 1000   21\n\nstr(german)\n\n'data.frame':   1000 obs. of  21 variables:\n $ status                 : Factor w/ 4 levels \"no checking account\",..: 1 1 2 1 1 1 1 1 4 2 ...\n $ duration               : int  18 9 12 12 12 10 8 6 18 24 ...\n $ credit_history         : Factor w/ 5 levels \"delay in paying off in the past\",..: 5 5 3 5 5 5 5 5 5 3 ...\n $ purpose                : Factor w/ 11 levels \"others\",\"car (new)\",..: 3 1 10 1 1 1 1 1 4 4 ...\n $ amount                 : int  1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ...\n $ savings                : Factor w/ 5 levels \"unknown/no savings account\",..: 1 1 2 1 1 1 1 1 1 3 ...\n $ employment_duration    : Factor w/ 5 levels \"unemployed\",\"&lt; 1 yr\",..: 2 3 4 3 3 2 4 2 1 1 ...\n $ installment_rate       : Ord.factor w/ 4 levels \"&gt;= 35\"&lt;\"25 &lt;= ... &lt; 35\"&lt;..: 4 2 2 3 4 1 1 2 4 1 ...\n $ personal_status_sex    : Factor w/ 4 levels \"male : divorced/separated\",..: 2 3 2 3 3 3 3 3 2 2 ...\n $ other_debtors          : Factor w/ 3 levels \"none\",\"co-applicant\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ present_residence      : Ord.factor w/ 4 levels \"&lt; 1 yr\"&lt;\"1 &lt;= ... &lt; 4 yrs\"&lt;..: 4 2 4 2 4 3 4 4 4 4 ...\n $ property               : Factor w/ 4 levels \"unknown / no property\",..: 2 1 1 1 2 1 1 1 3 4 ...\n $ age                    : int  21 36 23 39 38 48 39 40 65 23 ...\n $ other_installment_plans: Factor w/ 3 levels \"bank\",\"stores\",..: 3 3 3 3 1 3 3 3 3 3 ...\n $ housing                : Factor w/ 3 levels \"for free\",\"rent\",..: 1 1 1 1 2 1 2 2 2 1 ...\n $ number_credits         : Ord.factor w/ 4 levels \"1\"&lt;\"2-3\"&lt;\"4-5\"&lt;..: 1 2 1 2 2 2 2 1 2 1 ...\n $ job                    : Factor w/ 4 levels \"unemployed/unskilled - non-resident\",..: 3 3 2 2 2 2 2 2 1 1 ...\n $ people_liable          : Factor w/ 2 levels \"3 or more\",\"0 to 2\": 2 1 2 1 2 1 2 1 2 2 ...\n $ telephone              : Factor w/ 2 levels \"no\",\"yes (under customer name)\": 1 1 1 1 1 1 1 1 1 1 ...\n $ foreign_worker         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 1 1 2 2 ...\n $ credit_risk            : Factor w/ 2 levels \"bad\",\"good\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nOur dataset has 1000 observations and 21 columns. The variable we want to predict is credit_risk (either good or bad), i.e., we aim to classify people by their credit risk.\nWe also recommend the skimr package as it creates very well readable and understandable overviews:\n\nskimr::skim(german)\n\n\nData summary\n\n\nName\ngerman\n\n\nNumber of rows\n1000\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n18\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nstatus\n0\n1\nFALSE\n4\n…: 394, no : 274, …: 269, 0&lt;=: 63\n\n\ncredit_history\n0\n1\nFALSE\n5\nno : 530, all: 293, exi: 88, cri: 49\n\n\npurpose\n0\n1\nFALSE\n10\nfur: 280, oth: 234, car: 181, car: 103\n\n\nsavings\n0\n1\nFALSE\n5\nunk: 603, …: 183, …: 103, 100: 63\n\n\nemployment_duration\n0\n1\nFALSE\n5\n1 &lt;: 339, &gt;= : 253, 4 &lt;: 174, &lt; 1: 172\n\n\ninstallment_rate\n0\n1\nTRUE\n4\n&lt; 2: 476, 25 : 231, 20 : 157, &gt;= : 136\n\n\npersonal_status_sex\n0\n1\nFALSE\n4\nmal: 548, fem: 310, fem: 92, mal: 50\n\n\nother_debtors\n0\n1\nFALSE\n3\nnon: 907, gua: 52, co-: 41\n\n\npresent_residence\n0\n1\nTRUE\n4\n&gt;= : 413, 1 &lt;: 308, 4 &lt;: 149, &lt; 1: 130\n\n\nproperty\n0\n1\nFALSE\n4\nbui: 332, unk: 282, car: 232, rea: 154\n\n\nother_installment_plans\n0\n1\nFALSE\n3\nnon: 814, ban: 139, sto: 47\n\n\nhousing\n0\n1\nFALSE\n3\nren: 714, for: 179, own: 107\n\n\nnumber_credits\n0\n1\nTRUE\n4\n1: 633, 2-3: 333, 4-5: 28, &gt;= : 6\n\n\njob\n0\n1\nFALSE\n4\nski: 630, uns: 200, man: 148, une: 22\n\n\npeople_liable\n0\n1\nFALSE\n2\n0 t: 845, 3 o: 155\n\n\ntelephone\n0\n1\nFALSE\n2\nno: 596, yes: 404\n\n\nforeign_worker\n0\n1\nFALSE\n2\nno: 963, yes: 37\n\n\ncredit_risk\n0\n1\nFALSE\n2\ngoo: 700, bad: 300\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nduration\n0\n1\n20.90\n12.06\n4\n12.0\n18.0\n24.00\n72\n▇▇▂▁▁\n\n\namount\n0\n1\n3271.25\n2822.75\n250\n1365.5\n2319.5\n3972.25\n18424\n▇▂▁▁▁\n\n\nage\n0\n1\n35.54\n11.35\n19\n27.0\n33.0\n42.00\n75\n▇▆▃▁▁\n\n\n\n\n\nDuring an exploratory analysis meaningful discoveries could be:\n\nSkewed distributions\nMissing values\nEmpty / rare factor variables\n\nAn explanatory analysis is crucial to get a feeling for your data. On the other hand the data can be validated this way. Non-plausible data can be investigated or outliers can be removed.\nAfter feeling confident with the data, we want to do modeling now."
  },
  {
    "objectID": "gallery/basic/2020-03-11-basics-german-credit/index.html#modeling",
    "href": "gallery/basic/2020-03-11-basics-german-credit/index.html#modeling",
    "title": "German Credit Series - Basics",
    "section": "Modeling",
    "text": "Modeling\nConsidering how we are going to tackle the problem of classifying the credit risk relates closely to what mlr3 entities we will use.\nThe typical questions that arise when building a machine learning workflow are:\n\nWhat is the problem we are trying to solve?\nWhat are appropriate learning algorithms?\nHow do we evaluate “good” performance?\n\nMore systematically in mlr3 they can be expressed via five components:\n\nThe Task definition.\nThe Learner definition.\nThe training.\nThe prediction.\nThe evaluation via one or multiple Measures.\n\n\nTask Definition\nFirst, we are interested in the target which we want to model. Most supervised machine learning problems are regression or classification problems. However, note that other problems include unsupervised learning or time-to-event data (covered in mlr3proba).\nWithin mlr3, to distinguish between these problems, we define Tasks. If we want to solve a classification problem, we define a classification task – TaskClassif. For a regression problem, we define a regression task – TaskRegr.\nIn our case it is clearly our objective to model or predict the binary factor variable credit_risk. Thus, we define a TaskClassif:\n\ntask = as_task_classif(german, id = \"GermanCredit\", target = \"credit_risk\")\n\nNote that the German credit data is also given as an example task which ships with the mlr3 package. Thus, you actually don’t need to construct it yourself, just call tsk(\"german_credit\") to retrieve the object from the dictionary mlr_tasks.\n\n\nLearner Definition\nAfter having decided what should be modeled, we need to decide on how. This means we need to decide which learning algorithms, or Learners are appropriate. Using prior knowledge (e.g. knowing that it is a classification task or assuming that the classes are linearly separable) one ends up with one or more suitable Learners.\nMany learners can be obtained via the mlr3learners package. Additionally, many learners are provided via the mlr3extralearners package, from GitHub. These two resources combined account for a large fraction of standard learning algorithms. As mlr3 usually only wraps learners from packages, it is even easy to create a formal Learner by yourself. You may find the section about extending mlr3 in the mlr3book very helpful. If you happen to write your own Learner in mlr3, we would be happy if you share it with the mlr3 community.\nAll available Learners (i.e. all which you have installed from mlr3, mlr3learners, mlr3extralearners, or self-written ones) are registered in the dictionary mlr_learners:\n\nmlr_learners\n\n&lt;DictionaryLearner&gt; with 134 stored values\nKeys: classif.AdaBoostM1, classif.bart, classif.C50, classif.catboost, classif.cforest, classif.ctree,\n  classif.cv_glmnet, classif.debug, classif.earth, classif.featureless, classif.fnn, classif.gam,\n  classif.gamboost, classif.gausspr, classif.gbm, classif.glmboost, classif.glmer, classif.glmnet,\n  classif.IBk, classif.J48, classif.JRip, classif.kknn, classif.ksvm, classif.lda, classif.liblinear,\n  classif.lightgbm, classif.LMT, classif.log_reg, classif.lssvm, classif.mob, classif.multinom,\n  classif.naive_bayes, classif.nnet, classif.OneR, classif.PART, classif.qda, classif.randomForest,\n  classif.ranger, classif.rfsrc, classif.rpart, classif.svm, classif.xgboost, clust.agnes, clust.ap,\n  clust.cmeans, clust.cobweb, clust.dbscan, clust.diana, clust.em, clust.fanny, clust.featureless,\n  clust.ff, clust.hclust, clust.kkmeans, clust.kmeans, clust.MBatchKMeans, clust.mclust, clust.meanshift,\n  clust.pam, clust.SimpleKMeans, clust.xmeans, dens.kde_ks, dens.locfit, dens.logspline, dens.mixed,\n  dens.nonpar, dens.pen, dens.plug, dens.spline, regr.bart, regr.catboost, regr.cforest, regr.ctree,\n  regr.cubist, regr.cv_glmnet, regr.debug, regr.earth, regr.featureless, regr.fnn, regr.gam, regr.gamboost,\n  regr.gausspr, regr.gbm, regr.glm, regr.glmboost, regr.glmnet, regr.IBk, regr.kknn, regr.km, regr.ksvm,\n  regr.liblinear, regr.lightgbm, regr.lm, regr.lmer, regr.M5Rules, regr.mars, regr.mob, regr.nnet,\n  regr.randomForest, regr.ranger, regr.rfsrc, regr.rpart, regr.rsm, regr.rvm, regr.svm, regr.xgboost,\n  surv.akritas, surv.aorsf, surv.blackboost, surv.cforest, surv.coxboost, surv.coxtime, surv.ctree,\n  surv.cv_coxboost, surv.cv_glmnet, surv.deephit, surv.deepsurv, surv.dnnsurv, surv.flexible,\n  surv.gamboost, surv.gbm, surv.glmboost, surv.glmnet, surv.loghaz, surv.mboost, surv.nelson,\n  surv.obliqueRSF, surv.parametric, surv.pchazard, surv.penalized, surv.ranger, surv.rfsrc, surv.svm,\n  surv.xgboost\n\n\nFor our problem, a suitable learner could be one of the following: Logistic regression, CART, random forest (or many more).\nA learner can be initialized with the lrn() function and the name of the learner, e.g., lrn(\"classif.xxx\"). Use ?mlr_learners_xxx to open the help page of a learner named xxx.\nFor example, a logistic regression can be initialized in the following manner (logistic regression uses R’s glm() function and is provided by the mlr3learners package):\n\nlibrary(\"mlr3learners\")\nlearner_logreg = lrn(\"classif.log_reg\")\nprint(learner_logreg)\n\n&lt;LearnerClassifLogReg:classif.log_reg&gt;\n* Model: -\n* Parameters: list()\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: loglik, twoclass\n\n\n\n\nTraining\nTraining is the procedure, where a model is fitted on the (training) data.\n\nLogistic Regression\nWe start with the example of the logistic regression. However, you will immediately see that the procedure generalizes to any learner very easily.\nAn initialized learner can be trained on data using $train():\n\nlearner_logreg$train(task)\n\nTypically, in machine learning, one does not use the full data which is available but a subset, the so-called training data.\nTo efficiently perform a split of the data one could do the following:\n\ntrain_set = sample(task$row_ids, 0.8 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n\n80 percent of the data is used for training. The remaining 20 percent are used for evaluation at a subsequent later point in time. train_set is an integer vector referring to the selected rows of the original dataset:\n\nhead(train_set)\n\n[1] 836 679 129 930 509 471\n\n\nIn mlr3 the training with a subset of the data can be declared by the additional argument row_ids = train_set:\n\nlearner_logreg$train(task, row_ids = train_set)\n\nThe fitted model can be accessed via:\n\nlearner_logreg$model\n\n\nCall:  stats::glm(formula = task$formula(), family = \"binomial\", data = data, \n    model = FALSE)\n\nCoefficients:\n                                              (Intercept)                                                        age  \n                                                0.0688846                                                 -0.0159818  \n                                                   amount     credit_historycritical account/other credits elsewhere  \n                                                0.0001329                                                  0.4580373  \ncredit_historyno credits taken/all credits paid back duly     credit_historyexisting credits paid back duly till now  \n                                               -0.5087518                                                 -1.0249187  \n    credit_historyall credits at this bank paid back duly                                                   duration  \n                                               -1.5582920                                                  0.0360543  \n                                employment_duration&lt; 1 yr                        employment_duration1 &lt;= ... &lt; 4 yrs  \n                                                0.0720620                                                 -0.1262893  \n                      employment_duration4 &lt;= ... &lt; 7 yrs                                employment_duration&gt;= 7 yrs  \n                                               -0.5589136                                                 -0.1545512  \n                                         foreign_workerno                                                housingrent  \n                                                1.3647905                                                 -0.6199554  \n                                               housingown                                         installment_rate.L  \n                                               -0.6851413                                                  0.8838467  \n                                       installment_rate.Q                                         installment_rate.C  \n                                               -0.0385000                                                 -0.1072466  \n                                  jobunskilled - resident                               jobskilled employee/official  \n                                                0.9797910                                                  0.7657897  \n            jobmanager/self-empl./highly qualif. employee                                           number_credits.L  \n                                                0.5793418                                                  0.4081443  \n                                         number_credits.Q                                           number_credits.C  \n                                               -0.3386148                                                 -0.0262034  \n                                other_debtorsco-applicant                                     other_debtorsguarantor  \n                                                0.4995457                                                 -0.4840799  \n                            other_installment_plansstores                                other_installment_plansnone  \n                                                0.1844457                                                 -0.2035917  \n                                      people_liable0 to 2    personal_status_sexfemale : non-single or male : single  \n                                               -0.2306212                                                 -0.2797497  \n                personal_status_sexmale : married/widowed                         personal_status_sexfemale : single  \n                                               -0.7947385                                                 -0.4173530  \n                                      present_residence.L                                        present_residence.Q  \n                                                0.0948244                                                 -0.4654771  \n                                      present_residence.C                                       propertycar or other  \n                                                0.2317025                                                  0.2583020  \n        propertybuilding soc. savings agr./life insurance                                        propertyreal estate  \n                                                0.2595201                                                  0.6821534  \n                                         purposecar (new)                                          purposecar (used)  \n                                               -1.5120761                                                 -0.6224479  \n                               purposefurniture/equipment                                    purposeradio/television  \n                                               -0.7776132                                                 -0.1649750  \n                               purposedomestic appliances                                             purposerepairs  \n                                                0.1966830                                                  0.3824875  \n                                          purposevacation                                          purposeretraining  \n                                               -1.9184037                                                 -0.9364954  \n                                          purposebusiness                                       savings... &lt;  100 DM  \n                                               -1.2647440                                                 -0.2148135  \n                              savings100 &lt;= ... &lt;  500 DM                                savings500 &lt;= ... &lt; 1000 DM  \n                                               -0.5445696                                                 -1.4241969  \n                                    savings... &gt;= 1000 DM                                           status... &lt; 0 DM  \n                                               -1.0478097                                                 -0.4584854  \n                                   status0&lt;= ... &lt; 200 DM           status... &gt;= 200 DM / salary for at least 1 year  \n                                               -0.8798435                                                 -1.7552018  \n                       telephoneyes (under customer name)  \n                                               -0.1798432  \n\nDegrees of Freedom: 799 Total (i.e. Null);  745 Residual\nNull Deviance:      980.7 \nResidual Deviance: 702.6    AIC: 812.6\n\n\nThe stored object is a normal glm object and all its S3 methods work as expected:\n\nclass(learner_logreg$model)\n\n[1] \"glm\" \"lm\" \n\nsummary(learner_logreg$model)\n\n\nCall:\nstats::glm(formula = task$formula(), family = \"binomial\", data = data, \n    model = FALSE)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3992  -0.6812  -0.3642   0.6939   2.7540  \n\nCoefficients:\n                                                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                                                0.0688846  1.3333299   0.052 0.958797    \nage                                                       -0.0159818  0.0105444  -1.516 0.129605    \namount                                                     0.0001329  0.0000502   2.647 0.008110 ** \ncredit_historycritical account/other credits elsewhere     0.4580373  0.6523911   0.702 0.482623    \ncredit_historyno credits taken/all credits paid back duly -0.5087518  0.4802925  -1.059 0.289484    \ncredit_historyexisting credits paid back duly till now    -1.0249188  0.5343038  -1.918 0.055082 .  \ncredit_historyall credits at this bank paid back duly     -1.5582920  0.4858084  -3.208 0.001338 ** \nduration                                                   0.0360543  0.0106583   3.383 0.000718 ***\nemployment_duration&lt; 1 yr                                  0.0720620  0.4845297   0.149 0.881770    \nemployment_duration1 &lt;= ... &lt; 4 yrs                       -0.1262893  0.4577516  -0.276 0.782632    \nemployment_duration4 &lt;= ... &lt; 7 yrs                       -0.5589136  0.5016876  -1.114 0.265250    \nemployment_duration&gt;= 7 yrs                               -0.1545512  0.4625121  -0.334 0.738262    \nforeign_workerno                                           1.3647905  0.6425196   2.124 0.033660 *  \nhousingrent                                               -0.6199554  0.2705326  -2.292 0.021928 *  \nhousingown                                                -0.6851413  0.5581021  -1.228 0.219587    \ninstallment_rate.L                                         0.8838467  0.2472850   3.574 0.000351 ***\ninstallment_rate.Q                                        -0.0385001  0.2217440  -0.174 0.862161    \ninstallment_rate.C                                        -0.1072466  0.2281857  -0.470 0.638357    \njobunskilled - resident                                    0.9797910  0.7877153   1.244 0.213559    \njobskilled employee/official                               0.7657897  0.7627061   1.004 0.315358    \njobmanager/self-empl./highly qualif. employee              0.5793418  0.7790492   0.744 0.457087    \nnumber_credits.L                                           0.4081443  0.7356488   0.555 0.579026    \nnumber_credits.Q                                          -0.3386148  0.6189845  -0.547 0.584345    \nnumber_credits.C                                          -0.0262034  0.4717322  -0.056 0.955703    \nother_debtorsco-applicant                                  0.4995457  0.4504675   1.109 0.267452    \nother_debtorsguarantor                                    -0.4840799  0.4744964  -1.020 0.307635    \nother_installment_plansstores                              0.1844457  0.4733765   0.390 0.696804    \nother_installment_plansnone                               -0.2035917  0.2881916  -0.706 0.479911    \npeople_liable0 to 2                                       -0.2306212  0.2905750  -0.794 0.427386    \npersonal_status_sexfemale : non-single or male : single   -0.2797497  0.4602069  -0.608 0.543268    \npersonal_status_sexmale : married/widowed                 -0.7947384  0.4513890  -1.761 0.078297 .  \npersonal_status_sexfemale : single                        -0.4173530  0.5348044  -0.780 0.435165    \npresent_residence.L                                        0.0948244  0.2496873   0.380 0.704114    \npresent_residence.Q                                       -0.4654771  0.2293239  -2.030 0.042379 *  \npresent_residence.C                                        0.2317025  0.2240902   1.034 0.301150    \npropertycar or other                                       0.2583020  0.2934956   0.880 0.378812    \npropertybuilding soc. savings agr./life insurance          0.2595201  0.2712751   0.957 0.338735    \npropertyreal estate                                        0.6821535  0.4931574   1.383 0.166592    \npurposecar (new)                                          -1.5120761  0.4173677  -3.623 0.000291 ***\npurposecar (used)                                         -0.6224479  0.2979802  -2.089 0.036718 *  \npurposefurniture/equipment                                -0.7776132  0.2803938  -2.773 0.005549 ** \npurposeradio/television                                   -0.1649750  0.8289212  -0.199 0.842244    \npurposedomestic appliances                                 0.1966830  0.6322897   0.311 0.755751    \npurposerepairs                                             0.3824875  0.4657037   0.821 0.411469    \npurposevacation                                           -1.9184037  1.1948196  -1.606 0.108362    \npurposeretraining                                         -0.9364954  0.4014654  -2.333 0.019664 *  \npurposebusiness                                           -1.2647440  0.8064326  -1.568 0.116807    \nsavings... &lt;  100 DM                                      -0.2148135  0.3235401  -0.664 0.506724    \nsavings100 &lt;= ... &lt;  500 DM                               -0.5445696  0.5037257  -1.081 0.279660    \nsavings500 &lt;= ... &lt; 1000 DM                               -1.4241969  0.6223390  -2.288 0.022111 *  \nsavings... &gt;= 1000 DM                                     -1.0478097  0.3067055  -3.416 0.000635 ***\nstatus... &lt; 0 DM                                          -0.4584854  0.2481344  -1.848 0.064641 .  \nstatus0&lt;= ... &lt; 200 DM                                    -0.8798435  0.4219102  -2.085 0.037035 *  \nstatus... &gt;= 200 DM / salary for at least 1 year          -1.7552018  0.2629099  -6.676 2.45e-11 ***\ntelephoneyes (under customer name)                        -0.1798432  0.2322797  -0.774 0.438781    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 980.75  on 799  degrees of freedom\nResidual deviance: 702.58  on 745  degrees of freedom\nAIC: 812.58\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nRandom Forest\nJust like the logistic regression, we could train a random forest instead. We use the fast implementation from the ranger package. For this, we first need to define the learner and then actually train it.\nWe now additionally supply the importance argument (importance = \"permutation\"). Doing so, we override the default and let the learner do feature importance determination based on permutation feature importance:\n\nlearner_rf = lrn(\"classif.ranger\", importance = \"permutation\")\nlearner_rf$train(task, row_ids = train_set)\n\nWe can access the importance values using $importance():\n\nlearner_rf$importance()\n\n                 status                duration                  amount          credit_history                 savings \n           0.0352305897            0.0220794319            0.0126390139            0.0118909925            0.0097629159 \n                    age                property     employment_duration        installment_rate           other_debtors \n           0.0089588614            0.0065744135            0.0054059438            0.0039772325            0.0029554567 \n                purpose                     job                 housing     personal_status_sex       present_residence \n           0.0029153698            0.0019001816            0.0017784386            0.0016809300            0.0012840096 \n              telephone          number_credits other_installment_plans           people_liable          foreign_worker \n           0.0010803276            0.0009842075            0.0004556746            0.0003807613            0.0003657158 \n\n\nIn order to obtain a plot for the importance values, we convert the importance to a data.table and then process it with ggplot2:\n\nimportance = as.data.table(learner_rf$importance(), keep.rownames = TRUE)\ncolnames(importance) = c(\"Feature\", \"Importance\")\nggplot(importance, aes(x = reorder(Feature, Importance), y = Importance)) +\n  geom_col() + coord_flip() + xlab(\"\")\n\n\n\n\n\n\n\n\n\n\n\nPrediction\nLet’s see what the models predict.\nAfter training a model, the model can be used for prediction. Usually, prediction is the main purpose of machine learning models.\nIn our case, the model can be used to classify new credit applicants w.r.t. their associated credit risk (good vs. bad) on the basis of the features. Typically, machine learning models predict numeric values. In the regression case this is very natural. For classification, most models predict scores or probabilities. Based on these values, one can derive class predictions.\n\nPredict Classes\nFirst, we directly predict classes:\n\nprediction_logreg = learner_logreg$predict(task, row_ids = test_set)\nprediction_rf = learner_rf$predict(task, row_ids = test_set)\n\n\nprediction_logreg\n\n&lt;PredictionClassif&gt; for 200 observations:\n    row_ids truth response\n         18  good     good\n         23   bad      bad\n         26  good     good\n---                       \n        996   bad     good\n        997   bad      bad\n       1000   bad     good\n\n\n\nprediction_rf\n\n&lt;PredictionClassif&gt; for 200 observations:\n    row_ids truth response\n         18  good     good\n         23   bad      bad\n         26  good     good\n---                       \n        996   bad      bad\n        997   bad     good\n       1000   bad     good\n\n\nThe $predict() method returns a Prediction object. It can be converted to a data.table if one wants to use it downstream.\nWe can also display the prediction results aggregated in a confusion matrix:\n\nprediction_logreg$confusion\n\n        truth\nresponse bad good\n    bad   28   17\n    good  30  125\n\nprediction_rf$confusion\n\n        truth\nresponse bad good\n    bad   24   13\n    good  34  129\n\n\n\n\nPredict Probabilities\nMost learners may not only predict a class variable (“response”), but also their degree of “belief” / “uncertainty” in a given response. Typically, we achieve this by setting the $predict_type slot of a Learner to \"prob\". Sometimes this needs to be done before the learner is trained. Alternatively, we can directly create the learner with this option: lrn(\"classif.log_reg\", predict_type = \"prob\").\n\nlearner_logreg$predict_type = \"prob\"\n\n\nlearner_logreg$predict(task, row_ids = test_set)\n\n&lt;PredictionClassif&gt; for 200 observations:\n    row_ids truth response   prob.bad prob.good\n         18  good     good 0.24602549 0.7539745\n         23   bad      bad 0.88286383 0.1171362\n         26  good     good 0.05730414 0.9426959\n---                                            \n        996   bad     good 0.49783778 0.5021622\n        997   bad      bad 0.59947093 0.4005291\n       1000   bad     good 0.42568263 0.5743174\n\n\nNote that sometimes one needs to be cautious when dealing with the probability interpretation of the predictions.\n\n\n\nPerformance Evaluation\nTo measure the performance of a learner on new unseen data, we usually mimic the scenario of unseen data by splitting up the data into training and test set. The training set is used for training the learner, and the test set is only used for predicting and evaluating the performance of the trained learner. Numerous resampling methods (cross-validation, bootstrap) repeat the splitting process in different ways.\nWithin mlr3, we need to specify the resampling strategy using the rsmp() function:\n\nresampling = rsmp(\"holdout\", ratio = 2/3)\nprint(resampling)\n\n&lt;ResamplingHoldout&gt;: Holdout\n* Iterations: 1\n* Instantiated: FALSE\n* Parameters: ratio=0.6667\n\n\nHere, we use “holdout”, a simple train-test split (with just one iteration). We use the resample() function to undertake the resampling calculation:\n\nres = resample(task, learner = learner_logreg, resampling = resampling)\nres\n\n&lt;ResampleResult&gt; of 1 iterations\n* Task: GermanCredit\n* Learner: classif.log_reg\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n\n\nThe default score of the measure is included in the $aggregate() slot:\n\nres$aggregate()\n\nclassif.ce \n 0.2432432 \n\n\nThe default measure in this scenario is the classification error. Lower is better.\nWe can easily run different resampling strategies, e.g. repeated holdout (\"subsampling\"), or cross validation. Most methods perform repeated train/predict cycles on different data subsets and aggregate the result (usually as the mean). Doing this manually would require us to write loops. mlr3 does the job for us:\n\nresampling = rsmp(\"subsampling\", repeats = 10)\nrr = resample(task, learner = learner_logreg, resampling = resampling)\nrr$aggregate()\n\nclassif.ce \n 0.2474474 \n\n\nInstead, we could also run cross-validation:\n\nresampling = resampling = rsmp(\"cv\", folds = 10)\nrr = resample(task, learner = learner_logreg, resampling = resampling)\nrr$aggregate()\n\nclassif.ce \n     0.257 \n\n\nmlr3 features scores for many more measures. Here, we apply mlr_measures_classif.fpr for the false positive rate, and mlr_measures_classif.fnr for the false negative rate. Multiple measures can be provided as a list of measures (which can directly be constructed via msrs():\n\n# false positive rate\nrr$aggregate(msr(\"classif.fpr\"))\n\nclassif.fpr \n  0.1472895 \n\n# false positive rate and false negative\nmeasures = msrs(c(\"classif.fpr\", \"classif.fnr\"))\nrr$aggregate(measures)\n\nclassif.fpr classif.fnr \n  0.1472895   0.5197922 \n\n\nThere are a few more resampling methods, and quite a few more measures (implemented in mlr3measures). They are automatically registered in the respective dictionaries:\n\nmlr_resamplings\n\n&lt;DictionaryResampling&gt; with 9 stored values\nKeys: bootstrap, custom, custom_cv, cv, holdout, insample, loo, repeated_cv, subsampling\n\nmlr_measures\n\n&lt;DictionaryMeasure&gt; with 67 stored values\nKeys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier, classif.ce, classif.costs,\n  classif.dor, classif.fbeta, classif.fdr, classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,\n  classif.logloss, classif.mauc_au1p, classif.mauc_au1u, classif.mauc_aunp, classif.mauc_aunu,\n  classif.mbrier, classif.mcc, classif.npv, classif.ppv, classif.prauc, classif.precision, classif.recall,\n  classif.sensitivity, classif.specificity, classif.tn, classif.tnr, classif.tp, classif.tpr, clust.ch,\n  clust.db, clust.dunn, clust.silhouette, clust.wss, debug_classif, oob_error, regr.bias, regr.ktau,\n  regr.mae, regr.mape, regr.maxae, regr.medae, regr.medse, regr.mse, regr.msle, regr.pbias, regr.rae,\n  regr.rmse, regr.rmsle, regr.rrse, regr.rse, regr.rsq, regr.sae, regr.smape, regr.srho, regr.sse,\n  selected_features, sim.jaccard, sim.phi, time_both, time_predict, time_train\n\n\nTo get help on a resampling method, use ?mlr_resamplings_xxx, for a measure do ?mlr_measures_xxx. You can also browse the mlr3 reference online.\nNote that some measures, for example AUC, require the prediction of probabilities.\n\n\nPerformance Comparison and Benchmarks\nWe could compare Learners by evaluating resample() for each of them manually. However, benchmark() automatically performs resampling evaluations for multiple learners and tasks. benchmark_grid() creates fully crossed designs: Multiple Learners for multiple Tasks are compared w.r.t. multiple Resamplings.\n\nlearners = lrns(c(\"classif.log_reg\", \"classif.ranger\"), predict_type = \"prob\")\ngrid = benchmark_grid(\n  tasks = task,\n  learners = learners,\n  resamplings = rsmp(\"cv\", folds = 10)\n)\nbmr = benchmark(grid)\n\nCareful, large benchmarks may take a long time! This one should take less than a minute, however. In general, we want to use parallelization to speed things up on multi-core machines. For parallelization, mlr3 relies on the future package:\n\n# future::plan(\"multicore\") # uncomment for parallelization\n\nIn the benchmark we can compare different measures. Here, we look at the misclassification rate and the AUC:\n\nmeasures = msrs(c(\"classif.ce\", \"classif.auc\"))\nperformances = bmr$aggregate(measures)\nperformances[, c(\"learner_id\", \"classif.ce\", \"classif.auc\")]\n\n        learner_id classif.ce classif.auc\n1: classif.log_reg      0.250   0.7646559\n2:  classif.ranger      0.239   0.7977865\n\n\nWe see that the two models perform very similarly."
  },
  {
    "objectID": "gallery/basic/2020-03-11-basics-german-credit/index.html#deviating-from-hyperparameters-defaults",
    "href": "gallery/basic/2020-03-11-basics-german-credit/index.html#deviating-from-hyperparameters-defaults",
    "title": "German Credit Series - Basics",
    "section": "Deviating from hyperparameters defaults",
    "text": "Deviating from hyperparameters defaults\nThe previously shown techniques build the backbone of a mlr3-featured machine learning workflow. However, in most cases one would never proceed in the way we did. While many R packages have carefully selected default settings, they will not perform optimally in any scenario. Typically, we can select the values of such hyperparameters. The (hyper)parameters of a Learner can be accessed and set via its ParamSet $param_set:\n\nlearner_rf$param_set\n\n&lt;ParamSet&gt;\n                              id    class lower upper nlevels        default    parents       value\n 1:                        alpha ParamDbl  -Inf   Inf     Inf            0.5                       \n 2:       always.split.variables ParamUty    NA    NA     Inf &lt;NoDefault[3]&gt;                       \n 3:                class.weights ParamUty    NA    NA     Inf                                      \n 4:                      holdout ParamLgl    NA    NA       2          FALSE                       \n 5:                   importance ParamFct    NA    NA       4 &lt;NoDefault[3]&gt;            permutation\n 6:                   keep.inbag ParamLgl    NA    NA       2          FALSE                       \n 7:                    max.depth ParamInt     0   Inf     Inf                                      \n 8:                min.node.size ParamInt     1   Inf     Inf                                      \n 9:                     min.prop ParamDbl  -Inf   Inf     Inf            0.1                       \n10:                      minprop ParamDbl  -Inf   Inf     Inf            0.1                       \n11:                         mtry ParamInt     1   Inf     Inf &lt;NoDefault[3]&gt;                       \n12:                   mtry.ratio ParamDbl     0     1     Inf &lt;NoDefault[3]&gt;                       \n13:            num.random.splits ParamInt     1   Inf     Inf              1  splitrule            \n14:                  num.threads ParamInt     1   Inf     Inf              1                      1\n15:                    num.trees ParamInt     1   Inf     Inf            500                       \n16:                    oob.error ParamLgl    NA    NA       2           TRUE                       \n17:        regularization.factor ParamUty    NA    NA     Inf              1                       \n18:      regularization.usedepth ParamLgl    NA    NA       2          FALSE                       \n19:                      replace ParamLgl    NA    NA       2           TRUE                       \n20:    respect.unordered.factors ParamFct    NA    NA       3         ignore                       \n21:              sample.fraction ParamDbl     0     1     Inf &lt;NoDefault[3]&gt;                       \n22:                  save.memory ParamLgl    NA    NA       2          FALSE                       \n23: scale.permutation.importance ParamLgl    NA    NA       2          FALSE importance            \n24:                    se.method ParamFct    NA    NA       2        infjack                       \n25:                         seed ParamInt  -Inf   Inf     Inf                                      \n26:         split.select.weights ParamUty    NA    NA     Inf                                      \n27:                    splitrule ParamFct    NA    NA       3           gini                       \n28:                      verbose ParamLgl    NA    NA       2           TRUE                       \n29:                 write.forest ParamLgl    NA    NA       2           TRUE                       \n                              id    class lower upper nlevels        default    parents       value\n\nlearner_rf$param_set$values = list(verbose = FALSE)\n\nWe can choose parameters for our learners in two distinct manners. If we have prior knowledge on how the learner should be (hyper-)parameterized, the way to go would be manually entering the parameters in the parameter set. In most cases, however, we would want to tune the learner so that it can search “good” model configurations itself. For now, we only want to compare a few models.\nTo get an idea on which parameters can be manipulated, we can investigate the parameters of the original package version or look into the parameter set of the learner:\n\n## ?ranger::ranger\nas.data.table(learner_rf$param_set)[, .(id, class, lower, upper)]\n\n                              id    class lower upper\n 1:                        alpha ParamDbl  -Inf   Inf\n 2:       always.split.variables ParamUty    NA    NA\n 3:                class.weights ParamUty    NA    NA\n 4:                      holdout ParamLgl    NA    NA\n 5:                   importance ParamFct    NA    NA\n 6:                   keep.inbag ParamLgl    NA    NA\n 7:                    max.depth ParamInt     0   Inf\n 8:                min.node.size ParamInt     1   Inf\n 9:                     min.prop ParamDbl  -Inf   Inf\n10:                      minprop ParamDbl  -Inf   Inf\n11:                         mtry ParamInt     1   Inf\n12:                   mtry.ratio ParamDbl     0     1\n13:            num.random.splits ParamInt     1   Inf\n14:                  num.threads ParamInt     1   Inf\n15:                    num.trees ParamInt     1   Inf\n16:                    oob.error ParamLgl    NA    NA\n17:        regularization.factor ParamUty    NA    NA\n18:      regularization.usedepth ParamLgl    NA    NA\n19:                      replace ParamLgl    NA    NA\n20:    respect.unordered.factors ParamFct    NA    NA\n21:              sample.fraction ParamDbl     0     1\n22:                  save.memory ParamLgl    NA    NA\n23: scale.permutation.importance ParamLgl    NA    NA\n24:                    se.method ParamFct    NA    NA\n25:                         seed ParamInt  -Inf   Inf\n26:         split.select.weights ParamUty    NA    NA\n27:                    splitrule ParamFct    NA    NA\n28:                      verbose ParamLgl    NA    NA\n29:                 write.forest ParamLgl    NA    NA\n                              id    class lower upper\n\n\nFor the random forest two meaningful parameters which steer model complexity are num.trees and mtry. num.trees defaults to 500 and mtry to floor(sqrt(ncol(data) - 1)), in our case 4.\nIn the following we aim to train three different learners:\n\nThe default random forest.\nA random forest with low num.trees and low mtry.\nA random forest with high num.trees and high mtry.\n\nWe will benchmark their performance on the German credit dataset. For this we construct the three learners and set the parameters accordingly:\n\nrf_med = lrn(\"classif.ranger\", id = \"med\", predict_type = \"prob\")\n\nrf_low = lrn(\"classif.ranger\", id = \"low\", predict_type = \"prob\",\n  num.trees = 5, mtry = 2)\n\nrf_high = lrn(\"classif.ranger\", id = \"high\", predict_type = \"prob\",\n  num.trees = 1000, mtry = 11)\n\nOnce the learners are defined, we can benchmark them:\n\nlearners = list(rf_low, rf_med, rf_high)\ngrid = benchmark_grid(\n  tasks = task,\n  learners = learners,\n  resamplings = rsmp(\"cv\", folds = 10)\n)\n\n\nbmr = benchmark(grid)\nprint(bmr)\n\n&lt;BenchmarkResult&gt; of 30 rows with 3 resampling runs\n nr      task_id learner_id resampling_id iters warnings errors\n  1 GermanCredit        low            cv    10        0      0\n  2 GermanCredit        med            cv    10        0      0\n  3 GermanCredit       high            cv    10        0      0\n\n\nWe compare misclassification rate and AUC again:\n\nmeasures = msrs(c(\"classif.ce\", \"classif.auc\"))\nperformances = bmr$aggregate(measures)\nperformances[, .(learner_id, classif.ce, classif.auc)]\n\n   learner_id classif.ce classif.auc\n1:        low      0.278   0.7287261\n2:        med      0.240   0.7973700\n3:       high      0.233   0.7891017\n\nautoplot(bmr)\n\n\n\n\n\n\n\n\nThe “low” settings seem to underfit a bit, the “high” setting is comparable to the default setting “med”."
  },
  {
    "objectID": "gallery/basic/2020-03-11-basics-german-credit/index.html#outlook",
    "href": "gallery/basic/2020-03-11-basics-german-credit/index.html#outlook",
    "title": "German Credit Series - Basics",
    "section": "Outlook",
    "text": "Outlook\nThis tutorial was a detailed introduction to machine learning workflows within mlr3. Having followed this tutorial you should be able to run your first models yourself. Next to that we spiked into performance evaluation and benchmarking. Furthermore, we showed how to customize learners.\nThe next parts of the tutorial will go more into depth into additional mlr3 topics:\n\nPart II - Tuning introduces you to the mlr3tuning package\nPart III - Pipelines introduces you to the mlr3pipelines package"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html",
    "href": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html",
    "title": "German Credit Series - Tuning",
    "section": "",
    "text": "This is the second part of a serial of tutorials. The other parts of this series can be found here:\n\nPart I - Basics\nPart III - Pipelines\n\nWe will continue working with the German credit dataset. In Part I, we peeked into the dataset by using and comparing some learners with their default parameters. We will now see how to:\n\nTune hyperparameters for a given problem\nPerform nested resampling"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html#evaluation",
    "href": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html#evaluation",
    "title": "German Credit Series - Tuning",
    "section": "Evaluation",
    "text": "Evaluation\nWe will evaluate all hyperparameter configurations using 10-fold cross-validation. We use a fixed train-test split, i.e. the same splits for each evaluation. Otherwise, some evaluation could get unusually “hard” splits, which would make comparisons unfair.\n\ncv10 = rsmp(\"cv\", folds = 10)\n\n# fix the train-test splits using the $instantiate() method\ncv10$instantiate(task)\n\n# have a look at the test set instances per fold\ncv10$instance\n\n      row_id fold\n   1:     18    1\n   2:     19    1\n   3:     35    1\n   4:     38    1\n   5:     55    1\n  ---            \n 996:    973   10\n 997:    975   10\n 998:    981   10\n 999:    993   10\n1000:    998   10"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html#search-space-and-problem-definition",
    "href": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html#search-space-and-problem-definition",
    "title": "German Credit Series - Tuning",
    "section": "Search Space and Problem Definition",
    "text": "Search Space and Problem Definition\nFirst, we need to decide what Learner we want to optimize. We will use LearnerClassifKKNN, the “kernelized” k-nearest neighbor classifier. We will use kknn as a normal kNN without weighting first (i.e., using the rectangular kernel):\n\nknn = lrn(\"classif.kknn\", predict_type = \"prob\", kernel = \"rectangular\")\n\nAs a next step, we decide what parameters we optimize over. Before that, though, we are interested in the parameter set on which we could tune:\n\nknn$param_set\n\n\n\n            id    class lower upper nlevels\n1:           k ParamInt     1   Inf     Inf\n2:    distance ParamDbl     0   Inf     Inf\n3:      kernel ParamFct    NA    NA      10\n4:       scale ParamLgl    NA    NA       2\n5:     ykernel ParamUty    NA    NA     Inf\n6: store_model ParamLgl    NA    NA       2\n\n\nWe first tune the k parameter (i.e. the number of nearest neighbors), between 3 to 20. Second, we tune the distance function, allowing L1 and L2 distances. To do so, we use the paradox package to define a search space (see the online vignette for a more complete introduction.\n\nsearch_space = ps(\n  k = p_int(3, 20),\n  distance = p_int(1, 2)\n)\n\nAs a next step, we define a TuningInstanceSingleCrit that represents the problem we are trying to optimize.\n\ninstance_grid = TuningInstanceSingleCrit$new(\n  task = task,\n  learner = knn,\n  resampling = cv10,\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"none\"),\n  search_space = search_space\n)"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html#grid-search",
    "href": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html#grid-search",
    "title": "German Credit Series - Tuning",
    "section": "Grid Search",
    "text": "Grid Search\nAfter having set up a tuning instance, we can start tuning. Before that, we need a tuning strategy, though. A simple tuning method is to try all possible combinations of parameters: Grid Search. While it is very intuitive and simple, it is inefficient if the search space is large. For this simple use case, it suffices, though. We get the grid_search tuner via:\n\ntuner_grid = tnr(\"grid_search\", resolution = 18, batch_size = 36)\n\nTuning works by calling $optimize(). Note that the tuning procedure modifies our tuning instance (as usual for R6 class objects). The result can be found in the instance object. Before tuning it is empty:\n\ninstance_grid$result\n\nNULL\n\n\nNow, we tune:\n\ntuner_grid$optimize(instance_grid)\n\n   k distance learner_param_vals  x_domain classif.ce\n1: 7        1          &lt;list[3]&gt; &lt;list[2]&gt;       0.25\n\n\nThe result is returned by $optimize() together with its performance. It can be also accessed with the $result slot:\n\ninstance_grid$result\n\n   k distance learner_param_vals  x_domain classif.ce\n1: 7        1          &lt;list[3]&gt; &lt;list[2]&gt;       0.25\n\n\nWe can also look at the Archive of evaluated configurations:\n\nhead(as.data.table(instance_grid$archive))\n\n\n\n   k distance classif.ce runtime_learners           timestamp batch_nr warnings errors\n1: 3        1      0.273            1.458 2023-10-30 15:10:10        1        0      0\n2: 3        2      0.280            0.435 2023-10-30 15:10:10        1        0      0\n3: 4        1      0.290            0.790 2023-10-30 15:10:10        1        0      0\n4: 4        2      0.266            0.658 2023-10-30 15:10:10        1        0      0\n5: 5        1      0.268            0.716 2023-10-30 15:10:10        1        0      0\n6: 5        2      0.256            0.374 2023-10-30 15:10:10        1        0      0\n\n\nWe plot the performances depending on the sampled k and distance:\n\nggplot(as.data.table(instance_grid$archive),\n  aes(x = k, y = classif.ce, color = as.factor(distance))) +\n  geom_line() + geom_point(size = 3)\n\n\n\n\n\n\n\n\nOn average, the Euclidean distance (distance = 2) seems to work better. However, there is much randomness introduced by the resampling instance. So you, the reader, may see a different result, when you run the experiment yourself and set a different random seed. For k, we find that values between 7 and 13 perform well."
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html#random-search-and-transformation",
    "href": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html#random-search-and-transformation",
    "title": "German Credit Series - Tuning",
    "section": "Random Search and Transformation",
    "text": "Random Search and Transformation\nLet’s have a look at a larger search space. For example, we could tune all available parameters and limit k to large values (50). We also now tune the distance param continuously from 1 to 3 as a double and tune distance kernel and whether we scale the features.\nWe may find two problems when doing so:\nFirst, the resulting difference in performance between k = 3 and k = 4 is probably larger than the difference between k = 49 and k = 50. While 4 is 33% larger than 3, 50 is only 2 percent larger than 49. To account for this we will use a transformation function for k and optimize in log-space. We define the range for k from log(3) to log(50) and exponentiate in the transformation. Now, as k has become a double instead of an int (in the search space, before transformation), we round it in the extra_trafo.\n\nsearch_space_large = ps(\n  k = p_dbl(log(3), log(50)),\n  distance = p_dbl(1, 3),\n  kernel = p_fct(c(\"rectangular\", \"gaussian\", \"rank\", \"optimal\")),\n  scale = p_lgl(),\n  .extra_trafo = function(x, param_set) {\n    x$k = round(exp(x$k))\n    x\n  }\n)\n\nThe second problem is that grid search may (and often will) take a long time. For instance, trying out three different values for k, distance, kernel, and the two values for scale will take 54 evaluations. Because of this, we use a different search algorithm, namely the Random Search. We need to specify in the tuning instance a termination criterion. The criterion tells the search algorithm when to stop. Here, we will terminate after 36 evaluations:\n\ntuner_random = tnr(\"random_search\", batch_size = 36)\n\ninstance_random = TuningInstanceSingleCrit$new(\n  task = task,\n  learner = knn,\n  resampling = cv10,\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 36),\n  search_space = search_space_large\n)\n\n\ntuner_random$optimize(instance_random)\n\n          k distance   kernel scale learner_param_vals  x_domain classif.ce\n1: 1.683743 1.985146 gaussian  TRUE          &lt;list[4]&gt; &lt;list[4]&gt;      0.254\n\n\nLike before, we can review the Archive. It includes the points before and after the transformation. The archive includes a column for each parameter the Tuner sampled on the search space (values before the transformation) and additional columns with prefix x_domain_* that refer to the parameters used by the learner (values after the transformation):\n\nas.data.table(instance_random$archive)\n\n\n\n\n\n\n\nLet’s now investigate the performance by parameters. This is especially easy using visualization:\n\nggplot(as.data.table(instance_random$archive),\n  aes(x = x_domain_k, y = classif.ce, color = x_domain_scale)) +\n  geom_point(size = 3) + geom_line()\n\n\n\n\n\n\n\n\nThe previous plot suggests that scale has a strong influence on performance. For the kernel, there does not seem to be a strong influence:\n\nggplot(as.data.table(instance_random$archive),\n  aes(x = x_domain_k, y = classif.ce, color = x_domain_kernel)) +\n  geom_point(size = 3) + geom_line()"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html#example-tuning-with-a-larger-budget",
    "href": "gallery/basic/2020-03-11-mlr3tuning-tutorial-german-credit/index.html#example-tuning-with-a-larger-budget",
    "title": "German Credit Series - Tuning",
    "section": "Example: Tuning With A Larger Budget",
    "text": "Example: Tuning With A Larger Budget\nIt is always interesting to look at what could have been. The following dataset contains an optimization run result with 3600 evaluations – more than above by a factor of 100:\n\n\n\n\n\n\nThe scale effect is just as visible as before with fewer data:\n\nggplot(perfdata, aes(x = x_domain_k, y = classif.ce, color = scale)) +\n  geom_point(size = 2, alpha = 0.3)\n\n\n\n\n\n\n\n\nNow, there seems to be a visible pattern by kernel as well:\n\nggplot(perfdata, aes(x = x_domain_k, y = classif.ce, color = kernel)) +\n  geom_point(size = 2, alpha = 0.3)\n\n\n\n\n\n\n\n\nIn fact, if we zoom in to (5, 40) \\(\\times\\) (0.23, 0.28) and do decrease smoothing we see that different kernels have their optimum at different values of k:\n\nggplot(perfdata, aes(x = x_domain_k, y = classif.ce, color = kernel,\n  group = interaction(kernel, scale))) +\n  geom_point(size = 2, alpha = 0.3) + geom_smooth() +\n  xlim(5, 40) + ylim(0.23, 0.28)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhat about the distance parameter? If we select all results with k between 10 and 20 and plot distance and kernel we see an approximate relationship:\n\nggplot(perfdata[x_domain_k &gt; 10 & x_domain_k &lt; 20 & scale == TRUE],\n  aes(x = distance, y = classif.ce, color = kernel)) +\n  geom_point(size = 2) + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIn sum our observations are: The scale parameter is very influential, and scaling is beneficial. The distance type seems to be the least influential. There seems to be an interaction between ‘k’ and ‘kernel’."
  },
  {
    "objectID": "gallery/basic/2020-03-30-imbalanced-data/index.html",
    "href": "gallery/basic/2020-03-30-imbalanced-data/index.html",
    "title": "Imbalanced Data Handling with mlr3",
    "section": "",
    "text": "This use case compares different approaches to handle class imbalance for the Optical Recognition of Handwritten Digits (optdigits) binary classification data set using the mlr3 package. We mainly focus on undersampling the majority class, oversampling the minority class, and the SMOTE imbalance correction (Chawla et al. 2002) that enriches the minority class with synthetic data. The use case requires prior knowledge in basic ML concepts (issues imbalanced data, hyperparameter tuning, nested cross-validation). The R packages mlr3, mlr3pipelines and mlr3tuning will be used. You can find most of the content here also in the mlr3book explained in a more detailed way.\nThese steps are performed:\n\nRetrieve data sets from OpenML\nDefine imbalance correction pipeline Graphs (undersampling, oversampling and SMOTE) with mlr3pipelines\nAutotune the Graph together with a learner using mlr3tuning\nBenchmark the autotuned Graph and visualize the results using mlr3viz"
  },
  {
    "objectID": "gallery/basic/2020-03-30-imbalanced-data/index.html#visualize-benchmark-results",
    "href": "gallery/basic/2020-03-30-imbalanced-data/index.html#visualize-benchmark-results",
    "title": "Imbalanced Data Handling with mlr3",
    "section": "Visualize benchmark results",
    "text": "Visualize benchmark results\n\nbmr$aggregate(measure)\n\n   nr   task_id               learner_id resampling_id iters classif.fbeta\n1:  1 optdigits undersample.ranger.tuned       holdout     1     0.9453552\n2:  2 optdigits  oversample.ranger.tuned       holdout     1     0.9508197\n3:  3 optdigits       smote.ranger.tuned       holdout     1     0.9539295\nHidden columns: resample_result\n\n# one value per boxplot since we used holdout as outer resampling\nautoplot(bmr, measure = measure)"
  },
  {
    "objectID": "gallery/basic/2020-03-30-imbalanced-data/index.html#visualize-the-tuning-path",
    "href": "gallery/basic/2020-03-30-imbalanced-data/index.html#visualize-the-tuning-path",
    "title": "Imbalanced Data Handling with mlr3",
    "section": "Visualize the tuning path",
    "text": "Visualize the tuning path\nWith store_models = TRUE we allow the benchmark() function to store each single model that was computed during tuning. Therefore, we can plot the tuning path of the best learner from the subsampling iterations:\n\nlibrary(ggplot2)\nbmr_data_learners = as.data.table(bmr)$learner\nutune_path = bmr_data_learners[[1]]$model$tuning_instance$archive$data\nutune_gg = ggplot(utune_path, aes(x = undersample.ratio, y = classif.fbeta)) +\n  geom_point(size = 3) +\n  geom_line() + ylim(0.9, 1)\n\notune_path = bmr_data_learners[[2]]$model$tuning_instance$archive$data\notune_gg = ggplot(otune_path, aes(x = oversample.ratio, y = classif.fbeta)) +\n  geom_point(size = 3) +\n  geom_line() + ylim(0.9, 1)\n\nstune_path = bmr_data_learners[[3]]$model$tuning_instance$archive$data\nstune_gg = ggplot(stune_path, aes(\n  x = smote.dup_size,\n  y = classif.fbeta, col = factor(smote.K))) +\n  geom_point(size = 3) +\n  geom_line() + ylim(0.9, 1)\n\nlibrary(ggpubr)\nggarrange(utune_gg, otune_gg, stune_gg, common.legend = TRUE, nrow = 1)\n\n\n\n\n\n\n\n\nThe results show that oversampling the minority class (for simple oversampling as well as for SMOTE) and undersampling the majority class yield a better performance for this specific data set."
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html",
    "href": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html",
    "title": "German Credit Series - Pipelines",
    "section": "",
    "text": "This is the third part of a serial of use cases with the German credit dataset. The other parts of this series can be found here:\n\nPart I - Basics\nPart II - Tuning\n\nIn this tutorial, we continue working with the German credit dataset. We already used different Learners on it and tried to optimize their hyperparameters. Now we will do four additional things:\n\nWe preprocess the data as an integrated step of the model fitting process\nWe tune the associated preprocessing parameters\nWe stack multiple Learners in an ensemble model\nWe discuss some techniques that make Learners able to tackle challenging datasets that they could not handle otherwise (we are going to outline what challenging means in particular later on)"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#outline",
    "href": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#outline",
    "title": "German Credit Series - Pipelines",
    "section": "",
    "text": "This is the third part of a serial of use cases with the German credit dataset. The other parts of this series can be found here:\n\nPart I - Basics\nPart II - Tuning\n\nIn this tutorial, we continue working with the German credit dataset. We already used different Learners on it and tried to optimize their hyperparameters. Now we will do four additional things:\n\nWe preprocess the data as an integrated step of the model fitting process\nWe tune the associated preprocessing parameters\nWe stack multiple Learners in an ensemble model\nWe discuss some techniques that make Learners able to tackle challenging datasets that they could not handle otherwise (we are going to outline what challenging means in particular later on)"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#prerequisites",
    "href": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#prerequisites",
    "title": "German Credit Series - Pipelines",
    "section": "Prerequisites",
    "text": "Prerequisites\nFirst, load the packages we are going to use:\n\nlibrary(\"mlr3verse\")\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nWe again use the German credit dataset, but will restrict ourselves to the factorial features. To make things interesting or to make it a bit harder for our Learners, we introduce missing values in the dataset:\n\ntask = tsk(\"german_credit\")\ncredit_full = task$data()\ncredit = credit_full[, sapply(credit_full, FUN = is.factor), with = FALSE]\n\n# sample values to NA\ncredit = credit[, lapply(.SD, function(x) {\n  x[sample(c(TRUE, NA), length(x), replace = TRUE, prob = c(.9, .1))]\n})]\ncredit$credit_risk = credit_full$credit_risk\ntask = TaskClassif$new(\"GermanCredit\", credit, \"credit_risk\")\n\nWe instantiate a Resampling instance for this Task to be able to compare resampling performance:\n\ncv10 = rsmp(\"cv\")$instantiate(task)\n\nWe also might want to use multiple cores to reduce long run times of tuning runs.\n\nfuture::plan(\"multiprocess\")"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#intro",
    "href": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#intro",
    "title": "German Credit Series - Pipelines",
    "section": "Intro",
    "text": "Intro\nIn this use case, we will take a look at composite machine learning algorithms that may incorporate data preprocessing or the combination of multiple Learners (“ensemble methods”).\nWe use the mlr3pipelines package that enables us to chain PipeOps into data flow Graphs.\nAvailable PipeOps are listed in the mlr_pipeops dictionary:\n\nmlr_pipeops\n\n&lt;DictionaryPipeOp&gt; with 64 stored values\nKeys: boxcox, branch, chunk, classbalancing, classifavg, classweights, colapply, collapsefactors, colroles,\n  copy, datefeatures, encode, encodeimpact, encodelmer, featureunion, filter, fixfactors, histbin, ica,\n  imputeconstant, imputehist, imputelearner, imputemean, imputemedian, imputemode, imputeoor, imputesample,\n  kernelpca, learner, learner_cv, missind, modelmatrix, multiplicityexply, multiplicityimply, mutate, nmf,\n  nop, ovrsplit, ovrunite, pca, proxy, quantilebin, randomprojection, randomresponse, regravg,\n  removeconstants, renamecolumns, replicate, scale, scalemaxabs, scalerange, select, smote, spatialsign,\n  subsample, targetinvert, targetmutate, targettrafoscalerange, textvectorizer, threshold, tunethreshold,\n  unbranch, vtreat, yeojohnson"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#missing-value-imputation",
    "href": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#missing-value-imputation",
    "title": "German Credit Series - Pipelines",
    "section": "Missing Value Imputation",
    "text": "Missing Value Imputation\nWe have just introduced missing values into our data. While some Learners can deal with missing value, many cannot. Trying to train a random forest fails because of this:\n\nranger = lrn(\"classif.ranger\")\nranger$train(task)\n\nError: Task 'GermanCredit' has missing values in column(s) 'credit_history', 'employment_duration', 'foreign_worker', 'housing', 'installment_rate', 'job', 'number_credits', 'other_debtors', 'other_installment_plans', 'people_liable', 'personal_status_sex', 'present_residence', 'property', 'purpose', 'savings', 'status', 'telephone', but learner 'classif.ranger' does not support this\n\n\nWe can perform imputation of missing values using a PipeOp. To find out which imputation PipeOps are available, we do the following:\n\nmlr_pipeops$keys(\"^impute\")\n\n[1] \"imputeconstant\" \"imputehist\"     \"imputelearner\"  \"imputemean\"     \"imputemedian\"   \"imputemode\"    \n[7] \"imputeoor\"      \"imputesample\"  \n\n\nWe choose to impute factorial features using a new level (via PipeOpImputeOOR). Let’s use the PipeOp itself to create an imputed Task. This shows us how the PipeOp actually works:\n\nimputer = po(\"imputeoor\")\ntask_imputed = imputer$train(list(task))[[1]]\ntask_imputed$missings()\nhead(task_imputed$data())\n\n\n\n\n\n\n\nWe do not only need complete data during training but also during prediction. Using the same imputation heuristic for both is the most consistent strategy. This way the imputation strategy can, in fact, be seen as a part of the complete learner (which could be tuned).\nIf we used the imputed Task for Resampling, we would leak information from the test set into the training set. Therefore, it is mandatory to attach the imputation operator to the Learner itself, creating a GraphLearner:\n\ngraph_learner_ranger = as_learner(po(\"imputeoor\") %&gt;&gt;% ranger)\n\ngraph_learner_ranger$train(task)\n\nThis GraphLearner can be used for resampling – like an ordinary Learner:\n\nrr = resample(task, learner = graph_learner_ranger, resampling = cv10)\nrr$aggregate()\n\nclassif.ce \n     0.287"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#feature-filtering",
    "href": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#feature-filtering",
    "title": "German Credit Series - Pipelines",
    "section": "Feature Filtering",
    "text": "Feature Filtering\nTypically, sparse models, i.e. having models with few(er) features, are desirable. This is due to a variety of reasons, e.g., enhanced interpretability or decreased costs of acquiring data. Furthermore, sparse models may actually be associated with increased performance (especially if overfitting is anticipated). We can use feature filter to only keep features with the highest information. Filters are implemented in the mlr3filters package and listed in the following dictionary:\n\nmlr_filters\n\n&lt;DictionaryFilter&gt; with 21 stored values\nKeys: anova, auc, carscore, carsurvscore, cmim, correlation, disr, find_correlation, importance,\n  information_gain, jmi, jmim, kruskal_test, mim, mrmr, njmim, performance, permutation, relief,\n  selected_features, variance\n\n\nWe apply the FilterMIM (mutual information maximization) Filter as implemented in the praznik package. This Filter allows for the selection of the top-k features of best mutual information.\n\nfilter = flt(\"mim\")\nfilter$calculate(task_imputed)$scores\n\n                 status          credit_history                 savings                 purpose                property \n                 1.0000                  0.9375                  0.8750                  0.8125                  0.7500 \n                housing     employment_duration other_installment_plans     personal_status_sex           other_debtors \n                 0.6875                  0.6250                  0.5625                  0.5000                  0.4375 \n       installment_rate          foreign_worker                     job          number_credits               telephone \n                 0.3750                  0.3125                  0.2500                  0.1875                  0.1250 \n      present_residence           people_liable \n                 0.0625                  0.0000 \n\n\nMaking use of this Filter, you may wonder at which costs the reduction of the feature space comes. We can investigate the trade-off between features and performance by tuning. We incorporate our filtering strategy into the pipeline using PipeOpFilter. Like before, we need to perform imputation as the Filter also relies on complete data:\n\nfpipe = po(\"imputeoor\") %&gt;&gt;% po(\"filter\", flt(\"mim\"), filter.nfeat = 3)\nfpipe$train(task)[[1]]$head()\n\n   credit_risk                              credit_history                    savings\n1:        good     all credits at this bank paid back duly             ... &gt;= 1000 DM\n2:         bad no credits taken/all credits paid back duly unknown/no savings account\n3:        good     all credits at this bank paid back duly unknown/no savings account\n4:        good no credits taken/all credits paid back duly unknown/no savings account\n5:         bad    existing credits paid back duly till now unknown/no savings account\n6:        good                                    .MISSING             ... &gt;= 1000 DM\n                                       status\n1:                        no checking account\n2:                                 ... &lt; 0 DM\n3: ... &gt;= 200 DM / salary for at least 1 year\n4:                        no checking account\n5:                        no checking account\n6: ... &gt;= 200 DM / salary for at least 1 year\n\n\nWe can now tune over the mim.filter.nfeat parameter. It steers how many features are kept by the Filter and eventually used by the learner:\n\nsearch_space = ps(\n  mim.filter.nfeat = p_int(lower = 1, upper = length(task$feature_names))\n)\n\nThe problem is one-dimensional (i.e. only one parameter is tuned). Thus, we make use of a grid search. For higher dimensions, strategies like random search are more appropriate. The tuning procedure may take some time:\n\ninstance = tune(\n  tuner = tnr(\"grid_search\"),\n  task,\n  learner = fpipe %&gt;&gt;% lrn(\"classif.ranger\"),\n  resampling = cv10,\n  measure = msr(\"classif.ce\"),\n  search_space = search_space)\n\nWe can plot the performance against the number of features. If we do so, we see the possible trade-off between sparsity and predictive performance:\n\nautoplot(instance, type =  \"marginal\")"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#stacking",
    "href": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#stacking",
    "title": "German Credit Series - Pipelines",
    "section": "Stacking",
    "text": "Stacking\nWe want to build a model that is based on the predictions of other Learners. This means that we are in the state that we need predictions already during training. This is a very specific case that is luckily handled by PipeOpLearnerCV. PipeOpLearnerCV performs cross-validation during the training phase and returns the cross-validated predictions. We use \"prob\" predictions because they carry more information than response prediction:\n\ngraph_stack = po(\"imputeoor\") %&gt;&gt;%\n  gunion(list(\n    po(\"learner_cv\", lrn(\"classif.ranger\", predict_type = \"prob\")),\n    po(\"learner_cv\", lrn(\"classif.kknn\", predict_type = \"prob\")))) %&gt;&gt;%\n  po(\"featureunion\") %&gt;&gt;% lrn(\"classif.log_reg\")\n\nWe built a pretty complex Graph already. Therefore, we plot it:\n\ngraph_stack$plot(html = FALSE)\n\n\n\n\n\n\n\n\nWe now compare the performance of the stacked learner to the performance of the individual Learners:\n\ngrid = benchmark_grid(\n  task = task,\n  learner = list(\n    graph_stack,\n    as_learner(po(\"imputeoor\") %&gt;&gt;% lrn(\"classif.ranger\")),\n    as_learner(po(\"imputeoor\") %&gt;&gt;% lrn(\"classif.kknn\")),\n    as_learner(po(\"imputeoor\") %&gt;&gt;% lrn(\"classif.log_reg\"))),\n  resampling = cv10)\n\nbmr = benchmark(grid)\n\n\n\n                                                           learner_id classif.ce\n1: imputeoor.classif.ranger.classif.kknn.featureunion.classif.log_reg      0.282\n2:                                           imputeoor.classif.ranger      0.292\n3:                                             imputeoor.classif.kknn      0.299\n4:                                          imputeoor.classif.log_reg      0.283\n\n\nIf we train the stacked learner and look into the final Learner (the logistic regression), we can see how “important” each Learner of the stacked learner is:\n\ngraph_stack$train(task)\n\n$classif.log_reg.output\nNULL\n\nsummary(graph_stack$pipeops$classif.log_reg$state$model)\n\n\nCall:\nstats::glm(formula = task$formula(), family = \"binomial\", data = data, \n    model = FALSE)\n\nCoefficients: (2 not defined because of singularities)\n                         Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               -3.3179     0.3527  -9.406  &lt; 2e-16 ***\nclassif.ranger.prob.good   5.4010     0.5648   9.563  &lt; 2e-16 ***\nclassif.ranger.prob.bad        NA         NA      NA       NA    \nclassif.kknn.prob.good     0.8502     0.3169   2.683  0.00729 ** \nclassif.kknn.prob.bad          NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1221.7  on 999  degrees of freedom\nResidual deviance: 1054.1  on 997  degrees of freedom\nAIC: 1060.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe random forest has a higher contribution."
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#robustify-preventing-new-prediction-factor-levels-and-other-problems",
    "href": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#robustify-preventing-new-prediction-factor-levels-and-other-problems",
    "title": "German Credit Series - Pipelines",
    "section": "Robustify: Preventing new Prediction Factor Levels and other Problems",
    "text": "Robustify: Preventing new Prediction Factor Levels and other Problems\nWe now shift the context, using the complete German credit dataset:\n\ntask = tsk(\"german_credit\")\n\nThere is a potential practical problem for both, small data sets and data sets with covariates having many factor levels: It may occur that not all possible factor levels have been used by the Learner during training. This happens because these rare instances are simply not sampled. The prediction then may fail because the Learner does not know how to handle unseen factor levels:\n\ntask_unseen = task$clone()$filter(1:30)\nlearner_logreg = lrn(\"classif.log_reg\")\nlearner_logreg$train(task_unseen)\nlearner_logreg$predict(task)\n\nError in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor job has new levels unemployed/unskilled - non-resident\n\n\nNot only logistic regression but also many other Learners cannot handle new levels during prediction. Thus, we use PipeOpFixFactors to prevent that. PipeOpFixFactors introduces NA values for unseen levels. This means that we may need to impute afterwards. To solve this issue we can use PipeOpImputeSample, but with affect_columns set to only factorial features.\nAnother observation is that all-constant features may also be a problem:\n\ntask_constant = task$clone()$filter(1:2)\nlearner_logreg = lrn(\"classif.log_reg\")\nlearner_logreg$train(task_constant)\n\nError in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]): contrasts can be applied only to factors with 2 or more levels\n\n\nThis can be fixed using PipeOpRemoveConstants.\nBoth, handling unseen levels and all-constant features can be handled simultaneously using the following Graph:\n\nrobustify = po(\"fixfactors\") %&gt;&gt;%\n  po(\"removeconstants\") %&gt;&gt;%\n  po(\"imputesample\", affect_columns = selector_type(c(\"ordered\", \"factor\")))\n\nrobustify$plot(html = FALSE)\n\n\n\n\n\n\n\n\nThis robust learner works even in very pathological conditions:\n\ngraph_learner_robustify = as_learner(robustify %&gt;&gt;% learner_logreg)\n\ngraph_learner_robustify$train(task_constant)\ngraph_learner_robustify$predict(task)\n\n&lt;PredictionClassif&gt; for 1000 observations:\n    row_ids truth response\n          1  good     good\n          2   bad      bad\n          3  good     good\n---                       \n        998  good      bad\n        999   bad      bad\n       1000  good      bad"
  },
  {
    "objectID": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#your-ideas",
    "href": "gallery/basic/2020-03-11-mlr3pipelines-tutorial-german-credit/index.html#your-ideas",
    "title": "German Credit Series - Pipelines",
    "section": "Your Ideas",
    "text": "Your Ideas\nThere are various possibilities for preprocessing with PipeOps. You can try different methods for preprocessing and training. Feel free to discover this variety by yourself! Here are only a few hints that help when working with PipeOps:\n\nIt is not allowed to have two PipeOps with the same ID in a Graph\n\nInitialize a PipeOp with po(\"...\", id = \"xyz\") to change its ID on construction\n\nIf you build large Graphs involving complicated optimizations, like many \"learner_cv\", they may need a long time to train\nUse the affect_columns parameter if you want a PipeOp to only operate on part of the data\nUse po(\"select\") if you want to remove certain columns (possibly only along a single branch of multiple parallel branches). Both take selector_xxx() arguments, e.g. selector_type(\"integer\")\nYou may get the best performance if you actually inspect the features and see what kind of transformations work best for them (know your data!)\nSee what PipeOps are available by inspecting mlr_pipeops$keys(), and get help about them using ?mlr_pipeops_xxx"
  },
  {
    "objectID": "gallery/technical/2023-12-21-time-constraints/index.html",
    "href": "gallery/technical/2023-12-21-time-constraints/index.html",
    "title": "Time constraints in the mlr3 ecosystem",
    "section": "",
    "text": "Scope\nSetting time limits is an important consideration when tuning unreliable or unstable learning algorithms and when working on shared computing resources. The mlr3 ecosystem provides several mechanisms for setting time constraints for individual learners, tuning processes, and nested resampling.\n\n\nLearner\nThis section demonstrates how to impose time constraints using a support vector machine (SVM) as an illustrative example.\n\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3tuning)\n\nlearner = lrn(\"classif.svm\")\n\nApplying timeouts to the $train() and $predict() functions is essential for managing learners that may operate indefinitely. These time constraints are set independently for both the training and prediction stages. Generally, training a learner consumes more time than prediction. Certain learners, like k-nearest neighbors, lack a distinct training phase and require a timeout only during prediction. For the SVM’s training, we set a 10-second limit.\n\nlearner$timeout = c(train = 10, predict = Inf)\n\nTo effectively terminate the process if necessary, it’s important to run the training and prediction within a separate R process. The callr package is recommended for this encapsulation, as it tends to be more reliable than the evaluate package, especially for terminating externally compiled code. Note that using callr increases the runtime due to the overhead of starting an R process. Additionally, a fallback learner must be specified, such as \"classif.featureless\", to provide baseline predictions in case the primary learner is terminated.\n\nlearner$encapsulate(method = \"callr\", fallback = lrn(\"classif.featureless\"))\n\nThese time constraints are now integrated into the training, resampling, and benchmarking processes. For more information on encapsulation and fallback learners, see the mlr3book. The next section will focus on setting time limits for the entire tuning process.\n\n\nTuning\nWhen working with high-performance computing clusters, jobs are often bound by strict time constraints. Exceeding these limits results in the job being terminated and the loss of any results generated. Therefore, it’s important to ensure that the tuning process is designed to adhere to these time constraints.\nThe trm(\"runtime\") controls the duration of the tuning process. We must take into account that the terminator can only check if the time limit is reached between batches. We must therefore set the time lower than the runtime of the job. How much lower depends on the runtime or time limit of the individual learners. The last batch should be able to finish before the time limit of the cluster is reached.\n\nterminator = trm(\"run_time\", secs = 60)\n\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = terminator\n)\n\nWith these settings, our tuning operation is configured to run for 60 seconds, while individual learners are set to terminate after 10 seconds. This approach ensures the tuning process is efficient and adheres to the constraints imposed by the high-performance computing cluster.\n\n\nNested Resampling\nWhen using nested resampling, time constraints become more complex as they are applied across various levels. As before, the time limit for an individual learner during the tuning is set with $timeout. The time limit for the tuning processes in the auto tuners is controlled with the trm(\"runtime\"). It’s important to note that once the auto tuner enters the final phase of fitting the model and making predictions on the outer test set, the time limit governed by the terminator no longer applies. Additionally, the time limit previously set on the learner is temporarily deactivated, allowing the auto tuner to complete its task uninterrupted. However, a separate time limit can be assigned to each auto tuner using $timeout. This limit encompasses not only the tuning phase but also the time required for fitting the final model and predictions on the outer test set.\nThe best way to show this is with an example. We set the time limit for an individual learner to 10 seconds.\n\nlearner$timeout = c(train = 10, predict = Inf)\nlearner$encapsulate(method = \"callr\", fallback = lrn(\"classif.featureless\"))\n\nNext, we give each auto tuner 60 seconds to finish the tuning process.\n\nterminator = trm(\"run_time\", secs = 60)\n\nFurthermore, we impose a 120-second limit for resampling each auto tuner. This effectively divides the time allocation, with around 60 seconds for tuning and another 60 seconds for final model fitting and predictions on the outer test set.\n\nat = auto_tuner(\n  tuner = tnr(\"random_search\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"run_time\", secs = 60)\n)\n\nat$timeout = c(train = 100, predict = 20)\nat$encapsulate(method = \"callr\", fallback = lrn(\"classif.featureless\"))\n\nIn total, the entire nested resampling process is designed to be completed within 10 minutes (120 seconds multiplied by 5 folds).\n\nrr = resample(task, at, rsmp(\"cv\", folds = 5))\n\n\n\nConclusion\nWe delved into the setting of time constraints across different levels in the mlr3 ecosystem. From individual learners to the complexities of nested resampling, we’ve seen how effectively managing time limits can significantly enhance the efficiency and reliability of machine learning workflows. By utilizing the trm(\"runtime\") for tuning processes and setting $timeout for individual learners and auto tuners, we can ensure that our machine learning tasks are not only effective but also adhere to the practical time constraints of shared computational resources. For more information, see also the error handling section in the mlr3book.\n\n\nSession Information\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.1 (2024-06-14)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Berlin\n date     2024-10-18\n pandoc   3.1.3 @ /usr/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package        * version    date (UTC) lib source\n   backports        1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk            1.1.1      2024-10-16 [1] Github (mlr-org/bbotk@24d72a3)\n   checkmate        2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n   cli              3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n P codetools        0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   crayon           1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table     * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   digest           0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   evaluate         0.24.0     2024-06-10 [1] CRAN (R 4.4.1)\n   fastmap          1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   future           1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   globals          0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   htmltools        0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets      1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite         1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   knitr            1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   lgr              0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   listenv          0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   mlr3           * 0.21.0     2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3learners   * 0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3measures     1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc         0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3tuning     * 1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3website    * 0.0.0.9000 2024-10-17 [1] Github (mlr-org/mlr3website@bde3065)\n   palmerpenguins   0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox        * 1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly       1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   R6               2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   renv             1.0.3      2023-09-19 [1] CRAN (R 4.4.1)\n   rlang            1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown        2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   sessioninfo      1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   uuid             1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   xfun             0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml             2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "gallery/technical/2020-08-13-a-production-example-using-plumber-and-docker/index.html",
    "href": "gallery/technical/2020-08-13-a-production-example-using-plumber-and-docker/index.html",
    "title": "A Production Example Using Plumber and Docker",
    "section": "",
    "text": "Production with R has come a long way. In this tutorial, we give a brief example on how to write a REST API and deploy it (relying on the mlr3 ecosystem for the actual training and predicting). Most of this tutorial was inspired by other excellent posts and vignettes:\n\nR can API and So Can You!\nUsing docker to deploy an R plumber API\nAzureContainer’s vignette\n\nAll files presented in this tutorial are available here.\n\nModeling Background\n\nlibrary(data.table)\nlibrary(mlr3)\nlibrary(mlr3pipelines)\n\nWe will use a subset of the boston_housing Task. Our goal is to predict the median value of owner-occupied homes in USD 1000’s (target medv), using the features crim, tax and town (just to have factor, integer, and numeric feature types):\n\ndata = tsk(\"boston_housing\")$data()\ndata = data[, c(\"cmedv\", \"crim\", \"tax\", \"town\")]\n\n\ntask = TaskRegr$new(\"boston\", backend = data, target = \"cmedv\")\n\nLet’s create a toy pipeline:\nRegarding modeling, we will keep it very simple and use the rpart learner. Missing numerical features (which could happen during prediction) will be imputed by their median via PipeOpImputeMedian, while missing factorial features will be imputed using a new level via PipeOpImputeOOR. As PipeOpImputeOOR will introduce a new level, \".MISSING\" to impute missing values, we also use PipeOpFixFactors:\n\ng = po(\"imputemedian\") %&gt;&gt;%\n  po(\"imputeoor\") %&gt;&gt;%\n  po(\"fixfactors\") %&gt;&gt;%\n  lrn(\"regr.rpart\")\n\nWe wrap this Graph in a GraphLearner and can train on the Task:\n\ngl = GraphLearner$new(g)\ngl$train(task)\n\nWe can inspect the trained pipeline looking at:\n\ngl$model\n\nFurthermore, we can save the trained pipeline, i.e., as \"gl.rds\":\n\nsaveRDS(gl, \"gl.rds\")\n\nWe will also store some information regarding the features, i.e., the feature names, types and levels (you will later see, why we need to do this):\n\nfeature_info = list(\n  feature_names = task$feature_names,\n  feature_types = task$feature_types,\n  levels = task$levels()\n)\nsaveRDS(feature_info, \"feature_info.rds\")\n\nPutting everything in a file, train_gl.R looks like the following, which we can then source before moving on:\n\n# train_gl.R\n\nlibrary(mlr3)\nlibrary(mlr3pipelines)\n\ndata = tsk(\"boston_housing\")$data()\ndata = data[, c(\"medv\", \"crim\", \"tax\", \"town\")]\ntask = TaskRegr$new(\"boston\", backend = data, target = \"medv\")\n\ng = po(\"imputemedian\") %&gt;&gt;%\n  po(\"imputeoor\") %&gt;&gt;%\n  po(\"fixfactors\") %&gt;&gt;%\n  lrn(\"regr.rpart\")\n\ngl = GraphLearner$new(g)\n\ngl$train(task)\n\nsaveRDS(gl, \"gl.rds\")\n\nfeature_info = list(\n  feature_names = task$feature_names,\n  feature_types = task$feature_types,\n  levels = task$levels()\n)\n\nsaveRDS(feature_info, \"feature_info.rds\")\n\nOur goal of our REST (representational state transfer) API (application programming interface) will be to predict the medv of a new observation, i.e., it should do something like the following:\n\nnewdata = data.table(crim = 3.14, tax = 691, town = \"Newton\")\ngl$predict_newdata(newdata)\n\n&lt;PredictionRegr&gt; for 1 observations:\n row_ids truth response\n       1    NA 33.30106\n\n\nHowever, in our REST API, the newdata will be received at an endpoint that accepts a particular input. In the next section we will use plumber to set up our web service.\n\n\nUsing plumber to set up our REST API\nThe package plumber allows us to create a REST API by simply commenting existing R code. plumber makes use of these comments to define the web service. Running plumber::plumb on the commented R file then results in a runnable web service that other systems can interact with over a network.\nAs an endpoint for predicting the medv, we will use a POST request. This will allow us to enclose data in the body of the request message. More precisely, we assume that the data will be provided in the JSON format.\nWhen a POST request containing the data (in JSON format) is received our code must then:\n\nconvert the input (in JSON format) to a data.table with all feature columns matching their feature type\npredict the medv based on the input using our trained pipeline and provide an output that can be understood by the client\n\nTo make sure that all features match their feature type, we will later use the following function stored in the R file fix_feature_types.R:\n\n# fix_feature_types.R\n\nfix_feature_types = function(feature, feature_name, feature_info) {\n  id = match(feature_name, feature_info$feature_names)\n  feature_type = feature_info$feature_types$type[id]\n  switch(feature_type,\n    \"logical\"   = as.logical(feature),\n    \"integer\"   = as.integer(feature),\n    \"numeric\"   = as.numeric(feature),\n    \"character\" = as.character(feature),\n    \"factor\"    = factor(feature, levels = feature_info$levels[[feature_name]],\n      ordered = FALSE),\n    \"ordered\"   = factor(feature, levels = feature_info$levels[[feature_name]],\n      ordered = TRUE),\n    \"POSIXct\"   = as.POSIXct(feature)\n  )\n}\n\nfix_feature_types() can later be applied to the newdata, and will make sure, that all incoming features are converted to their expected feature type as in the original Task we used for training our pipeline (and this is the reason, why we stored the information about the features earlier). Note that in our tutorial we only have factor, integer, and numeric features, but fix_feature_types() should also work for all other supported feature_types listed in mlr_reflections$task_feature_types. However, it may need some customization depending on your own production environment to make the conversions meaningful.\nThe following R file, predict_gl.R loads our trained pipepline and feature information and provides an endpoint for a POST request, \"/predict_medv\". The incoming data then is converted using jsonlite::fromJSON. We expect the incoming data to either be JSON objects in an array or nested JSON objects and therefore we bind the converted vectors row-wise to a data.table using data.table::rbindlist. We then convert all features to their expected feature_types (using the fix_feature_types() function as defined above) and can finally predict the medv using our trained pipeline. As no default serialization from R6 objects to JSON objects exists (yet), we wrap the Prediction in a data.table (of course we could also only return the numeric prediction values):\n\n# predict_gl.R\n\nlibrary(data.table)\nlibrary(jsonlite)\nlibrary(mlr3)\nlibrary(mlr3pipelines)\n\nsource(\"fix_feature_types.R\")\n\ngl = readRDS(\"gl.rds\")\n\nfeature_info = readRDS(\"feature_info.rds\")\n\n#* @post /predict_medv\nfunction(req) {\n  # get the JSON string from the post body\n  newdata = fromJSON(req$postBody, simplifyVector = FALSE)\n  # expect either JSON objects in an array or nested JSON objects\n  newdata = rbindlist(newdata, use.names = TRUE)\n  # convert all features in place to their expected feature_type\n  newdata[, colnames(newdata) := mlr3misc::pmap(\n    list(.SD, colnames(newdata)),\n    fix_feature_types,\n    feature_info = feature_info)]\n  # predict and return as a data.table\n  as.data.table(gl$predict_newdata(newdata))\n  # or only the numeric values\n  # gl$predict_newdata(newdata)$response\n}\n\nNote that the only difference to a regular R file is the comment\n\n#* @post /predict_medv`\n\ntelling plumber to construct the endpoint \"/predict_medv\" for a POST request.\nWe can then run plumber::plumb. The following code sets up the web service locally on your personal machine at port 1030 (we use such a high number because some systems require administrator rights to allow processes to listen to lower ports):\n\nlibrary(plumber)\nr = plumb(\"predict_gl.R\")\nr$run(port = 1030, host = \"0.0.0.0\")\n\nCongratulations, your first REST API is running on your local machine. We can test it by providing some data, using curl via the command line:\n\ncurl --data '[{\"crim\":3.14, \"tax\":691, \"town\":\"Newton\"}]' \"http://127.0.0.1:1030/predict_medv\"\n\nThis should return the predicted medv:\n\n[{\"row_id\":1,\"response\":\"32.2329\"}]\n\nAlternatively, we can also use the httr::POST function within R:\n\nnewdata = '[{\"crim\":3.14, \"tax\":691, \"town\":\"Newton\"}]'\nresp = httr::POST(url = \"http://127.0.0.1:1030/predict_medv\",\n  body = newdata, encode = \"json\")\nhttr::content(resp)\n\nWe can further play around a bit more and provide more than a single new observation and also check whether our feature type conversion and missing value imputation works:\n\nnewdata = '[\n  {\"crim\":3.14, \"tax\":691, \"town\":\"Newton\"},\n  {\"crim\":\"not_a_number\", \"tax\":3.14, \"town\":\"Munich\"},\n  {\"tax\":\"not_a_number\", \"town\":31, \"crim\":99}\n]'\nresp = httr::POST(url = \"http://127.0.0.1:1030/predict_medv\",\n  body = newdata, encode = \"json\")\nhttr::content(resp)\n\nNote that you can also use jsonlite::toJSON to convert a data.frame to JSON data for your toy examples here.\nIn the following final section we want to use Docker to run a virtual machine as a container (an instance of a snapshot of a machine at a moment in time).\n\n\nUsing Docker to Deploy our REST API\nA Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application. Suppose we want to run our REST API on an Amazon Web Service or Microsoft’s Azure Cloud. Then we can use a Docker container to easily set up our web service without going through the hassle of configuring manually our hosting instance.\nWe are going to need two things: An image and a container. An image defines the OS and software while the container is the actual running instance of the image. To build a Docker image we have to specify a Dockerfile. Note that it is sensible to set up the whole project in its own directory, e.g., ~/mlr3_api.\nEvery Dockerfile starts with a FROM statement describing the image we are building our image from. In our case we want an R based image that ideally already has plumber and its dependencies installed. Luckily, the trestletech/plumber image exists:\n\nFROM trestletech/plumber\n\nWe then install the R packages needed to set up our REST API (note that we can skip jsonlite, because plumber already depends on it):\n\nRUN R -e 'install.packages(c(\"data.table\", \"mlr3\", \"mlr3pipelines\"))'\n\nNext, we copy our trained pipeline (gl.rds), our stored feature information (feature_info.rds), our R file to convert features, (fix_feature_types.R) and our R file to predict (predict_gl.R) to a new directory /data and set this as the working directory:\n\nRUN mkdir /data\nCOPY gl.rds /data\nCOPY feature_info.rds /data\nCOPY fix_feature_types.R /data\nCOPY predict_gl.R /data\nWORKDIR /data\n\nFinally, we listen on port 1030 and start the server (this is analogously done as manually calling plumber::plumb on the predict_gl.R file and running it):\n\nEXPOSE 1030\nENTRYPOINT [\"R\", \"-e\", \\\n    \"r = plumber::plumb('/data/predict_gl.R'); r$run(port = 1030, host = '0.0.0.0')\"]\n\nThe complete Dockerfile looks like the following:\n\nFROM trestletech/plumber\n\nRUN R -e 'install.packages(c(\"data.table\", \"mlr3\", \"mlr3misc\", \"mlr3pipelines\"))'\n\nRUN mkdir /data\nCOPY gl.rds /data\nCOPY feature_info.rds /data\nCOPY fix_feature_types.R /data\nCOPY predict_gl.R /data\nWORKDIR /data\n\nEXPOSE 1030\nENTRYPOINT [\"R\", \"-e\", \\\n    \"r = plumber::plumb('/data/predict_gl.R'); r$run(port = 1030, host = '0.0.0.0')\"]\n\nTo build the image we open a terminal in the mlr3_api directory and run:\n\ndocker build -t mlr3-plumber-demo .\n\nThis may take quite some time.\nTo finally run the container, simply use:\n\ndocker run --rm -p 1030:1030 mlr3-plumber-demo\n\nYou can then proceed to provide some data via curl or httr::POST (to the same local address, because the Docker container is still running on your local machine).\nTo stop all running containers use:\n\ndocker stop $(docker ps -a -q)\n\nFinally, you can proceed to deploy your container to an Amazon Web Service or an Azure Cloud. For the latter, the package AzureContainers is especially helpful. If you do plan to do this note that the plumber service above is exposed over HTTP, and there is no authentication layer making it insecure. You may think about adding a layer of authentification and restricting the service to HTTPS.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package        * version    date (UTC) lib source\n   backports        1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   checkmate        2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n   cli              3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n P codetools        0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   crayon           1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table     * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   digest           0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   evaluate         1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fastmap          1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   future           1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   globals          0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   htmltools        0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets      1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite         1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   knitr            1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   lgr              0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   listenv          0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   mlr3           * 0.21.0     2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3misc         0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines  * 0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3website    * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   palmerpenguins   0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox          1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly       1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   R6               2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   renv             1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang            1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown        2.28       2024-08-17 [1] CRAN (R 4.4.1)\n P rpart            4.1.23     2023-12-05 [?] CRAN (R 4.4.0)\n   sessioninfo      1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   stringi          1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   uuid             1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   withr            3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun             0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml             2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html",
    "href": "gallery/technical/2025-01-12-efs/index.html",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "",
    "text": "In this post we will show how we can use the mlr3fselect R package to perform wrapped-based ensemble feature selection on a given dataset. Wrapper-based ensemble feature selection involves applying stability selection techniques (resampling of the data) to create robust feature subsets by leveraging multiple ML models in wrapper-based feature selection strategies.\nSome papers from which we draw the ideas for this tutorial:\n\nStability selection, i.e. drawing multiple subsamples from a dataset and performing feature selection on each (Meinshausen and Bühlmann 2010). Stability selection helps ensure that the selected features are robust to variations in the training data, increasing the reliability of the feature selection process.\nThe ensemble idea for feature selection, i.e. using multiple methods or models to perform feature selection on a dataset (Saeys, Abeel, and Van De Peer 2008). This combines the strengths of different approaches to achieve more comprehensive results and alleviates biases that may arise from each individual approach for feature selection.\n\n\n\n\n\n\n\nNote\n\n\n\nWe also support embedded-based ensemble feature selection, see the function mlr3fselect::embedded_ensemble_fselect() for more details."
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#intro",
    "href": "gallery/technical/2025-01-12-efs/index.html#intro",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "",
    "text": "In this post we will show how we can use the mlr3fselect R package to perform wrapped-based ensemble feature selection on a given dataset. Wrapper-based ensemble feature selection involves applying stability selection techniques (resampling of the data) to create robust feature subsets by leveraging multiple ML models in wrapper-based feature selection strategies.\nSome papers from which we draw the ideas for this tutorial:\n\nStability selection, i.e. drawing multiple subsamples from a dataset and performing feature selection on each (Meinshausen and Bühlmann 2010). Stability selection helps ensure that the selected features are robust to variations in the training data, increasing the reliability of the feature selection process.\nThe ensemble idea for feature selection, i.e. using multiple methods or models to perform feature selection on a dataset (Saeys, Abeel, and Van De Peer 2008). This combines the strengths of different approaches to achieve more comprehensive results and alleviates biases that may arise from each individual approach for feature selection.\n\n\n\n\n\n\n\nNote\n\n\n\nWe also support embedded-based ensemble feature selection, see the function mlr3fselect::embedded_ensemble_fselect() for more details."
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#libraries",
    "href": "gallery/technical/2025-01-12-efs/index.html#libraries",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(mlr3verse)\nlibrary(fastVoteR) # for feature ranking\nlibrary(ggplot2)\nlibrary(future)\nlibrary(progressr)"
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#dataset",
    "href": "gallery/technical/2025-01-12-efs/index.html#dataset",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "Dataset",
    "text": "Dataset\nWe will use the sonar dataset, which is a binary classification task:\n\ntask = tsk(\"sonar\")\ntask\n\n&lt;TaskClassif:sonar&gt; (208 x 61): Sonar: Mines vs. Rocks\n* Target: Class\n* Properties: twoclass\n* Features (60):\n  - dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2, V20, V21, V22, V23, V24, V25, V26,\n    V27, V28, V29, V3, V30, V31, V32, V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43, V44, V45,\n    V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55, V56, V57, V58, V59, V6, V60, V7, V8, V9"
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#efs-workflow",
    "href": "gallery/technical/2025-01-12-efs/index.html#efs-workflow",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "EFS Workflow",
    "text": "EFS Workflow\nThe ensemble feature selection (EFS) workflow is the following (in parentheses we provide the arguments for the mlr3fselect::ensemble_fselect() function that implements this process):\n\nRepeatedly split a dataset to train/test sets (init_resampling), e.g. by subsampling \\(B\\) times.\nChoose \\(M\\) learners (learners).\n\nPerform wrapped-based feature selection on each train set from (1) using each of the models from (2). This process results in a ‘best’ feature (sub)set and a final trained model using these best features, for each combination of train set and learner (\\(B \\times M\\) combinations in total).\nScore the final models on the respective test sets.\n\nTo guide the feature selection process (3) we need to choose:\n\nAn optimization algorithm (fselector), e.g. Recursive Feature Elimination (RFE)\nAn inner resampling technique (inner_resampling), e.g. 5-fold cross-validation (CV)\nAn inner measure (inner_measure), e.g. classification error\nA stopping criterion for the feature selection (terminator), i.e. how many iterations should the optimization algorithm run\n\n\n\n\n\n\n\nNote\n\n\n\nThe inner_measure (used for finding the best feature subset in each train set) and measure (assesses performance on each test set) can be different.\n\n\n\nParallelization\nInternally, ensemble_fselect() performs a full mlr3::benchmark(), the results of which can be stored with the argument store_benchmark_result. The process is fully parallelizable, where every job is a (init resampling iteration, learner) combination. So it’s better to make sure that each RFE optimization (done via mlr3fselect::auto_fselector) is single-threaded.\nBelow we show the code that setups the configuration for the parallelization:\n\n# Parallelization for EFS: use 10 cores\nplan(\"multisession\", workers = 10)\n\n\n\nRFE\nFor each (train set, learner) combination we will run a Recursive Feature Elimination (RFE) optimization algorithm. We configure the algorithm to start with all features of the task, remove the 80% less important features in each iteration, and stop when 2 features are reached. In each RFE iteration, a 5-fold CV resampling of the given task takes place and a learner is trained and used for prediction on the test folds. The outcome of each RFE iteration is the average CV error (performance estimate) and the feature importances (by default the average of the feature ranks from each fold). Practically, for the sonar dataset, we will have 15 RFE iterations, with the following feature subset sizes:\n60 48 38 30 24 19 15 12 10 8  6  5  4  3  2\nThe best feature set will be chosen as the one with the lowest 5-fold CV error. i.e. the best performance estimate in the inner resampling.\nIn mlr3 code, we specify the RFE fselector as:\n\nrfe = fs(\"rfe\", n_features = 2, feature_fraction = 0.8)\n\nSee this gallery post for more details on RFE optimization.\n\n\n\n\n\n\nNote\n\n\n\n\nUsing RFE as the feature selection optimization algorithm means that all learners need to have the \"importance\" property.\n\n\n\n\n\nLearners\nWe define a list() with the following classification learners (parameters are set at default values):\n\nXGBoost with early stopping\nA tree\nA random forest (RF)\nA Support Vector Machine (SVM)\n\n\nmax_nrounds = 500\n\nlearners = list(\n  lrn(\"classif.xgboost\", id = \"xgb\", nrounds = max_nrounds,\n      early_stopping_rounds = 20, validate = \"test\"),\n  lrn(\"classif.rpart\", id = \"tree\"),\n  lrn(\"classif.ranger\", id = \"rf\", importance = \"permutation\"),\n  lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\", kernel = \"linear\")\n)\n\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to perform tuning while also performing wrapper-based feature selection. This practically means that we would use an AutoTuner learner with its own inner resampling scheme and tuning space in the above list. The whole process would then be a double (nested) cross-validation with outer loop the \\(B\\) subsample iterations, which is computationally taxing. Models that need minimum to no tuning (e.g. like Random Forests) are therefore ideal candidates for wrapper-based ensemble feature selection.\n\n\n\n\nCallbacks\nSince SVM doesn’t support importance scores by itself, we convert the coefficients of the trained linear SVM model to importance scores via a callback:\n\nsvm_rfe = clbk(\"mlr3fselect.svm_rfe\")\nsvm_rfe\n\n&lt;CallbackBatchFSelect:mlr3fselect.svm_rfe&gt;: SVM-RFE Callback\n* Active Stages: on_optimization_begin\n\n\n\nAlso, since the XGBoost learner performs internal tuning via early stopping, where the test folds in the inner cross-validation resampling scheme act as validation sets, we need to define the following callback:\n\ninternal_ss = ps(\n  nrounds = p_int(upper = max_nrounds, aggr = function(x) as.integer(mean(unlist(x))))\n)\n\nxgb_clbk = clbk(\"mlr3fselect.internal_tuning\", internal_search_space = internal_ss)\nxgb_clbk\n\n&lt;CallbackBatchFSelect:mlr3fselect.internal_tuning&gt;: Internal Tuning\n* Active Stages: on_auto_fselector_after_final_model, on_auto_fselector_before_final_model,\n  on_eval_before_archive, on_optimization_end\n\n\nThis practically sets the boosting rounds of the final XGBoost model (after the RFE optimization is finished) as the average boosting rounds from each subsequent training fold (corresponding to the model trained with the ‘best’ feature subset). For example, since we’re performing a 5-fold inner CV, we would have 5 different early-stopped boosting nrounds, from which we will use the average value to train the final XGBoost model using the whole train set.\n\nFor all learners we will prefer sparser models during the RFE optimization process. This means that across all RFE iterations, we will choose as ‘best’ feature subset the one that has the minimum number of features and its performance is within one standard error of the feature set with the best performance (e.g. the lowest classification error). This can be achieved with the following callback:\n\none_se_clbk = clbk(\"mlr3fselect.one_se_rule\")\none_se_clbk\n\n&lt;CallbackBatchFSelect:mlr3fselect.one_se_rule&gt;: One Standard Error Rule Callback\n* Active Stages: on_optimization_end"
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#execute-efs",
    "href": "gallery/technical/2025-01-12-efs/index.html#execute-efs",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "Execute EFS",
    "text": "Execute EFS\nUsing the mlr3fselect::ensemble_fselect() function, we split the sonar task to \\(B = 50\\) subsamples (each corresponding to a 80%/20% train/test set split) and perform RFE in each train set using each of the \\(M = 4\\) learners.\nFor a particular (train set, learner) combination, the RFE process will evaluate the \\(15\\) feature subsets mentioned above. Using the inner 5-fold CV resampling scheme, the average CV classification error will be used to find the best feature subset. Using only features from this best feature set, a final model will be trained using all the observations from each trained set. Lastly, the performance of this final model will be assessed on the corresponding test set using the classification accuracy metric.\n\nset.seed(42)\nefs = ensemble_fselect(\n  fselector = rfe,\n  task = task,\n  learners = learners,\n  init_resampling = rsmp(\"subsampling\", repeats = 50, ratio = 0.8),\n  inner_resampling = rsmp(\"cv\", folds = 5),\n  inner_measure = msr(\"classif.ce\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"none\"),\n  # following list must be named with the learners' ids\n  callbacks = list(\n    xgb  = list(one_se_clbk, xgb_clbk),\n    tree = list(one_se_clbk),\n    rf   = list(one_se_clbk),\n    svm  = list(one_se_clbk, svm_rfe)\n  ),\n  store_benchmark_result = FALSE\n)\n\nThe result is stored in an EnsembleFSResult object, which can use to visualize the results, rank the features and assess the stability of the ensemble feature selection process, among others."
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#result-object",
    "href": "gallery/technical/2025-01-12-efs/index.html#result-object",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "Result Object",
    "text": "Result Object\nPrinting the result object provides some initial information:\n\nprint(efs)\n\n&lt;EnsembleFSResult&gt; with 4 learners and 50 initial resamplings\n     resampling_iteration    learner_id n_features\n                    &lt;int&gt;        &lt;char&gt;      &lt;int&gt;\n  1:                    1 xgb.fselector          8\n  2:                    2 xgb.fselector         30\n  3:                    3 xgb.fselector         60\n  4:                    4 xgb.fselector         19\n  5:                    5 xgb.fselector          6\n ---                                              \n196:                   46 svm.fselector         24\n197:                   47 svm.fselector         19\n198:                   48 svm.fselector         15\n199:                   49 svm.fselector         19\n200:                   50 svm.fselector          8\n\n\nAs we can see, we have \\(M \\times B = 4 \\times 50 = 200\\) (init resampling, learner) combinations. We can inspect the actual data.table result:\n\nefs$result\n\n        learner_id resampling_iteration classif.acc                    features n_features classif.ce_inner\n            &lt;char&gt;                &lt;int&gt;       &lt;num&gt;                      &lt;list&gt;      &lt;int&gt;            &lt;num&gt;\n  1: xgb.fselector                    1   0.6904762  V11,V12,V16,V21,V36,V4,...          8        0.1081996\n  2: xgb.fselector                    2   0.9285714  V1,V10,V11,V12,V13,V15,...         30        0.1627451\n  3: xgb.fselector                    3   0.8333333  V1,V10,V11,V12,V13,V14,...         60        0.1203209\n  4: xgb.fselector                    4   0.8571429 V11,V12,V15,V16,V21,V27,...         19        0.1447415\n  5: xgb.fselector                    5   0.7142857     V10,V11,V16,V31,V36,V45          6        0.1319073\n ---                                                                                                       \n196: svm.fselector                   46   0.7619048  V1,V11,V12,V14,V23,V25,...         24        0.1440285\n197: svm.fselector                   47   0.7380952 V11,V15,V23,V30,V31,V33,...         19        0.1331551\n198: svm.fselector                   48   0.7619048 V12,V17,V31,V32,V36,V37,...         15        0.1809269\n199: svm.fselector                   49   0.7142857 V11,V12,V14,V17,V20,V24,...         19        0.1864528\n200: svm.fselector                   50   0.7619048 V11,V14,V23,V36,V39,V40,...          8        0.1629234\n                            importance\n                                &lt;list&gt;\n  1:       6.8,6.8,6.4,3.8,3.6,3.4,...\n  2: 27.2,26.8,25.6,22.0,22.0,21.6,...\n  3: 60.0,57.6,53.8,52.0,51.4,49.6,...\n  4: 19.0,13.4,13.0,12.8,12.2,11.0,...\n  5:           4.4,4.2,4.2,3.8,2.2,2.2\n ---                                  \n196: 24.0,21.6,20.2,18.4,17.6,16.8,...\n197: 16.6,16.2,16.2,15.0,14.8,14.4,...\n198: 14.0,13.6,13.0,10.4, 9.8, 9.2,...\n199: 17.6,16.0,15.6,14.4,13.4,12.8,...\n200:       8.0,5.8,5.6,5.2,4.6,3.2,...\n\n\nFor each learner (\"learner_id\") and dataset subsample (\"resampling_iteration\") we get:\n\nThe ‘best’ feature subsets (\"features\")\nThe number of ‘best’ features (\"nfeatures\")\nThe importances for these ‘best’ features (\"importance\") - this output column we get only because RFE optimization was used\nThe inner optimization performance scores on the train sets (\"classif.ce_inner\")\nThe performance scores on the test sets (\"classif.acc\")\n\nSince there are two ways in this process to evaluate performance, we can always check which is the active measure:\n\nefs$active_measure\n\n[1] \"outer\"\n\n\nBy default the active measure is the \"outer\", i.e. the measure used to evaluate each learner’s performance in the test sets. In our case that was the classification accuracy:\n\nefs$measure\n\n&lt;MeasureClassifSimple:classif.acc&gt;: Classification Accuracy\n* Packages: mlr3, mlr3measures\n* Range: [0, 1]\n* Minimize: FALSE\n* Average: macro\n* Parameters: list()\n* Properties: -\n* Predict type: response\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the following sections we can use the inner optimization scores (i.e. \"classif.ce_inner\") by executing efs$set_active_measure(\"inner\"). This affects all methods and plots that use performance scores."
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#performance",
    "href": "gallery/technical/2025-01-12-efs/index.html#performance",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "Performance",
    "text": "Performance\nWe can view the performance scores of the different learners used in the ensemble feature selection process. Each box represents the distribution of scores across different resampling iterations for a particular learner.\n\nautoplot(efs, type = \"performance\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nWe observe that RF has better classification accuracy on the test sets of the \\(50\\) subsamples, followed by XGBoost, then the SVM and last the tree model."
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#number-of-selected-features",
    "href": "gallery/technical/2025-01-12-efs/index.html#number-of-selected-features",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "Number of Selected Features",
    "text": "Number of Selected Features\nContinuing, we can plot the number of features selected by each learner in the different resampling iterations:\n\nautoplot(efs, type = \"n_features\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(breaks = seq(0, 60, 10))\n\n\n\n\n\n\n\n\nWe observe that RF needed more features to achieve the best average performance, followed by SVM, then XGBoost and the tree model was the model using the least features (but with worst performance)."
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#pareto-plot",
    "href": "gallery/technical/2025-01-12-efs/index.html#pareto-plot",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "Pareto Plot",
    "text": "Pareto Plot\nBoth performance scores and number of features selected by the RFE optimization process can be visualized jointly in the Pareto plot. Here we also draw the Pareto front, i.e. the set of points that represent the trade-off between the number of features and performance (classification accuracy). As we see below, these points are derived from multiple learners and resamplings:\n\nautoplot(efs, type = \"pareto\", theme = theme_minimal(base_size = 14)) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Empirical Pareto front\")\n\n\n\n\n\n\n\n\nWe can also draw an estimated Pareto front curve by fitting a linear model with the inverse of the number of selected features (\\(1/x\\)) of the empirical Pareto front as input, and the associated performance scores as output:\n\nautoplot(efs, type = \"pareto\", pareto_front = \"estimated\", \n         theme = theme_minimal(base_size = 14)) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Estimated Pareto front\")"
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#knee-point-identification",
    "href": "gallery/technical/2025-01-12-efs/index.html#knee-point-identification",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "Knee Point Identification",
    "text": "Knee Point Identification\nNo matter the type of Pareto front that we chose, specialized methods are available to identify knee points, i.e. points of the Pareto front with an optimal trade-off between performance and number of selected features.\nBy default, we use the geometry-based Normal-Boundary Intersection (NBI) method. This approach calculates the perpendicular distance of each point from the line connecting the first (worst performance, minimum number of features) and last (best performance, maximum number of features) point of the Pareto front. The knee point is then identified as the point with the maximum distance from this line (Das 1999).\nUsing the empirical and estimated Pareto fronts, we observe that the optimal knee points correspond to different numbers of features:\n\nefs$knee_points()\n\n   n_features classif.acc\n        &lt;num&gt;       &lt;num&gt;\n1:         10   0.9047619\n\nefs$knee_points(type = \"estimated\")\n\n   n_features classif.acc\n        &lt;int&gt;       &lt;num&gt;\n1:          8   0.8597253\n\n\n\n\n\n\n\n\nTipNumber of features cutoff\n\n\n\nThe number of features at the identified knee point provides a cutoff for prioritizing features when working with a ranked feature list (see “Feature Ranking” section)."
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#stability",
    "href": "gallery/technical/2025-01-12-efs/index.html#stability",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "Stability",
    "text": "Stability\nThe stabm R package (Bommert and Lang 2021) implements many measures for the assessment of the stability of feature selection, i.e. the similarity between the selected feature sets (\"features\" column in the EnsembleFSResult object). We can use these measures to assess and visualize the stability across all resampling iterations and learners (global = \"TRUE\") or per each learner separately (global = \"FALSE\").\nThe default stability measure is the Jaccard Index:\n\nefs$stability(stability_measure = \"jaccard\", global = TRUE)\n\n[1] 0.2640504\n\n\nStability per learner:\n\nefs$stability(stability_measure = \"jaccard\", global = FALSE)\n\n xgb.fselector tree.fselector   rf.fselector  svm.fselector \n     0.3657964      0.3554681      0.4716744      0.3119381 \n\n\nWe observe that the RF model was the most stable in identifying similar predictive features across the different subsamples of the dataset, while the SVM model the least stable.\nTo visualize stability, the following code generates a stability barplot:\n\nautoplot(efs, type = \"stability\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\nAlternatively, the Nogueira stability measure can be used, which unlike the Jaccard Index, it’s a chance-corrected similarity measure (Nogueira, Sechidis, and Brown 2018):\n\nautoplot(efs, type = \"stability\", stability_measure = \"nogueira\", \n         stability_args = list(p = task$n_features), \n         theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "gallery/technical/2025-01-12-efs/index.html#feature-ranking",
    "href": "gallery/technical/2025-01-12-efs/index.html#feature-ranking",
    "title": "Wrapper-based Ensemble Feature Selection",
    "section": "Feature Ranking",
    "text": "Feature Ranking\nUsing the Pareto method, we demonstrated how we can identify a reasonable cutoff for the number of selected features. Now we will focus on how to create a consensus ranked feature list based on the results of the ensemble feature selection.\nThe most straightforward ranking is obtained by counting how often each feature appears in the ‘best’ feature subsets (\"features\"). Below we show the top 8 features, i.e. up to the cutoff derived from the knee point of the estimated Pareto front. The column \"score\" represents these counts, while the column \"norm_score\" is the feature selection frequency or also known as selection probability (Meinshausen and Bühlmann 2010):\n\nefs$feature_ranking(method = \"av\", use_weights = FALSE, committee_size = 8)\n\n   feature score norm_score borda_score\n    &lt;char&gt; &lt;num&gt;      &lt;num&gt;       &lt;num&gt;\n1:     V12   179      0.895   1.0000000\n2:     V11   170      0.850   0.9830508\n3:      V9   123      0.615   0.9661017\n4:     V45   121      0.605   0.9491525\n5:     V16   118      0.590   0.9322034\n6:     V36   113      0.565   0.9152542\n7:     V49   104      0.520   0.8983051\n8:      V4    99      0.495   0.8813559\n\n\nIn the language of Voting Theory, we call the method that generates these counts approval voting (method = \"av\") (Lackner and Skowron 2023). Using this framework, learners act as voters, features act as candidates and voters select certain candidates (features). The primary objective is to compile these selections into a consensus ranked list of features (a committee). The committee_size specifies how many (top-ranked) features to return.\nInternally, $feature_ranking() uses the fastVoteR R package, which supports more advanced ranking methods. For example, we can perform weighted ranking, by considering the varying performance (accuracy) of each learner. This results in the same top 8 features but with slightly different ordering:\n\nefs$feature_ranking(method = \"av\", use_weights = TRUE, committee_size = 8)\n\n   feature     score norm_score borda_score\n    &lt;char&gt;     &lt;num&gt;      &lt;num&gt;       &lt;num&gt;\n1:     V12 134.78571  0.8995710   1.0000000\n2:     V11 127.33333  0.8498331   0.9830508\n3:     V45  94.45238  0.6303830   0.9661017\n4:      V9  93.71429  0.6254569   0.9491525\n5:     V16  89.76190  0.5990783   0.9322034\n6:     V36  87.97619  0.5871603   0.9152542\n7:     V49  80.64286  0.5382171   0.8983051\n8:      V4  76.64286  0.5115207   0.8813559\n\n\nAdditionally, alternative ranking methods are supported. Below, we use satisfaction approval voting (SAV), which ranks features by normalizing approval scores based on the number of features a model has selected. Specifically, models that select more features distribute their “approval” across a larger set, reducing the contribution to each selected feature. Conversely, features chosen by models with fewer selected features receive higher weights, as their selection reflects stronger individual importance. This approach ensures that sparsely selected features are prioritized in the ranking, leading to a different set of top-ranked features compared to standard approval voting. For instance, in the example above, the \"V10\" feature enters the top 8 features, replacing \"V4\":\n\nefs$feature_ranking(method = \"sav\", committee_size = 8)\n\n   feature     score norm_score borda_score\n    &lt;char&gt;     &lt;num&gt;      &lt;num&gt;       &lt;num&gt;\n1:     V11 15.353545  0.9100050   1.0000000\n2:     V12 14.107691  0.8361632   0.9830508\n3:     V16  7.698460  0.4562879   0.9661017\n4:     V45  6.811607  0.4037241   0.9491525\n5:      V9  6.443311  0.3818952   0.9322034\n6:     V36  6.060615  0.3592128   0.9152542\n7:     V10  5.955446  0.3529794   0.8983051\n8:     V49  4.741014  0.2810000   0.8813559"
  },
  {
    "objectID": "gallery/optimization/2023-01-16-hotstart/index.html",
    "href": "gallery/optimization/2023-01-16-hotstart/index.html",
    "title": "Hotstarting",
    "section": "",
    "text": "Scope\nHotstarting a learner resumes the training from an already fitted model. An example would be to train an already fit XGBoost model for an additional 500 boosting iterations. In mlr3, we call this process Hotstarting, where a learner has access to a cache of already trained models which is called a mlr3::HoststartStack We distinguish between forward and backward hotstarting. We start this post with backward hotstarting and then talk about the less efficient forward hotstarting.\n\n\nBackward Hotstarting\nIn this example, we optimize the hyperparameters of a random forest and use hotstarting to reduce the runtime. Hotstarting a random forest backwards is very simple. The model remains unchanged and only a subset of the trees is used for prediction i.e. a new model is not fitted. For example, a random forest is trained with 1000 trees and a specific hyperparameter configuration. If another random forest with 500 trees but with the same hyperparameter configuration has to be trained, the model with 1000 trees is copied and only 500 trees are used for prediction.\nWe load the ranger learner and set the search space from the Bischl et al. (2021) article.\n\nlibrary(mlr3verse)\n\nlearner = lrn(\"classif.ranger\",\n  mtry.ratio      = to_tune(0, 1),\n  replace         = to_tune(),\n  sample.fraction = to_tune(1e-1, 1),\n  num.trees       = to_tune(1, 2000)\n)\n\nWe activate hotstarting with the allow_hotstart option. When running a grid search with hotstarting, the grid is sorted by the hot start parameter. This means the models with 2000 trees are trained first. The models with less than 2000 trees hot start on the 2000 trees models which allows the training to be completed immediately.\n\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 5, batch_size = 5),\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  allow_hotstart = TRUE\n)\n\nFor comparison, we perform the same tuning without hotstarting.\n\ninstance_2 = tune(\n  tuner = tnr(\"grid_search\", resolution = 5, batch_size = 5),\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  allow_hotstart = FALSE\n)\n\nWe plot the time of completion of each batch (see Figure 1). Each batch includes 5 configurations. We can see that tuning with hotstarting is slower at first. As soon as all models are fitted with 2000 trees, the tuning runs much faster and overtakes the tuning without hotstarting.\n\n\n\n\n\n\n\n\nFigure 1: Time of completion of each batch with and without hotstarting.\n\n\n\n\n\n\n\nForward Hotstarting\nForward hotstarting is currently only supported by XGBoost. However, we have observed that hotstarting only provides a speed advantage for very large datasets and models with more than 5000 boosting rounds. The reason is that copying the models from the main process to the workers is a major bottleneck. The parallelization package future copies the models sequentially to the workers. Consequently, it takes a long time until the last worker can even start. Moreover, copying itself consumes a lot of time, and copying the model back from the worker blocks the main process again. During the development process, we overestimated the speed benefits of hotstarting and underestimated the overhead of parallelization. We can therefore only advise against using forward hotstarting during tuning. It is much more efficient to use the internal early-stopping mechanism of XGBoost. This eliminates the need to copy models to the worker. See the gallery post on early stopping for an example. We might improve the efficiency of the hotstarting mechanism in the future, if there are convincing use cases.\n\n\nManual Hotstarting\nNevertheless, forward hotstarting can be useful without parallelization. If you have an already trained model and want to add more boosting iteration to it. In this example, the learner_5000 is the already trained model. We create a new learner with the same hyperparameters but double the number of boosting iteration. To activate hotstarting, we create a HotstartStack and copy it to the $hotstart_stack slot of the new learner.\n\ntask = tsk(\"spam\")\n\nlearner_5000 = lrn(\"classif.xgboost\", nrounds = 5000, eta = 0.1)\nlearner_5000$train(task)\n\nlearner_10000 = lrn(\"classif.xgboost\", nrounds = 10000, eta = 0.1)\nlearner_10000$hotstart_stack = HotstartStack$new(learner_5000)\nlearner_10000$train(task)\n\nTraining the initial model took 59.885 seconds.\n\nlearner_5000$state$train_time\n\n[1] 59.885\n\n\nAdding 5000 boosting rounds took 46.837 seconds.\n\nlearner_10000$state$train_time - learner_5000$state$train_time\n\n[1] 46.837\n\n\nTraining the model from the beginning would have taken about two minutes. This means, without parallelization, we get the expected speed advantage.\n\n\nConclusion\nWe have seen how mlr3 enables to reduce the training time, by building on a hotstart stack of already trained learners. One has to be careful, however, when using forward hotstarting during tuning because of the high parallelization overhead that arises from copying the models between the processes. If a model has an internal early stopping implementation, it should usually be relied upon instead of using the mlr3 hotstarting mechanism. However, manual forward hotstarting can be helpful in some situations when we do not want to train a large model from the beginning.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   farver              2.1.2      2024-05-13 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future              1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2           * 3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools           0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   labeling            0.4.3      2023-08-29 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect         1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband       0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3misc            0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning          1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox             1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n   scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   viridisLite         0.4.2      2023-05-02 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\nReferences\n\nBischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2021. “Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges.” arXiv:2107.05847 [Cs, Stat], July. http://arxiv.org/abs/2107.05847."
  },
  {
    "objectID": "gallery/optimization/2023-02-01-shadow-variable-search/index.html",
    "href": "gallery/optimization/2023-02-01-shadow-variable-search/index.html",
    "title": "Shadow Variable Search on the Pima Indian Diabetes Data Set",
    "section": "",
    "text": "Scope\nFeature selection is the process of finding an optimal set of features to improve the performance, interpretability and robustness of machine learning algorithms. In this article, we introduce the Shadow Variable Search algorithm which is a wrapper method for feature selection. Wrapper methods iteratively add features to the model that optimize a performance measure. As an example, we will search for the optimal set of features for a support vector machine on the Pima Indian Diabetes data set. We assume that you are already familiar with the basic building blocks of the mlr3 ecosystem. If you are new to feature selection, we recommend reading the feature selection chapter of the mlr3book first. Some knowledge about mlr3pipelines is beneficial but not necessary to understand the example.\n\n\nShadow Variable Search\nAdding shadow variables to a data set is a well-known method in machine learning (Wu, Boos, and Stefanski 2007; Thomas et al. 2017). The idea is to add permutated copies of the original features to the data set. These permutated copies are called shadow variables or pseudovariables and the permutation breaks any relationship with the target variable, making them useless for prediction. The subsequent search is similar to the sequential forward selection algorithm, where one new feature is added in each iteration of the algorithm. This new feature is selected as the one that improves the performance of the model the most. This selection is computationally expensive, as one model for each of the not yet included features has to be trained. The difference between shadow variable search and sequential forward selection is that the former uses the selection of a shadow variable as the termination criterion. Selecting a shadow variable means that the best improvement is achieved by adding a feature that is unrelated to the target variable. Consequently, the variables not yet selected are most likely also correlated to the target variable only by chance. Therefore, only the previously selected features have a true influence on the target variable.\nmlr3fselect is the feature selection package of the mlr3 ecosystem. It implements the shadow variable search algorithm. We load all packages of the ecosystem with the mlr3verse package.\n\nlibrary(mlr3verse)\n\nWe retrieve the shadow variable search optimizer with the fs() function. The algorithm has no control parameters.\n\noptimizer = fs(\"shadow_variable_search\")\n\n\n\nTask and Learner\nThe objective of the Pima Indian Diabetes data set is to predict whether a person has diabetes or not. The data set includes 768 patients with 8 measurements (see Figure 1).\n\ntask = tsk(\"pima\")\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(data.table)\n\ndata = melt(as.data.table(task), id.vars = task$target_names, measure.vars = task$feature_names)\n\nggplot(data, aes(x = value, fill = diabetes)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ variable, ncol = 8, scales = \"free\") +\n  scale_fill_viridis_d(end = 0.8) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank())\n\n\n\n\n\n\n\n\nFigure 1: Distribution of the features in the Pima Indian Diabetes data set.\n\n\n\n\n\nThe data set contains missing values.\n\ntask$missings()\n\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure  triceps \n       0        0        5      374       11        0        0       35      227 \n\n\nSupport vector machines cannot handle missing values. We impute the missing values with the histogram imputation method.\n\nlearner = po(\"imputehist\") %&gt;&gt;% lrn(\"classif.svm\", predict_type = \"prob\")\n\n\n\nFeature Selection\nNow we define the feature selection problem by using the fsi() function that constructs an FSelectInstanceBatchSingleCrit. In addition to the task and learner, we have to select a resampling strategy and performance measure to determine how the performance of a feature subset is evaluated. We pass the \"none\" terminator because the shadow variable search algorithm terminates by itself.\n\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\")\n)\n\nWe are now ready to start the shadow variable search. To do this, we simply pass the instance to the $optimize() method of the optimizer.\n\noptimizer$optimize(instance)\n\n      age glucose insulin   mass pedigree pregnant pressure triceps                  features n_features classif.auc\n   &lt;lgcl&gt;  &lt;lgcl&gt;  &lt;lgcl&gt; &lt;lgcl&gt;   &lt;lgcl&gt;   &lt;lgcl&gt;   &lt;lgcl&gt;  &lt;lgcl&gt;                    &lt;list&gt;      &lt;int&gt;       &lt;num&gt;\n1:   TRUE    TRUE   FALSE   TRUE     TRUE    FALSE    FALSE   FALSE age,glucose,mass,pedigree          4    0.835165\n\n\nThe optimizer returns the best feature set and the corresponding estimated performance.\nFigure 2 shows the optimization path of the feature selection. The feature glucose was selected first and in the following iterations age, mass and pedigree. Then a shadow variable was selected and the feature selection was terminated.\n\n\nCode\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(mlr3misc)\nlibrary(viridisLite)\n\ndata = as.data.table(instance$archive)[order(-classif.auc), head(.SD, 1), by = batch_nr][order(batch_nr)]\ndata[, features := map_chr(features, str_collapse)]\ndata[, batch_nr := as.character(batch_nr)]\n\nggplot(data, aes(x = batch_nr, y = classif.auc)) +\n  geom_bar(\n    stat = \"identity\",\n    width = 0.5,\n    fill = viridis(1, begin = 0.5),\n    alpha = 0.8) +\n  geom_text(\n    data = data,\n    mapping = aes(x = batch_nr, y = 0, label = features),\n    hjust = 0,\n    nudge_y = 0.05,\n    color = \"white\",\n    size = 5\n    ) +\n  coord_flip() +\n  xlab(\"Iteration\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 2: Optimization path of the shadow variable search.\n\n\n\n\n\nThe archive contains all evaluated feature sets. We can see that each feature has a corresponding shadow variable. We only show the variables age, glucose and insulin and their shadow variables here.\n\nas.data.table(instance$archive)[, .(age, glucose, insulin, permuted__age, permuted__glucose, permuted__insulin, classif.auc)]\n\n       age glucose insulin permuted__age permuted__glucose permuted__insulin classif.auc\n    &lt;lgcl&gt;  &lt;lgcl&gt;  &lt;lgcl&gt;        &lt;lgcl&gt;            &lt;lgcl&gt;            &lt;lgcl&gt;       &lt;num&gt;\n 1:   TRUE   FALSE   FALSE         FALSE             FALSE             FALSE   0.6437052\n 2:  FALSE    TRUE   FALSE         FALSE             FALSE             FALSE   0.7598155\n 3:  FALSE   FALSE    TRUE         FALSE             FALSE             FALSE   0.4900280\n 4:  FALSE   FALSE   FALSE         FALSE             FALSE             FALSE   0.6424026\n 5:  FALSE   FALSE   FALSE         FALSE             FALSE             FALSE   0.5690107\n---                                                                                     \n54:   TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8266713\n55:   TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8063568\n56:   TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8244232\n57:   TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8234605\n58:   TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8164784\n\n\n\n\nFinal Model\nThe learner we use to make predictions on new data is called the final model. The final model is trained with the optimal feature set on the full data set. We subset the task to the optimal feature set and train the learner.\n\ntask$select(instance$result_feature_set)\nlearner$train(task)\n\nThe trained model can now be used to predict new, external data.\n\n\nConclusion\nThe shadow variable search is a fast feature selection method that is easy to use. More information on the theoretical background can be found in Wu, Boos, and Stefanski (2007) and Thomas et al. (2017). If you want to know more about feature selection in general, we recommend having a look at our book.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   e1071               1.7-16     2024-09-16 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   farver              2.1.2      2024-05-13 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future              1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2           * 3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools           0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   labeling            0.4.3      2023-08-29 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect         1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband       0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3measures        1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc          * 0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning          1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox             1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   proxy               0.4-27     2022-06-09 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n   scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   viridisLite       * 0.4.2      2023-05-02 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\nReferences\n\nThomas, Janek, Tobias Hepp, Andreas Mayr, and Bernd Bischl. 2017. “Probing for Sparse and Fast Variable Selection with Model-Based Boosting.” Computational and Mathematical Methods in Medicine 2017 (July): e1421409. https://doi.org/10.1155/2017/1421409.\n\n\nWu, Yujun, Dennis D Boos, and Leonard A Stefanski. 2007. “Controlling Variable Selection by the Addition of Pseudovariables.” Journal of the American Statistical Association 102 (477): 235–43. https://doi.org/10.1198/016214506000000843."
  },
  {
    "objectID": "gallery/optimization/2021-01-19-integer-hyperparameters-in-tuners-for-real-valued-search-spaces/index.html",
    "href": "gallery/optimization/2021-01-19-integer-hyperparameters-in-tuners-for-real-valued-search-spaces/index.html",
    "title": "Integer Hyperparameters in Tuners for Real-valued Search Spaces",
    "section": "",
    "text": "requireNamespace(\"kknn\")\n\nLoading required namespace: kknn"
  },
  {
    "objectID": "gallery/optimization/2021-01-19-integer-hyperparameters-in-tuners-for-real-valued-search-spaces/index.html#introduction",
    "href": "gallery/optimization/2021-01-19-integer-hyperparameters-in-tuners-for-real-valued-search-spaces/index.html#introduction",
    "title": "Integer Hyperparameters in Tuners for Real-valued Search Spaces",
    "section": "Introduction",
    "text": "Introduction\nTuner for real-valued search spaces are not able to tune on integer hyperparameters. However, it is possible to round the real values proposed by a Tuner to integers before passing them to the learner in the evaluation. We show how to apply a parameter transformation to a ParamSet and use this set in the tuning process.\nWe load the mlr3verse package which pulls in the most important packages for this example.\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/optimization/2021-01-19-integer-hyperparameters-in-tuners-for-real-valued-search-spaces/index.html#task-and-learner",
    "href": "gallery/optimization/2021-01-19-integer-hyperparameters-in-tuners-for-real-valued-search-spaces/index.html#task-and-learner",
    "title": "Integer Hyperparameters in Tuners for Real-valued Search Spaces",
    "section": "Task and Learner",
    "text": "Task and Learner\nIn this example, we use the k-Nearest-Neighbor classification learner. We want to tune the integer-valued hyperparameter k which defines the numbers of neighbors.\n\nlearner = lrn(\"classif.kknn\")\nprint(learner$param_set$params$k)\n\nNULL"
  },
  {
    "objectID": "gallery/optimization/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline/index.html",
    "href": "gallery/optimization/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline/index.html",
    "title": "Practical Tuning Series - Tune a Preprocessing Pipeline",
    "section": "",
    "text": "Scope\nThis is the second part of the practical tuning series. The other parts can be found here:\n\nPart I - Tune a Support Vector Machine\nPart III - Build an Automated Machine Learning System\nPart IV - Tuning and Parallel Processing\n\nIn this post, we build a simple preprocessing pipeline and tune it. For this, we are using the mlr3pipelines extension package. First, we start by imputing missing values in the Pima Indians Diabetes data set. After that, we encode a factor column to numerical dummy columns in the data set. Next, we combine both preprocessing steps to a Graph and create a GraphLearner. Finally, nested resampling is used to compare the performance of two imputation methods.\n\n\nPrerequisites\nWe load the mlr3verse package which pulls in the most important packages for this example.\n\nlibrary(mlr3verse)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented. The lgr package is used for logging in all mlr3 packages. The mlr3 logger prints the logging messages from the base package, whereas the bbotk logger is responsible for logging messages from the optimization packages (e.g. mlr3tuning ).\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nIn this example, we use the Pima Indians Diabetes data set which is used to predict whether or not a patient has diabetes. The patients are characterized by 8 numeric features of which some have missing values. We alter the data set by categorizing the feature pressure (blood pressure) into the categories \"low\", \"mid\", and \"high\".\n\n# retrieve the task from mlr3\ntask = tsk(\"pima\")\n\n# create data frame with categorized pressure feature\ndata = task$data(cols = \"pressure\")\nbreaks = quantile(data$pressure, probs = c(0, 0.33, 0.66, 1), na.rm = TRUE)\ndata$pressure = cut(data$pressure, breaks, labels = c(\"low\", \"mid\", \"high\"))\n\n# overwrite the feature in the task\ntask$cbind(data)\n\n# generate a quick textual overview\nskimr::skim(task$data())\n\n\nData summary\n\n\nName\ntask$data()\n\n\nNumber of rows\n768\n\n\nNumber of columns\n9\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndiabetes\n0\n1.00\nFALSE\n2\nneg: 500, pos: 268\n\n\npressure\n36\n0.95\nFALSE\n3\nlow: 282, mid: 245, hig: 205\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nage\n0\n1.00\n33.24\n11.76\n21.00\n24.00\n29.00\n41.00\n81.00\n▇▃▁▁▁\n\n\nglucose\n5\n0.99\n121.69\n30.54\n44.00\n99.00\n117.00\n141.00\n199.00\n▁▇▇▃▂\n\n\ninsulin\n374\n0.51\n155.55\n118.78\n14.00\n76.25\n125.00\n190.00\n846.00\n▇▂▁▁▁\n\n\nmass\n11\n0.99\n32.46\n6.92\n18.20\n27.50\n32.30\n36.60\n67.10\n▅▇▃▁▁\n\n\npedigree\n0\n1.00\n0.47\n0.33\n0.08\n0.24\n0.37\n0.63\n2.42\n▇▃▁▁▁\n\n\npregnant\n0\n1.00\n3.85\n3.37\n0.00\n1.00\n3.00\n6.00\n17.00\n▇▃▂▁▁\n\n\ntriceps\n227\n0.70\n29.15\n10.48\n7.00\n22.00\n29.00\n36.00\n99.00\n▆▇▁▁▁\n\n\n\n\n\nWe choose the xgboost algorithm from the xgboost package as learner.\n\nlearner = lrn(\"classif.xgboost\", nrounds = 100, id = \"xgboost\", verbose = 0)\n\n\n\nMissing Values\nThe task has missing data in five columns.\n\nround(task$missings() / task$nrow, 2)\n\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure  triceps \n    0.00     0.00     0.01     0.49     0.01     0.00     0.00     0.05     0.30 \n\n\nThe xgboost learner has an internal method for handling missing data but some learners cannot handle missing values. We will try to beat the internal method in terms of predictive performance. The mlr3pipelines package offers various methods to impute missing values.\n\nmlr_pipeops$keys(\"^impute\")\n\n[1] \"imputeconstant\" \"imputehist\"     \"imputelearner\"  \"imputemean\"     \"imputemedian\"   \"imputemode\"    \n[7] \"imputeoor\"      \"imputesample\"  \n\n\nWe choose the PipeOpImputeOOR that adds the new factor level \".MISSING\". to factorial features and imputes numerical features by constant values shifted below the minimum (default) or above the maximum.\n\nimputer = po(\"imputeoor\")\nprint(imputer)\n\nPipeOp: &lt;imputeoor&gt; (not trained)\nvalues: &lt;min=TRUE, offset=1, multiplier=1&gt;\nInput channels &lt;name [train type, predict type]&gt;:\n  input [Task,Task]\nOutput channels &lt;name [train type, predict type]&gt;:\n  output [Task,Task]\n\n\nAs the output suggests, the in- and output of this pipe operator is a Task for both the training and the predict step. We can manually train the pipe operator to check its functionality:\n\ntask_imputed = imputer$train(list(task))[[1]]\ntask_imputed$missings()\n\ndiabetes      age pedigree pregnant  glucose  insulin     mass pressure  triceps \n       0        0        0        0        0        0        0        0        0 \n\n\nLet’s compare an observation with missing values to the observation with imputed observation.\n\nrbind(\n  task$data()[8,],\n  task_imputed$data()[8,]\n)\n\n   diabetes   age glucose insulin  mass pedigree pregnant pressure triceps\n     &lt;fctr&gt; &lt;num&gt;   &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;   &lt;fctr&gt;   &lt;num&gt;\n1:      neg    29     115      NA  35.3    0.134       10     &lt;NA&gt;      NA\n2:      neg    29     115    -819  35.3    0.134       10 .MISSING     -86\n\n\nNote that OOR imputation is in particular useful for tree-based models, but should not be used for linear models or distance-based models.\n\n\nFactor Encoding\nThe xgboost learner cannot handle categorical features. Therefore, we must to convert factor columns to numerical dummy columns. For this, we argument the xgboost learner with automatic factor encoding.\nThe PipeOpEncode encodes factor columns with one of six methods. In this example, we use one-hot encoding which creates a new binary column for each factor level.\n\nfactor_encoding = po(\"encode\", method = \"one-hot\")\n\nWe manually trigger the encoding on the task.\n\nfactor_encoding$train(list(task))\n\n$output\n&lt;TaskClassif:pima&gt; (768 x 11): Pima Indian Diabetes\n* Target: diabetes\n* Properties: twoclass\n* Features (10):\n  - dbl (10): age, glucose, insulin, mass, pedigree, pregnant, pressure.high, pressure.low, pressure.mid,\n    triceps\n\n\nThe factor column pressure has been converted to the three binary columns \"pressure.low\", \"pressure.mid\", and \"pressure.high\".\n\n\nConstructing the Pipeline\nWe created two preprocessing steps which could be used to create a new task with encoded factor variables and imputed missing values. However, if we do this before resampling, information from the test can leak into our training step which typically leads to overoptimistic performance measures. To avoid this, we add the preprocessing steps to the Learner itself, creating a GraphLearner. For this, we create a Graph first.\n\ngraph = po(\"encode\") %&gt;&gt;%\n  po(\"imputeoor\") %&gt;&gt;%\n  learner\nplot(graph, html = FALSE)\n\n\n\n\n\n\n\n\nWe use as_learner() to wrap the Graph into a GraphLearner with which allows us to use the graph like a normal learner.\n\ngraph_learner = as_learner(graph)\n\n# short learner id for printing\ngraph_learner$id = \"graph_learner\"\n\nThe GraphLearner can be trained and used for making predictions. Instead of calling $train() or $predict() manually, we will directly use it for resampling. We choose a 3-fold cross-validation as the resampling strategy.\n\nresampling = rsmp(\"cv\", folds = 3)\n\nrr = resample(task = task, learner = graph_learner, resampling = resampling)\n\n\nrr$score()[, c(\"iteration\", \"task_id\", \"learner_id\", \"resampling_id\", \"classif.ce\"), with = FALSE]\n\n   iteration task_id    learner_id resampling_id classif.ce\n       &lt;int&gt;  &lt;char&gt;        &lt;char&gt;        &lt;char&gt;      &lt;num&gt;\n1:         1    pima graph_learner            cv  0.2851562\n2:         2    pima graph_learner            cv  0.2460938\n3:         3    pima graph_learner            cv  0.2968750\n\n\nFor each resampling iteration, the following steps are performed:\n\nThe task is subsetted to the training indices.\nThe factor encoder replaces factor features with dummy columns in the training task.\nThe OOR imputer determines values to impute from the training task and then replaces all missing values with learned imputation values.\nThe learner is applied on the modified training task and the model is stored inside the learner.\n\nNext is the predict step:\n\nThe task is subsetted to the test indices.\nThe factor encoder replaces all factor features with dummy columns in the test task.\nThe OOR imputer replaces all missing values of the test task with the imputation values learned on the training set.\nThe learner’s predict method is applied on the modified test task.\n\nBy following this procedure, it is guaranteed that no information can leak from the training step to the predict step.\n\n\nTuning the Pipeline\nLet’s have a look at the parameter set of the GraphLearner. It consists of the xgboost hyperparameters, and additionally, the parameter of the PipeOp encode and imputeoor. All hyperparameters are prefixed with the id of the respective PipeOp or learner.\n\nas.data.table(graph_learner$param_set)[, c(\"id\", \"class\", \"lower\", \"upper\", \"nlevels\"), with = FALSE]\n\n                                     id    class lower upper nlevels\n                                 &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n 1:                       encode.method ParamFct    NA    NA       5\n 2:               encode.affect_columns ParamUty    NA    NA     Inf\n 3:                       imputeoor.min ParamLgl    NA    NA       2\n 4:                    imputeoor.offset ParamDbl     0   Inf     Inf\n 5:                imputeoor.multiplier ParamDbl     0   Inf     Inf\n 6:            imputeoor.affect_columns ParamUty    NA    NA     Inf\n 7:                       xgboost.alpha ParamDbl     0   Inf     Inf\n 8:               xgboost.approxcontrib ParamLgl    NA    NA       2\n 9:                  xgboost.base_score ParamDbl  -Inf   Inf     Inf\n10:                     xgboost.booster ParamFct    NA    NA       3\n11:                   xgboost.callbacks ParamUty    NA    NA     Inf\n12:           xgboost.colsample_bylevel ParamDbl     0     1     Inf\n13:            xgboost.colsample_bynode ParamDbl     0     1     Inf\n14:            xgboost.colsample_bytree ParamDbl     0     1     Inf\n15:                      xgboost.device ParamUty    NA    NA     Inf\n16: xgboost.disable_default_eval_metric ParamLgl    NA    NA       2\n17:       xgboost.early_stopping_rounds ParamInt     1   Inf     Inf\n18:                         xgboost.eta ParamDbl     0     1     Inf\n19:                 xgboost.eval_metric ParamUty    NA    NA     Inf\n20:            xgboost.feature_selector ParamFct    NA    NA       5\n21:                       xgboost.feval ParamUty    NA    NA     Inf\n22:                       xgboost.gamma ParamDbl     0   Inf     Inf\n23:                 xgboost.grow_policy ParamFct    NA    NA       2\n24:     xgboost.interaction_constraints ParamUty    NA    NA     Inf\n25:              xgboost.iterationrange ParamUty    NA    NA     Inf\n26:                      xgboost.lambda ParamDbl     0   Inf     Inf\n27:                 xgboost.lambda_bias ParamDbl     0   Inf     Inf\n28:                     xgboost.max_bin ParamInt     2   Inf     Inf\n29:              xgboost.max_delta_step ParamDbl     0   Inf     Inf\n30:                   xgboost.max_depth ParamInt     0   Inf     Inf\n31:                  xgboost.max_leaves ParamInt     0   Inf     Inf\n32:                    xgboost.maximize ParamLgl    NA    NA       2\n33:            xgboost.min_child_weight ParamDbl     0   Inf     Inf\n34:                     xgboost.missing ParamDbl  -Inf   Inf     Inf\n35:        xgboost.monotone_constraints ParamUty    NA    NA     Inf\n36:                     xgboost.nrounds ParamInt     1   Inf     Inf\n37:              xgboost.normalize_type ParamFct    NA    NA       2\n38:                     xgboost.nthread ParamInt     1   Inf     Inf\n39:                  xgboost.ntreelimit ParamInt     1   Inf     Inf\n40:           xgboost.num_parallel_tree ParamInt     1   Inf     Inf\n41:                   xgboost.objective ParamUty    NA    NA     Inf\n42:                    xgboost.one_drop ParamLgl    NA    NA       2\n43:                xgboost.outputmargin ParamLgl    NA    NA       2\n44:                 xgboost.predcontrib ParamLgl    NA    NA       2\n45:             xgboost.predinteraction ParamLgl    NA    NA       2\n46:                    xgboost.predleaf ParamLgl    NA    NA       2\n47:               xgboost.print_every_n ParamInt     1   Inf     Inf\n48:                xgboost.process_type ParamFct    NA    NA       2\n49:                   xgboost.rate_drop ParamDbl     0     1     Inf\n50:                xgboost.refresh_leaf ParamLgl    NA    NA       2\n51:                     xgboost.reshape ParamLgl    NA    NA       2\n52:          xgboost.seed_per_iteration ParamLgl    NA    NA       2\n53:             xgboost.sampling_method ParamFct    NA    NA       2\n54:                 xgboost.sample_type ParamFct    NA    NA       2\n55:                   xgboost.save_name ParamUty    NA    NA     Inf\n56:                 xgboost.save_period ParamInt     0   Inf     Inf\n57:            xgboost.scale_pos_weight ParamDbl  -Inf   Inf     Inf\n58:                   xgboost.skip_drop ParamDbl     0     1     Inf\n59:                xgboost.strict_shape ParamLgl    NA    NA       2\n60:                   xgboost.subsample ParamDbl     0     1     Inf\n61:                       xgboost.top_k ParamInt     0   Inf     Inf\n62:                    xgboost.training ParamLgl    NA    NA       2\n63:                 xgboost.tree_method ParamFct    NA    NA       5\n64:      xgboost.tweedie_variance_power ParamDbl     1     2     Inf\n65:                     xgboost.updater ParamUty    NA    NA     Inf\n66:                     xgboost.verbose ParamInt     0     2       3\n67:                   xgboost.watchlist ParamUty    NA    NA     Inf\n68:                   xgboost.xgb_model ParamUty    NA    NA     Inf\n                                     id    class lower upper nlevels\n\n\nWe will tune the encode method.\n\ngraph_learner$param_set$values$encode.method = to_tune(c(\"one-hot\", \"treatment\"))\n\nWe define a tuning instance and use grid search since we want to try all encode methods.\n\ninstance = tune(\n  tuner = tnr(\"grid_search\"),\n  task = task,\n  learner = graph_learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\")\n)\n\nThe archive shows us the performance of the model with different encoding methods.\n\nprint(instance$archive)\n\n&lt;ArchiveBatchTuning&gt; with 2 evaluations\n   encode.method classif.ce warnings errors batch_nr\n          &lt;char&gt;      &lt;num&gt;    &lt;int&gt;  &lt;int&gt;    &lt;int&gt;\n1:       one-hot       0.26        0      0        1\n2:     treatment       0.25        0      0        2\n   encode.method classif.ce x_domain_encode.method warnings errors batch_nr\n          &lt;char&gt;      &lt;num&gt;                 &lt;char&gt;    &lt;int&gt;  &lt;int&gt;    &lt;int&gt;\n1:       one-hot       0.26                one-hot        0      0        1\n2:     treatment       0.25              treatment        0      0        2\n\n\n\n\nNested Resampling\nWe create one GraphLearner with imputeoor and test it against a GraphLearner that uses the internal imputation method of xgboost. Applying nested resampling ensures a fair comparison of the predictive performances.\n\ngraph_1 = po(\"encode\") %&gt;&gt;%\n  learner\ngraph_learner_1 = GraphLearner$new(graph_1)\n\ngraph_learner_1$param_set$values$encode.method = to_tune(c(\"one-hot\", \"treatment\"))\n\nat_1 = auto_tuner(\n  learner = graph_learner_1,\n  resampling = resampling,\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"none\"),\n  tuner = tnr(\"grid_search\"),\n  store_models = TRUE\n)\n\n\ngraph_2 = po(\"encode\") %&gt;&gt;%\n  po(\"imputeoor\") %&gt;&gt;%\n  learner\ngraph_learner_2 = GraphLearner$new(graph_2)\n\ngraph_learner_2$param_set$values$encode.method = to_tune(c(\"one-hot\", \"treatment\"))\n\nat_2 = auto_tuner(\n  learner = graph_learner_2,\n  resampling = resampling,\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"none\"),\n  tuner = tnr(\"grid_search\"),\n  store_models = TRUE\n)\n\nWe run the benchmark.\n\nresampling_outer = rsmp(\"cv\", folds = 3)\ndesign = benchmark_grid(task, list(at_1, at_2), resampling_outer)\n\nbmr = benchmark(design, store_models = TRUE)\n\nWe compare the aggregated performances on the outer test sets which give us an unbiased performance estimate of the GraphLearners with the different encoding methods.\n\nbmr$aggregate()\n\n      nr task_id                     learner_id resampling_id iters classif.ce\n   &lt;int&gt;  &lt;char&gt;                         &lt;char&gt;        &lt;char&gt; &lt;int&gt;      &lt;num&gt;\n1:     1    pima           encode.xgboost.tuned            cv     3  0.2669271\n2:     2    pima encode.imputeoor.xgboost.tuned            cv     3  0.2903646\nHidden columns: resample_result\n\nautoplot(bmr)\n\n\n\n\n\n\n\n\nNote that in practice, it is required to tune preprocessing hyperparameters jointly with the hyperparameters of the learner. Otherwise, comparing preprocessing steps is not feasible and can lead to wrong conclusions.\nApplying nested resampling can be shortened by using the auto_tuner()-shortcut.\n\ngraph_1 = po(\"encode\") %&gt;&gt;% learner\ngraph_learner_1 = as_learner(graph_1)\ngraph_learner_1$param_set$values$encode.method = to_tune(c(\"one-hot\", \"treatment\"))\n\nat_1 = auto_tuner(\n  method = \"grid_search\",\n  learner = graph_learner_1,\n  resampling = resampling,\n  measure = msr(\"classif.ce\"),\n  store_models = TRUE)\n\ngraph_2 = po(\"encode\") %&gt;&gt;% po(\"imputeoor\") %&gt;&gt;% learner\ngraph_learner_2 = as_learner(graph_2)\ngraph_learner_2$param_set$values$encode.method = to_tune(c(\"one-hot\", \"treatment\"))\n\nat_2 = auto_tuner(\n  method = \"grid_search\",\n  learner = graph_learner_2,\n  resampling = resampling,\n  measure = msr(\"classif.ce\"),\n  store_models = TRUE)\n\ndesign = benchmark_grid(task, list(at_1, at_2), rsmp(\"cv\", folds = 3))\n\nbmr = benchmark(design, store_models = TRUE)\n\n\n\nFinal Model\nWe train the chosen GraphLearner with the AutoTuner to get a final model with optimized hyperparameters.\n\nat_2$train(task)\n\nThe trained model can now be used to make predictions on new data at_2$predict(). The pipeline ensures that the preprocessing is always a part of the train and predict step.\n\n\nResources\nThe mlr3book includes chapters on pipelines and hyperparameter tuning. The mlr3cheatsheets contain frequently used commands and workflows of mlr3.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   base64enc           0.1-3      2015-07-28 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   farver              2.1.2      2024-05-13 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future              1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2             3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools           0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   igraph              2.0.3      2024-03-13 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   labeling            0.4.3      2023-08-29 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n P Matrix              1.7-0      2024-04-26 [?] CRAN (R 4.4.0)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect       * 1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband       0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3measures        1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc            0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning          1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox             1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   purrr               1.0.2      2023-08-10 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   repr                1.1.7      2024-03-22 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n   scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   skimr               2.1.5      2022-12-23 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   stringr             1.5.1      2023-11-14 [1] CRAN (R 4.4.1)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyr               1.3.1      2024-01-24 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   viridisLite         0.4.2      2023-05-02 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   xgboost             1.7.8.1    2024-07-24 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "gallery/optimization/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/index.html",
    "href": "gallery/optimization/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/index.html",
    "title": "Practical Tuning Series - Tuning and Parallel Processing",
    "section": "",
    "text": "Scope\nThis is the fourth part of the practical tuning series. The other parts can be found here:\n\nPart I - Tune a Support Vector Machine\nPart II - Tune a Preprocessing Pipeline\nPart III - Build an Automated Machine Learning System\n\nIn this post, we teach how to run various jobs in mlr3 in parallel. The goal is to map computational jobs (e.g. evaluation of one configuration) to a pool of workers (usually physical CPU cores, sometimes remote computational nodes) to reduce the run time needed for tuning.\n\n\nPrerequisites\nWe load the mlr3verse package which pulls in the most important packages for this example. Additionally, make sure you have installed the packages future and future.apply.\n\nlibrary(mlr3verse)\n\nWe decrease the verbosity of the logger to keep the output clearly represented. The lgr package is used for logging in all mlr3 packages. The mlr3 logger prints the logging messages from the base package, whereas the bbotk logger is responsible for logging messages from the optimization packages (e.g. mlr3tuning ).\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\n\n\nParallel Backend\nThe workers are specified by the parallel backend which orchestrates starting up, shutting down, and communication with the workers. On a single machine, multisession and multicore are common backends. The multisession backend spawns new background R processes. It is available on all platforms.\n\nfuture::plan(\"multisession\")\n\nThe multicore backend uses forked R processes which allows the workers to access R objects in a shared memory. This reduces the overhead since R objects are only copied in memory if they are modified. Unfortunately, forking processes is not supported on Windows and when running R from within RStudio.\n\nfuture::plan(\"multicore\")\n\nBoth backends support the workers argument that specifies the number of used cores.\nUse this code if your code should run with the multicore backend when possible.\n\nif (future::supportsMulticore()) {\n  future::plan(future::multicore)\n} else {\n  future::plan(future::multisession)\n}\n\n\n\nResampling\nThe resample() and benchmark() functions in mlr3 can be executed in parallel. The parallelization is triggered by simply declaring a plan via future::plan().\n\nfuture::plan(\"multisession\")\n\ntask = tsk(\"pima\")\nlearner = lrn(\"classif.rpart\") # classification tree\nresampling = rsmp(\"cv\", folds = 3)\n\nresample(task, learner, resampling)\n\n&lt;ResampleResult&gt; with 3 resampling iterations\n task_id    learner_id resampling_id iteration     prediction_test warnings errors\n    pima classif.rpart            cv         1 &lt;PredictionClassif&gt;        0      0\n    pima classif.rpart            cv         2 &lt;PredictionClassif&gt;        0      0\n    pima classif.rpart            cv         3 &lt;PredictionClassif&gt;        0      0\n\n\nThe 3-fold cross-validation gives us 3 jobs since each resampling iteration is executed in parallel.\nThe benchmark() function accepts a design of experiments as input where each experiment is defined as a combination of a task, a learner, and a resampling strategy. For each experiment, resampling is performed. The nested loop over experiments and resampling iterations is flattened so that all resampling iterations of all experiments can be executed in parallel.\n\nfuture::plan(\"multisession\")\n\ntasks = list(tsk(\"pima\"), tsk(\"iris\"))\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\n\ngrid = benchmark_grid(tasks, learner, resampling)\n\nbenchmark(grid)\n\n&lt;BenchmarkResult&gt; of 6 rows with 2 resampling runs\n nr task_id    learner_id resampling_id iters warnings errors\n  1    pima classif.rpart            cv     3        0      0\n  2    iris classif.rpart            cv     3        0      0\n\n\nThe 2 experiments and the 3-fold cross-validation result in 6 jobs which are executed in parallel.\n\n\nTuning\nThe mlr3tuning package internally calls benchmark() during tuning. If the tuner is capable of suggesting multiple configurations per iteration (such as random search, grid search, or hyperband), these configurations represent individual experiments, and the loop flattening of benchmark() is triggered. E.g., all resampling iterations of all hyperparameter configurations on a grid can be executed in parallel.\n\nfuture::plan(\"multisession\")\n\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$values$cp = to_tune(0.001, 0.1)\nlearner$param_set$values$minsplit = to_tune(1, 10)\n\ninstance = tune(\n  tuner = tnr(\"random_search\", batch_size = 5), # random search suggests 5 configurations per batch\n  task = tsk(\"pima\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10\n)\n\nThe batch size of 5 and the 3-fold cross-validation gives us 15 jobs. This is done twice because of the limit of 10 evaluations in total.\n\n\nNested Resampling\nNested resampling results in two nested resampling loops. For this, an AutoTuner is passed to resample() or benchmark(). We can choose different parallelization backends for the inner and outer resampling loop, respectively. We just have to pass a list of backends.\n\n# Runs the outer loop in parallel and the inner loop sequentially\nfuture::plan(list(\"multisession\", \"sequential\"))\n\n# Runs the outer loop sequentially and the inner loop in parallel\nfuture::plan(list(\"sequential\", \"multisession\"))\n\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$values$cp = to_tune(0.001, 0.1)\nlearner$param_set$values$minsplit = to_tune(1, 10)\n\nrr = tune_nested(\n  tuner = tnr(\"random_search\", batch_size = 5), # random search suggests 5 configurations per batch\n  task = tsk(\"pima\"),\n  learner = learner,\n  inner_resampling = rsmp (\"cv\", folds = 3),\n  outer_resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10\n)\n\nWhile nesting real parallelization backends is often unintended and causes unnecessary overhead, it is useful in some distributed computing setups. It can be achieved with future by forcing a fixed number of workers for each loop.\n\n# Runs both loops in parallel\nfuture::plan(list(future::tweak(\"multisession\", workers = 2),\n                  future::tweak(\"multisession\", workers = 4)))\n\nThis example would run on 8 cores (= 2 * 4) on the local machine.\n\n\nResources\nThe mlr3book includes a chapters on parallelization. The mlr3cheatsheets contain frequently used commands and workflows of mlr3.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future            * 1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2             3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools           0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect       * 1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband       0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3measures        1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc            0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning          1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox             1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n P rpart               4.1.23     2023-12-05 [?] CRAN (R 4.4.0)\n   scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "gallery/optimization/2021-03-11-practical-tuning-series-build-an-automated-machine-learning-system/index.html",
    "href": "gallery/optimization/2021-03-11-practical-tuning-series-build-an-automated-machine-learning-system/index.html",
    "title": "Practical Tuning Series - Build an Automated Machine Learning System",
    "section": "",
    "text": "Scope\nThis is the third part of the practical tuning series. The other parts can be found here:\n\nPart I - Tune a Support Vector Machine\nPart II - Tune a Preprocessing Pipeline\nPart IV - Tuning and Parallel Processing\n\nIn this post, we implement a simple automated machine learning (AutoML) system which includes preprocessing, a switch between multiple learners and hyperparameter tuning. For this, we build a pipeline with the mlr3pipelines extension package. Additionally, we use nested resampling to get an unbiased performance estimate of our AutoML system.\n\n\nPrerequisites\nWe load the mlr3verse package which pulls in the most important packages for this example.\n\nlibrary(mlr3verse)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented. The lgr package is used for logging in all mlr3 packages. The mlr3 logger prints the logging messages from the base package, whereas the bbotk logger is responsible for logging messages from the optimization packages (e.g. mlr3tuning ).\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nIn this example, we use the Pima Indians Diabetes data set which is used to to predict whether or not a patient has diabetes. The patients are characterized by 8 numeric features and some have missing values.\n\ntask = tsk(\"pima\")\n\n\n\nBranching\nWe use three popular machine learning algorithms: k-nearest-neighbors, support vector machines and random forests.\n\nlearners = list(\n  lrn(\"classif.kknn\", id = \"kknn\"),\n  lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\"),\n  lrn(\"classif.ranger\", id = \"ranger\")\n)\n\nThe PipeOpBranch allows us to specify multiple alternatives paths. In this graph, the paths lead to the different learner models. The selection hyperparameter controls which path is executed i.e., which learner is used to fit a model. It is important to use the PipeOpBranch after the branching so that the outputs are merged into one result object. We visualize the graph with branching below.\n\ngraph =\n  po(\"branch\", options = c(\"kknn\", \"svm\", \"ranger\")) %&gt;&gt;%\n  gunion(lapply(learners, po)) %&gt;&gt;%\n  po(\"unbranch\")\ngraph$plot(html = FALSE)\n\n\n\n\n\n\n\n\nAlternatively, we can use the ppl()-shortcut to load a predefined graph from the mlr_graphs dictionary. For this, the learner list must be named.\n\nlearners = list(\n  kknn = lrn(\"classif.kknn\", id = \"kknn\"),\n  svm = lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\"),\n  ranger = lrn(\"classif.ranger\", id = \"ranger\")\n)\n\ngraph = ppl(\"branch\", lapply(learners, po))\n\n\n\nPreprocessing\nThe task has missing data in five columns.\n\nround(task$missings() / task$nrow, 2)\n\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure  triceps \n    0.00     0.00     0.01     0.49     0.01     0.00     0.00     0.05     0.30 \n\n\nThe pipeline \"robustify\" function creates a preprocessing pipeline based on our task. The resulting pipeline imputes missing values with PipeOpImputeHist and creates a dummy column (PipeOpMissInd) which indicates the imputed missing values. Internally, this creates two paths and the results are combined with PipeOpFeatureUnion. In contrast to PipeOpBranch, both paths are executed. Additionally, \"robustify\" adds PipeOpEncode to encode factor columns and PipeOpRemoveConstants to remove features with a constant value.\n\ngraph = ppl(\"robustify\", task = task, factors_to_numeric = TRUE) %&gt;&gt;%\n  graph\nplot(graph, html = FALSE)\n\n\n\n\n\n\n\n\nWe could also create the preprocessing pipeline manually.\n\ngunion(list(po(\"imputehist\"),\n  po(\"missind\", affect_columns = selector_type(c(\"numeric\", \"integer\"))))) %&gt;&gt;%\n  po(\"featureunion\") %&gt;&gt;%\n  po(\"encode\") %&gt;&gt;%\n  po(\"removeconstants\")\n\nGraph with 5 PipeOps:\n              ID         State        sccssors          prdcssors\n          &lt;char&gt;        &lt;char&gt;          &lt;char&gt;             &lt;char&gt;\n      imputehist &lt;&lt;UNTRAINED&gt;&gt;    featureunion                   \n         missind &lt;&lt;UNTRAINED&gt;&gt;    featureunion                   \n    featureunion &lt;&lt;UNTRAINED&gt;&gt;          encode imputehist,missind\n          encode &lt;&lt;UNTRAINED&gt;&gt; removeconstants       featureunion\n removeconstants &lt;&lt;UNTRAINED&gt;&gt;                             encode\n\n\n\n\nGraph Learner\nWe use as_learner() to create a GraphLearner which encapsulates the pipeline and can be used like a learner.\n\ngraph_learner = as_learner(graph)\n\nThe parameter set of the graph learner includes all hyperparameters from all contained learners. The hyperparameter ids are prefixed with the corresponding learner ids. The hyperparameter branch.selection controls which learner is used.\n\nas.data.table(graph_learner$param_set)[, .(id, class, lower, upper, nlevels)]\n\n                                              id    class lower upper nlevels\n                                          &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n 1:           removeconstants_prerobustify.ratio ParamDbl     0     1     Inf\n 2:         removeconstants_prerobustify.rel_tol ParamDbl     0   Inf     Inf\n 3:         removeconstants_prerobustify.abs_tol ParamDbl     0   Inf     Inf\n 4:       removeconstants_prerobustify.na_ignore ParamLgl    NA    NA       2\n 5:  removeconstants_prerobustify.affect_columns ParamUty    NA    NA     Inf\n 6:                    imputehist.affect_columns ParamUty    NA    NA     Inf\n 7:                                missind.which ParamFct    NA    NA       2\n 8:                                 missind.type ParamFct    NA    NA       4\n 9:                       missind.affect_columns ParamUty    NA    NA     Inf\n10:                  imputesample.affect_columns ParamUty    NA    NA     Inf\n11:                                encode.method ParamFct    NA    NA       5\n12:                        encode.affect_columns ParamUty    NA    NA     Inf\n13:          removeconstants_postrobustify.ratio ParamDbl     0     1     Inf\n14:        removeconstants_postrobustify.rel_tol ParamDbl     0   Inf     Inf\n15:        removeconstants_postrobustify.abs_tol ParamDbl     0   Inf     Inf\n16:      removeconstants_postrobustify.na_ignore ParamLgl    NA    NA       2\n17: removeconstants_postrobustify.affect_columns ParamUty    NA    NA     Inf\n18:                                       kknn.k ParamInt     1   Inf     Inf\n19:                                kknn.distance ParamDbl     0   Inf     Inf\n20:                                  kknn.kernel ParamFct    NA    NA      10\n21:                                   kknn.scale ParamLgl    NA    NA       2\n22:                                 kknn.ykernel ParamUty    NA    NA     Inf\n23:                             kknn.store_model ParamLgl    NA    NA       2\n24:                                svm.cachesize ParamDbl  -Inf   Inf     Inf\n25:                            svm.class.weights ParamUty    NA    NA     Inf\n26:                                    svm.coef0 ParamDbl  -Inf   Inf     Inf\n27:                                     svm.cost ParamDbl     0   Inf     Inf\n28:                                    svm.cross ParamInt     0   Inf     Inf\n29:                          svm.decision.values ParamLgl    NA    NA       2\n30:                                   svm.degree ParamInt     1   Inf     Inf\n31:                                  svm.epsilon ParamDbl     0   Inf     Inf\n32:                                   svm.fitted ParamLgl    NA    NA       2\n33:                                    svm.gamma ParamDbl     0   Inf     Inf\n34:                                   svm.kernel ParamFct    NA    NA       4\n35:                                       svm.nu ParamDbl  -Inf   Inf     Inf\n36:                                    svm.scale ParamUty    NA    NA     Inf\n37:                                svm.shrinking ParamLgl    NA    NA       2\n38:                                svm.tolerance ParamDbl     0   Inf     Inf\n39:                                     svm.type ParamFct    NA    NA       2\n40:                                 ranger.alpha ParamDbl  -Inf   Inf     Inf\n41:                ranger.always.split.variables ParamUty    NA    NA     Inf\n42:                         ranger.class.weights ParamUty    NA    NA     Inf\n43:                               ranger.holdout ParamLgl    NA    NA       2\n44:                            ranger.importance ParamFct    NA    NA       4\n45:                            ranger.keep.inbag ParamLgl    NA    NA       2\n46:                             ranger.max.depth ParamInt     0   Inf     Inf\n47:                            ranger.min.bucket ParamInt     1   Inf     Inf\n48:                         ranger.min.node.size ParamInt     1   Inf     Inf\n49:                               ranger.minprop ParamDbl  -Inf   Inf     Inf\n50:                                  ranger.mtry ParamInt     1   Inf     Inf\n51:                            ranger.mtry.ratio ParamDbl     0     1     Inf\n52:                     ranger.num.random.splits ParamInt     1   Inf     Inf\n53:                            ranger.node.stats ParamLgl    NA    NA       2\n54:                           ranger.num.threads ParamInt     1   Inf     Inf\n55:                             ranger.num.trees ParamInt     1   Inf     Inf\n56:                             ranger.oob.error ParamLgl    NA    NA       2\n57:                 ranger.regularization.factor ParamUty    NA    NA     Inf\n58:               ranger.regularization.usedepth ParamLgl    NA    NA       2\n59:                               ranger.replace ParamLgl    NA    NA       2\n60:             ranger.respect.unordered.factors ParamFct    NA    NA       3\n61:                       ranger.sample.fraction ParamDbl     0     1     Inf\n62:                           ranger.save.memory ParamLgl    NA    NA       2\n63:          ranger.scale.permutation.importance ParamLgl    NA    NA       2\n64:                             ranger.se.method ParamFct    NA    NA       2\n65:                                  ranger.seed ParamInt  -Inf   Inf     Inf\n66:                  ranger.split.select.weights ParamUty    NA    NA     Inf\n67:                             ranger.splitrule ParamFct    NA    NA       3\n68:                               ranger.verbose ParamLgl    NA    NA       2\n69:                          ranger.write.forest ParamLgl    NA    NA       2\n70:                             branch.selection ParamFct    NA    NA       3\n                                              id    class lower upper nlevels\n\n\n\n\nTune the pipeline\nWe will only tune one hyperparameter for each learner in this example. Additionally, we tune the branching parameter which selects one of the three learners. We have to specify that a hyperparameter is only valid for a certain learner by using depends = branch.selection == &lt;learner_id&gt;.\n\n# branch\ngraph_learner$param_set$values$branch.selection =\n  to_tune(c(\"kknn\", \"svm\", \"ranger\"))\n\n# kknn\ngraph_learner$param_set$values$kknn.k =\n  to_tune(p_int(3, 50, logscale = TRUE, depends = branch.selection == \"kknn\"))\n\n# svm\ngraph_learner$param_set$values$svm.cost =\n  to_tune(p_dbl(-1, 1, trafo = function(x) 10^x, depends = branch.selection == \"svm\"))\n\n# ranger\ngraph_learner$param_set$values$ranger.mtry =\n  to_tune(p_int(1, 8, depends = branch.selection == \"ranger\"))\n\n# short learner id for printing\ngraph_learner$id = \"graph_learner\"\n\nWe define a tuning instance and select a random search which is stopped after 20 evaluated configurations.\n\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = task,\n  learner = graph_learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 20\n)\n\nThe following shows a quick way to visualize the tuning results.\n\nautoplot(instance, type = \"marginal\",\n  cols_x = c(\"x_domain_kknn.k\", \"x_domain_svm.cost\", \"ranger.mtry\"))\n\n\n\n\n\n\n\n\n\n\nFinal Model\nWe add the optimized hyperparameters to the graph learner and train the learner on the full dataset.\n\nlearner = as_learner(graph)\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n\nThe trained model can now be used to make predictions on new data. A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (instance$result_y) as the model’s performance. Instead we have to use nested resampling to get an unbiased performance estimate.\n\n\nNested Resampling\nWe use nested resampling to get an unbiased estimate of the predictive performance of our graph learner.\n\ngraph_learner = as_learner(graph)\ngraph_learner$param_set$values$branch.selection =\n  to_tune(c(\"kknn\", \"svm\", \"ranger\"))\ngraph_learner$param_set$values$kknn.k =\n  to_tune(p_int(3, 50, logscale = TRUE, depends = branch.selection == \"kknn\"))\ngraph_learner$param_set$values$svm.cost =\n  to_tune(p_dbl(-1, 1, trafo = function(x) 10^x, depends = branch.selection == \"svm\"))\ngraph_learner$param_set$values$ranger.mtry =\n  to_tune(p_int(1, 8, depends = branch.selection == \"ranger\"))\ngraph_learner$id = \"graph_learner\"\n\ninner_resampling = rsmp(\"cv\", folds = 3)\nat = auto_tuner(\n  learner = graph_learner,\n  resampling = inner_resampling,\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 10),\n  tuner = tnr(\"random_search\")\n)\n\nouter_resampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, at, outer_resampling, store_models = TRUE)\n\nWe check the inner tuning results for stable hyperparameters. This means that the selected hyperparameters should not vary too much. We might observe unstable models in this example because the small data set and the low number of resampling iterations might introduce too much randomness. Usually, we aim for the selection of stable hyperparameters for all outer training sets.\n\nextract_inner_tuning_results(rr)\n\n\n\n\n\n\n\nNext, we want to compare the predictive performances estimated on the outer resampling to the inner resampling. Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.\n\nrr$score()[, .(iteration, task_id, learner_id, resampling_id, classif.ce)]\n\n   iteration task_id          learner_id resampling_id classif.ce\n       &lt;int&gt;  &lt;char&gt;              &lt;char&gt;        &lt;char&gt;      &lt;num&gt;\n1:         1    pima graph_learner.tuned            cv  0.2695312\n2:         2    pima graph_learner.tuned            cv  0.2578125\n3:         3    pima graph_learner.tuned            cv  0.2343750\n\n\nThe aggregated performance of all outer resampling iterations is essentially the unbiased performance of the graph learner with optimal hyperparameter found by random search.\n\nrr$aggregate()\n\nclassif.ce \n 0.2539062 \n\n\nApplying nested resampling can be shortened by using the tune_nested()-shortcut.\n\ngraph_learner = as_learner(graph)\ngraph_learner$param_set$values$branch.selection =\n  to_tune(c(\"kknn\", \"svm\", \"ranger\"))\ngraph_learner$param_set$values$kknn.k =\n  to_tune(p_int(3, 50, logscale = TRUE, depends = branch.selection == \"kknn\"))\ngraph_learner$param_set$values$svm.cost =\n  to_tune(p_dbl(-1, 1, trafo = function(x) 10^x, depends = branch.selection == \"svm\"))\ngraph_learner$param_set$values$ranger.mtry =\n  to_tune(p_int(1, 8, depends = branch.selection == \"ranger\"))\ngraph_learner$id = \"graph_learner\"\n\nrr = tune_nested(\n  tuner = tnr(\"random_search\"),\n  task = task,\n  learner = graph_learner,\n  inner_resampling = rsmp(\"cv\", folds = 3),\n  outer_resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10,\n)\n\n\n\nResources\nThe mlr3book includes chapters on pipelines and hyperparameter tuning. The mlr3cheatsheets contain frequently used commands and workflows of mlr3.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   bslib               0.8.0      2024-07-29 [1] CRAN (R 4.4.1)\n   cachem              1.1.0      2024-05-16 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   crosstalk           1.2.1      2023-11-23 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   DT                  0.33       2024-04-04 [1] CRAN (R 4.4.1)\n   e1071               1.7-16     2024-09-16 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   farver              2.1.2      2024-05-13 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future              1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2             3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools           0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   igraph              2.0.3      2024-03-13 [1] CRAN (R 4.4.1)\n   jquerylib           0.1.4      2021-04-26 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   kknn                1.3.1      2016-03-26 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   labeling            0.4.3      2023-08-29 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n P Matrix              1.7-0      2024-04-26 [?] CRAN (R 4.4.0)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect         1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband       0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3measures        1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc            0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning          1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox             1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   patchwork           1.3.0      2024-09-16 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   proxy               0.4-27     2022-06-09 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   ranger              0.16.0     2023-11-12 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n   sass                0.4.9      2024-03-15 [1] CRAN (R 4.4.1)\n   scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   viridisLite         0.4.2      2023-05-02 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "torch_pipeops.html",
    "href": "torch_pipeops.html",
    "title": "Torch Pipeline Operators",
    "section": "",
    "text": "Torch Pipeline Operators\nmlr3torch supports a number of PipeOp building blocks to define neural networks, including preprocessing and data augmentation steps."
  },
  {
    "objectID": "filters.html",
    "href": "filters.html",
    "title": "Feature Selection Filter",
    "section": "",
    "text": "Feature Filters quantify the importance of each feature of a Task by assigning them a numerical score. In a second step, features can be selected by either selecting a fixed absolute or relative frequency of the best features, or by thresholding on the score value.\nThe Filter PipeOp allows to use filters as a preprocessing step.\n\n\n\n\n\n\n\n\nUse the \\(-\\log_{10}()\\)-transformed \\(p\\)-values of a Kruskal-Wallis rank sum test (implemented in kruskal.test()) for filtering features of the Pima Indian Diabetes tasks.\n\nlibrary(\"mlr3verse\")\n\nLoading required package: mlr3\n\n# retrieve a task\ntask = tsk(\"pima\")\n\n# retrieve a filter\nfilter = flt(\"kruskal_test\")\n\n# calculate scores\nfilter$calculate(task)\n\n# access scores\nfilter$scores\n\n  glucose       age      mass   insulin   triceps  pregnant  pedigree  pressure \n39.885381 16.942901 16.740864 13.127828  9.158113  7.426955  5.922431  5.788607 \n\n# plot scores\nautoplot(filter)\n\n\n\n\n\n\n\n# subset task to 3 most important features\ntask$select(head(names(filter$scores), 3))\ntask$feature_names\n\n[1] \"age\"     \"glucose\" \"mass\""
  },
  {
    "objectID": "filters.html#example-usage",
    "href": "filters.html#example-usage",
    "title": "Feature Selection Filter",
    "section": "",
    "text": "Use the \\(-\\log_{10}()\\)-transformed \\(p\\)-values of a Kruskal-Wallis rank sum test (implemented in kruskal.test()) for filtering features of the Pima Indian Diabetes tasks.\n\nlibrary(\"mlr3verse\")\n\nLoading required package: mlr3\n\n# retrieve a task\ntask = tsk(\"pima\")\n\n# retrieve a filter\nfilter = flt(\"kruskal_test\")\n\n# calculate scores\nfilter$calculate(task)\n\n# access scores\nfilter$scores\n\n  glucose       age      mass   insulin   triceps  pregnant  pedigree  pressure \n39.885381 16.942901 16.740864 13.127828  9.158113  7.426955  5.922431  5.788607 \n\n# plot scores\nautoplot(filter)\n\n\n\n\n\n\n\n# subset task to 3 most important features\ntask$select(head(names(filter$scores), 3))\ntask$feature_names\n\n[1] \"age\"     \"glucose\" \"mass\""
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mlr3: Machine Learning in R",
    "section": "",
    "text": "The mlr3 ecosystem is the framework for machine learning in R.\nAn open-source collection of R packages providing a unified interface for machine learning in the R language. Successor of mlr.\nA scientifically designed and easy to learn interface.\n\n\n\n\n\n\n\nMore than 100 connected machine learning algorithms.\n\n\n\n\n\n\n\nLight on dependencies.\n\n\n\n\n\n\n\nConvenient parallelization with the future package.\n\n\n\n\n\n\n\nState-of-the-art optimization algorithms.\n\n\n\n\n\n\n\nDataflow programming with pipelines."
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "mlr3: Machine Learning in R",
    "section": "Get Started",
    "text": "Get Started\nThere are many packages in the mlr3 ecosystem that you may want to use. You can install the full mlr3 universe at once with:\n\ninstall.packages(\"mlr3verse\")\n\nYou can also use our Docker images."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "mlr3: Machine Learning in R",
    "section": "Resources",
    "text": "Resources\n\nOur book “Applied Machine Learning Using mlr3 in R” is the central entry point to mlr3 ecosystem. This essential guide covers key aspects of machine learning, from building and evaluating predictive models to advanced techniques like hyperparameter tuning for peak performance. It delves into constructing comprehensive machine learning pipelines, encompassing data pre-processing, modeling, and prediction aggregation.\nThe book is primarily aimed at researchers, practitioners, and graduate students who use machine learning or who are interested in using it. It can be used as a textbook for an introductory or advanced machine learning class that uses R, as a reference for people who work with machine learning methods, and in industry for exploratory experiments in machine learning.\nIn addition to the book, there are many other resources to learn more about mlr3. The gallery contains a collection of case studies that demonstrate the functionality of mlr3. The cheatsheets provide a quick overview of the most important functions. The resources section contains links to talks, courses, and other material."
  },
  {
    "objectID": "team/about-lennart.html",
    "href": "team/about-lennart.html",
    "title": "Lennart Schneider",
    "section": "",
    "text": "PhD Student at LMU Munich. Interested in black box optimization, HPO and AutoML. Mainly working on mlr3mbo."
  },
  {
    "objectID": "team/about-raphael.html",
    "href": "team/about-raphael.html",
    "title": "Raphael Sonabend",
    "section": "",
    "text": "Postdoc at Imperial College London. I am the main developer of mlr3proba and also the previous maintainer of mlr3extralearners."
  },
  {
    "objectID": "team/about-florian.html",
    "href": "team/about-florian.html",
    "title": "Florian Pfisterer",
    "section": "",
    "text": "PhD Student at LMU Munich. I am interested in projects on the intersection of Meta-Learning, AutoML and Algorithmic Fairness. Mainly working on mlr3pipelines and mlr3keras/mlr3torch."
  },
  {
    "objectID": "team/about-lars.html",
    "href": "team/about-lars.html",
    "title": "Lars Kotthoff",
    "section": "",
    "text": "Computer Science Professor at University of Wyoming, contributes small pieces here and there."
  },
  {
    "objectID": "gallery-all.html",
    "href": "gallery-all.html",
    "title": "All Posts",
    "section": "",
    "text": "All Posts\n\n\n  \n    \n      Wrapper-based Ensemble Feature Selection\n      Find the most stable and predictive features using multiple learners and resampling techniques.\n\n      2025-01-12 - John Zobolas\n    \n  \n    \n      Time constraints in the mlr3 ecosystem\n      Set time limits for learners, tuning and nested resampling.\n\n      2023-12-21 - Marc Becker\n    \n  \n    \n      Analyzing the Runtime Performance of tidymodels and mlr3\n      Compare the runtime performance of tidymodels and mlr3.\n\n      2023-10-30 - Marc Becker\n    \n  \n    \n      Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)\n      Demonstrate use of survival BART on the lung dataset via mlr3proba and distr6.\n\n      2023-10-25 - John Zobolas\n    \n  \n    \n      Spatial Data in the mlr3 Ecosystem\n      Run a land cover classification of the city of Leipzig.\n\n      2023-02-27 - Marc Becker\n    \n  \n    \n      Recursive Feature Elimination on the Sonar Data Set\n      Utilize the built-in feature importance of models.\n\n      2023-02-07 - Marc Becker\n    \n  \n    \n      Shadow Variable Search on the Pima Indian Diabetes Data Set\n      Run a feature selection with permutated features.\n\n      2023-02-01 - Marc Becker, Sebastian Fischer\n    \n  \n    \n      Default Hyperparameter Configuration\n      Run the default hyperparameter configuration of learners as a baseline.\n\n      2023-01-31 - Marc Becker\n    \n  \n    \n      Hyperband Series - Data Set Subsampling\n      Optimize the hyperparameters of a Support Vector Machine with Hyperband.\n\n      2023-01-16 - Marc Becker, Sebastian Fischer\n    \n  \n    \n      Hotstarting\n      Resume the training of learners.\n\n      2023-01-16 - Marc Becker, Sebastian Fischer\n    \n  \n    \n      Hyperband Series - Iterative Training\n      Optimize the hyperparameters of an XGBoost model with Hyperband.\n\n      2023-01-15 - Marc Becker, Sebastian Fischer\n    \n  \n    \n      Visualization in mlr3\n      Quickly plot the mlr3 ecosystem.\n\n      2022-12-22 - Marc Becker\n    \n  \n    \n      Practical Tuning Series - Tuning and Parallel Processing\n      Run various jobs in mlr3 in parallel.\n\n      2021-03-12 - Marc Becker, Theresa Ullmann, Michel Lang, Bernd Bischl, Jakob Richter, Martin Binder\n    \n  \n    \n      Practical Tuning Series - Build an Automated Machine Learning System\n      Implement a simple automated machine learning system.\n\n      2021-03-11 - Marc Becker, Theresa Ullmann, Michel Lang, Bernd Bischl, Jakob Richter, Martin Binder\n    \n  \n    \n      Practical Tuning Series - Tune a Preprocessing Pipeline\n      Build a simple preprocessing pipeline and tune it.\n\n      2021-03-10 - Marc Becker, Theresa Ullmann, Michel Lang, Bernd Bischl, Jakob Richter, Martin Binder\n    \n  \n    \n      Practical Tuning Series - Tune a Support Vector Machine\n      Optimize the hyperparameters of a support vector machine.\n\n      2021-03-09 - Marc Becker, Theresa Ullmann, Michel Lang, Bernd Bischl, Jakob Richter, Martin Binder\n    \n  \n    \n      Tuning a Complex Graph\n      Tune a preprocessing pipeline and multiple tuners at once.\n\n      2021-02-03 - Lennart Schneider\n    \n  \n    \n      Integer Hyperparameters in Tuners for Real-valued Search Spaces\n      Optimize integer hyperparameters with tuners that can only propose real numbers.\n\n      2021-01-19 - Marc Becker\n    \n  \n    \n      Threshold Tuning for Classification Tasks\n      Adjust the probability thresholds of classes.\n\n      2020-10-14 - Florian Pfisterer\n    \n  \n    \n      Liver Patient Classification Based on Diagnostic Measures\n      Tune and benchmark pipelines.\n\n      2020-09-11 - Julian Lange, Jae-Eun Nam, Viet Tran, Simon Wiegrebe, Henri Funk (Editor)\n    \n  \n    \n      Comparison of Decision Boundaries of Classification Learners\n      Visualize the decision boundaries of multiple classification learners on some artificial data sets.\n\n      2020-08-14 - Michel Lang\n    \n  \n    \n      A Production Example Using Plumber and Docker\n      Write a REST API using plumber and deploy it using Docker.\n\n      2020-08-13 - Lennart Schneider\n    \n  \n    \n      Target Transformations via Pipelines\n      Transform the target variable.\n\n      2020-06-15 - Lennart Schneider\n    \n  \n    \n      mlr3 and OpenML - Moneyball Use Case\n      Download data from OpenML data and impute missing values.\n\n      2020-05-04 - Philipp Kopper\n    \n  \n    \n      Feature Engineering of Date-Time Variables\n      Engineer features using date-time variables.\n\n      2020-05-02 - Lennart Schneider\n    \n  \n    \n      Tuning a Stacked Learner\n      Tune a multilevel stacking model.\n\n      2020-04-27 - Milan Dragicevic, Giuseppe Casalicchio\n    \n  \n    \n      A Pipeline for the Titanic Data Set - Advanced\n      Create new features and impute missing values with a pipeline.\n\n      2020-04-27 - Florian Pfisterer\n    \n  \n    \n      Pipelines, Selectors, Branches\n      Build a preprocessing pipeline with branching.\n\n      2020-04-23 - Milan Dragicevic, Giuseppe Casalicchio\n    \n  \n    \n      Regression Chains\n      Handle multi-target regression with regression chains.\n\n      2020-04-18 - Lennart Schneider\n    \n  \n    \n      Imbalanced Data Handling with mlr3\n      Handle imbalanced data with oversampling, undersampling, and SMOTE imbalance correction.\n\n      2020-03-30 - Giuseppe Casalicchio\n    \n  \n    \n      Resampling - Stratified, Blocked and Predefined\n      Apply stratified, block and custom resampling.\n\n      2020-03-30 - Milan Dragicevic, Giuseppe Casalicchio\n    \n  \n    \n      A Pipeline for the Titanic Data Set - Basics\n      Build a graph.\n\n      2020-03-12 - Florian Pfisterer\n    \n  \n    \n      German Credit Series - Pipelines\n      Impute missing values, filter features and stack Learners.\n\n      2020-03-11 - Martin Binder, Florian Pfisterer\n    \n  \n    \n      German Credit Series - Tuning\n      Optimize Hyperparameters and apply nested resampling.\n\n      2020-03-11 - Martin Binder, Florian Pfisterer\n    \n  \n    \n      German Credit Series - Basics\n      Train different models.\n\n      2020-03-11 - Martin Binder, Florian Pfisterer, Michel Lang\n    \n  \n    \n      Select Uncorrelated Features\n      Remove correlated features with a pipeline.\n\n      2020-02-25 - Martin Binder, Florian Pfisterer\n    \n  \n    \n      Tuning Over Multiple Learners\n      Tune over multiple learners for a single task.\n\n      2020-02-01 - Jakob Richter, Bernd Bischl\n    \n  \n    \n      Impute Missing Variables\n      Augment a Random Forest with automatic imputation.\n\n      2020-01-31 - Florian Pfisterer\n    \n  \n    \n      Encode Factor Levels for xgboost\n      Encode factor variables in a task.\n\n      2020-01-31 - Michel Lang\n    \n  \n    \n      House Prices in King County\n      Apply multiple preprocessing steps, fit a model and visualize the results.\n\n      2020-01-30 - Florian Pfisterer\n    \n  \n\nNo matching items"
  },
  {
    "objectID": "benchmarks/benchmark_rush.html",
    "href": "benchmarks/benchmark_rush.html",
    "title": "mlr3 - Runtime and Memory Benchmarks",
    "section": "",
    "text": "Scope\nThis report analyzes the runtime and memory usage of rush across the most recent package versions. It focuses on the core methods $push_running_tasks(), $push_results() and $fetch_finished_tasks(). This report helps users assess whether observed runtimes fall within expected ranges. Substantial anomalies in runtime should be reported by opening a GitHub issue. Benchmarks are executed on a high‑performance cluster optimized for multi‑core throughput rather than single‑core speed. Consequently, single‑core runtimes may be faster on a modern local machine.\n\n\nSummary of Latest rush Version\nWe summarize the results for the latest rush version. The overhead of $push_running_tasks() and $push_results() is below a half millisecond. The runtime of $fetch_finished_tasks() depends on the number of tasks and the cache size. When no tasks are cached, the runtime for 1 to 1000 tasks is around 1 ms When one new task is fetched and the rest is cached, the runtime ranges between 12 and 37 ms depending of the cache size.\n\n\nPush Running Tasks\nThe runtime of $push_running_tasks().\n\nxss = list(list(x1 = 1))\nkeys = rush$push_running_tasks(xss)\n\n\n\n\n\n\n\nRuntime of $push_running_tasks() by rush version.\n\n\nrush\nNumber of Parameters\nNumber of Tasks\nRuntime, ms\n\n\n\n\n10 Parameters\n\n\n0.4.1\n10\n1\n0.48\n\n\n1 Parameter\n\n\n0.4.1\n1\n1\n0.49\n\n\n\n\n\n\n\n\n\nPush Results\nThe runtime of $push_results().\n\nyss = list(list(y = 1))\nrush$push_results(keys, yss)\n\n\n\n\n\n\n\nRuntime of $push_results() by rush version.\n\n\nrush\nNumber of Parameters\nNumber of Tasks\nRuntime, ms\n\n\n\n\n10 Parameters\n\n\n0.4.1\n10\n1\n0.37\n\n\n1 Parameter\n\n\n0.4.1\n1\n1\n0.37\n\n\n\n\n\n\n\n\n\nFetch Finished Tasks\nThe runtime of $fetch_finished_tasks().\n\nrush$fetch_finished_tasks()\n\n\n\n\n\n\n\nRuntime of $fetch_finished_tasks() by number of tasks and rush version.\n\n\nrush\nNumber of Parameters\nNumber of Tasks\nRuntime, ms\n\n\n\n\n10 Parameters\n\n\n0.4.1\n10\n1\n1.4\n\n\n0.4.1\n10\n10\n1.8\n\n\n0.4.1\n10\n100\n5.7\n\n\n0.4.1\n10\n1,000\n37\n\n\n0.4.1\n10\n10,000\n390\n\n\n0.4.1\n10\n100,000\n4400\n\n\n1 Parameter\n\n\n0.4.1\n1\n1\n1.4\n\n\n0.4.1\n1\n10\n1.8\n\n\n0.4.1\n1\n100\n5.3\n\n\n0.4.1\n1\n1,000\n33\n\n\n0.4.1\n1\n10,000\n320\n\n\n0.4.1\n1\n100,000\n3400\n\n\n\n\n\n\n\n\n\nFetch Finished Tasks with Cache\nThe runtime of $fetch_finished_tasks() when one new task is fetched and the other tasks are cached.\n\nrush$fetch_finished_tasks()\n\n\n\n\n\n\n\nRuntime of $fetch_finished_tasks() with cache by cache size and rush version.\n\n\nrush\nNumber of Parameters\nNumber of Tasks\nRuntime, ms\n\n\n\n\n10 Parameters\n\n\n0.4.1\n10\n1\n0.92\n\n\n0.4.1\n10\n10\n0.94\n\n\n0.4.1\n10\n100\n1.2\n\n\n0.4.1\n10\n1,000\n4.0\n\n\n0.4.1\n10\n10,000\n51\n\n\n0.4.1\n10\n100,000\n970\n\n\n1 Parameter\n\n\n0.4.1\n1\n1\n0.91\n\n\n0.4.1\n1\n10\n0.94\n\n\n0.4.1\n1\n100\n1.1\n\n\n0.4.1\n1\n1,000\n2.2\n\n\n0.4.1\n1\n10,000\n15\n\n\n0.4.1\n1\n100,000\n300\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: “Runtime of fetching finished tasks with (red lines) and without (blue lines) caching.” “The x-axis shows the number of tasks fetched from the database.” “The fetching with caching gets one new task from the database and the rest from the cache.” “The fetching without caching gets all tasks from the database.”"
  },
  {
    "objectID": "benchmarks/benchmarks_mlr3tuning.html",
    "href": "benchmarks/benchmarks_mlr3tuning.html",
    "title": "mlr3tuning - Runtime and Memory Benchmarks",
    "section": "",
    "text": "Scope\nThis report analyzes the runtime and memory usage of mlr3tuning across versions. It evaluates tune() and tune_nested() in sequential and parallel modes. Given the size of the mlr3 ecosystem, performance bottlenecks can arise at multiple stages. This report helps users judge whether observed runtimes and memory footprints are within expected ranges. Substantial anomalies should be reported via a GitHub issue. Benchmarks are executed on a high‑performance cluster optimized for multi‑core throughput rather than single‑core speed. Consequently, runtimes may be faster on a modern local machine.\n\n\nSummary of Latest mlr3tuning Version\nThe benchmarks are comprehensive, so we summarize results for the latest mlr3tuning version. We measure runtime and memory for random search with 1,000 resampling iterations on the spam dataset with 1,000 and 10,000 instances. Nested resampling uses 10 outer iterations and the same random search in the inner loop. Overhead introduced by tune() and tune_nested() must be interpreted relative to model training time. For 1 s training time, overhead is minimal. For 100 ms training time, overhead is approximately 20%. For 10 ms training time, overhead approximately doubles to triples total runtime. For 1 ms training time, total runtime is about 15 to 25 times the bare model training time. Memory usage for tune() and tune_nested() ranges between 370 MB and 670 MB. An empty R session consumes 131 MB. mlr3tuning parallelizes over resampling iterations using the future package. Parallel execution adds overhead from worker initialization, so we compare parallel and sequential runtimes. For all training times, parallel tune() reduces total runtime. Memory increases with core count because each worker is a separate R session. Using 10 cores requires around 1.5 GB. tune_nested() parallelizes over the outer resampling loop. Across all training times, the parallel version is faster than the sequential version. Total memory usage is approximately 3.6 GB.\n\n\nTune\nThe runtime and memory usage of tune() are measured across mlr3tuning versions. A random search is used with a batch size of 1,000. Models are trained on the spam dataset with 1,000 and 10,000 instances.\n\ntask = tsk(\"spam\")\n\nlearner = lrn(\"classif.rpart\",\n  cp = to_tune(0, 1))\n\ntune(\n  tune = tnr(\"random_search\", batch_size = 1000),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 1000),\n  store_benchmark_result = FALSE,\n  store_models = FALSE\n)\n\n\n\n\n\n\n\nRuntime and memory usage of tune() by mlr3tuning version and task size. The k factors indicate how many times longer total runtime is than the model training time. The subscripts denote reference training times in milliseconds; for example, k100 corresponds to 100 ms. A green background highlights cases where the total runtime is less than three times the model training time. The pk factors report the speedup of parallel relative to sequential execution. The pk factor is omitted when parallel execution is slower than sequential execution.\n\n\nmlr3tuning Version\nTask Size\nOverhead, s\nk1000\nk100\nk10\nk1\nMemory, mb\npk1\npk10\npk100\npk1000\n\n\n\n\n1000 Observations\n\n\n1.5.0\n1000\n22\n1.0\n1.2\n3.2\n23\n366\n2.1\n2.7\n5.8\n9.2\n\n\n1.4.0\n1000\n21\n1.0\n1.2\n3.1\n22\n366\n2.0\n2.6\n5.7\n9.2\n\n\n1.3.0\n1000\n21\n1.0\n1.2\n3.1\n22\n365\n1.9\n2.5\n5.7\n9.2\n\n\n1.2.1\n1000\n21\n1.0\n1.2\n3.1\n22\n427\n2.0\n2.6\n5.7\n9.2\n\n\n10000 Observations\n\n\n1.5.0\n10000\n24\n1.0\n1.2\n3.4\n25\n443\n1.9\n2.4\n5.4\n9.1\n\n\n1.4.0\n10000\n24\n1.0\n1.2\n3.4\n25\n403\n1.9\n2.5\n5.4\n9.1\n\n\n1.3.0\n10000\n23\n1.0\n1.2\n3.3\n24\n434\n1.9\n2.4\n5.4\n9.1\n\n\n1.2.1\n10000\n23\n1.0\n1.2\n3.3\n24\n446\n1.9\n2.4\n5.4\n9.1\n\n\n\n\n\n\n\n\n\nNested Tuning\nThe runtime and memory usage of tune_nested() are measured across mlr3tuning versions. The outer resampling performs 10 iterations, and the inner random search evaluates 1,000 feature subsets. Models are trained on the spam dataset with 1,000 and 10,000 instances.\n\ntask = tsk(\"spam\")\n\nlearner = lrn(\"classif.rpart\",\n  cp = to_tune(0, 1))\n\ntune_nested(\n  tuner = tnr(\"random_search\", batch_size = 1000),\n  task = task,\n  learner = learner,\n  inner_resampling = rsmp(\"holdout\"),\n  outer_resampling = rsmp(\"subsampling\", repeats = 10),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 1000),\n  store_tune_instance = FALSE,\n  store_benchmark_result = FALSE,\n  store_models = FALSE\n)\n\n\n\n\n\n\n\nRuntime and memory usage of tune_nested() by mlr3tuning version and task size. The k factors indicate how many times longer total runtime is than the model training time. The subscripts denote reference training times in milliseconds; for example, k100 corresponds to 100 ms. A green background highlights cases where the total runtime is less than three times the model training time. The pk factors report the speedup of parallel relative to sequential execution. The pk factor is omitted when parallel execution is slower than sequential execution.\n\n\nmlr3tuning Version\nTask Size\nOverhead, s\nk1000\nk100\nk10\nk1\nMemory, mb\npk1\npk10\npk100\npk1000\n\n\n\n\n1000 Observations\n\n\n1.5.0\n1000\n24\n1.0\n1.2\n3.4\n25\n352\n7.0\n7.6\n9.2\n9.9\n\n\n1.4.0\n1000\n23\n1.0\n1.2\n3.3\n24\n352\n6.6\n7.3\n9.1\n9.9\n\n\n1.3.0\n1000\n24\n1.0\n1.2\n3.4\n25\n347\n7.7\n8.2\n9.4\n9.9\n\n\n1.2.1\n1000\n23\n1.0\n1.2\n3.3\n24\n346\n7.5\n8.1\n9.4\n9.9\n\n\n10000 Observations\n\n\n1.5.0\n10000\n25\n1.0\n1.2\n3.5\n26\n351\n3.7\n4.4\n7.4\n9.6\n\n\n1.4.0\n10000\n23\n1.0\n1.2\n3.3\n24\n357\n4.8\n5.6\n8.2\n9.7\n\n\n1.3.0\n10000\n23\n1.0\n1.2\n3.3\n24\n385\n4.8\n5.6\n8.2\n9.8\n\n\n1.2.1\n10000\n23\n1.0\n1.2\n3.3\n24\n345\n4.8\n5.6\n8.2\n9.7"
  },
  {
    "objectID": "benchmarks/benchmarks_paradox.html",
    "href": "benchmarks/benchmarks_paradox.html",
    "title": "paradox - Runtime Benchmarks",
    "section": "",
    "text": "Scope\nThis report analyzes the runtime and memory usage of the paradox package across different versions. The benchmarks vary the type and the size of the parameter space.\nGiven the extensive package ecosystem of mlr3, performance bottlenecks can occur at multiple stages. This report aims to help users determine whether the runtime of their workflows falls within expected ranges. If significant runtime or memory anomalies are observed, users are encouraged to report them by opening a GitHub issue.\nBenchmarks are conducted on a high-performance cluster optimized for multi-core performance rather than single-core speed. Consequently, runtimes may be faster on a local machine.\n\n\nSummary of Latest paradox Version\nThe benchmarks are comprehensive; therefore, we present a summary of the most important functions in the latest paradox version. We start by measuring the runtime of the ps() function when initializing a parameter space in the size of 5, 50 and 500 parameters. The runtime increases from 2 to 10 to 100 ms but is independent of the type of the parameters. Getting values with the $get_values() method takes around 100 µs. The $set_values() method takes a half millisecond for 5 values and 14 ms for 500 values.\n\n\nInitialize\nThe runtime of the initialize() method is measured for different paradox versions.\n\nparam_set = ps(\n  a = p_dbl(lower = 0, upper = 1),\n  b = p_dbl(lower = 0, upper = 1),\n  c = p_dbl(lower = 0, upper = 1),\n  d = p_dbl(lower = 0, upper = 1),\n  e = p_dbl(lower = 0, upper = 1)\n)\n\n\n\n\n\n\n\nRuntime of the ps() function by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n2.4\n\n\n1.0.1\ncategorical\n5\n2.1\n\n\n1.1.0\nmixed\n5\n2.0\n\n\n1.0.1\nmixed\n5\n1.9\n\n\n1.1.0\nnumeric\n5\n1.6\n\n\n1.0.1\nnumeric\n5\n1.9\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n11\n\n\n1.0.1\ncategorical\n50\n11\n\n\n1.1.0\nmixed\n50\n11\n\n\n1.0.1\nmixed\n50\n9.9\n\n\n1.1.0\nnumeric\n50\n8.6\n\n\n1.0.1\nnumeric\n50\n9.5\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n110\n\n\n1.0.1\ncategorical\n500\n110\n\n\n1.1.0\nmixed\n500\n99\n\n\n1.0.1\nmixed\n500\n89\n\n\n1.1.0\nnumeric\n500\n87\n\n\n1.0.1\nnumeric\n500\n74\n\n\n\n\n\n\n\n\n\nIds\nThe runtime usage of the param_set$ids() method is measured for different paradox versions.\n\nparam_set$ids()\n\n\n\n\n\n\n\nRuntime of the $ids() method by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n0.037\n\n\n1.0.1\ncategorical\n5\n0.039\n\n\n1.1.0\nmixed\n5\n0.036\n\n\n1.0.1\nmixed\n5\n0.038\n\n\n1.1.0\nnumeric\n5\n0.038\n\n\n1.0.1\nnumeric\n5\n0.035\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n0.036\n\n\n1.0.1\ncategorical\n50\n0.034\n\n\n1.1.0\nmixed\n50\n0.038\n\n\n1.0.1\nmixed\n50\n0.034\n\n\n1.1.0\nnumeric\n50\n0.038\n\n\n1.0.1\nnumeric\n50\n0.038\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n0.038\n\n\n1.0.1\ncategorical\n500\n0.034\n\n\n1.1.0\nmixed\n500\n0.038\n\n\n1.0.1\nmixed\n500\n0.034\n\n\n1.1.0\nnumeric\n500\n0.035\n\n\n1.0.1\nnumeric\n500\n0.034\n\n\n\n\n\n\n\nSubset the parameter space by class.\n\nparam_set$ids(class = \"numeric\")\n\n\n\n\n\n\n\nRuntime of the $ids() method by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n1.1\n\n\n1.0.1\ncategorical\n5\n1.1\n\n\n1.1.0\nmixed\n5\n1.1\n\n\n1.0.1\nmixed\n5\n1.3\n\n\n1.1.0\nnumeric\n5\n1.1\n\n\n1.0.1\nnumeric\n5\n1.1\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n1.2\n\n\n1.0.1\ncategorical\n50\n1.1\n\n\n1.1.0\nmixed\n50\n1.1\n\n\n1.0.1\nmixed\n50\n1.1\n\n\n1.1.0\nnumeric\n50\n1.2\n\n\n1.0.1\nnumeric\n50\n1.0\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n1.1\n\n\n1.0.1\ncategorical\n500\n1.1\n\n\n1.1.0\nmixed\n500\n1.1\n\n\n1.0.1\nmixed\n500\n1.1\n\n\n1.1.0\nnumeric\n500\n1.1\n\n\n1.0.1\nnumeric\n500\n1.1\n\n\n\n\n\n\n\nSubset the parameter space by tags.\n\nparam_set$ids(tags = \"a\")\n\n\n\n\n\n\n\nRuntime of the $ids() method by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n0.044\n\n\n1.0.1\ncategorical\n5\n0.042\n\n\n1.1.0\nmixed\n5\n0.042\n\n\n1.0.1\nmixed\n5\n0.037\n\n\n1.1.0\nnumeric\n5\n0.040\n\n\n1.0.1\nnumeric\n5\n0.038\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n0.044\n\n\n1.0.1\ncategorical\n50\n0.039\n\n\n1.1.0\nmixed\n50\n0.044\n\n\n1.0.1\nmixed\n50\n0.044\n\n\n1.1.0\nnumeric\n50\n0.063\n\n\n1.0.1\nnumeric\n50\n0.044\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n0.067\n\n\n1.0.1\ncategorical\n500\n0.061\n\n\n1.1.0\nmixed\n500\n0.059\n\n\n1.0.1\nmixed\n500\n0.059\n\n\n1.1.0\nnumeric\n500\n0.060\n\n\n1.0.1\nnumeric\n500\n0.058\n\n\n\n\n\n\n\nSubset the parameter space by tags and class.\n\nparam_set$ids(tags = \"a\", class = \"numeric\")\n\n\n\n\n\n\n\nRuntime of the $get_values() method by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n2.2\n\n\n1.0.1\ncategorical\n5\n2.1\n\n\n1.1.0\nmixed\n5\n2.1\n\n\n1.0.1\nmixed\n5\n2.1\n\n\n1.1.0\nnumeric\n5\n2.1\n\n\n1.0.1\nnumeric\n5\n2.2\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n2.1\n\n\n1.0.1\ncategorical\n50\n2.1\n\n\n1.1.0\nmixed\n50\n2.1\n\n\n1.0.1\nmixed\n50\n2.1\n\n\n1.1.0\nnumeric\n50\n2.1\n\n\n1.0.1\nnumeric\n50\n2.1\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n2.2\n\n\n1.0.1\ncategorical\n500\n2.3\n\n\n1.1.0\nmixed\n500\n2.3\n\n\n1.0.1\nmixed\n500\n2.4\n\n\n1.1.0\nnumeric\n500\n2.3\n\n\n1.0.1\nnumeric\n500\n2.3\n\n\n\n\n\n\n\n\n\nGet Values\nThe runtime of the $get_values() method is measured for different paradox versions.\n\nparam_set$get_values()\n\n\n\n\n\n\n\nRuntime of the $get_values() method by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n0.11\n\n\n1.0.1\ncategorical\n5\n0.11\n\n\n1.1.0\nmixed\n5\n0.11\n\n\n1.0.1\nmixed\n5\n0.11\n\n\n1.1.0\nnumeric\n5\n0.10\n\n\n1.0.1\nnumeric\n5\n0.10\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n0.11\n\n\n1.0.1\ncategorical\n50\n0.11\n\n\n1.1.0\nmixed\n50\n0.11\n\n\n1.0.1\nmixed\n50\n0.10\n\n\n1.1.0\nnumeric\n50\n0.11\n\n\n1.0.1\nnumeric\n50\n0.11\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n0.12\n\n\n1.0.1\ncategorical\n500\n0.14\n\n\n1.1.0\nmixed\n500\n0.14\n\n\n1.0.1\nmixed\n500\n0.14\n\n\n1.1.0\nnumeric\n500\n0.15\n\n\n1.0.1\nnumeric\n500\n0.15\n\n\n\n\n\n\n\nSubset the parameter space by class.\n\nparam_set$get_values(class = \"numeric\")\n\n\n\n\n\n\n\nRuntime of the $get_values() method by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n1.2\n\n\n1.0.1\ncategorical\n5\n1.2\n\n\n1.1.0\nmixed\n5\n1.2\n\n\n1.0.1\nmixed\n5\n1.2\n\n\n1.1.0\nnumeric\n5\n1.2\n\n\n1.0.1\nnumeric\n5\n1.2\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n1.2\n\n\n1.0.1\ncategorical\n50\n1.2\n\n\n1.1.0\nmixed\n50\n1.2\n\n\n1.0.1\nmixed\n50\n1.1\n\n\n1.1.0\nnumeric\n50\n1.2\n\n\n1.0.1\nnumeric\n50\n1.2\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n1.2\n\n\n1.0.1\ncategorical\n500\n1.2\n\n\n1.1.0\nmixed\n500\n1.3\n\n\n1.0.1\nmixed\n500\n1.3\n\n\n1.1.0\nnumeric\n500\n1.3\n\n\n1.0.1\nnumeric\n500\n1.2\n\n\n\n\n\n\n\nSubset the parameter space by tags.\n\nparam_set$get_values(tags = \"a\")\n\n\n\n\n\n\n\nRuntime of the $get_values() method by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n0.11\n\n\n1.0.1\ncategorical\n5\n0.11\n\n\n1.1.0\nmixed\n5\n0.11\n\n\n1.0.1\nmixed\n5\n0.11\n\n\n1.1.0\nnumeric\n5\n0.11\n\n\n1.0.1\nnumeric\n5\n0.10\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n0.12\n\n\n1.0.1\ncategorical\n50\n0.11\n\n\n1.1.0\nmixed\n50\n0.11\n\n\n1.0.1\nmixed\n50\n0.12\n\n\n1.1.0\nnumeric\n50\n0.12\n\n\n1.0.1\nnumeric\n50\n0.12\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n0.15\n\n\n1.0.1\ncategorical\n500\n0.15\n\n\n1.1.0\nmixed\n500\n0.15\n\n\n1.0.1\nmixed\n500\n0.14\n\n\n1.1.0\nnumeric\n500\n0.15\n\n\n1.0.1\nnumeric\n500\n0.15\n\n\n\n\n\n\n\nSubset the parameter space by tags and class.\n\nparam_set$get_values(tags = \"a\", class = \"numeric\")\n\n\n\n\n\n\n\nRuntime of the $get_values() method by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n2.3\n\n\n1.0.1\ncategorical\n5\n2.3\n\n\n1.1.0\nmixed\n5\n2.3\n\n\n1.0.1\nmixed\n5\n2.3\n\n\n1.1.0\nnumeric\n5\n2.3\n\n\n1.0.1\nnumeric\n5\n2.2\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n2.1\n\n\n1.0.1\ncategorical\n50\n2.1\n\n\n1.1.0\nmixed\n50\n2.3\n\n\n1.0.1\nmixed\n50\n2.3\n\n\n1.1.0\nnumeric\n50\n2.3\n\n\n1.0.1\nnumeric\n50\n2.2\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n2.4\n\n\n1.0.1\ncategorical\n500\n2.5\n\n\n1.1.0\nmixed\n500\n2.5\n\n\n1.0.1\nmixed\n500\n2.5\n\n\n1.1.0\nnumeric\n500\n2.3\n\n\n1.0.1\nnumeric\n500\n2.3\n\n\n\n\n\n\n\n\n\nSet Values\nThe runtime of the $set_values() method is measured for different paradox versions.\n\nparam_set$set_values(a = 1)\n\n\n\n\n\n\n\nRuntime of the $set_values() method by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n0.56\n\n\n1.0.1\ncategorical\n5\n0.51\n\n\n1.1.0\nmixed\n5\n0.77\n\n\n1.0.1\nmixed\n5\n0.73\n\n\n1.1.0\nnumeric\n5\n0.61\n\n\n1.0.1\nnumeric\n5\n0.59\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n1.7\n\n\n1.0.1\ncategorical\n50\n1.8\n\n\n1.1.0\nmixed\n50\n2.0\n\n\n1.0.1\nmixed\n50\n1.9\n\n\n1.1.0\nnumeric\n50\n1.8\n\n\n1.0.1\nnumeric\n50\n1.8\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n14\n\n\n1.0.1\ncategorical\n500\n13\n\n\n1.1.0\nmixed\n500\n14\n\n\n1.0.1\nmixed\n500\n14\n\n\n1.1.0\nnumeric\n500\n14\n\n\n1.0.1\nnumeric\n500\n13\n\n\n\n\n\n\n\n\n\nLower and Upper\nThe runtime of getting the lower and upper bounds of the parameter space is measured for different paradox versions.\n\nparam_set$lower\n\n\n\n\n\n\n\nRuntime of the $lower field by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n0.015\n\n\n1.0.1\ncategorical\n5\n0.015\n\n\n1.1.0\nmixed\n5\n0.015\n\n\n1.0.1\nmixed\n5\n0.015\n\n\n1.1.0\nnumeric\n5\n0.016\n\n\n1.0.1\nnumeric\n5\n0.015\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n0.016\n\n\n1.0.1\ncategorical\n50\n0.016\n\n\n1.1.0\nmixed\n50\n0.015\n\n\n1.0.1\nmixed\n50\n0.015\n\n\n1.1.0\nnumeric\n50\n0.016\n\n\n1.0.1\nnumeric\n50\n0.015\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n0.015\n\n\n1.0.1\ncategorical\n500\n0.015\n\n\n1.1.0\nmixed\n500\n0.015\n\n\n1.0.1\nmixed\n500\n0.015\n\n\n1.1.0\nnumeric\n500\n0.015\n\n\n1.0.1\nnumeric\n500\n0.015\n\n\n\n\n\n\n\nRuntime of $upper().\n\nparam_set$upper\n\n\n\n\n\n\n\nRuntime of the $upper field by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n0.015\n\n\n1.0.1\ncategorical\n5\n0.015\n\n\n1.1.0\nmixed\n5\n0.016\n\n\n1.0.1\nmixed\n5\n0.015\n\n\n1.1.0\nnumeric\n5\n0.015\n\n\n1.0.1\nnumeric\n5\n0.015\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n0.016\n\n\n1.0.1\ncategorical\n50\n0.015\n\n\n1.1.0\nmixed\n50\n0.015\n\n\n1.0.1\nmixed\n50\n0.015\n\n\n1.1.0\nnumeric\n50\n0.016\n\n\n1.0.1\nnumeric\n50\n0.015\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n0.015\n\n\n1.0.1\ncategorical\n500\n0.015\n\n\n1.1.0\nmixed\n500\n0.015\n\n\n1.0.1\nmixed\n500\n0.015\n\n\n1.1.0\nnumeric\n500\n0.015\n\n\n1.0.1\nnumeric\n500\n0.015\n\n\n\n\n\n\n\n\n\nLevels\nThe runtime of getting the levels of the parameter space is measured for different paradox versions.\n\nparam_set$levels\n\n\n\n\n\n\n\nRuntime of the $levels field by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n0.016\n\n\n1.0.1\ncategorical\n5\n0.015\n\n\n1.1.0\nmixed\n5\n0.015\n\n\n1.0.1\nmixed\n5\n0.015\n\n\n1.1.0\nnumeric\n5\n0.015\n\n\n1.0.1\nnumeric\n5\n0.015\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n0.016\n\n\n1.0.1\ncategorical\n50\n0.016\n\n\n1.1.0\nmixed\n50\n0.016\n\n\n1.0.1\nmixed\n50\n0.016\n\n\n1.1.0\nnumeric\n50\n0.016\n\n\n1.0.1\nnumeric\n50\n0.016\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n0.015\n\n\n1.0.1\ncategorical\n500\n0.015\n\n\n1.1.0\nmixed\n500\n0.015\n\n\n1.0.1\nmixed\n500\n0.020\n\n\n1.1.0\nnumeric\n500\n0.015\n\n\n1.0.1\nnumeric\n500\n0.016\n\n\n\n\n\n\n\n\n\nClass\nThe runtime of getting the classes of the parameter space is measured for different paradox versions.\n\nparam_set$class\n\n\n\n\n\n\n\nRuntime of the $class field by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n0.015\n\n\n1.0.1\ncategorical\n5\n0.015\n\n\n1.1.0\nmixed\n5\n0.015\n\n\n1.0.1\nmixed\n5\n0.015\n\n\n1.1.0\nnumeric\n5\n0.015\n\n\n1.0.1\nnumeric\n5\n0.015\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n0.016\n\n\n1.0.1\ncategorical\n50\n0.015\n\n\n1.1.0\nmixed\n50\n0.015\n\n\n1.0.1\nmixed\n50\n0.015\n\n\n1.1.0\nnumeric\n50\n0.015\n\n\n1.0.1\nnumeric\n50\n0.016\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n0.015\n\n\n1.0.1\ncategorical\n500\n0.015\n\n\n1.1.0\nmixed\n500\n0.015\n\n\n1.0.1\nmixed\n500\n0.015\n\n\n1.1.0\nnumeric\n500\n0.016\n\n\n1.0.1\nnumeric\n500\n0.017\n\n\n\n\n\n\n\n\n\nSubspaces\nThe runtime of subsetting the parameter space into subspaces of size 1 is measured.\n\nparam_set$subspaces()\n\n\n\n\n\n\n\nRuntime of the $subspaces() method by paradox version, space size, parameter type and paradox version. The table includes runtimes for spaces of size 5, 50 and 500 parameters and for numeric, categorical and mixed parameters.\n\n\nparadox\nType\nSize\nRuntime, ms\n\n\n\n\n5 Parameters\n\n\n1.1.0\ncategorical\n5\n14\n\n\n1.0.1\ncategorical\n5\n14\n\n\n1.1.0\nmixed\n5\n16\n\n\n1.0.1\nmixed\n5\n13\n\n\n1.1.0\nnumeric\n5\n13\n\n\n1.0.1\nnumeric\n5\n15\n\n\n50 Parameters\n\n\n1.1.0\ncategorical\n50\n140\n\n\n1.0.1\ncategorical\n50\n140\n\n\n1.1.0\nmixed\n50\n150\n\n\n1.0.1\nmixed\n50\n140\n\n\n1.1.0\nnumeric\n50\n130\n\n\n1.0.1\nnumeric\n50\n140\n\n\n500 Parameters\n\n\n1.1.0\ncategorical\n500\n1600\n\n\n1.0.1\ncategorical\n500\n1600\n\n\n1.1.0\nmixed\n500\n1700\n\n\n1.0.1\nmixed\n500\n1500\n\n\n1.1.0\nnumeric\n500\n1700\n\n\n1.0.1\nnumeric\n500\n1500"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "You are welcome to open a Pull Request on GitHub. Before doing so, please read our PR Guidelines.\nIf you\n\nwant to add a new Learner, PipeOp or Measure you can check out the chapter extending of the mlr3 book.\nare interested in creating a new R package for the mlr3 ecosystem or collaborate on an existing one, you can contact us on Mattermost.\n\nAdditional developer information lives in the mlr3 GitHub Wiki."
  },
  {
    "objectID": "tasks.html",
    "href": "tasks.html",
    "title": "Tasks",
    "section": "",
    "text": "The mlr3 packages also ship with some data sets, readily usable as Task objects. The goal of these tasks is to quickly demonstrate the capabilities of the packages.\n\n\n\n\n\n\n\n\nCreate a classification task from the data set in the palmerpenguins package.\n\nlibrary(\"mlr3verse\")\n\n# create a task\ntask = tsk(\"breast_cancer\")\ntask\n\n\n── &lt;TaskClassif&gt; (683x10): Wisconsin Breast Cancer ─────────────────────────────\n• Target: class\n• Target classes: malignant (positive class, 35%), benign (65%)\n• Properties: twoclass\n• Features (9):\n  • ord (9): bare_nuclei, bl_cromatin, cell_shape, cell_size, cl_thickness,\n  epith_c_size, marg_adhesion, mitoses, normal_nucleoli\n\n# get the dimensions\nc(task$nrow, task$ncol)\n\n[1] 683  10\n\n# check for missing values\ntask$missings()\n\n          class     bare_nuclei     bl_cromatin      cell_shape       cell_size \n              0               0               0               0               0 \n   cl_thickness    epith_c_size   marg_adhesion         mitoses normal_nucleoli \n              0               0               0               0               0 \n\n# plot class frequencies\nautoplot(task)"
  },
  {
    "objectID": "tasks.html#example-usage",
    "href": "tasks.html#example-usage",
    "title": "Tasks",
    "section": "",
    "text": "Create a classification task from the data set in the palmerpenguins package.\n\nlibrary(\"mlr3verse\")\n\n# create a task\ntask = tsk(\"breast_cancer\")\ntask\n\n\n── &lt;TaskClassif&gt; (683x10): Wisconsin Breast Cancer ─────────────────────────────\n• Target: class\n• Target classes: malignant (positive class, 35%), benign (65%)\n• Properties: twoclass\n• Features (9):\n  • ord (9): bare_nuclei, bl_cromatin, cell_shape, cell_size, cl_thickness,\n  epith_c_size, marg_adhesion, mitoses, normal_nucleoli\n\n# get the dimensions\nc(task$nrow, task$ncol)\n\n[1] 683  10\n\n# check for missing values\ntask$missings()\n\n          class     bare_nuclei     bl_cromatin      cell_shape       cell_size \n              0               0               0               0               0 \n   cl_thickness    epith_c_size   marg_adhesion         mitoses normal_nucleoli \n              0               0               0               0               0 \n\n# plot class frequencies\nautoplot(task)"
  },
  {
    "objectID": "benchmarks/benchmarks_mlr3fselect.html",
    "href": "benchmarks/benchmarks_mlr3fselect.html",
    "title": "mlr3fselect - Runtime and Memory Benchmarks",
    "section": "",
    "text": "Scope\nThis report analyzes runtime and memory usage of mlr3fselect across recent versions. It evaluates fselect() and fselect_nested() in sequential and parallel execution. Given the size of the mlr3 ecosystem, performance bottlenecks can arise at multiple stages. This report enables users to assess whether observed runtimes and memory footprints are within expected ranges. Substantial anomalies should be reported via a GitHub issue. Benchmarks are executed on a high‑performance cluster optimized for multi‑core throughput rather than single‑core speed. Runtimes on modern local machines may therefore differ.\n\n\nSummary of Latest mlr3fselect Version\nThe benchmarks are comprehensive, so we summarize results for the latest mlr3fselect version. We measure runtime and memory for random search with 1,000 resampling iterations on the spam dataset with 1,000 and 10,000 instances. Nested resampling uses 10 outer iterations and the same random search in the inner loop with a holdout resampling. Overhead introduced by fselect() and fselect_nested() must be interpreted relative to the model training time. For 1 s training time, overhead is minimal. For 100 ms training time, overhead is approximately 20%. For 10 ms training time, overhead approximately doubles to triples total runtime. For 1 ms training time, total runtime is about 16 to 20 times the bare model training time. Memory usage for fselect() and fselect_nested() ranges between 450 MB and 550 MB. An empty R session consumes 131 MB. mlr3fselect parallelizes over resampling iterations using the future package. Parallel execution adds overhead from worker initialization, so we compare parallel and sequential runtimes. Parallel fselect() reduces total runtime for all training times. Memory increases with core count because each worker is a separate R session. Using 10 cores requires around 1.8 GB. fselect_nested() parallelizes over the outer resampling loop. Across all training times, the parallel version is faster than the sequential version. Total memory usage is approximately 3.3 GB.\n\n\nFeature Selection\nWe measure runtime and memory usage of fselect() across mlr3fselect versions. Random search is used with batch_size = 1000. Models are trained on the spam dataset with 1,000 and 10,000 instances.\n\ntask = tsk(\"spam\")\n\nlearner = lrn(\"classif.rpart\")\n\nfselect(\n  fselector = fs(\"random_search\", batch_size = 1000),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 1000),\n  store_benchmark_result = FALSE,\n  store_models = FALSE\n)\n\n\n\n\n\n\n\nRuntime and memory usage of fselect() by mlr3fselect version and task size. The k factors indicate how many times longer total runtime is than the model training time. The subscripts denote reference training times in milliseconds; for example, k100 corresponds to 100 ms. A green background marks cases where total runtime is less than three times the model training time. The pk factors report the speedup of parallel relative to sequential execution. The pk factor is omitted when parallel execution is slower than sequential execution.\n\n\nmlr3fselect\nTask Size\nOverhead, s\nk1000\nk100\nk10\nk1\nMemory, mb\npk1\npk10\npk100\npk1000\n\n\n\n\n1000 Observations\n\n\n1.5.0\n1000\n17\n1.0\n1.2\n2.7\n18\n323\n1.4\n2.0\n5.2\n9.0\n\n\n1.4.0\n1000\n25\n1.0\n1.2\n3.5\n26\n516\n1.2\n1.6\n4.1\n8.5\n\n\n1.3.0\n1000\n23\n1.0\n1.2\n3.3\n24\n469\n1.3\n1.7\n4.4\n8.7\n\n\n10000 Observations\n\n\n1.5.0\n10000\n19\n1.0\n1.2\n2.9\n20\n379\n1.4\n1.9\n4.9\n8.9\n\n\n1.4.0\n10000\n26\n1.0\n1.3\n3.6\n27\n524\n1.2\n1.6\n4.0\n8.4\n\n\n1.3.0\n10000\n25\n1.0\n1.3\n3.5\n26\n513\n1.3\n1.7\n4.2\n8.5\n\n\n\n\n\n\n\n\n\nNested Feature Selection\nWe measure runtime and memory usage of fselect_nested() across mlr3fselect versions. The outer resampling performs 10 iterations, and the inner random search evaluates 1,000 feature subsets. Models are trained on the spam dataset with 1,000 and 10,000 instances.\n\ntask = tsk(\"spam\")\n\nlearner = lrn(\"classif.rpart\")\n\nfselect_nested(\n  fselector = fs(\"random_search\", batch_size = 1000),\n  task = task,\n  learner = learner,\n  inner_resampling = rsmp(\"holdout\"),\n  outer_resampling = rsmp(\"subsampling\", repeats = 10),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 1000),\n  store_fselect_instance = FALSE,\n  store_benchmark_result = FALSE,\n  store_models = FALSE\n)\n\n\n\n\n\n\n\nRuntime and memory usage of fselect_nested() by mlr3fselect version and task size. The k factors indicate how many times longer total runtime is than the model training time. The subscripts denote reference training times in milliseconds; for example, k100 corresponds to 100 ms. A green background marks cases where total runtime is less than three times the model training time. The pk factors report the speedup of parallel relative to sequential execution. The pk factor is omitted when parallel execution is slower than sequential execution.\n\n\nmlr3fselect\nTask Size\nOverhead, s\nk1000\nk100\nk10\nk1\nMemory, mb\npk1\npk10\npk100\npk1000\n\n\n\n\n1000 Observations\n\n\n1.5.0\n1000\n19\n1.0\n1.2\n2.9\n20\n289\n6.4\n7.2\n9.1\n9.9\n\n\n1.4.0\n1000\n27\n1.0\n1.3\n3.7\n28\n350\n7.1\n7.7\n9.2\n9.9\n\n\n1.3.0\n1000\n25\n1.0\n1.3\n3.5\n26\n341\n7.6\n8.1\n9.4\n9.9\n\n\n10000 Observations\n\n\n1.5.0\n10000\n20\n1.0\n1.2\n3.0\n21\n310\n4.6\n5.5\n8.3\n9.8\n\n\n1.4.0\n10000\n27\n1.0\n1.3\n3.7\n28\n372\n5.0\n5.6\n8.2\n9.7\n\n\n1.3.0\n10000\n26\n1.0\n1.3\n3.6\n27\n353\n5.3\n6.0\n8.4\n9.8"
  },
  {
    "objectID": "benchmarks/benchmarks_mlr3.html",
    "href": "benchmarks/benchmarks_mlr3.html",
    "title": "mlr3 - Runtime and Memory Benchmarks",
    "section": "",
    "text": "Scope\nThis report analyzes the runtime and memory usage of mlr3 across the four most recent package versions. It focuses on the learner methods $train() and $predict() and on the evaluation functions resample() and benchmark(). The benchmarks quantify the runtime overhead introduced by mlr3 and the memory usage. Overhead is reported relative to the training time of the underlying models. The study varies dataset size and the number of resampling iterations. All experiments also assess the effect of parallelization on runtime and memory. The impact of encapsulation is examined by comparing alternative encapsulation methods.\nGiven the size of the mlr3 ecosystem, performance bottlenecks can arise at multiple stages. This report helps users assess whether observed runtimes fall within expected ranges. Substantial anomalies in runtime or memory should be reported by opening a GitHub issue. Benchmarks are executed on a high‑performance cluster optimized for multi‑core throughput rather than single‑core speed. Consequently, single‑core runtimes may be faster on a modern local machine.\n\n\nSummary of Latest mlr3 Version\nThe benchmarks are comprehensive, so we summarize the results for the latest mlr3 version. The runtime overhead of mlr3 must be interpreted relative to model training and prediction times. For instance, if ranger::ranger() takes 100 ms to train and lrn(\"classif.ranger\")$train() takes 110 milliseconds, the overhead is 10%. If the same model requires 1 second to train, the overhead is 1%. The overhead is shown relative to the training time of the models with the factors k_1, k_10, k_100, and k_1000. The subscript denotes the model’s training time in milliseconds. The factors pk_1, pk_10, pk_100, and pk_1000 report the speedup of parallel over sequential execution.\nWe first consider $train(). For models with training times of 1000 ms and 100 ms, the overhead is minimal. When training takes 10 ms, runtime approximately doubles. For 1 ms models, overhead is roughly ten times the bare model training time.\nThe overhead of $predict() is comparable to $train(), and dataset size has only a minor effect. $predict_newdata() converts newdata to a task and then predicts, which roughly doubles the overhead relative to $predict(). The recently introduced $predict_newdata_fast() is substantially faster than $predict_newdata(). For models with 10 ms prediction time, the overhead is about 10%. For models with 1 ms prediction time, the overhead is about 50%.\nThe overhead of resample() and benchmark() is small for 1000 ms and 100 ms models. For 10 ms models, the total runtime is approximately twice the bare training time. For 1 ms models, the total runtime is approximately ten times the bare training time. An empty R session consumes 131 MB of memory. Resampling with 10 iterations uses approximately 164 MB, increasing to about 225 MB for 1000 iterations. Memory usage for benchmark() is comparable to resample().\nmlr3 parallelizes over resampling iterations via the future package. Parallel execution adds overhead due to worker initialization. We therefore compare parallel and sequential runtimes. For 1 s models, parallel resample() and benchmark() reduce total runtime. For 100 ms models, parallelization is advantageous primarily for 100 or 1000 iterations. For 10 ms and 1 ms models, parallel execution overtakes sequential execution mainly at 1000 iterations. Memory grows with the number of cores because each core launches a separate R session. Using 10 cores results in a total memory footprint of approximately 1.2 GB.\nEncapsulation captures and logs conditions such as messages, warnings, and errors without interrupting control flow. Encapsulation via callr introduces approximately 1 s of additional runtime per model training. Encapsulation via evaluate adds negligible runtime overhead.\n\n\nTrain\nThe runtime and memory usage of $train() are measured for different mlr3 versions.\n\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.featureless\")\n\nlearner$train(task)\n\n\n\n\n\n\n\nRuntime and memory usage of $train() by mlr3 version and dataset size. The k factors indicate how many times longer the total runtime is compared to the model training time. The numbers represent the model training time itself e.g. k100 are models trained for 100 ms. A green background highlights cases where the total runtime is less than three times the model training time. The table includes runtime and memory usage for tasks of size 10, 100, 1,000 and 10,000.\n\n\nmlr3\nTask Size\nOverhead, ms\nk1000\nk100\nk10\nk1\nMemory, mb\n\n\n\n\n10 Observations\n\n\n1.3.0\n10\n5\n1.0\n1.1\n1.5\n6.2\n147\n\n\n1.2.0\n10\n5\n1.0\n1.1\n1.5\n6.3\n146\n\n\n1.1.0\n10\n5\n1.0\n1.0\n1.5\n5.9\n151\n\n\n100 Observations\n\n\n1.3.0\n100\n5\n1.0\n1.1\n1.5\n6.2\n150\n\n\n1.2.0\n100\n5\n1.0\n1.1\n1.5\n6.2\n148\n\n\n1.1.0\n100\n5\n1.0\n1.0\n1.5\n6.0\n146\n\n\n1000 Observations\n\n\n1.3.0\n1000\n5\n1.0\n1.1\n1.5\n6.5\n150\n\n\n1.2.0\n1000\n5\n1.0\n1.1\n1.5\n6.4\n148\n\n\n1.1.0\n1000\n5\n1.0\n1.1\n1.5\n6.2\n146\n\n\n10000 Observations\n\n\n1.3.0\n10000\n6\n1.0\n1.1\n1.6\n7.0\n174\n\n\n1.2.0\n10000\n6\n1.0\n1.1\n1.6\n7.1\n172\n\n\n1.1.0\n10000\n6\n1.0\n1.1\n1.6\n6.9\n174\n\n\n\n\n\n\n\n\n\nPredict\nThe runtime of $predict() is measured across mlr3 versions.\n\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.featureless\")\n\nlearner$train(task)\n\nlearner$predict(task)\n\n\n\n\n\n\n\nRuntime and memory usage of $predict() by mlr3 version and dataset size. The k factors indicate how many times longer the total runtime is compared to the model training time. The numbers represent the model training time itself e.g. k100 are models trained for 100 ms. A green background highlights cases where the total runtime is less than three times the model training time. The table includes runtime and memory usage for tasks of size 10, 100, 1,000 and 10,000.\n\n\nmlr3\nTask Size\nOverhead, ms\nk1000\nk100\nk10\nk1\nMemory, mb\n\n\n\n\n10 Observations\n\n\n1.3.0\n10\n4\n1.0\n1.0\n1.4\n4.9\n148\n\n\n1.2.0\n10\n4\n1.0\n1.0\n1.4\n4.9\n147\n\n\n1.1.0\n10\n5\n1.0\n1.1\n1.5\n6.2\n146\n\n\n100 Observations\n\n\n1.3.0\n100\n4\n1.0\n1.0\n1.4\n4.9\n146\n\n\n1.2.0\n100\n4\n1.0\n1.0\n1.4\n5.0\n146\n\n\n1.1.0\n100\n5\n1.0\n1.1\n1.5\n6.2\n147\n\n\n1000 Observations\n\n\n1.3.0\n1000\n4\n1.0\n1.0\n1.4\n5.0\n151\n\n\n1.2.0\n1000\n4\n1.0\n1.0\n1.4\n5.1\n151\n\n\n1.1.0\n1000\n5\n1.0\n1.1\n1.5\n6.4\n148\n\n\n10000 Observations\n\n\n1.3.0\n10000\n5\n1.0\n1.1\n1.5\n6.0\n174\n\n\n1.2.0\n10000\n5\n1.0\n1.1\n1.5\n6.1\n169\n\n\n1.1.0\n10000\n6\n1.0\n1.1\n1.6\n7.2\n177\n\n\n\n\n\n\n\n\n\nPredict Newdata\nThe runtime of $predict_newdata() is measured across mlr3 versions.\n\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.featureless\")\n\nlearner$train(task)\n\nlearner$predict_newdata(newdata)\n\n\n\n\n\n\n\nRuntime and memory usage of $predict_newdata() by mlr3 version and dataset size. The k factors indicate how many times longer the total runtime is compared to the model training time. The numbers represent the model training time itself e.g. k100 are models trained for 100 ms. A green background highlights cases where the total runtime is less than three times the model training time. The table includes runtime and memory usage for tasks of size 10, 100, 1,000 and 10,000.\n\n\nmlr3\nTask Size\nOverhead, ms\nk1000\nk100\nk10\nk1\nMemory, mb\n\n\n\n\n10 Observations\n\n\n1.3.0\n10\n20\n1.0\n1.2\n3.0\n21\n156\n\n\n1.2.0\n10\n20\n1.0\n1.2\n3.0\n21\n152\n\n\n1.1.0\n10\n21\n1.0\n1.2\n3.1\n22\n152\n\n\n100 Observations\n\n\n1.3.0\n100\n20\n1.0\n1.2\n3.0\n21\n151\n\n\n1.2.0\n100\n20\n1.0\n1.2\n3.0\n21\n153\n\n\n1.1.0\n100\n21\n1.0\n1.2\n3.1\n22\n157\n\n\n1000 Observations\n\n\n1.3.0\n1000\n21\n1.0\n1.2\n3.1\n22\n153\n\n\n1.2.0\n1000\n21\n1.0\n1.2\n3.1\n22\n154\n\n\n1.1.0\n1000\n22\n1.0\n1.2\n3.2\n23\n154\n\n\n10000 Observations\n\n\n1.3.0\n10000\n33\n1.0\n1.3\n4.3\n34\n182\n\n\n1.2.0\n10000\n33\n1.0\n1.3\n4.3\n34\n173\n\n\n1.1.0\n10000\n34\n1.0\n1.3\n4.4\n35\n181\n\n\n\n\n\n\n\n\n\nPredict Newdata Fast\nThe runtime of $predict_newdata_fast() is measured across mlr3 versions.\n\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.featureless\")\n\nlearner$train(task)\n\nlearner$predict_newdata_fast(task)\n\n\n\n\n\n\n\nRuntime and memory usage of $predict_newdata_fast() by mlr3 version and dataset size. The k factors indicate how many times longer the total runtime is compared to the model training time. The numbers represent the model training time itself e.g. k100 are models trained for 100 ms. A green background highlights cases where the total runtime is less than three times the model training time. The table includes runtime and memory usage for tasks of size 10, 100, 1,000 and 10,000.\n\n\nmlr3\nTask Size\nOverhead, ms\nk1000\nk100\nk10\nk1\nMemory, mb\n\n\n\n\n10 Observations\n\n\n1.3.0\n10\n0\n1.0\n1.0\n1.0\n1.3\n149\n\n\n1.2.0\n10\n0\n1.0\n1.0\n1.0\n1.3\n151\n\n\n1.1.0\n10\n0\n1.0\n1.0\n1.0\n1.3\n150\n\n\n100 Observations\n\n\n1.3.0\n100\n0\n1.0\n1.0\n1.0\n1.3\n150\n\n\n1.2.0\n100\n0\n1.0\n1.0\n1.0\n1.3\n153\n\n\n1.1.0\n100\n0\n1.0\n1.0\n1.0\n1.3\n150\n\n\n1000 Observations\n\n\n1.3.0\n1000\n0\n1.0\n1.0\n1.0\n1.4\n154\n\n\n1.2.0\n1000\n0\n1.0\n1.0\n1.0\n1.4\n153\n\n\n1.1.0\n1000\n0\n1.0\n1.0\n1.0\n1.4\n155\n\n\n10000 Observations\n\n\n1.3.0\n10000\n1\n1.0\n1.0\n1.1\n2.0\n162\n\n\n1.2.0\n10000\n1\n1.0\n1.0\n1.1\n2.1\n165\n\n\n1.1.0\n10000\n1\n1.0\n1.0\n1.1\n2.0\n160\n\n\n\n\n\n\n\n\n\nResampling\nThe runtime and memory usage of resample() are measured across mlr3 versions. The number of resampling iterations (evals) is set to 1000, 100, and 10. We also measure the runtime of resample() with future::multisession parallelization on 10 cores.\n\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.featureless\")\n\nresampling = rsmp(\"subsampling\", repeats = evals)\n\nresample(task, learner, resampling)\n\n\n\n\n\n\n\nRuntime and memory usage of resample() by mlr3 version and resampling iterations on the spam dataset with 1,000 observations. The k factors indicate how many times longer the total runtime is compared to the model training time. The numbers represent the model training time itself e.g. k100 are models trained for 100 ms. A green background highlights cases where the total runtime is less than three times the model training time. The pk factors indicate how many times faster the parallel runtime is compared to the sequential runtime. No pk factor is shown when the parallel runtime is slower than the sequential runtime.\n\n\nmlr3\nResampling Iterations\nRuntime, s\nk1000\nk100\nk10\nk1\nMemory, mb\npk1000\npk100\npk10\npk1\n\n\n\n\n10 Resampling Iterations\n\n\n1.3.0\n10\n139\n1.0\n1.1\n2.4\n15\n150\n43\n4.8\n1.6\n1.1\n\n\n1.2.0\n10\n140\n1.0\n1.1\n2.4\n15\n147\n43\n4.8\n1.6\n1.1\n\n\n1.1.0\n10\n135\n1.0\n1.1\n2.4\n15\n148\n43\n4.8\n1.6\n1.1\n\n\n100 Resampling Iterations\n\n\n1.3.0\n100\n1285\n1.0\n1.1\n2.3\n14\n156\n82\n9.2\n7.0\n5.8\n\n\n1.2.0\n100\n1261\n1.0\n1.1\n2.3\n14\n155\n83\n9.2\n7.0\n5.8\n\n\n1.1.0\n100\n1207\n1.0\n1.1\n2.2\n13\n156\n83\n9.2\n6.9\n5.7\n\n\n1000 Resampling Iterations\n\n\n1.3.0\n1000\n12772\n1.0\n1.1\n2.3\n14\n268\n76\n8.5\n5.2\n4.0\n\n\n1.2.0\n1000\n12600\n1.0\n1.1\n2.3\n14\n264\n75\n8.4\n5.1\n3.8\n\n\n1.1.0\n1000\n11993\n1.0\n1.1\n2.2\n13\n274\n75\n8.3\n4.9\n3.6\n\n\n\n\n\n\n\n\n\nBenchmark\nThe runtime and memory usage of benchmark() are measured across mlr3 versions. The number of resampling iterations (evals) is set to 1000, 100, and 10. We also measure the runtime of benchmark() with future::multisession parallelization on 10 cores.\n\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.featureless\")\nresampling = rsmp(\"subsampling\", repeats = evals / 5)\n\ndesign = benchmark_grid(task, replicate(5, learner), resampling)\n\nbenchmark(design)\n\n\n\n\n\n\n\nRuntime and memory usage of benchmark() by mlr3 version and resampling iterations on the spam dataset with 1,000 observations. The k factors indicate how many times longer the total runtime is compared to the model training time. The numbers represent the model training time itself e.g. k100 are models trained for 100 ms. A green background highlights cases where the total runtime is less than three times the model training time. The pk factors indicate how many times faster the parallel runtime is compared to the sequential runtime. No pk factor is shown when the parallel runtime is slower than the sequential runtime.\n\n\nmlr3\nResampling Iterations\nRuntime, s\nk1000\nk100\nk10\nk1\nMemory, mb\npk1000\npk100\npk10\npk1\n\n\n\n\n10 Resampling Iterations\n\n\n1.3.0\n10\n158\n1.0\n1.2\n2.6\n17\n151\n9.0\n1.0\n—\n—\n\n\n1.2.0\n10\n161\n1.0\n1.2\n2.6\n17\n152\n9.1\n1.0\n—\n—\n\n\n1.1.0\n10\n150\n1.0\n1.2\n2.5\n16\n151\n9.1\n1.0\n—\n—\n\n\n100 Resampling Iterations\n\n\n1.3.0\n100\n1291\n1.0\n1.1\n2.3\n14\n154\n45\n5.0\n1.7\n1.1\n\n\n1.2.0\n100\n1280\n1.0\n1.1\n2.3\n14\n155\n45\n5.0\n1.7\n1.1\n\n\n1.1.0\n100\n1236\n1.0\n1.1\n2.2\n13\n156\n46\n5.1\n1.7\n1.1\n\n\n1000 Resampling Iterations\n\n\n1.3.0\n1000\n12601\n1.0\n1.1\n2.3\n14\n260\n61\n6.8\n3.0\n2.1\n\n\n1.2.0\n1000\n13883\n1.0\n1.1\n2.4\n15\n260\n61\n6.9\n3.2\n2.2\n\n\n1.1.0\n1000\n14129\n1.0\n1.1\n2.4\n15\n258\n62\n7.0\n3.3\n2.3\n\n\n\n\n\n\n\n\n\nEncapsulation\nThe runtime and memory usage of $train() are measured for different encapsulation methods and mlr3 versions.\n\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.featureless\")\nlearner$encapsulate(method, fallback = lrn(\"classif.featureless\"))\n\nlearner$train(task)\n\n\n\n\n\n\n\nRuntime and memory usage of $train() by mlr3 version and encapsulation method. The k factors indicate how many times longer the total runtime is compared to the model training time. The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.\n\n\nmlr3\nMethod\nRuntime, s\nk1000\nk100\nk10\nk1\nMemory, mb\n\n\n\n\nNo Encapsulation\n\n\n1.3.0\nnone\n16\n1.0\n1.2\n2.6\n17\n150\n\n\n1.2.0\nnone\n15\n1.0\n1.1\n2.5\n16\n150\n\n\n1.1.0\nnone\n13\n1.0\n1.1\n2.3\n14\n149\n\n\nEvaluate\n\n\n1.3.0\nevaluate\n32\n1.0\n1.3\n4.2\n33\n152\n\n\n1.2.0\nevaluate\n30\n1.0\n1.3\n4.0\n31\n148\n\n\n1.1.0\nevaluate\n27\n1.0\n1.3\n3.7\n28\n152\n\n\nCallr\n\n\n1.3.0\ncallr\n3696\n4.7\n38\n370\n3,700\n151\n\n\n1.2.0\ncallr\n3452\n4.5\n36\n350\n3,500\n149\n\n\n1.1.0\ncallr\n1935\n2.9\n20\n190\n1,900\n149"
  },
  {
    "objectID": "benchmarks/benchmarks_async.html",
    "href": "benchmarks/benchmarks_async.html",
    "title": "Asynchronous Optimization Runtime Benchmarks",
    "section": "",
    "text": "This report evaluates the performance of asynchronous optimization algorithms on high-performance clusters. It aims to help users assess whether their workflow runtimes fall within expected ranges. If significant utilization anomalies arise, users are encouraged to report them via a GitHub issue.\nThe study optimizes eight hyperparameters of the XGBoost learner using the Bank Marketing dataset. This dataset, derived from a Portuguese bank’s marketing campaign, includes client demographics, financial details, and previous interactions to predict term deposit subscriptions. To ensure a fair comparison, 20% of the data was reserved for a holdout set, while the remaining data underwent 3-fold cross-validation during optimization. Performance was measured using the area under the receiver operating characteristic curve (AUC).\nThis report primarily focuses on computing resource utilization rather than algorithmic performance. A rigorous comparison of algorithm performance would require additional datasets and nested resampling. Experiments were conducted on 10 nodes, each with 128 cores, totaling 1,280 workers. The optimization ran for 30 minutes, consuming 640 CPU hours."
  },
  {
    "objectID": "benchmarks/benchmarks_async.html#hyperband",
    "href": "benchmarks/benchmarks_async.html#hyperband",
    "title": "Asynchronous Optimization Runtime Benchmarks",
    "section": "Hyperband",
    "text": "Hyperband\nFigure 4 illustrates the number of hyperparameter configurations evaluated per stage for ASHA and ASHA Hotstart. As expected, the number of configurations decreases by a factor of 3 at each stage, following the successive halving principle. With hotstarting, more configurations are evaluated in all stages. This increase is due to the reduced training time, as models continue training from previous stages rather than restarting from scratch. As a result, ASHA Hotstart improves resource efficiency, enabling a greater number of configurations to be explored within the same computational budget.\n\n\n\n\n\n\n\n\nFigure 4: Number of evaluated hyperparameter configurations per stage for ASHA and ASHA Hotstart.\n\n\n\n\n\nTable 2 presents the mean runtime of the learners at each stage of the ASHA algorithms. As expected, the runtime increases with the number of boosting rounds, since later stages allocate more computational resources to promising configurations. However, in the hotstarting variant, the runtime is lower in the later stages. This is because models continue training from previous stages rather than starting from scratch.\n\n\n\n\nTable 2: Mean runtime of the learners, number of configurations and boosting rounds at each stage for ASHA and ASHA Hotstart.\n\n\n\n\n\n\n\n\n\nTuner\nStage\nMean Runtime (s)\nNumber of Configurations\nNumber of Boosting Rounds\n\n\n\n\nasha\n1\n2\n285,031\n9\n\n\nasha\n2\n4\n95,114\n27\n\n\nasha\n3\n11\n32,099\n81\n\n\nasha\n4\n27\n10,790\n243\n\n\nasha\n5\n75\n3,464\n729\n\n\nasha\n6\n192\n959\n2187\n\n\nasha_hotstart\n1\n2\n342,489\n9\n\n\nasha_hotstart\n2\n3\n113,986\n27\n\n\nasha_hotstart\n3\n8\n38,315\n81\n\n\nasha_hotstart\n4\n19\n12,950\n243\n\n\nasha_hotstart\n5\n53\n4,211\n729\n\n\nasha_hotstart\n6\n135\n1,237\n2187\n\n\n\n\n\n\n\n\n\n\nAs an example, Figure 5 presents the marginal plot of the learning rate (eta) and the regularization parameter (alpha) from the ASHA Hotstart run. The plot visualizes the configurations evaluated by the algorithm at each stage, where each point represents a configuration and the color indicates the corresponding holdout AUC value. In the first stage, a large number of configurations are tested quickly with a small training budget. The plot reveals that high alpha values lead to lower AUC scores and are therefore not promoted to the next stage. As the algorithm progresses, it shifts focus toward the more promising regions of the hyperparameter space. By the final stages, fewer configurations remain, but they benefit from a greater number of boosting rounds. The algorithm tends to favor configurations with moderate eta values, suggesting that extreme learning rates are less effective in this scenario.\n\n\n\n\n\n\n\n\nFigure 5: Marginal plot of eta and alpha for ASHA Hotstart. Each point represents a configuration evaluated by ASHA Hotstart. The color shows the holdout AUC value. The facets show the number of boosting rounds or stages."
  },
  {
    "objectID": "benchmarks/benchmarks_async.html#adbo",
    "href": "benchmarks/benchmarks_async.html#adbo",
    "title": "Asynchronous Optimization Runtime Benchmarks",
    "section": "ADBO",
    "text": "ADBO\nIn addition to measuring the runtime of the learners, we also tracked the runtime of the surrogate model training and the runtime of the acquisition optimizer (Table 3). The following table presents the proportional share of each component—learner training, surrogate model training, and acquisition optimization—relative to the total worker runtime. This breakdown provides insights into how computational resources are distributed among different aspects of the optimization process. Understanding these contributions helps in identifying potential bottlenecks and optimizing the balance between model evaluation and search efficiency.\n\n\n\n\nTable 3: Share of the runtime of the learners, surrogate model, and acquisition optimizer on the runtime of the workers for ADBO.\n\n\n\n\n\n\n\n\n\nMean Utilization Runtime Learners\nMean Utilization Runtime Acquisition Optimizer\nMean Utilization Surrogate\nMean Utilization\n\n\n\n\n94.3%\n2.8%\n0.7%\n97.9%"
  },
  {
    "objectID": "pipeops.html",
    "href": "pipeops.html",
    "title": "Pipeline Operators",
    "section": "",
    "text": "Pipeline Operators\nObjects of class PipeOp are the building blocks to compose linear machine learning pipelines and non-linear Graphs. The base objects are implemented in mlr3pipelines."
  },
  {
    "objectID": "team/about-bernd.html",
    "href": "team/about-bernd.html",
    "title": "Bernd Bischl",
    "section": "",
    "text": "Professor of Statistical Learning and Data Science at the LMU Munich. I created mlr a long time ago at the beginning of my PhD. Nowadays, I spend most of my time in project supervision, code reviews and helping to design new parts of the framework. I was part of the design process of nearly all parts of the new mlr3, but nearly all code has been written by the other developers."
  },
  {
    "objectID": "team/about-sebastian.html",
    "href": "team/about-sebastian.html",
    "title": "Sebastian Fischer",
    "section": "",
    "text": "Research Engineer at LMU Munich. Working on mlr3torch, mlr3oml and maintaining mlr3extralearners."
  },
  {
    "objectID": "team/about-marc.html",
    "href": "team/about-marc.html",
    "title": "Marc Becker",
    "section": "",
    "text": "Research engineer at the LMU Munich and main developer of the mlr3 optimization packages."
  },
  {
    "objectID": "team/about-patrick.html",
    "href": "team/about-patrick.html",
    "title": "Patrick Schratz",
    "section": "",
    "text": "R consultant in Zurich, Switzerland. PhD Candidate in environmental modeling. Mainly contributing to spatiotemporal packages. Maintainer of the old mlr package."
  },
  {
    "objectID": "team/about-michel.html",
    "href": "team/about-michel.html",
    "title": "Michel Lang",
    "section": "",
    "text": "Postdoc at the TU Dortmund and one of the main developers of mlr. I’ve worked on many internal parts of mlr and started to implement support for survival analysis. Now main developer of mlr3."
  },
  {
    "objectID": "book.html",
    "href": "book.html",
    "title": "Book",
    "section": "",
    "text": "The “mlr3book” is a free manual written in an online-book style available in two versions.\nIt is the main reference for the mlr3 ecosystem.\n\nHTML\nPDF"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Why is there only the rpart learner?\nHow can I use parallelization?\nWhy is the parallelization with the future package slow?\nWhy is the parallelization of tuning slow?\nWhy are the CPUs on my system not fully utilized when using parallelization?\nHow can I use time constraints when tuning?\nWhy is method X slower when used via mlr3?\nPreprocessing factor levels\nMemory Problems\nHow can I suppress logging output of learners on the R console\nA learner trained with an old mlr3 version does not work anymore\nCaching of knitr/rmarkdown chunks does not work with mlr3\nHow to keep all mlr3 packages up-to-date?"
  },
  {
    "objectID": "faq.html#learner",
    "href": "faq.html#learner",
    "title": "Frequently Asked Questions",
    "section": "Why is there only the rpart learner?",
    "text": "Why is there only the rpart learner?\nThe base package mlr3 ships only with regression and classification trees from the rpart package and some learners for debugging. A selection of popular learners can be found in the extension package mlr3learners. Survival learners are provided by mlr3proba, cluster learners via mlr3cluster. Additional learners can be found in the extension packages mlr3extralearners. If your favorite learner is missing, please open a learner request. An overview of all learners can be found on our website."
  },
  {
    "objectID": "faq.html#parallelization",
    "href": "faq.html#parallelization",
    "title": "Frequently Asked Questions",
    "section": "How can I use parallelization?",
    "text": "How can I use parallelization?\nParallelization is supported when training learners, resampling, tuning and predicting. We recommend reading the section about Parallelization in the mlr3book."
  },
  {
    "objectID": "faq.html#parallelization-slow",
    "href": "faq.html#parallelization-slow",
    "title": "Frequently Asked Questions",
    "section": "Why is the parallelization with the future package slow?",
    "text": "Why is the parallelization with the future package slow?\nStarting and terminating workers as well as possible communication between workers comes at a price in the form of additionally required runtime which is called parallelization overhead. This overhead strongly varies between parallelization backends and must be carefully weighed against the runtime of the sequential execution to determine if parallelization is worth the effort. When resampling or tuning a fast-fitting learner, it helps to chunk multiple resampling iterations into a single computational job. The option mlr3.exec_chunk_bins determines the number of chunks to split the resampling iterations into. For example, when running a benchmark with 100 resampling iterations, options(\"mlr3.exec_chunk_bins\" = 4) creates 4 computational jobs with 25 resampling iterations each. This reduces the parallelization overhead and speeds up the execution. The parallelization of the BLAS library can interfere with future parallelization due to over-utilization of the available cores. Install RhpcBLASctl so that mlr3 can turn off the parallelization of BLAS. RhpcBLASctl can only be included as an optional dependency due to licensing issues."
  },
  {
    "objectID": "faq.html#tuning-slow",
    "href": "faq.html#tuning-slow",
    "title": "Frequently Asked Questions",
    "section": "Why is the parallelization of tuning slow?",
    "text": "Why is the parallelization of tuning slow?\nTuning can also suffer from the parallelization overhead described above. Additionally, the batch size of the tuner can have a large impact on the runtime. Setting an optimal batch size is explained in the section Parallelization of Tuning of the mlr3book."
  },
  {
    "objectID": "faq.html#parallelization-cpu",
    "href": "faq.html#parallelization-cpu",
    "title": "Frequently Asked Questions",
    "section": "Why are the CPUs on my system not fully utilized when using parallelization?",
    "text": "Why are the CPUs on my system not fully utilized when using parallelization?\nIf there are few jobs with dissimilar runtimes, the system may end up waiting for the last chunk to finish, while other resources are idle. This is referred to as synchronization overhead. When minimizing the synchronization overhead, a too large chunk size can lead to a situation where the last chunk takes much longer than the others. This can be avoided by setting mlr3.exec_chunk_bins to a smaller value than the number of cores available on the system."
  },
  {
    "objectID": "faq.html#how-can-i-use-time-constraints-when-tuning",
    "href": "faq.html#how-can-i-use-time-constraints-when-tuning",
    "title": "Frequently Asked Questions",
    "section": "How can I use time constraints when tuning?",
    "text": "How can I use time constraints when tuning?\nTime constraints can be set for individual learners, tuning processes, and nested resampling. The gallery post Time constraints in the mlr3 ecosystem provides an overview of the different options."
  },
  {
    "objectID": "faq.html#mlr3-default-slower",
    "href": "faq.html#mlr3-default-slower",
    "title": "Frequently Asked Questions",
    "section": "Why is method X slower when used via mlr3?",
    "text": "Why is method X slower when used via mlr3?\nBy default, we set the number of threads of learners to 1 to avoid conflicts with parallelization. Therefore, the default configuration of a learner may be significantly slower than the default configuration of the method when used directly."
  },
  {
    "objectID": "faq.html#factor-levels",
    "href": "faq.html#factor-levels",
    "title": "Frequently Asked Questions",
    "section": "Preprocessing factor levels",
    "text": "Preprocessing factor levels\nWhen working with mlr3, it is important to avoid using special characters in the levels of factor variables. The presence of symbols such as +, -, &lt;, &gt;, =, or spaces in the factor levels can cause errors during model training (depends on the learner used and if the formula interface is utilized, e.g. as in the surv.parametric learner). While underscores (_) and dots (.) are generally safe to use, other special characters should be avoided. To ensure smooth operation and prevent errors, please follow these guidelines:\n\nUse descriptive labels with no special characters: Assign meaningful and descriptive labels to factor levels that do not include special characters. For example, instead of 60+ for a factor level of an age feature, use 60_above.\nUse factor encoding: Incorporate a pre-processing step in your data pipeline (e.g. see mlr_pipeops_encode to make sure factors are one-hot encoded, alleviating problems that may arise from factor levels that incorporate strange symbols."
  },
  {
    "objectID": "faq.html#memory-problems",
    "href": "faq.html#memory-problems",
    "title": "Frequently Asked Questions",
    "section": "Memory Problems",
    "text": "Memory Problems\nOne explanation for why mlr3 might in some cases use an unusual amount of memory, is when packages are installed with the --with-keep.source flag. This configuration option is enabled by default when managing dependencies via renv, see issue #1713. To opt out of this default run the code below, e.g. by adding it to your .Rprofile:\n\noptions(\"install.opts\" = \"--without-keep.source\")"
  },
  {
    "objectID": "faq.html#suppress-output",
    "href": "faq.html#suppress-output",
    "title": "Frequently Asked Questions",
    "section": "How can I suppress logging output of learners on the R console",
    "text": "How can I suppress logging output of learners on the R console\nSome learners are quite verbose during their train or predict step, and this clutters the R console. Note that this is different than controlling the generic mlr3 logger, which is covered under Logging. Most of these learners provide some option in their paramset to control output behavior. Another option is to simply use Encapsulation, likely in the evaluate mode, running the learner in the same R session, but with caught exceptions and redirected output.\n\nlibrary(mlr3)\nlibrary(mlr3learners)\nmytask = tsk(\"iris\")\n# manual option\nmylearner = lrn(\"classif.nnet\", trace = TRUE)\n# generic option\nmylearner$encapsulate(method = \"evaluate\", fallback = lrn(\"classif.featureless\"))\n\nWarning: \n✖ The fallback learner 'classif.featureless' and the base learner\n  'classif.nnet' have different predict types: 'response' != 'prob'.\n→ Class: Mlr3WarningConfigFallbackPredictType\n\nmylearner$train(mytask)\n\nWarning in rbindlist(unname(map(.x, .f, ...)), use.names = TRUE, fill = .fill,\n: Column 3 ['condition'] of item 1 is length 0. This (and 0 others like it) has\nbeen filled with NA (NULL for list columns) to make each item uniform."
  },
  {
    "objectID": "faq.html#old-model-broken",
    "href": "faq.html#old-model-broken",
    "title": "Frequently Asked Questions",
    "section": "An object saved with an old mlr3 version does not work anymore",
    "text": "An object saved with an old mlr3 version does not work anymore\nIt is possible that a saved Learner that was trained with an old mlr3 version does not work with a different version of mlr3. In general, we recommend saving the computational environment using a tool like renv so this can later be restored and avoiding such situations alltogether. If this is not an option, a possible workaround is to construct the same learner in the currently used mlr3 version and manually set its $state to the one of the saved learner. This is illustrated below:\n\nUsing an old mlr3 version:\n\nlearner = lrn(\"classif.rpart\")\nlearner$train(tsk(\"iris\"))\nsaveRDS(learner, \"learner.rds\")\n\nWith a subsequent mlr3 version:\n\nlearner = lrn(\"classif.rpart\")\nlearner_old = readRDS(\"learner.rds\")\nlearner$state = learner_old$state\n\n\nNote that this is risky and not guaranteed to work because of various reasons: * You might have now loaded a different version of the learner library (in this case the rpart pacakge). * The internals (such as the structure of the internal $state) might have changed between the versions.\nTherefore, be careful when attempting this solution and double-check that the learner behaves sensibly.\nSince mlr3 1.0.0 we test if old objects are compatible with the current version of mlr3. We found the following incompatibility issues:\n\nTask objects stored with mlr3 &lt;= 1.1.0 cannot be loaded with mlr3 &gt;= 1.2.0 because the $data() method of the DataBackend class lost the data_format argument. This also makes the Task objects stored in ResampleResult and BenchmarkResult objects incompatible.\nResampleResult and BenchmarkResult objects stored with paradox &lt;= 1.0.1 cannot be loaded with paradox &gt;= 1.1.0 because presence and allow_token were added to the $check() method of the ParamSet class. Other objects with ParamSet objects could also be affected."
  },
  {
    "objectID": "faq.html#caching-knitr",
    "href": "faq.html#caching-knitr",
    "title": "Frequently Asked Questions",
    "section": "Caching of knitr/rmarkdown chunks does not work with mlr3",
    "text": "Caching of knitr/rmarkdown chunks does not work with mlr3\n{knitr} per default uses R’s lazy-load database to store the results of individual chunks. The lazy-load database is an internal feature of R, and has issues handling active bindings (https://github.com/r-lib/R6/issues/152). Fortunately, it is possible to disable lazy-loading by setting the chunk option cache.lazy to FALSE:\n\nknitr::opts_chunk$set(cache = TRUE, cache.lazy = FALSE)"
  },
  {
    "objectID": "faq.html#update-packages",
    "href": "faq.html#update-packages",
    "title": "Frequently Asked Questions",
    "section": "How to keep all mlr3 packages up-to-date?",
    "text": "How to keep all mlr3 packages up-to-date?\nEither run R’s update.packages() to update all installed packages, or run\ndevtools::update_packages(\"mlr3verse\", dependencies = TRUE)\nto update only packages from the mlr3verse. Note that this also updates recursive dependencies not listed as a direct import."
  },
  {
    "objectID": "gallery/optimization/2023-02-07-recursive-feature-elimination/index.html",
    "href": "gallery/optimization/2023-02-07-recursive-feature-elimination/index.html",
    "title": "Recursive Feature Elimination on the Sonar Data Set",
    "section": "",
    "text": "Scope\nFeature selection is the process of finding an optimal subset of features in order to improve the performance, interpretability and robustness of machine learning algorithms. In this article, we introduce the wrapper feature selection method Recursive Feature Elimination. Wrapper methods iteratively select features that optimize a performance measure. As an example, we will search for the optimal set of features for a gradient boosting machine and support vector machine on the Sonar data set. We assume that you are already familiar with the basic building blocks of the mlr3 ecosystem. If you are new to feature selection, we recommend reading the feature selection chapter of the mlr3book first.\n\n\nRecursive Feature Elimination\nRecursive Feature Elimination (RFE) is a widely used feature selection method for high-dimensional data sets. The idea is to iteratively remove the least predictive feature from a model until the desired number of features is reached. This feature is determined by the built-in feature importance method of the model. Currently, RFE works with support vector machines (SVM), decision tree algorithms and gradient boosting machines (GBM). Supported learners are tagged with the \"importance\" property. For a full list of supported learners, see the learner page on the mlr-org website and search for \"importance\".\nGuyon et al. (2002) developed the RFE algorithm for SVMs (SVM-RFE) to select informative genes in cancer classification. The importance of the features is given by the weight vector of a linear support vector machine. This method was later extended to other machine learning algorithms. The only requirement is that the models can internally measure the feature importance. The random forest algorithm offers multiple options for measuring feature importance. The commonly used methods are the mean decrease in accuracy (MDA) and the mean decrease in impurity (MDI). The MDA measures the decrease in accuracy for a feature if it was randomly permuted in the out-of-bag sample. The MDI is the total reduction in node impurity when the feature is used for splitting. Gradient boosting algorithms like XGBoost, LightGBM and GBM use similar methods to measure the importance of the features.\nResampling strategies can be combined with the algorithm in different ways. The frameworks scikit-learn (Pedregosa et al. 2011) and caret (Kuhn 2008) implement a variant called recursive feature elimination with cross-validation (RFE-CV) that estimates the optimal number of features with cross-validation first. Then one more RFE is carried out on the complete dataset with the optimal number of features as the final feature set size. The RFE implementation in mlr3 can rank and aggregate importance scores across resampling iterations. We will explore both variants in more detail below.\nmlr3fselect is the feature selection package of the mlr3 ecosystem. It implements the RFE and RFE-CV algorithm. We load all packages of the ecosystem with the mlr3verse package.\n\nlibrary(mlr3verse)\n\nWe retrieve the RFE optimizer with the fs() function.\n\noptimizer = fs(\"rfe\",\n  n_features = 1,\n  feature_number = 1,\n  aggregation = \"rank\")\n\nThe algorithm has multiple control parameters. The optimizer stops when the number of features equals n_features. The parameters feature_number, feature_fraction and subset_size determine the number of features that are removed in each iteration. The feature_number option removes a fixed number of features in each iteration, whereas feature_fraction removes a fraction of the features. The subset_size argument is a vector that specifies exactly how many features are removed in each iteration. The parameters are mutually exclusive and the default is feature_fraction = 0.5. Usually, RFE fits a new model in each resampling iteration and calculates the feature importance again. We can deactivate this behavior by setting recursive = FALSE. The selection of feature subsets in all iterations is then based solely on the importance scores of the first model trained with all features. When running an RFE with a resampling strategy like cross-validation, multiple models and importance scores are generated. The aggregation parameter determines how the importance scores are aggregated. The option \"rank\" ranks the importance scores in each iteration and then averages the ranks of the features. The feature with the lowest average rank is removed. The option \"mean\" averages the importance scores of the features across the iterations. The \"mean\" should only be used if the learner’s importance scores can be reasonably averaged.\n\n\nTask\nThe objective of the Sonar data set is to predict whether a sonar signal bounced off a metal cylinder or a rock. The data set includes 60 numerical features (see Figure 1).\n\ntask = tsk(\"sonar\")\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(data.table)\n\ndata = melt(as.data.table(task), id.vars = task$target_names, measure.vars = task$feature_names)\ndata = data[c(\"V1\", \"V10\", \"V11\", \"V12\", \"V13\", \"V14\"), , on = \"variable\"]\n\nggplot(data, aes(x = value, fill = Class)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ variable, ncol = 6, scales = \"free\") +\n  scale_fill_viridis_d(end = 0.8) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank())\n\n\n\n\n\n\n\n\nFigure 1: Distribution of the first 5 features in the Sonar dataset.\n\n\n\n\n\n\n\nGradient Boosting Machine\nWe start with the GBM learner and set the predict type to \"prob\" to obtain class probabilities.\n\nlearner = lrn(\"classif.gbm\",\n  distribution = \"bernoulli\",\n  predict_type = \"prob\")\n\nNow we define the feature selection problem by using the fsi() function that constructs an FSelectInstanceBatchSingleCrit. In addition to the task and learner, we have to select a resampling strategy and performance measure to determine how the performance of a feature subset is evaluated. We pass the \"none\" terminator because the n_features parameter of the optimizer determines when the feature selection stops.\n\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 6),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\"))\n\nWe are now ready to start the RFE. To do this, we simply pass the instance to the $optimize() method of the optimizer.\n\noptimizer$optimize(instance)\n\nThe optimizer saves the best feature set and the corresponding estimated performance in instance$result.\nFigure 2 shows the optimization path of the feature selection. We observe that the performance increases first as the number of features decreases. As soon as informative features are removed, the performance drops.\n\n\nCode\nlibrary(viridisLite)\nlibrary(mlr3misc)\n\ndata = as.data.table(instance$archive)\ndata[, n:= map_int(importance, length)]\n\nggplot(data, aes(x = n, y = classif.auc)) +\n  geom_line(\n    color = viridis(1, begin = 0.5),\n    linewidth = 1) +\n  geom_point(\n    fill = viridis(1, begin = 0.5),\n    shape = 21,\n    size = 3,\n    stroke = 0.5,\n    alpha = 0.8) +\n  xlab(\"Number of Features\") +\n  scale_x_reverse() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 2: Performance of the gradient-boosting models depending on the number of features.\n\n\n\n\n\nThe importance scores of the feature sets are recorded in the archive.\n\nas.data.table(instance$archive)[, list(features, classif.auc, importance)]\n\n                      features classif.auc                                                importance\n                        &lt;list&gt;       &lt;num&gt;                                                    &lt;list&gt;\n 1: V1,V10,V11,V12,V13,V14,...   0.8929304 58.83333,58.83333,54.50000,54.00000,53.33333,52.50000,...\n 2: V1,V10,V11,V12,V13,V15,...   0.9177811 57.33333,56.00000,54.00000,53.66667,50.50000,50.00000,...\n 3: V1,V10,V11,V12,V13,V15,...   0.9045253 54.83333,54.66667,54.66667,53.00000,51.83333,51.33333,...\n 4: V1,V10,V11,V12,V13,V15,...   0.8927833 56.00000,55.83333,53.00000,52.00000,50.16667,50.00000,...\n 5: V1,V10,V11,V12,V13,V15,...   0.9016274 55.50000,53.50000,51.33333,50.00000,49.00000,48.50000,...\n---                                                                                                 \n56:         V11,V12,V16,V48,V9   0.8311625              4.166667,3.333333,2.833333,2.500000,2.166667\n57:             V11,V12,V16,V9   0.8216772                       3.833333,2.666667,2.000000,1.500000\n58:                V11,V12,V16   0.8065807                                2.833333,1.833333,1.333333\n59:                    V11,V12   0.8023780                                         1.833333,1.166667\n60:                        V11   0.7515904                                                         1\n\n\n\n\nSupport Vector Machine\nNow we will select the optimal feature set for an SVM with a linear kernel. The importance scores are the weights of the model.\n\nlearner = lrn(\"classif.svm\",\n  type = \"C-classification\",\n  kernel = \"linear\",\n  predict_type = \"prob\")\n\nThe SVM learner does not support the calculation of importance scores at first. The reason is that importance scores cannot be determined with all kernels. This can be seen by the missing \"importance\" property.\n\nlearner$properties\n\n[1] \"multiclass\" \"twoclass\"  \n\n\nUsing the \"mlr3fselect.svm_rfe\" callback however makes it possible to use a linear SVM with the RFE optimizer. The callback adds the $importance() method internally to the learner. We load the callback with the clbk() function and pass it as the \"callback\" argument to fsi().\n\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 6),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\"),\n  callback = clbk(\"mlr3fselect.svm_rfe\"))\n\nWe start the feature selection.\n\noptimizer$optimize(instance)\n\nFigure 3 shows the average performance of the SVMs depending on the number of features. We can see that the performance increases significantly with a reduced feature set.\n\n\nCode\nlibrary(viridisLite)\nlibrary(mlr3misc)\n\ndata = as.data.table(instance$archive)\ndata[, n:= map_int(importance, length)]\n\nggplot(data, aes(x = n, y = classif.auc)) +\n  geom_line(\n    color = viridis(1, begin = 0.5),\n    linewidth = 1) +\n  geom_point(\n    fill = viridis(1, begin = 0.5),\n    shape = 21,\n    size = 3,\n    stroke = 0.5,\n    alpha = 0.8) +\n  xlab(\"Number of Features\") +\n  scale_x_reverse() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 3: Performance of the support vector machines depending on the number of features.\n\n\n\n\n\nFor datasets with a lot of features, it is more efficient to remove several features per iteration. We show an example where 25% of the features are removed in each iteration.\n\noptimizer = fs(\"rfe\", n_features = 1, feature_fraction = 0.75)\n\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 6),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\"),\n  callback = clbk(\"mlr3fselect.svm_rfe\"))\n\noptimizer$optimize(instance)\n\nFigure 4 shows a similar optimization curve as Figure 3 but with fewer evaluated feature sets.\n\n\nCode\nlibrary(viridisLite)\nlibrary(mlr3misc)\n\ndata = as.data.table(instance$archive)\ndata[, n:= map_int(importance, length)]\n\nggplot(data, aes(x = n, y = classif.auc)) +\n  geom_line(\n    color = viridis(1, begin = 0.5),\n    linewidth = 1) +\n  geom_point(\n    fill = viridis(1, begin = 0.5),\n    shape = 21,\n    size = 3,\n    stroke = 0.5,\n    alpha = 0.8) +\n  xlab(\"Number of Features\") +\n  scale_x_reverse() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 4: Optimization path of the feature selection.\n\n\n\n\n\n\n\nRecursive Feature Elimination with Cross Validation\nRFE-CV estimates the optimal number of features before selecting a feature set. For this, an RFE is run in each resampling iteration and the number of features with the best mean performance is selected (see Figure 5). Then one more RFE is carried out on the complete dataset with the optimal number of features as the final feature set size.\n\n\n\n\n\n\n%%{ init: { 'flowchart': { 'curve': 'bump' } } }%%\nflowchart TB\n    cross-validation[3-Fold Cross-Validation]\n    cross-validation--&gt;rfe-1\n    cross-validation--&gt;rfe-2\n    cross-validation--&gt;rfe-3\n    subgraph rfe-1[RFE 1]\n    direction TB\n    f14[4 Features]\n    f13[3 Features]\n    f12[2 Features]\n    f11[1 Features]\n    f14--&gt;f13--&gt;f12--&gt;f11\n    style f13 fill:#ccea84\n    end\n    subgraph rfe-2[RFE 2]\n    direction TB\n    f24[4 Features]\n    f23[3 Features]\n    f22[2 Features]\n    f21[1 Features]\n    f24--&gt;f23--&gt;f22--&gt;f21\n    style f23 fill:#ccea84\n    end\n    subgraph rfe-3[RFE 3]\n    direction TB\n    f34[4 Features]\n    f33[3 Features]\n    f32[2 Features]\n    f31[1 Features]\n    f34--&gt;f33--&gt;f32--&gt;f31\n    style f33 fill:#ccea84\n    end\n    all_obs[All Observations]\n    rfe-1--&gt;all_obs\n    rfe-2--&gt;all_obs\n    rfe-3--&gt;all_obs\n    all_obs --&gt; rfe\n    subgraph rfe[RFE]\n    direction TB\n    f54[4 Features]\n    f53[3 Features]\n    f54--&gt;f53\n    style f53 fill:#8e6698\n    end\n\n\n\n\nFigure 5: Example of an RFE-CV. The optimal number of features is estimated with a 3-fold cross-validation. One RFE is executed with each train-test split (RFE 1 to RFE 3). The number of features with the best mean performance (green rectangles) is used as the size of the final feature set. A final RFE is performed on all observations. The algorithm stops when the optimal feature set size is reached (purple rectangle) and the optimized feature set is returned.\n\n\n\n\n\nWe retrieve the RFE-CV optimizer. RFE-CV has almost the same control parameters as the RFE optimizer. The only difference is that no aggregation is needed.\n\noptimizer = fs(\"rfecv\",\n  n_features = 1,\n  feature_number = 1)\n\nThe chosen resampling strategy is used to estimate the optimal number of features. The 6-fold cross-validation results in 6 RFE runs. You can choose any other resampling strategy with multiple iterations. Let’s start the feature selection.\n\nlearner = lrn(\"classif.svm\",\n  type = \"C-classification\",\n  kernel = \"linear\",\n  predict_type = \"prob\")\n\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 6),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\"),\n  callback = clbk(\"mlr3fselect.svm_rfe\"))\n\noptimizer$optimize(instance)\n\n\n\n\n\n\n\nWarning\n\n\n\nThe performance of the optimal feature set is calculated on the complete data set and should not be reported as the performance of the final model. Estimate the performance of the final model with nested resampling.\n\n\nWe visualize the selection of the optimal number of features. Each point is the mean performance of the number of features. We achieved the best performance with 19 features.\n\n\nCode\nlibrary(ggplot2)\nlibrary(viridisLite)\nlibrary(mlr3misc)\n\ndata = as.data.table(instance$archive)[!is.na(iteration), ]\naggr = data[, list(\"y\" = mean(unlist(.SD))), by = \"batch_nr\", .SDcols = \"classif.auc\"]\naggr[, batch_nr := 61 - batch_nr]\n\ndata[, n:= map_int(importance, length)]\n\nggplot(aggr, aes(x = batch_nr, y = y)) +\n  geom_line(\n    color = viridis(1, begin = 0.5),\n    linewidth = 1) +\n  geom_point(\n    fill = viridis(1, begin = 0.5),\n    shape = 21,\n    size = 3,\n    stroke = 0.5,\n    alpha = 0.8) +\n  geom_vline(\n    xintercept = aggr[y == max(y)]$batch_nr,\n    colour = viridis(1, begin = 0.33),\n    linetype = 3\n  ) +\n  xlab(\"Number of Features\") +\n  ylab(\"Mean AUC\") +\n  scale_x_reverse() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 6: Estimation of the optimal number of features. The best mean performance is achieved with 19 features (blue line).\n\n\n\n\n\nThe archive contains the extra column \"iteration\" that indicates in which resampling iteration the feature set was evaluated. The feature subsets of the final RFE run have no value in the \"iteration\" column because they were evaluated on the complete data set.\n\nas.data.table(instance$archive)[, list(features, classif.auc, iteration, importance)]\n\n                       features classif.auc iteration                                                      importance\n                         &lt;list&gt;       &lt;num&gt;     &lt;int&gt;                                                          &lt;list&gt;\n  1: V1,V10,V11,V12,V13,V14,...   0.8782895         1       2.864018,1.532774,1.408485,1.399930,1.326165,1.167745,...\n  2: V1,V10,V11,V12,V13,V14,...   0.7026144         2       2.056442,1.706077,1.258703,1.191762,1.190752,1.178514,...\n  3: V1,V10,V11,V12,V13,V14,...   0.8790850         3       1.950412,1.887710,1.820891,1.616219,1.231928,1.138675,...\n  4: V1,V10,V11,V12,V13,V14,...   0.8125000         4 2.6958580,1.5623759,1.4990138,1.3902109,0.9385757,0.9232132,...\n  5: V1,V10,V11,V12,V13,V14,...   0.8807018         5       2.487483,1.470778,1.356517,1.033764,0.635383,0.575074,...\n ---                                                                                                                 \n398:  V1,V11,V12,V16,V23,V3,...   0.9605275        NA 2.0089739,1.1047492,1.0011253,0.6602411,0.6015470,0.5431803,...\n399:  V1,V12,V16,V23,V3,V30,...   0.9595988        NA 1.8337471,1.1937962,0.9853467,0.7751384,0.7296726,0.6222569,...\n400:  V1,V12,V16,V23,V3,V30,...   0.9589486        NA 1.8824952,1.2468164,1.0106654,0.8090618,0.6983925,0.6568389,...\n401:  V1,V12,V16,V23,V3,V30,...   0.9559766        NA 2.3872902,0.9094028,0.8809098,0.8277941,0.7841591,0.7792772,...\n402:  V1,V12,V16,V23,V3,V30,...   0.9521687        NA 1.9485133,1.1482257,1.1098823,0.9591012,0.8234140,0.8118616,...\n\n\n\n\nFinal Model\nThe learner we use to make predictions on new data is called the final model. The final model is trained with the optimal feature set on the full data set. The optimal set consists of 19 features and is stored in instance$result_feature_set. We subset the task to the optimal feature set and train the learner.\n\ntask$select(instance$result_feature_set)\nlearner$train(task)\n\nThe trained model can now be used to predict new, external data.\n\n\nConclusion\nThe RFE algorithm is a valuable feature selection method, especially for high-dimensional datasets with only a few observations. The numerous settings of the algorithm in mlr3 make it possible to apply it to many datasets and learners. If you want to know more about feature selection in general, we recommend having a look at our book.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   e1071               1.7-16     2024-09-16 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   farver              2.1.2      2024-05-13 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future              1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   gbm                 2.2.2      2024-06-28 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2           * 3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools           0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   labeling            0.4.3      2023-08-29 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n P Matrix              1.7-0      2024-04-26 [?] CRAN (R 4.4.0)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect       * 1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband       0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3measures        1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc          * 0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning          1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox             1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   proxy               0.4-27     2022-06-09 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n   scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n P survival            3.7-0      2024-06-05 [?] CRAN (R 4.4.0)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   viridisLite       * 0.4.2      2023-05-02 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\nReferences\n\nGuyon, Isabelle, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. 2002. “Gene Selection for Cancer Classification Using Support Vector Machines.” Machine Learning 46 (1): 389–422. https://doi.org/10.1023/A:1012487302797.\n\n\nKuhn, Max. 2008. “Building Predictive Models in r Using the Caret Package.” Journal of Statistical Software 28 (November): 1–26. https://doi.org/10.18637/jss.v028.i05.\n\n\nPedregosa, Fabian, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12 (85): 2825–30. http://jmlr.org/papers/v12/pedregosa11a.html."
  },
  {
    "objectID": "gallery/optimization/2020-10-14-threshold-tuning/index.html",
    "href": "gallery/optimization/2020-10-14-threshold-tuning/index.html",
    "title": "Threshold Tuning for Classification Tasks",
    "section": "",
    "text": "Predicting probabilities in classification tasks allows us to adjust the probability thresholds required for assigning an observation to a certain class. This can lead to improved classification performance, especially for cases where we e.g. aim to balance off metrics such as false positive and false negative rates.\nThis is for example often done in ROC Analysis. The mlr3book also has a chapter on ROC Analysis) for the interested reader. This post does not focus on ROC analysis, but instead focusses on the general problem of adjusting classification thresholds for arbitrary metrics.\nThis post assumes some familiarity with the mlr3, and also the mlr3pipelines and mlr3tuning packages, as both are used during the post. The mlr3book contains more details on those two packages. This post is a more in-depth version of the article on threshold tuning in the mlr3book."
  },
  {
    "objectID": "gallery/optimization/2020-10-14-threshold-tuning/index.html#intro",
    "href": "gallery/optimization/2020-10-14-threshold-tuning/index.html#intro",
    "title": "Threshold Tuning for Classification Tasks",
    "section": "",
    "text": "Predicting probabilities in classification tasks allows us to adjust the probability thresholds required for assigning an observation to a certain class. This can lead to improved classification performance, especially for cases where we e.g. aim to balance off metrics such as false positive and false negative rates.\nThis is for example often done in ROC Analysis. The mlr3book also has a chapter on ROC Analysis) for the interested reader. This post does not focus on ROC analysis, but instead focusses on the general problem of adjusting classification thresholds for arbitrary metrics.\nThis post assumes some familiarity with the mlr3, and also the mlr3pipelines and mlr3tuning packages, as both are used during the post. The mlr3book contains more details on those two packages. This post is a more in-depth version of the article on threshold tuning in the mlr3book."
  },
  {
    "objectID": "gallery/optimization/2020-10-14-threshold-tuning/index.html#prerequisites",
    "href": "gallery/optimization/2020-10-14-threshold-tuning/index.html#prerequisites",
    "title": "Threshold Tuning for Classification Tasks",
    "section": "Prerequisites",
    "text": "Prerequisites\nWe load the mlr3verse package which pulls in the most important packages for this example.\n\nlibrary(mlr3)\nlibrary(mlr3pipelines)\nlibrary(mlr3tuning)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/optimization/2020-10-14-threshold-tuning/index.html#thresholds-a-short-intro",
    "href": "gallery/optimization/2020-10-14-threshold-tuning/index.html#thresholds-a-short-intro",
    "title": "Threshold Tuning for Classification Tasks",
    "section": "Thresholds: A short intro",
    "text": "Thresholds: A short intro\nIn order to understand thresholds, we will quickly showcase the effect of setting different thresholds:\nFirst we create a learner that predicts probabilities and use it to predict on holdout data, storing the prediction.\n\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nrr = resample(tsk(\"pima\"), learner, rsmp(\"holdout\"))\nprd = rr$prediction()\nprd\n\n&lt;PredictionClassif&gt; for 256 observations:\n row_ids truth response  prob.pos  prob.neg\n       4   neg      neg 0.1057692 0.8942308\n       6   neg      neg 0.0200000 0.9800000\n      10   pos      neg 0.1428571 0.8571429\n     ---   ---      ---       ---       ---\n     764   neg      neg 0.2777778 0.7222222\n     766   neg      neg 0.0200000 0.9800000\n     767   pos      pos 0.8000000 0.2000000\n\n\nIf we now look at the confusion matrix, the off-diagonal elements are errors made by our model (false positives and false negatives) while on-diagol ements are where our model predicted correctly.\n\n# Print confusion matrix\nprd$confusion\n\n        truth\nresponse pos neg\n     pos  53  27\n     neg  37 139\n\n# Print False Positives and False Negatives\nprd$score(list(msr(\"classif.fp\"), msr(\"classif.fn\")))\n\nclassif.fp classif.fn \n        27         37 \n\n\nBy adjusting the classification threshold, in this case the probability required to predict the positive class, we can now trade off predicting more positive cases (first row) against predicting fewer negative cases (second row) or vice versa.\n\n# Lower threshold: More positives\nprd$set_threshold(0.25)$confusion\n\n        truth\nresponse pos neg\n     pos  78  71\n     neg  12  95\n\n\n\n# Higher threshold: Fewer positives\nprd$set_threshold(0.75)$confusion\n\n        truth\nresponse pos neg\n     pos  52  20\n     neg  38 146\n\n\nThis threshold value can now be adjusted optimally for a given measure, such as accuracy. How this can be done is discussed in the following section."
  },
  {
    "objectID": "gallery/optimization/2020-10-14-threshold-tuning/index.html#adjusting-thresholds-two-strategies",
    "href": "gallery/optimization/2020-10-14-threshold-tuning/index.html#adjusting-thresholds-two-strategies",
    "title": "Threshold Tuning for Classification Tasks",
    "section": "Adjusting thresholds: Two strategies",
    "text": "Adjusting thresholds: Two strategies\nCurrently mlr3pipelines offers two main strategies towards adjusting classification thresholds. We can either expose the thresholds as a hyperparameter of the Learner by using PipeOpThreshold. This allows us to tune the thresholds via an outside optimizer from mlr3tuning.\nAlternatively, we can also use PipeOpTuneThreshold which automatically tunes the threshold after each learner fit.\nIn this blog-post, we’ll go through both strategies."
  },
  {
    "objectID": "gallery/optimization/2020-10-14-threshold-tuning/index.html#pipeopthreshold",
    "href": "gallery/optimization/2020-10-14-threshold-tuning/index.html#pipeopthreshold",
    "title": "Threshold Tuning for Classification Tasks",
    "section": "PipeOpThreshold",
    "text": "PipeOpThreshold\nPipeOpThreshold can be put directly after a Learner.\nA simple example would be:\n\ngr = lrn(\"classif.rpart\", predict_type = \"prob\") %&gt;&gt;% po(\"threshold\")\nl = GraphLearner$new(gr)\n\nNote, that predict_type = “prob” is required for po(\"threshold\") to have any effect.\nThe thresholds are now exposed as a hyperparameter of the GraphLearner we created:\n\nas.data.table(l$param_set)[, .(id, class, lower, upper, nlevels)]\n\n                              id    class lower upper nlevels\n                          &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n 1:             classif.rpart.cp ParamDbl     0     1     Inf\n 2:     classif.rpart.keep_model ParamLgl    NA    NA       2\n 3:     classif.rpart.maxcompete ParamInt     0   Inf     Inf\n 4:       classif.rpart.maxdepth ParamInt     1    30      30\n 5:   classif.rpart.maxsurrogate ParamInt     0   Inf     Inf\n 6:      classif.rpart.minbucket ParamInt     1   Inf     Inf\n 7:       classif.rpart.minsplit ParamInt     1   Inf     Inf\n 8: classif.rpart.surrogatestyle ParamInt     0     1       2\n 9:   classif.rpart.usesurrogate ParamInt     0     2       3\n10:           classif.rpart.xval ParamInt     0   Inf     Inf\n11:         threshold.thresholds ParamUty    NA    NA     Inf\n\n\nWe can now tune those thresholds from the outside as follows:\nBefore tuning, we have to define which hyperparameters we want to tune over. In this example, we only tune over the thresholds parameter of the threshold PipeOp. you can easily imagine, that we can also jointly tune over additional hyperparameters, i.e. rpart’s cp parameter.\nAs the Task we aim to optimize for is a binary task, we can simply specify the threshold parameter:\n\nsearch_space = ps(\n  threshold.thresholds = p_dbl(lower = 0, upper = 1)\n)\n\nWe now create a AutoTuner, which automatically tunes the supplied learner over the ParamSet we supplied above.\n\nat = auto_tuner(\n  tuner = tnr(\"random_search\"),\n  learner = l,\n  resampling = rsmp(\"cv\", folds = 3L),\n  measure = msr(\"classif.ce\"),\n  search_space = search_space,\n  term_evals = 5L,\n)\n\nat$train(tsk(\"german_credit\"))\n\nFor multi-class Tasks, this is a little more complicated. We have to use a trafo to transform a set of ParamDbl into the desired format for threshold.thresholds: A named numeric vector containing the thresholds. This can be easily achieved via a trafo function:\n\nsearch_space = ps(\n  versicolor = p_dbl(lower = 0, upper = 1),\n  setosa = p_dbl(lower = 0, upper = 1),\n  virginica = p_dbl(lower = 0, upper = 1),\n  .extra_trafo = function(x, param_set) {\n    list(threshold.thresholds = mlr3misc::map_dbl(x, identity))\n  }\n)\n\nInside the .exta_trafo, we simply collect all set params into a named vector via map_dbl and store it in the threshold.thresholds slot expected by the learner.\nAgain, we create a AutoTuner, which automatically tunes the supplied learner over the ParamSet we supplied above.\n\nat_2 = auto_tuner(\n  tuner = tnr(\"random_search\"),\n  learner = l,\n  resampling = rsmp(\"cv\", folds = 3L),\n  measure = msr(\"classif.ce\"),\n  search_space = search_space,\n  term_evals = 5L,\n)\n\nat_2$train(tsk(\"iris\"))\n\nOne drawback of this strategy is, that this requires us to fit a new model for each new threshold setting. While setting a threshold and computing performance is relatively cheap, fitting the learner is often more computationally demanding. A better strategy is therefore often to optimize the thresholds separately after each model fit."
  },
  {
    "objectID": "gallery/optimization/2020-10-14-threshold-tuning/index.html#pipeoptunethreshold",
    "href": "gallery/optimization/2020-10-14-threshold-tuning/index.html#pipeoptunethreshold",
    "title": "Threshold Tuning for Classification Tasks",
    "section": "PipeOpTuneThreshold",
    "text": "PipeOpTuneThreshold\nPipeOpTuneThreshold on the other hand works together with PipeOpLearnerCV. It directly optimizes the cross-validated predictions made by this PipeOp.\nA simple example would be:\n\ngr = po(\"learner_cv\", lrn(\"classif.rpart\", predict_type = \"prob\")) %&gt;&gt;%\n  po(\"tunethreshold\")\nl2 = GraphLearner$new(gr)\n\nNote, that predict_type = “prob” is required for po(\"tunethreshold\") to have any effect. Additionally, note that this time no threshold parameter is exposed, it is automatically tuned internally.\n\nas.data.table(l2$param_set)[, .(id, class, lower, upper, nlevels)]\n\n                                        id    class lower upper nlevels\n                                    &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n 1:        classif.rpart.resampling.method ParamFct    NA    NA       2\n 2:         classif.rpart.resampling.folds ParamInt     2   Inf     Inf\n 3: classif.rpart.resampling.keep_response ParamLgl    NA    NA       2\n 4:                       classif.rpart.cp ParamDbl     0     1     Inf\n 5:               classif.rpart.keep_model ParamLgl    NA    NA       2\n 6:               classif.rpart.maxcompete ParamInt     0   Inf     Inf\n 7:                 classif.rpart.maxdepth ParamInt     1    30      30\n 8:             classif.rpart.maxsurrogate ParamInt     0   Inf     Inf\n 9:                classif.rpart.minbucket ParamInt     1   Inf     Inf\n10:                 classif.rpart.minsplit ParamInt     1   Inf     Inf\n11:           classif.rpart.surrogatestyle ParamInt     0     1       2\n12:             classif.rpart.usesurrogate ParamInt     0     2       3\n13:                     classif.rpart.xval ParamInt     0   Inf     Inf\n14:           classif.rpart.affect_columns ParamUty    NA    NA     Inf\n15:                  tunethreshold.measure ParamUty    NA    NA     Inf\n16:                tunethreshold.optimizer ParamUty    NA    NA     Inf\n17:                tunethreshold.log_level ParamUty    NA    NA     Inf\n\n\nIf we now use the GraphLearner, it automatically adjusts the thresholds during prediction.\nNote that we can set ResamplingInsample as a resampling strategy for PipeOpLearnerCV in order to evaluate predictions on the “training” data. This is generally not advised, as it might lead to over-fitting on the thresholds but can significantly reduce runtime.\nFinally, we can compare no threshold tuning to the tunethreshold approach:\n\nComparison of the approaches\n\nbmr = benchmark(benchmark_grid(\n  learners = list(no_tuning = lrn(\"classif.rpart\"), internal = l2),\n  tasks = tsk(\"german_credit\"),\n  rsmp(\"cv\", folds = 3L)\n))\n\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\nOptimInstanceSingleCrit is deprecated. Use OptimInstanceBatchSingleCrit instead.\n\n\n\nbmr$aggregate(list(msr(\"classif.ce\"), msr(\"classif.fnr\")))\n\n      nr       task_id                  learner_id resampling_id iters classif.ce classif.fnr\n   &lt;int&gt;        &lt;char&gt;                      &lt;char&gt;        &lt;char&gt; &lt;int&gt;      &lt;num&gt;       &lt;num&gt;\n1:     1 german_credit               classif.rpart            cv     3  0.2760095  0.12723983\n2:     2 german_credit classif.rpart.tunethreshold            cv     3  0.2879916  0.04485325\nHidden columns: resample_result"
  },
  {
    "objectID": "gallery/optimization/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/index.html",
    "href": "gallery/optimization/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/index.html",
    "title": "Practical Tuning Series - Tune a Support Vector Machine",
    "section": "",
    "text": "requireNamespace(\"DiceKriging\")\n\nLoading required namespace: DiceKriging\n\n\n\nScope\nThis is the first part of the practical tuning series. The other parts can be found here:\n\nPart II - Tune a Preprocessing Pipeline\nPart III - Build an Automated Machine Learning System\nPart IV - Tuning and Parallel Processing\n\nIn this post, we demonstrate how to optimize the hyperparameters of a support vector machine (SVM). We are using the mlr3 machine learning framework with the mlr3tuning extension package.\nFirst, we start by showing the basic building blocks of mlr3tuning and tune the cost and gamma hyperparameters of an SVM with a radial basis function on the Iris data set. After that, we use transformations to tune the both hyperparameters on the logarithmic scale. Next, we explain the importance of dependencies to tune hyperparameters like degree which are dependent on the choice of kernel. After that, we fit an SVM with optimized hyperparameters on the full dataset. Finally, nested resampling is used to compute an unbiased performance estimate of our tuned SVM.\n\n\nPrerequisites\nWe load the mlr3verse package which pulls in the most important packages for this example.\n\nlibrary(mlr3verse)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented. The lgr package is used for logging in all mlr3 packages. The mlr3 logger prints the logging messages from the base package, whereas the bbotk logger is responsible for logging messages from the optimization packages (e.g. mlr3tuning ).\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nIn the example, we use the Iris data set which classifies 150 flowers in three species of Iris. The flowers are characterized by sepal length and width and petal length and width. The Iris data set allows us to quickly fit models to it. However, the influence of hyperparameter tuning on the predictive performance might be minor. Other data sets might give more meaningful tuning results.\n\n# retrieve the task from mlr3\ntask = tsk(\"iris\")\n\n# generate a quick textual overview using the skimr package\nskimr::skim(task$data())\n\n\nData summary\n\n\nName\ntask$data()\n\n\nNumber of rows\n150\n\n\nNumber of columns\n5\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSpecies\n0\n1\nFALSE\n3\nset: 50, ver: 50, vir: 50\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPetal.Length\n0\n1\n3.76\n1.77\n1.0\n1.6\n4.35\n5.1\n6.9\n▇▁▆▇▂\n\n\nPetal.Width\n0\n1\n1.20\n0.76\n0.1\n0.3\n1.30\n1.8\n2.5\n▇▁▇▅▃\n\n\nSepal.Length\n0\n1\n5.84\n0.83\n4.3\n5.1\n5.80\n6.4\n7.9\n▆▇▇▅▂\n\n\nSepal.Width\n0\n1\n3.06\n0.44\n2.0\n2.8\n3.00\n3.3\n4.4\n▁▆▇▂▁\n\n\n\n\n\nWe choose the support vector machine implementation from the e1071 package (which is based on LIBSVM) and use it as a classification machine by setting type to \"C-classification\".\n\nlearner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\")\n\n\n\nTuning Search Space\nFor tuning, it is important to create a search space that defines the type and range of the hyperparameters. A learner stores all information about its hyperparameters in the slot $param_set. Not all parameters are tunable. We have to choose a subset of the hyperparameters we want to tune.\n\nas.data.table(learner$param_set)[, .(id, class, lower, upper, nlevels)]\n\nWe use the to_tune() function to define the range over which the hyperparameter should be tuned. We opt for the cost and gamma hyperparameters of the radial kernel and set the tuning ranges with lower and upper bounds.\n\nlearner$param_set$values$cost = to_tune(0.1, 10)\nlearner$param_set$values$gamma = to_tune(0, 5)\n\n\n\nTuning\nWe specify how to evaluate the performance of the different hyperparameter configurations. For this, we choose 3-fold cross validation as the resampling strategy and the classification error as the performance measure.\n\nresampling = rsmp(\"cv\", folds = 3)\nmeasure = msr(\"classif.ce\")\n\nUsually, we have to select a budget for the tuning. This is done by choosing a Terminator, which stops the tuning e.g. after a performance level is reached or after a given time. However, some tuners like grid search terminate themselves. In this case, we choose a terminator that never stops and the tuning is not stopped before all grid points are evaluated.\n\nterminator = trm(\"none\")\n\nAt this point, we can construct a TuningInstanceBatchSingleCrit that describes the tuning problem.\n\ninstance = ti(\n  task = task,\n  learner = learner,\n  resampling = resampling,\n  measure = measure,\n  terminator = terminator\n)\n\nprint(instance)\n\n&lt;TuningInstanceBatchSingleCrit&gt;\n* State:  Not optimized\n* Objective: &lt;ObjectiveTuningBatch:classif.svm_on_iris&gt;\n* Search Space:\n       id    class lower upper nlevels\n   &lt;char&gt;   &lt;char&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt;\n1:   cost ParamDbl   0.1    10     Inf\n2:  gamma ParamDbl   0.0     5     Inf\n* Terminator: &lt;TerminatorNone&gt;\n\n\nFinally, we have to choose a Tuner. Grid Search discretizes numeric parameters into a given resolution and constructs a grid from the Cartesian product of these sets. Categorical parameters produce a grid over all levels specified in the search space. In this example, we only use a resolution of 5 to keep the runtime low. Usually, a higher resolution is used to create a denser grid.\n\ntuner = tnr(\"grid_search\", resolution = 5)\n\nprint(tuner)\n\n&lt;TunerBatchGridSearch&gt;: Grid Search\n* Parameters: batch_size=1, resolution=5\n* Parameter classes: ParamLgl, ParamInt, ParamDbl, ParamFct\n* Properties: dependencies, single-crit, multi-crit\n* Packages: mlr3tuning, bbotk\n\n\nWe can preview the proposed configurations by using generate_design_grid(). This function is internally executed by TunerBatchGridSearch.\n\ngenerate_design_grid(learner$param_set$search_space(), resolution = 5)\n\n&lt;Design&gt; with 25 rows:\n      cost gamma\n     &lt;num&gt; &lt;num&gt;\n 1:  0.100  0.00\n 2:  0.100  1.25\n 3:  0.100  2.50\n 4:  0.100  3.75\n 5:  0.100  5.00\n 6:  2.575  0.00\n 7:  2.575  1.25\n 8:  2.575  2.50\n 9:  2.575  3.75\n10:  2.575  5.00\n11:  5.050  0.00\n12:  5.050  1.25\n13:  5.050  2.50\n14:  5.050  3.75\n15:  5.050  5.00\n16:  7.525  0.00\n17:  7.525  1.25\n18:  7.525  2.50\n19:  7.525  3.75\n20:  7.525  5.00\n21: 10.000  0.00\n22: 10.000  1.25\n23: 10.000  2.50\n24: 10.000  3.75\n25: 10.000  5.00\n      cost gamma\n\n\nWe trigger the tuning by passing the TuningInstanceBatchSingleCrit to the $optimize() method of the Tuner. The instance is modified in-place.\n\ntuner$optimize(instance)\n\n    cost gamma learner_param_vals  x_domain classif.ce\n   &lt;num&gt; &lt;num&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n1: 2.575   2.5          &lt;list[4]&gt; &lt;list[2]&gt; 0.04666667\n\n\nWe plot the performances depending on the evaluated cost and gamma values.\n\nautoplot(instance, type = \"surface\", cols_x = c(\"cost\", \"gamma\"),\n  learner = lrn(\"regr.km\"))\n\n\n\n\n\n\n\n\n\n\nThe points mark the evaluated cost and gamma values. We should not infer the performance of new values from the heatmap since it is only an interpolation. However, we can see the general interaction between the hyperparameters.\nTuning a learner can be shortened by using the tune()-shortcut.\n\nlearner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\")\nlearner$param_set$values$cost = to_tune(0.1, 10)\nlearner$param_set$values$gamma = to_tune(0, 5)\n\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 5),\n  task = tsk(\"iris\"),\n  learner = learner,\n  resampling = rsmp (\"holdout\"),\n  measure = msr(\"classif.ce\")\n)\n\n\n\nTransformation\nNext, we want to tune the cost and gamma hyperparameter more efficiently. It is recommended to tune cost and gamma on the logarithmic scale (Hsu, Chang, and Lin 2003). The log transformation emphasizes smaller cost and gamma values but also creates large values. Therefore, we use a log transformation to emphasize this region of the search space with a denser grid.\nGenerally speaking, transformations can be used to convert hyperparameters to a new scale. These transformations are applied before the proposed configuration is passed to the Learner. We can directly define the transformation in the to_tune() function. The lower and upper bound is set on the original scale.\n\nlearner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\")\n\n# tune from 2^-15 to 2^15 on a log scale\nlearner$param_set$values$cost = to_tune(p_dbl(-15, 15, trafo = function(x) 2^x))\n\n# tune from 2^-15 to 2^5 on a log scale\nlearner$param_set$values$gamma = to_tune(p_dbl(-15, 5, trafo = function(x) 2^x))\n\nTransformations to the log scale are the ones most commonly used. We can use a shortcut for this transformation. The lower and upper bound is set on the transformed scale.\n\nlearner$param_set$values$cost = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\nlearner$param_set$values$gamma = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\n\nWe use the tune()-shortcut to run the tuning.\n\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 5),\n  task = task,\n  learner = learner,\n  resampling = resampling,\n  measure = measure\n)\n\nThe hyperparameter values after the transformation are stored in the x_domain column as lists. We can expand these lists into multiple columns by using as.data.table(). The hyperparameter names are prefixed by x_domain.\n\nas.data.table(instance$archive)[, .(cost, gamma, x_domain_cost, x_domain_gamma)]\n\n          cost      gamma x_domain_cost x_domain_gamma\n         &lt;num&gt;      &lt;num&gt;         &lt;num&gt;          &lt;num&gt;\n 1:  11.512925   5.756463  1.000000e+05   3.162278e+02\n 2:   0.000000 -11.512925  1.000000e+00   1.000000e-05\n 3:  -5.756463  -5.756463  3.162278e-03   3.162278e-03\n 4:  11.512925  -5.756463  1.000000e+05   3.162278e-03\n 5:   0.000000  -5.756463  1.000000e+00   3.162278e-03\n 6:  -5.756463   0.000000  3.162278e-03   1.000000e+00\n 7: -11.512925   0.000000  1.000000e-05   1.000000e+00\n 8:   5.756463   0.000000  3.162278e+02   1.000000e+00\n 9:   0.000000   0.000000  1.000000e+00   1.000000e+00\n10:  11.512925  11.512925  1.000000e+05   1.000000e+05\n11:   0.000000   5.756463  1.000000e+00   3.162278e+02\n12:   5.756463 -11.512925  3.162278e+02   1.000000e-05\n13:   5.756463  11.512925  3.162278e+02   1.000000e+05\n14: -11.512925   5.756463  1.000000e-05   3.162278e+02\n15:   5.756463   5.756463  3.162278e+02   3.162278e+02\n16:   5.756463  -5.756463  3.162278e+02   3.162278e-03\n17:  11.512925 -11.512925  1.000000e+05   1.000000e-05\n18:  -5.756463   5.756463  3.162278e-03   3.162278e+02\n19: -11.512925 -11.512925  1.000000e-05   1.000000e-05\n20:  11.512925   0.000000  1.000000e+05   1.000000e+00\n21:  -5.756463  11.512925  3.162278e-03   1.000000e+05\n22: -11.512925  11.512925  1.000000e-05   1.000000e+05\n23:  -5.756463 -11.512925  3.162278e-03   1.000000e-05\n24: -11.512925  -5.756463  1.000000e-05   3.162278e-03\n25:   0.000000  11.512925  1.000000e+00   1.000000e+05\n          cost      gamma x_domain_cost x_domain_gamma\n\n\nWe plot the performances depending on the evaluated cost and gamma values.\n\nlibrary(ggplot2)\nlibrary(scales)\nautoplot(instance, type = \"points\", cols_x = c(\"x_domain_cost\", \"x_domain_gamma\")) +\n  scale_x_continuous(\n    trans = log2_trans(),\n    breaks = trans_breaks(\"log10\", function(x) 10^x),\n    labels = trans_format(\"log10\", math_format(10^.x))) +\n  scale_y_continuous(\n    trans = log2_trans(),\n    breaks = trans_breaks(\"log10\", function(x) 10^x),\n    labels = trans_format(\"log10\", math_format(10^.x)))\n\n\n\n\n\n\n\n\n\n\nDependencies\nDependencies ensure that certain parameters are only proposed depending on values of other hyperparameters. We want to tune the degree hyperparameter that is only needed for the polynomial kernel.\n\nlearner = lrn(\"classif.svm\", type = \"C-classification\")\n\nlearner$param_set$values$cost = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\nlearner$param_set$values$gamma = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\n\nlearner$param_set$values$kernel = to_tune(c(\"polynomial\", \"radial\"))\nlearner$param_set$values$degree = to_tune(1, 4)\n\nThe dependencies are already stored in the learner parameter set.\n\nlearner$param_set$deps\n\nIndices: &lt;id&gt;, &lt;on__id&gt;\n       id     on                  cond\n   &lt;char&gt; &lt;char&gt;                &lt;list&gt;\n1:  coef0 kernel &lt;Condition:CondAnyOf&gt;\n2:   cost   type &lt;Condition:CondEqual&gt;\n3: degree kernel &lt;Condition:CondEqual&gt;\n4:  gamma kernel &lt;Condition:CondAnyOf&gt;\n5:     nu   type &lt;Condition:CondEqual&gt;\n\n\nThe gamma hyperparameter depends on the kernel being polynomial, radial or sigmoid\n\nlearner$param_set$deps$cond[[5]]\n\nCondEqual: x == nu-classification\n\n\nwhereas the degree hyperparameter is solely used by the polynomial kernel.\n\nlearner$param_set$deps$cond[[3]]\n\nCondEqual: x == polynomial\n\n\nWe preview the grid to show the effect of the dependencies.\n\ngenerate_design_grid(learner$param_set$search_space(), resolution = 2)\n\n&lt;Design&gt; with 12 rows:\n         cost degree     gamma     kernel\n        &lt;num&gt;  &lt;int&gt;     &lt;num&gt;     &lt;char&gt;\n 1: -11.51293      1 -11.51293 polynomial\n 2: -11.51293     NA -11.51293     radial\n 3: -11.51293      1  11.51293 polynomial\n 4: -11.51293     NA  11.51293     radial\n 5: -11.51293      4 -11.51293 polynomial\n 6: -11.51293      4  11.51293 polynomial\n 7:  11.51293      1 -11.51293 polynomial\n 8:  11.51293     NA -11.51293     radial\n 9:  11.51293      1  11.51293 polynomial\n10:  11.51293     NA  11.51293     radial\n11:  11.51293      4 -11.51293 polynomial\n12:  11.51293      4  11.51293 polynomial\n\n\nThe value for degree is NA if the dependency on the kernel is not satisfied.\nWe use the tune()-shortcut to run the tuning.\n\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 3),\n  task = task,\n  learner = learner,\n  resampling = resampling,\n  measure = measure\n)\n\n\ninstance$result\n\n       cost degree     gamma     kernel learner_param_vals  x_domain classif.ce\n      &lt;num&gt;  &lt;int&gt;     &lt;num&gt;     &lt;char&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n1: 11.51293      1 -11.51293 polynomial          &lt;list[5]&gt; &lt;list[4]&gt;       0.02\n\n\n\n\nFinal Model\nWe add the optimized hyperparameters to the learner and train the learner on the full dataset.\n\nlearner = lrn(\"classif.svm\")\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n\nThe trained model can now be used to make predictions on new data. A common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (instance$result_y) as the model’s performance. These scores might be biased and overestimate the ability of the fitted model to predict with new data. Instead, we have to use nested resampling to get an unbiased performance estimate.\n\n\nNested Resampling\nTuning should not be performed on the same resampling sets which are used for evaluating the model itself, since this would result in a biased performance estimate. Nested resampling uses an outer and inner resampling to separate the tuning from the performance estimation of the model. We can use the AutoTuner class for running nested resampling. The AutoTuner wraps a Learner and tunes the hyperparameter of the learner during $train(). This is our inner resampling loop.\n\nlearner = lrn(\"classif.svm\", type = \"C-classification\")\nlearner$param_set$values$cost = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\nlearner$param_set$values$gamma = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\nlearner$param_set$values$kernel = to_tune(c(\"polynomial\", \"radial\"))\nlearner$param_set$values$degree = to_tune(1, 4)\n\nresampling_inner = rsmp(\"cv\", folds = 3)\nterminator = trm(\"none\")\ntuner = tnr(\"grid_search\", resolution = 3)\n\nat = auto_tuner(\n  learner = learner,\n  resampling = resampling_inner,\n  measure = measure,\n  terminator = terminator,\n  tuner = tuner,\n  store_models = TRUE)\n\nWe put the AutoTuner into a resample() call to get the outer resampling loop.\n\nresampling_outer = rsmp(\"cv\", folds = 3)\nrr = resample(task = task, learner = at, resampling = resampling_outer, store_models = TRUE)\n\nWe check the inner tuning results for stable hyperparameters. This means that the selected hyperparameters should not vary too much. We might observe unstable models in this example because the small data set and the low number of resampling iterations might introduce too much randomness. Usually, we aim for the selection of stable hyperparameters for all outer training sets.\n\nextract_inner_tuning_results(rr)[, .SD, .SDcols = !c(\"learner_param_vals\", \"x_domain\")]\n\n   iteration      cost degree    gamma     kernel classif.ce task_id        learner_id resampling_id\n       &lt;int&gt;     &lt;num&gt;  &lt;int&gt;    &lt;num&gt;     &lt;char&gt;      &lt;num&gt;  &lt;char&gt;            &lt;char&gt;        &lt;char&gt;\n1:         1   0.00000      3  0.00000 polynomial 0.03980986    iris classif.svm.tuned            cv\n2:         2   0.00000     NA  0.00000     radial 0.02970885    iris classif.svm.tuned            cv\n3:         3 -11.51293      1 11.51293 polynomial 0.03000594    iris classif.svm.tuned            cv\n\n\nNext, we want to compare the predictive performances estimated on the outer resampling to the inner resampling (extract_inner_tuning_results(rr)). Significantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.\n\nrr$score()[, .(iteration, task_id, learner_id, resampling_id, classif.ce)]\n\n   iteration task_id        learner_id resampling_id classif.ce\n       &lt;int&gt;  &lt;char&gt;            &lt;char&gt;        &lt;char&gt;      &lt;num&gt;\n1:         1    iris classif.svm.tuned            cv       0.06\n2:         2    iris classif.svm.tuned            cv       0.08\n3:         3    iris classif.svm.tuned            cv       0.00\n\n\nThe archives of the AutoTuners allows us to inspect all evaluated hyperparameters configurations with the associated predictive performances.\n\nextract_inner_tuning_archives(rr, unnest = NULL, exclude_columns = c(\"resample_result\", \"uhash\", \"x_domain\", \"timestamp\"))\n\n     iteration      cost degree     gamma     kernel classif.ce runtime_learners warnings errors batch_nr task_id\n         &lt;int&gt;     &lt;num&gt;  &lt;int&gt;     &lt;num&gt;     &lt;char&gt;      &lt;num&gt;            &lt;num&gt;    &lt;int&gt;  &lt;int&gt;    &lt;int&gt;  &lt;char&gt;\n  1:         1   0.00000     NA -11.51293     radial 0.69994058            0.013        0      0        1    iris\n  2:         1  11.51293     NA -11.51293     radial 0.04991087            0.008        0      0        2    iris\n  3:         1   0.00000      4  11.51293 polynomial 0.22846108            0.064        0      0        3    iris\n  4:         1   0.00000      3   0.00000 polynomial 0.03980986            0.009        0      0        4    iris\n  5:         1   0.00000      1 -11.51293 polynomial 0.69994058            0.009        0      0        5    iris\n ---                                                                                                             \n104:         3  11.51293     NA  11.51293     radial 0.64022579            0.009        0      0       32    iris\n105:         3  11.51293      1 -11.51293 polynomial 0.03000594            0.011        0      0       33    iris\n106:         3 -11.51293      3 -11.51293 polynomial 0.58140226            0.007        0      0       34    iris\n107:         3  11.51293      3  11.51293 polynomial 0.09031491            0.008        0      0       35    iris\n108:         3  11.51293      1   0.00000 polynomial 0.07040998            0.015        0      0       36    iris\n            learner_id resampling_id\n                &lt;char&gt;        &lt;char&gt;\n  1: classif.svm.tuned            cv\n  2: classif.svm.tuned            cv\n  3: classif.svm.tuned            cv\n  4: classif.svm.tuned            cv\n  5: classif.svm.tuned            cv\n ---                                \n104: classif.svm.tuned            cv\n105: classif.svm.tuned            cv\n106: classif.svm.tuned            cv\n107: classif.svm.tuned            cv\n108: classif.svm.tuned            cv\n\n\nThe aggregated performance of all outer resampling iterations is essentially the unbiased performance of an SVM with optimal hyperparameter found by grid search.\n\nrr$aggregate()\n\nclassif.ce \n0.04666667 \n\n\nApplying nested resampling can be shortened by using the tune_nested()-shortcut.\n\nlearner = lrn(\"classif.svm\", type = \"C-classification\")\nlearner$param_set$values$cost = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\nlearner$param_set$values$gamma = to_tune(p_dbl(1e-5, 1e5, logscale = TRUE))\nlearner$param_set$values$kernel = to_tune(c(\"polynomial\", \"radial\"))\nlearner$param_set$values$degree = to_tune(1, 4)\n\nrr = tune_nested(\n  tuner = tnr(\"grid_search\", resolution = 3),\n  task = tsk(\"iris\"),\n  learner = learner,\n  inner_resampling = rsmp (\"cv\", folds = 3),\n  outer_resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n)\n\n\n\nResources\nThe mlr3book includes chapters on tuning spaces and hyperparameter tuning. The mlr3cheatsheets contain frequently used commands and workflows of mlr3.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   base64enc           0.1-3      2015-07-28 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   DiceKriging         1.6.0      2021-02-23 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   e1071               1.7-16     2024-09-16 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   farver              2.1.2      2024-05-13 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future              1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2           * 3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools           0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   labeling            0.4.3      2023-08-29 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect         1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband       0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3measures        1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc            0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning          1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox             1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   proxy               0.4-27     2022-06-09 [1] CRAN (R 4.4.1)\n   purrr               1.0.2      2023-08-10 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   repr                1.1.7      2024-03-22 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n   scales            * 1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   skimr               2.1.5      2022-12-23 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   stringr             1.5.1      2023-11-14 [1] CRAN (R 4.4.1)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyr               1.3.1      2024-01-24 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   viridisLite         0.4.2      2023-05-02 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\nReferences\n\nHsu, Chih-wei, Chih-chung Chang, and Chih-Jen Lin. 2003. “A Practical Guide to Support Vector Classification.”"
  },
  {
    "objectID": "gallery/optimization/2023-01-16-hyperband-subsampling/index.html",
    "href": "gallery/optimization/2023-01-16-hyperband-subsampling/index.html",
    "title": "Hyperband Series - Data Set Subsampling",
    "section": "",
    "text": "Scope\nWe continue working with the Hyperband optimization algorithm (Li et al. 2018). The previous post used the number of boosting iterations of an XGBoost model as the resource. However, Hyperband is not limited to machine learning algorithms that are trained iteratively. The resource can also be the number of features, the training time of a model, or the size of the training data set. In this post, we will tune a support vector machine and use the size of the training data set as the fidelity parameter. The time to train a support vector machine and the performance increases with the size of the data set. This makes the data set size a suitable fidelity parameter for Hyperband. This is the second part of the Hyperband series. The first part can be found here Hyperband Series - Iterative Training. If you don’t know much about Hyperband, check out the first post which explains the algorithm in detail. We assume that you are already familiar with tuning in the mlr3 ecosystem. If not, you should start with the book chapter on optimization or the Hyperparameter Optimization on the Palmer Penguins Data Set post. A little knowledge about mlr3pipelines is beneficial but not necessary to understand the example.\n\n\nHyperparameter Optimization\nIn this post, we will optimize the hyperparameters of the support vector machine on the Sonar data set. We begin by constructing a classification machine by setting type to \"C-classification\".\n\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\")\n\nThe mlr3pipelines package features a PipeOp for subsampling.\n\npo(\"subsample\")\n\nPipeOp: &lt;subsample&gt; (not trained)\nvalues: &lt;frac=0.6321, stratify=FALSE, replace=FALSE&gt;\nInput channels &lt;name [train type, predict type]&gt;:\n  input [Task,Task]\nOutput channels &lt;name [train type, predict type]&gt;:\n  output [Task,Task]\n\n\nThe PipeOp controls the size of the training data set with the frac parameter. We connect the PipeOp with the learner and get a GraphLearner.\n\ngraph_learner = as_learner(\n  po(\"subsample\") %&gt;&gt;%\n  learner\n)\n\nThe graph learner subsamples and then fits a support vector machine on the data subset. The parameter set of the graph learner is a combination of the parameter sets of the PipeOp and learner.\n\nas.data.table(graph_learner$param_set)[, .(id, lower, upper, levels)]\n\n                    id lower upper                             levels\n                &lt;char&gt; &lt;num&gt; &lt;num&gt;                             &lt;list&gt;\n 1:     subsample.frac     0   Inf                             [NULL]\n 2: subsample.stratify    NA    NA                         TRUE,FALSE\n 3:  subsample.replace    NA    NA                         TRUE,FALSE\n 4:      svm.cachesize  -Inf   Inf                             [NULL]\n 5:  svm.class.weights    NA    NA                             [NULL]\n---                                                                  \n15:             svm.nu  -Inf   Inf                             [NULL]\n16:          svm.scale    NA    NA                             [NULL]\n17:      svm.shrinking    NA    NA                         TRUE,FALSE\n18:      svm.tolerance     0   Inf                             [NULL]\n19:           svm.type    NA    NA C-classification,nu-classification\n\n\nNext, we create the search space. We use TuneToken to mark which hyperparameters should be tuned. We have to prefix the hyperparameters with the id of the PipeOps. The subsample.frac is the fidelity parameter that must be tagged with \"budget\" in the search space. The data set size is increased from 3.7% to 100%. For the other hyperparameters, we took the search space for support vector machines from the Kuehn et al. (2018) article. This search space works for a wide range of data sets.\n\ngraph_learner$param_set$set_values(\n  subsample.frac  = to_tune(p_dbl(3^-3, 1, tags = \"budget\")),\n  svm.kernel      = to_tune(c(\"linear\", \"polynomial\", \"radial\")),\n  svm.cost        = to_tune(1e-4, 1e3, logscale = TRUE),\n  svm.gamma       = to_tune(1e-4, 1e3, logscale = TRUE),\n  svm.tolerance   = to_tune(1e-4, 2, logscale = TRUE),\n  svm.degree      = to_tune(2, 5)\n)\n\nSupport vector machines often crash or never finish the training with certain hyperparameter configurations. We set a timeout of 30 seconds and a fallback learner to handle these cases.\n\ngraph_learner$encapsulate(method = \"evaluate\", fallback = lrn(\"classif.featureless\"))\ngraph_learner$timeout = c(train = 30, predict = 30)\n\nLet’s create the tuning instance. We use the \"none\" terminator because Hyperband controls the termination itself.\n\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = graph_learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\ninstance\n\n&lt;TuningInstanceBatchSingleCrit&gt;\n* State:  Not optimized\n* Objective: &lt;ObjectiveTuningBatch:subsample.svm_on_sonar&gt;\n* Search Space:\n               id    class       lower     upper nlevels\n           &lt;char&gt;   &lt;char&gt;       &lt;num&gt;     &lt;num&gt;   &lt;num&gt;\n1: subsample.frac ParamDbl  0.03703704 1.0000000     Inf\n2:       svm.cost ParamDbl -9.21034037 6.9077553     Inf\n3:     svm.degree ParamInt  2.00000000 5.0000000       4\n4:      svm.gamma ParamDbl -9.21034037 6.9077553     Inf\n5:     svm.kernel ParamFct          NA        NA       3\n6:  svm.tolerance ParamDbl -9.21034037 0.6931472     Inf\n* Terminator: &lt;TerminatorNone&gt;\n\n\nWe load the Hyperband tuner and set eta = 3.\n\nlibrary(\"mlr3hyperband\")\n\ntuner = tnr(\"hyperband\", eta = 3)\n\nUsing eta = 3 and a lower bound of 3.7% for the data set size, results in the following schedule. Configurations with the same data set size are evaluated in parallel.\n\n\n\n\n\n\nNow we are ready to start the tuning.\n\ntuner$optimize(instance)\n\nThe best model is a support vector machine with a polynomial kernel.\n\ninstance$result[, .(subsample.frac, svm.cost, svm.degree, svm.gamma, svm.kernel, svm.tolerance, classif.ce)]\n\n   subsample.frac  svm.cost svm.degree svm.gamma svm.kernel svm.tolerance classif.ce\n            &lt;num&gt;     &lt;num&gt;      &lt;int&gt;     &lt;num&gt;     &lt;char&gt;         &lt;num&gt;      &lt;num&gt;\n1:              1 -6.149713          3 0.9764394 polynomial     -8.985273  0.1873706\n\n\nThe archive contains all evaluated configurations. We look at the 8 configurations that were evaluated on the complete data set. The configuration with the best classification error on the full data set was sampled in bracket 2. The classification error was estimated to be 26% on 33% of the data set and increased to 19% on the full data set (see green line in Figure 1).\n\n\n\n\n\n\n\n\nFigure 1: Optimization path of the 8 configurations evaluated on the complete data set.\n\n\n\n\n\n\n\nConclusion\nUsing the data set size as the budget parameter in Hyperband allows the tuning of machine learning models that are not trained iteratively. We have tried to keep the runtime of the example low. For your optimization, you should use cross-validation and run multiple iterations of Hyperband.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   crosstalk           1.2.1      2023-11-23 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   farver              2.1.2      2024-05-13 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future              1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2           * 3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools         * 0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   labeling            0.4.3      2023-08-29 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect         1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband     * 0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3measures        1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc            0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning        * 1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox           * 1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   reactable         * 0.4.4      2023-03-12 [1] CRAN (R 4.4.1)\n   reactR              0.6.1      2024-09-14 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n   scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   viridisLite         0.4.2      2023-05-02 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\n\n\n\nReferences\n\nKuehn, Daniel, Philipp Probst, Janek Thomas, and Bernd Bischl. 2018. “Automatic Exploration of Machine Learning Experiments on OpenML.” https://arxiv.org/abs/1806.10961.\n\n\nLi, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.” Journal of Machine Learning Research 18 (185): 1–52. https://jmlr.org/papers/v18/16-558.html."
  },
  {
    "objectID": "gallery/optimization/2023-01-15-hyperband-xgboost/index.html",
    "href": "gallery/optimization/2023-01-15-hyperband-xgboost/index.html",
    "title": "Hyperband Series - Iterative Training",
    "section": "",
    "text": "Scope\nIncreasingly large data sets and search spaces make hyperparameter optimization a time-consuming task. Hyperband (Li et al. 2018) solves this by approximating the performance of a configuration on a simplified version of the problem such as a small subset of the training data, with just a few training epochs in a neural network, or with only a small number of iterations in a gradient-boosting model. After starting randomly sampled configurations, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones. This type of optimization is called multi-fidelity optimization. The fidelity parameter is part of the search space and controls the tradeoff between the runtime and accuracy of the performance approximation. In this post, we will optimize XGBoost and use the number of boosting iterations as the fidelity parameter. This means Hyperband will allocate more boosting iterations to well-performing configurations. The number of boosting iterations increases the time to train a model and improves the performance until the model is overfitting to the training data. It is therefore a suitable fidelity parameter. We assume that you are already familiar with tuning in the mlr3 ecosystem. If not, you should start with the book chapter on optimization or the Hyperparameter Optimization on the Palmer Penguins Data Set post. This is the first part of the Hyperband series. The second part can be found here Hyperband Series - Data Set Subsampling.\n\n\nHyperband\nHyperband is an advancement of the Successive Halving algorithm by Jamieson and Talwalkar (2016). Successive Halving is initialized with the number of starting configurations \\(n\\), the proportion of configurations discarded in each stage \\(\\eta\\), and the minimum \\(r{_{min}}\\) and maximum \\(r{_{max}}\\) budget of a single evaluation. The algorithm starts by sampling \\(n\\) random configurations and allocating the minimum budget \\(r{_{min}}\\) to them. The configurations are evaluated and \\(\\frac{1}{\\eta}\\) of the worst-performing configurations are discarded. The remaining configurations are promoted to the next stage and evaluated on a larger budget. This continues until one or more configurations are evaluated on the maximum budget \\(r{_{max}}\\) and the best performing configuration is selected. The number of stages is calculated so that each stage consumes approximately the same budget. This sometimes results in the minimum budget having to be slightly adjusted by the algorithm. Successive Halving has the disadvantage that is not clear whether we should choose a large \\(n\\) and try many configurations on a small budget or choose a small \\(n\\) and train more configurations on the full budget.\nHyperband solves this problem by running Successive Halving with different numbers of stating configurations. The algorithm is initialized with the same parameters as Successive Halving but without \\(n\\). Each run of Successive Halving is called a bracket and starts with a different budget \\(r{_{0}}\\). A smaller starting budget means that more configurations can be tried out. The most explorative bracket allocated the minimum budget \\(r{_{min}}\\). The next bracket increases the starting budget by a factor of \\(\\eta\\). In each bracket, the starting budget increases further until the last bracket \\(s = 0\\) essentially performs a random search with the full budget \\(r{_{max}}\\). The number of brackets \\(s{_{max}} + 1\\) is calculated with \\(s{_{max}} = {\\log_\\eta \\frac{r{_{max}} }{r{_{min}}}}\\). Under the condition that \\(r{_{0}}\\) increases by \\(\\eta\\) with each bracket, \\(r{_{min}}\\) sometimes has to be adjusted slightly in order not to use more than \\(r{_{max}}\\) resources in the last bracket. The number of configurations in the base stages is calculated so that each bracket uses approximately the same amount of budget. The following table shows a full run of the Hyperband algorithm. The bracket \\(s = 3\\) is the most explorative bracket and \\(s = 0\\) performance a random search on the full budget.\n\n\n\n\n\n\n\n\nFigure 1: Hyperband schedule with \\(\\eta = 2\\) , \\(r{_{min}} = 1\\) and \\(r{_{max}} = 8\\)\n\n\n\n\nThe Hyperband implementation in mlr3hyperband evaluates configurations with the same budget in parallel. This results in all brackets finishing at approximately the same time. The colors in Figure 1 indicate batches that are evaluated in parallel.\n\n\nHyperparameter Optimization\nIn this practical example, we will optimize the hyperparameters of XGBoost on the Spam data set. We begin by loading the XGBoost learner..\n\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.xgboost\")\n\nThe next thing we do is define the search space. The nrounds parameter controls the number of boosting iterations. We set a range from 16 to 128 boosting iterations. This is used as \\(r{_{min}}\\) and \\(r{_{max}}\\) by the Hyperband algorithm. We need to tag the parameter with \"budget\" to identify it as a fidelity parameter. For the other hyperparameters, we take the search space for XGBoost from the Bischl et al. (2021) article. This search space works for a wide range of data sets.\n\nlearner$param_set$set_values(\n  nrounds           = to_tune(p_int(16, 128, tags = \"budget\")),\n  eta               = to_tune(1e-4, 1, logscale = TRUE),\n  max_depth         = to_tune(1, 20),\n  colsample_bytree  = to_tune(1e-1, 1),\n  colsample_bylevel = to_tune(1e-1, 1),\n  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),\n  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),\n  subsample         = to_tune(1e-1, 1)\n)\n\nWe construct the tuning instance. We use the \"none\" terminator because Hyperband terminates itself when all brackets are evaluated.\n\ninstance = ti(\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\ninstance\n\n&lt;TuningInstanceBatchSingleCrit&gt;\n* State:  Not optimized\n* Objective: &lt;ObjectiveTuningBatch:classif.xgboost_on_spam&gt;\n* Search Space:\n                  id    class     lower      upper nlevels\n              &lt;char&gt;   &lt;char&gt;     &lt;num&gt;      &lt;num&gt;   &lt;num&gt;\n1:             alpha ParamDbl -6.907755   6.907755     Inf\n2: colsample_bylevel ParamDbl  0.100000   1.000000     Inf\n3:  colsample_bytree ParamDbl  0.100000   1.000000     Inf\n4:               eta ParamDbl -9.210340   0.000000     Inf\n5:            lambda ParamDbl -6.907755   6.907755     Inf\n6:         max_depth ParamInt  1.000000  20.000000      20\n7:           nrounds ParamInt 16.000000 128.000000     113\n8:         subsample ParamDbl  0.100000   1.000000     Inf\n* Terminator: &lt;TerminatorNone&gt;\n\n\nWe load the Hyperband tuner and set eta = 2. Hyperband can start from the beginning when the last bracket is evaluated. We control the number of Hyperband runs with the repetition argument. The setting repetition = Inf is useful when a terminator should stop the optimization.\n\nlibrary(\"mlr3hyperband\")\n\ntuner = tnr(\"hyperband\", eta = 2, repetitions = 1)\n\nThe Hyperband implementation in mlr3hyperband evaluates configurations with the same budget in parallel. This results in all brackets finishing at approximately the same time. You can think of it as going diagonally through Figure 1. Using eta = 2 and a range from 16 to 128 boosting iterations results in the following schedule.\n\n\n\n\n\n\nNow we are ready to start the tuning.\n\ntuner$optimize(instance)\n\nThe result of a run is the configuration with the best performance. This does not necessarily have to be a configuration evaluated with the highest budget since we can overfit the data with too many boosting iterations.\n\ninstance$result[, .(nrounds, eta, max_depth, colsample_bytree, colsample_bylevel, lambda, alpha, subsample)]\n\n   nrounds       eta max_depth colsample_bytree colsample_bylevel   lambda     alpha subsample\n     &lt;num&gt;     &lt;num&gt;     &lt;int&gt;            &lt;num&gt;             &lt;num&gt;    &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n1:     128 -1.985313        14        0.2378133         0.3688266 -3.76799 -6.126724 0.5369803\n\n\nThe archive of a Hyperband run has the additional columns \"bracket\" and \"stage\".\n\nas.data.table(instance$archive)[, .(bracket, stage, classif.ce, eta, max_depth, colsample_bytree)]\n\n    bracket stage classif.ce        eta max_depth colsample_bytree\n      &lt;int&gt; &lt;num&gt;      &lt;num&gt;      &lt;num&gt;     &lt;int&gt;            &lt;num&gt;\n 1:       3     0 0.08083442 -4.6441571        19        0.4649268\n 2:       3     0 0.07627119 -3.6868263         8        0.6186939\n 3:       3     0 0.07757497 -4.2078328        20        0.9856336\n 4:       3     0 0.05215124 -1.6405650        15        0.6790190\n 5:       3     0 0.06453716 -1.2375660         4        0.8636816\n---                                                               \n31:       0     0 0.08018253 -3.6652572        20        0.8791154\n32:       3     3 0.04823990 -1.6405650        15        0.6790190\n33:       2     2 0.03911343 -1.9853130        14        0.2378133\n34:       1     1 0.06779661 -5.0578038        10        0.3014021\n35:       1     1 0.06714472 -0.2398598        11        0.7152166\n\n\n\n\nConclusion\nThe handling of Hyperband in mlr3tuning is very similar to that of other tuners. We only have to select an additional fidelity parameter and tag it with \"budget\". We have tried to keep the runtime of the example low. For your optimization, you should use cross-validation and increase the maximum number of boosting rounds. The Bischl et al. (2021) search space suggests 5000 boosting rounds. Check out our next post on Hyperband which uses the size of the training data set as the fidelity parameter.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace        * 2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   crosstalk           1.2.1      2023-11-23 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future              1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2             3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools         * 0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect         1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband     * 0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3measures        1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc            0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning        * 1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox           * 1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   reactable         * 0.4.4      2023-03-12 [1] CRAN (R 4.4.1)\n   reactR              0.6.1      2024-09-14 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n   scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\nReferences\n\nBischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2021. “Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges.” arXiv:2107.05847 [Cs, Stat], July. http://arxiv.org/abs/2107.05847.\n\n\nJamieson, Kevin, and Ameet Talwalkar. 2016. “Non-Stochastic Best Arm Identification and Hyperparameter Optimization.” In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, edited by Arthur Gretton and Christian C. Robert, 51:240–48. Proceedings of Machine Learning Research. Cadiz, Spain: PMLR. http://proceedings.mlr.press/v51/jamieson16.html.\n\n\nLi, Lisha, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2018. “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.” Journal of Machine Learning Research 18 (185): 1–52. https://jmlr.org/papers/v18/16-558.html."
  },
  {
    "objectID": "gallery/optimization/2023-01-31-default-configuration/index.html",
    "href": "gallery/optimization/2023-01-31-default-configuration/index.html",
    "title": "Default Hyperparameter Configuration",
    "section": "",
    "text": "Scope\nThe predictive performance of modern machine learning algorithms is highly dependent on the choice of their hyperparameter configuration. Options for setting hyperparameters are tuning, manual selection by the user, and using the default configuration of the algorithm. The default configurations are chosen to work with a wide range of data sets but they usually do not achieve the best predictive performance. When tuning a learner in mlr3, we can run the default configuration as a baseline. Seeing how well it performs will tell us whether tuning pays off. If the optimized configurations perform worse, we could expand the search space or try a different optimization algorithm. Of course, it could also be that tuning on the given data set is simply not worth it.\nProbst, Boulesteix, and Bischl (2019) studied the tunability of machine learning algorithms. They found that the tunability of algorithms varies widely. Algorithms like glmnet and XGBoost are highly tunable, while algorithms like random forests work well with their default configuration. The highly tunable algorithms should thus beat their baselines more easily with optimized hyperparameters. In this article, we will tune the hyperparameters of a random forest and compare the performance of the default configuration with the optimized configurations.\n\n\nExample\nWe tune the hyperparameters of the ranger learner on the spam data set. The search space is taken from Bischl et al. (2021).\n\nlibrary(mlr3tuning)\nlibrary(mlr3learners)\n\nlearner = lrn(\"classif.ranger\",\n  mtry.ratio      = to_tune(0, 1),\n  replace         = to_tune(),\n  sample.fraction = to_tune(1e-1, 1),\n  num.trees       = to_tune(1, 2000)\n)\n\nWhen creating the tuning instance, we pass the mlr3tuning.default_configuration callback to test the default hyperparameter configuration. The default configuration is evaluated in the first batch of the tuning run. The other batches use the specified tuning method. In this example, they are randomly drawn configurations.\n\ninstance = tune(\n  tuner = tnr(\"random_search\", batch_size = 5),\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp (\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  term_evals = 51,\n  callbacks = clbk(\"mlr3tuning.default_configuration\")\n)\n\nThe default configuration is recorded in the first row of the archive. The other rows contain the results of the random search.\n\nas.data.table(instance$archive)[, .(batch_nr, mtry.ratio, replace, sample.fraction, num.trees, classif.ce)]\n\n    batch_nr mtry.ratio replace sample.fraction num.trees classif.ce\n 1:        1  0.1228070    TRUE       1.0000000       500 0.05345502\n 2:        2  0.3501304   FALSE       0.7508930      1333 0.05475880\n 3:        2  0.6235093   FALSE       0.3830663       682 0.06388527\n 4:        2  0.8002110   FALSE       0.8686475       466 0.06127771\n 5:        2  0.2390842    TRUE       0.4383263      1081 0.06258149\n---                                                                 \n47:       11  0.2220490    TRUE       0.3372232       486 0.06062581\n48:       11  0.9806011   FALSE       0.6418448       773 0.06323338\n49:       11  0.8375713   FALSE       0.2742567      1964 0.06779661\n50:       11  0.9514603   FALSE       0.9537379       626 0.07170795\n51:       11  0.8203689   FALSE       0.8481546       295 0.05867014\n\n\nWe plot the performances of the evaluated hyperparameter configurations. The blue line connects the best configuration of each batch. We see that the default configuration already performs well and the optimized configurations can not beat it.\n\nlibrary(mlr3viz)\n\nautoplot(instance, type = \"performance\")\n\n\n\n\n\n\n\n\n\n\nConlcusion\nThe time required to test the default configuration is negligible compared to the time required to run the hyperparameter optimization. It gives us a valuable indication of whether our tuning is properly configured. Running the default configuration as a baseline is a good practice that should be used in every tuning run.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package        * version    date (UTC) lib source\n   backports        1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk            1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate        2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n   cli              3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n P codetools        0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace       2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon           1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table     * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   digest           0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   dplyr            1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   evaluate         1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi            1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   farver           2.1.2      2024-05-13 [1] CRAN (R 4.4.1)\n   fastmap          1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   future           1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply     1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   generics         0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2          3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals          0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue             1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gridExtra        2.3        2017-09-09 [1] CRAN (R 4.4.1)\n   gtable           0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools        0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets      1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite         1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   knitr            1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   labeling         0.4.3      2023-08-29 [1] CRAN (R 4.4.1)\n P lattice          0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr              0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle        1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv          0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr         2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P Matrix           1.7-0      2024-04-26 [?] CRAN (R 4.4.0)\n   mlr3           * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3learners   * 0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3measures     1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc         0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3tuning     * 1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3viz        * 0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website    * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   munsell          0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n   palmerpenguins   0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox        * 1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly       1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar           1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig        2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   R6               2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   ranger           0.16.0     2023-11-12 [1] CRAN (R 4.4.1)\n   Rcpp             1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   renv             1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang            1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown        2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   scales           1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo      1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   stringi          1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   tibble           3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyselect       1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8             1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid             1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs            0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   viridis          0.6.5      2024-01-29 [1] CRAN (R 4.4.1)\n   viridisLite      0.4.2      2023-05-02 [1] CRAN (R 4.4.1)\n   withr            3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun             0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml             2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\nReferences\n\nBischl, Bernd, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas, et al. 2021. “Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges.” arXiv:2107.05847 [Cs, Stat], July. http://arxiv.org/abs/2107.05847.\n\n\nProbst, Philipp, Anne-Laure Boulesteix, and Bernd Bischl. 2019. “Tunability: Importance of Hyperparameters of Machine Learning Algorithms.” Journal of Machine Learning Research 20 (53): 1–32. http://jmlr.org/papers/v20/18-444.html."
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html",
    "title": "Visualization in mlr3",
    "section": "",
    "text": "We showcase the visualization functions of the mlr3 ecosystem. The mlr3viz package creates a plot for almost all mlr3 objects. This post displays all available plots with their reproducible code. We start with plots of the base mlr3 objects. This includes boxplots of tasks, dendrograms of cluster learners and ROC curves of predictions. After that, we tune a classification tree and visualize the results. Finally, we show visualizations for filters.\n\n\n\n\n\n\nNote\n\n\n\nThis article will be updated whenever a new plot is available in mlr3viz."
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#classification",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#classification",
    "title": "Visualization in mlr3",
    "section": "Classification",
    "text": "Classification\nWe begin with plots of the classification task Palmer Penguins. We plot the class frequency of the target variable.\n\nlibrary(mlr3verse)\nlibrary(mlr3viz)\n\ntask = tsk(\"penguins\")\ntask$select(c(\"body_mass\", \"bill_length\"))\n\nautoplot(task, type = \"target\")\n\n\n\n\n\n\n\n\nThe \"duo\" plot shows the distribution of multiple features.\n\nautoplot(task, type = \"duo\")\n\n\n\n\n\n\n\n\nThe \"pairs\" plot shows the pairwise comparison of multiple features. The classes of the target variable are shown in different colors.\n\nautoplot(task, type = \"pairs\")"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#regression",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#regression",
    "title": "Visualization in mlr3",
    "section": "Regression",
    "text": "Regression\nNext, we plot the regression task mtcars. We create a boxplot of the target variable.\n\ntask = tsk(\"mtcars\")\ntask$select(c(\"am\", \"carb\"))\n\nautoplot(task, type = \"target\")\n\n\n\n\n\n\n\n\nThe \"pairs\" plot shows the pairwise comparison of mutiple features and the target variable.\n\nautoplot(task, type = \"pairs\")"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#cluster",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#cluster",
    "title": "Visualization in mlr3",
    "section": "Cluster",
    "text": "Cluster\nFinally, we plot the cluster task US Arrests. The \"pairs\" plot shows the pairwise comparison of mutiple features.\n\nlibrary(mlr3cluster)\n\ntask = mlr_tasks$get(\"usarrests\")\n\nautoplot(task, type = \"pairs\")"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#classification-1",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#classification-1",
    "title": "Visualization in mlr3",
    "section": "Classification",
    "text": "Classification\nThe \"prediction\" plot shows the decision boundary of a classification learner and the true class labels as points.\n\ntask = tsk(\"pima\")$select(c(\"age\", \"pedigree\"))\nlearner = lrn(\"classif.rpart\")\nlearner$train(task)\n\nautoplot(learner, type = \"prediction\", task)\n\n\n\n\n\n\n\n\nUsing probabilities.\n\ntask = tsk(\"pima\")$select(c(\"age\", \"pedigree\"))\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nlearner$train(task)\n\nautoplot(learner, type = \"prediction\", task)"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#regression-1",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#regression-1",
    "title": "Visualization in mlr3",
    "section": "Regression",
    "text": "Regression\nThe \"prediction\" plot of a regression learner illustrates the decision boundary and the true response as points.\n\ntask = tsk(\"boston_housing\")$select(\"age\")\nlearner = lrn(\"regr.rpart\")\nlearner$train(task)\n\nautoplot(learner, type = \"prediction\", task)\n\n\n\n\n\n\n\n\nWhen using two features, the response surface is plotted in the background.\n\ntask = tsk(\"boston_housing\")$select(c(\"age\", \"rm\"))\nlearner = lrn(\"regr.rpart\")\nlearner$train(task)\n\nautoplot(learner, type = \"prediction\", task)"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#glmnet",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#glmnet",
    "title": "Visualization in mlr3",
    "section": "GLMNet",
    "text": "GLMNet\nThe classification and regression GLMNet learner is equipped with a plot function.\n\nlibrary(mlr3data)\n\ntask = tsk(\"ilpd\")\ntask$select(setdiff(task$feature_names, \"gender\"))\nlearner = lrn(\"classif.glmnet\")\nlearner$train(task)\n\nautoplot(learner, type = \"ggfortify\")\n\n\n\n\n\n\n\n\n\ntask = tsk(\"mtcars\")\nlearner = lrn(\"regr.glmnet\")\nlearner$train(task)\n\nautoplot(learner, type = \"ggfortify\")"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#rpart",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#rpart",
    "title": "Visualization in mlr3",
    "section": "Rpart",
    "text": "Rpart\nWe plot a classification tree of the rpart package. We have to fit the learner with keep_model = TRUE to keep the model object.\n\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\", keep_model = TRUE)\nlearner$train(task)\n\nautoplot(learner, type = \"ggparty\")\n\n\n\n\n\n\n\n\nWe can also plot regression trees.\n\ntask = tsk(\"mtcars\")\nlearner = lrn(\"regr.rpart\", keep_model = TRUE)\nlearner$train(task)\n\nautoplot(learner, type = \"ggparty\")"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#clusthierachical",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#clusthierachical",
    "title": "Visualization in mlr3",
    "section": "ClustHierachical",
    "text": "ClustHierachical\nThe \"dend\" plot shows the result of the hierarchical clustering of the data.\n\nlibrary(mlr3cluster)\n\ntask = tsk(\"usarrests\")\nlearner = lrn(\"clust.hclust\")\nlearner$train(task)\n\nautoplot(learner, type = \"dend\", task = task)\n\n\n\n\n\n\n\n\nThe \"scree\" type plots the number of clusters and the height.\n\nautoplot(learner, type = \"scree\")"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#classification-2",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#classification-2",
    "title": "Visualization in mlr3",
    "section": "Classification",
    "text": "Classification\nWe plot the predictions of a classification learner. The \"stacked\" plot shows the predicted and true class labels.\n\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\npred = learner$train(task)$predict(task)\n\nautoplot(pred, type = \"stacked\")\n\n\n\n\n\n\n\n\nThe ROC curve plots the true positive rate against the false positive rate at different thresholds.\n\nautoplot(pred, type = \"roc\")\n\n\n\n\n\n\n\n\nThe precision-recall curve plots the precision against the recall at different thresholds.\n\nautoplot(pred, type = \"prc\")\n\n\n\n\n\n\n\n\nThe \"threshold\" plot varies the threshold of a binary classification and plots against the resulting performance.\n\nautoplot(pred, type = \"threshold\")"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#regression-2",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#regression-2",
    "title": "Visualization in mlr3",
    "section": "Regression",
    "text": "Regression\nThe predictions of a regression learner are often presented as a scatterplot of truth and predicted response.\n\ntask = tsk(\"boston_housing\")\nlearner = lrn(\"regr.rpart\")\npred = learner$train(task)$predict(task)\n\nautoplot(pred, type = \"xy\")\n\n\n\n\n\n\n\n\nAdditionally, we plot the response with the residuals.\n\nautoplot(pred, type = \"residual\")\n\n\n\n\n\n\n\n\nWe can also plot the distribution of the residuals.\n\nautoplot(pred, type = \"histogram\")"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#cluster-1",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#cluster-1",
    "title": "Visualization in mlr3",
    "section": "Cluster",
    "text": "Cluster\nThe predictions of a cluster learner are often presented as a scatterplot of the data points colored by the cluster.\n\nlibrary(mlr3cluster)\n\ntask = tsk(\"usarrests\")\nlearner = lrn(\"clust.kmeans\", centers = 3)\npred = learner$train(task)$predict(task)\n\nautoplot(pred, task, type = \"scatter\")\n\n\n\n\n\n\n\n\nThe \"sil\" plot shows the silhouette width of the clusters. The dashed line is the mean silhouette width.\n\nautoplot(pred, task, type = \"sil\")\n\n\n\n\n\n\n\n\nThe \"pca\" plot shows the first two principal components of the data colored by the cluster.\n\nautoplot(pred, task, type = \"pca\")"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#classification-3",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#classification-3",
    "title": "Visualization in mlr3",
    "section": "Classification",
    "text": "Classification\nThe \"boxplot\" shows the distribution of the performance measures.\n\ntask = tsk(\"sonar\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nresampling = rsmp(\"cv\")\nrr = resample(task, learner, resampling)\n\nautoplot(rr, type = \"boxplot\")\n\n\n\n\n\n\n\n\nWe can also plot the distribution of the performance measures as a “histogram”.\n\nautoplot(rr, type = \"histogram\")\n\n\n\n\n\n\n\n\nThe ROC curve plots the true positive rate against the false positive rate at different thresholds.\n\nautoplot(rr, type = \"roc\")\n\n\n\n\n\n\n\n\nThe precision-recall curve plots the precision against the recall at different thresholds.\n\nautoplot(rr, type = \"prc\")\n\n\n\n\n\n\n\n\nThe \"prediction\" plot shows two features and the predicted class in the background. Points mark the observations of the test set and the color presents the truth.\n\ntask = tsk(\"pima\")\ntask$filter(seq(100))\ntask$select(c(\"age\", \"glucose\"))\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n\nautoplot(rr, type = \"prediction\")\n\n\n\n\n\n\n\n\nAlternatively, we can plot class probabilities.\n\ntask = tsk(\"pima\")\ntask$filter(seq(100))\ntask$select(c(\"age\", \"glucose\"))\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n\nautoplot(rr, type = \"prediction\")\n\n\n\n\n\n\n\n\nIn addition to the test set, we can also plot the train set.\n\ntask = tsk(\"pima\")\ntask$filter(seq(100))\ntask$select(c(\"age\", \"glucose\"))\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\", predict_sets = c(\"train\", \"test\"))\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n\nautoplot(rr, type = \"prediction\", predict_sets = c(\"train\", \"test\"))\n\n\n\n\n\n\n\n\nThe \"prediction\" plot can also show categorical features.\n\ntask = tsk(\"german_credit\")\ntask$filter(seq(100))\ntask$select(c(\"housing\", \"employment_duration\"))\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n\nautoplot(rr, type = \"prediction\")"
  },
  {
    "objectID": "gallery/technical/2022-12-22-mlr3viz/index.html#regression-3",
    "href": "gallery/technical/2022-12-22-mlr3viz/index.html#regression-3",
    "title": "Visualization in mlr3",
    "section": "Regression",
    "text": "Regression\nThe “prediction” plot shows one feature and the response. Points mark the observations of the test set.\n\ntask = tsk(\"boston_housing\")\ntask$select(\"age\")\ntask$filter(seq(100))\nlearner = lrn(\"regr.rpart\")\nresampling = rsmp(\"cv\", folds  = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n\nautoplot(rr, type = \"prediction\")\n\n\n\n\n\n\n\n\nAdditionally, we can add confidence bounds.\n\ntask = tsk(\"boston_housing\")\ntask$select(\"age\")\ntask$filter(seq(100))\nlearner = lrn(\"regr.lm\", predict_type = \"se\")\nresampling = rsmp(\"cv\", folds  = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n\nautoplot(rr, type = \"prediction\")\n\n\n\n\n\n\n\n\nAnd add the train set.\n\ntask = tsk(\"boston_housing\")\ntask$select(\"age\")\ntask$filter(seq(100))\nlearner = lrn(\"regr.lm\", predict_type = \"se\", predict_sets = c(\"train\", \"test\"))\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n\nautoplot(rr, type = \"prediction\", predict_sets = c(\"train\", \"test\"))\n\n\n\n\n\n\n\n\nWe can also add the prediction surface to the background.\n\ntask = tsk(\"boston_housing\")\ntask$select(c(\"age\", \"rm\"))\ntask$filter(seq(100))\nlearner = lrn(\"regr.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, learner, resampling, store_models = TRUE)\n\nautoplot(rr, type = \"prediction\")"
  },
  {
    "objectID": "gallery/technical/2023-10-30-tidymodels/index.html",
    "href": "gallery/technical/2023-10-30-tidymodels/index.html",
    "title": "Analyzing the Runtime Performance of tidymodels and mlr3",
    "section": "",
    "text": "In the realm of data science, machine learning frameworks play an important role in streamlining and accelerating the development of analytical workflows. Among these, tidymodels and mlr3 stand out as prominent tools within the R community. They provide a unified interface for data preprocessing, model training, resampling and tuning. The streamlined and accelerated development process, while efficient, typically results in a trade-off concerning runtime performance. This article undertakes a detailed comparison of the runtime efficiency of tidymodels and mlr3, focusing on their performance in training, resampling, and tuning machine learning models. Specifically, we assess the time efficiency of these frameworks in running the rpart::rpart() and ranger::ranger() models, using the Sonar dataset as a test case. Additionally, the study delves into analyzing the runtime overhead of these frameworks by comparing their performance against training the models without a framework. Through this comparative analysis, the article aims to provide valuable insights into the operational trade-offs of using these advanced machine learning frameworks in practical data science applications."
  },
  {
    "objectID": "gallery/technical/2023-10-30-tidymodels/index.html#train-the-models",
    "href": "gallery/technical/2023-10-30-tidymodels/index.html#train-the-models",
    "title": "Analyzing the Runtime Performance of tidymodels and mlr3",
    "section": "Train the Models",
    "text": "Train the Models\nOur benchmark starts with the fundamental task of model training. To facilitate a direct comparison, we have structured our presentation into two distinct segments. On the left, we demonstrate the initialization of the rpart model, employing both mlr3 and tidymodels frameworks. The rpart model is a decision tree classifier, which is a simple and fast-fitting algorithm for classification tasks. Simultaneously, on the right, we turn our attention to the initialization of the ranger model, known for its efficient implementation of the random forest algorithm. Our aim is to mirror the configuration as closely as possible across both frameworks, maintaining consistency in parameters and settings.\n# tidymodels\ntm_mod = decision_tree() %&gt;%\n  set_engine(\"rpart\",\n    xval = 0L) %&gt;%\n  set_mode(\"classification\")\n\n# mlr3\nlearner = lrn(\"classif.rpart\",\n  xval = 0L)\n# tidymodels\ntm_mod = rand_forest(trees = 1000L) %&gt;%\n  set_engine(\"ranger\",\n    num.threads = 1L,\n    seed = 1) %&gt;%\n  set_mode(\"classification\")\n\n# mlr3\nlearner = lrn(\"classif.ranger\",\n  num.trees = 1000L,\n  num.threads = 1L,\n  seed = 1,\n  verbose = FALSE,\n  predict_type = \"prob\")\n\n\n\n\n\nWe measure the runtime for the train functions within each framework. The result of the train function is a trained model in both frameworks. In addition, we invoke the rpart() and ranger() functions to establish a baseline for the minimum achievable runtime. This allows us to not only assess the efficiency of the train functions in each framework but also to understand how they perform relative to the base packages.\n\n# tidymodels train\nfit(tm_mod, formula, data = data)\n\n# mlr3 train\nlearner$train(task)\n\nWhen training an rpart model, tidymodels demonstrates superior speed, outperforming mlr3 (Table 1). Notably, the mlr3 package requires approximately twice the time compared to the baseline.\nA key observation from our results is the significant relative overhead when using a framework for rpart model training. Given that rpart inherently requires a shorter training time, the additional processing time introduced by the frameworks becomes more pronounced. This aspect highlights the trade-off between the convenience offered by these frameworks and their impact on runtime for quicker tasks.\nConversely, when we shift our focus to training a ranger model, the scenario changes (Table 2). Here, the runtime performance of ranger is strikingly similar across both tidymodels and mlr3. This equality in execution time can be attributed to the inherently longer training duration required by ranger models. As a result, the relative overhead introduced by either framework becomes minimal, effectively diminishing in the face of the more time-intensive training process. This pattern suggests that for more complex or time-consuming tasks, the choice of framework may have a less significant impact on overall runtime performance.\n\n\n\n\n\n\nTable 1: Average runtime in milliseconds of training rpart depending on the framework.\n\n\n\n\n\n\nFramework\nLQ\nMedian\nUQ\n\n\n\n\nbase\n11\n11\n12\n\n\nmlr3\n23\n23\n24\n\n\ntidymodels\n18\n18\n19\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Average runtime in milliseconds of training ranger depending on the framework.\n\n\n\n\n\n\nFramework\nLQ\nMedian\nUQ\n\n\n\n\nbase\n286\n322\n347\n\n\nmlr3\n301\n335\n357\n\n\ntidymodels\n310\n342\n362"
  },
  {
    "objectID": "gallery/technical/2023-10-30-tidymodels/index.html#resample-sequential",
    "href": "gallery/technical/2023-10-30-tidymodels/index.html#resample-sequential",
    "title": "Analyzing the Runtime Performance of tidymodels and mlr3",
    "section": "Resample Sequential",
    "text": "Resample Sequential\nWe proceed to evaluate the runtime performance of the resampling functions within both frameworks, specifically under conditions without parallelization. This step involves the generation of resampling splits, including 3-fold, 6-fold, and 9-fold cross-validation. Additionally, we run a 100 times repeated 3-fold cross-validation.\nWe generate the same resampling splits for both frameworks. This consistency is key to ensuring that any observed differences in runtime are attributable to the frameworks themselves, rather than variations in the resampling process.\nIn our pursuit of a fair and balanced comparison, we address certain inherent differences between the two frameworks. Notably, tidymodels inherently includes scoring of the resampling results as part of its process. To align the comparison, we replicate this scoring step in mlr3, thus maintaining a level field for evaluation. Furthermore, mlr3 inherently saves predictions during the resampling process. To match this, we activate the saving of the predictions in tidymodels.\n\n# tidymodels resample\ncontrol = control_grid(save_pred = TRUE)\nmetrics = metric_set(accuracy)\n\ntm_wf =\n  workflow() %&gt;%\n  add_model(tm_mod) %&gt;%\n  add_formula(formula)\n\nfit_resamples(tm_wf, folds, metrics = metrics, control = control)\n\n# mlr3 resample\nmeasure = msr(\"classif.acc\")\n\nrr = resample(task, learner, resampling)\nrr$score(measure)\n\nWhen resampling the fast-fitting rpart model, mlr3 demonstrates a notable edge in speed, as detailed in Table 3. In contrast, when it comes to resampling the more computationally intensive ranger models, the performance of tidymodels and mlr3 converges closely (Table 4). This parity in performance is particularly noteworthy, considering the differing internal mechanisms and optimizations of tidymodels and mlr3. A consistent trend observed across both frameworks is a linear increase in runtime proportional to the number of folds in cross-validation (Figure 1).\n\n\n\n\n\n\nTable 3: Average runtime in milliseconds of rpart depending on the framework and resampling strategy.\n\n\n\n\n\n\nFramework\nResampling\nLQ\nMedian\nUQ\n\n\n\n\nmlr3\ncv3\n188\n196\n210\n\n\ntidymodels\ncv3\n233\n242\n257\n\n\nmlr3\ncv6\n343\n357\n379\n\n\ntidymodels\ncv6\n401\n415\n436\n\n\nmlr3\ncv9\n500\n520\n548\n\n\ntidymodels\ncv9\n568\n588\n616\n\n\nmlr3\nrcv100\n15526\n16023\n16777\n\n\ntidymodels\nrcv100\n16409\n16876\n17527\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4: Average runtime in milliseconds of ranger depending on the framework and resampling strategy.\n\n\n\n\n\n\nFramework\nResampling\nLQ\nMedian\nUQ\n\n\n\n\nmlr3\ncv3\n923\n1004\n1062\n\n\ntidymodels\ncv3\n916\n981\n1023\n\n\nmlr3\ncv6\n1990\n2159\n2272\n\n\ntidymodels\ncv6\n2089\n2176\n2239\n\n\nmlr3\ncv9\n3074\n3279\n3441\n\n\ntidymodels\ncv9\n3260\n3373\n3453\n\n\nmlr3\nrcv100\n85909\n88642\n91381\n\n\ntidymodels\nrcv100\n87828\n88822\n89843\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Average runtime, measured in milliseconds, for cross-validations using rpart (displayed on the left) and ranger (on the right). The comparison encompasses variations across different frameworks and the number of folds in the cross-validation."
  },
  {
    "objectID": "gallery/technical/2023-10-30-tidymodels/index.html#resample-parallel",
    "href": "gallery/technical/2023-10-30-tidymodels/index.html#resample-parallel",
    "title": "Analyzing the Runtime Performance of tidymodels and mlr3",
    "section": "Resample Parallel",
    "text": "Resample Parallel\nWe conducted a second set of resampling function tests, this time incorporating parallelization to explore its impact on runtime efficiency. In this phase, we utilized doFuture and doParallel as the primary parallelization packages for tidymodels, recognizing their robust support and compatibility. Meanwhile, for mlr3, the future package was employed to facilitate parallel processing.\nOur findings, as presented in the respective tables (Table 5 and Table 6), reveal interesting dynamics about parallelization within the frameworks. When the number of folds in the resampling process is doubled, we observe only a marginal increase in the average runtime. This pattern suggests a significant overhead associated with initializing the parallel workers, a factor that becomes particularly influential in the overall efficiency of the parallelization process.\nIn the case of the rpart model, the parallelization overhead appears to outweigh the potential speedup benefits, as illustrated in the left section of Figure 2. This result indicates that for less complex models like rpart, where individual training times are relatively short, the initialization cost of parallel workers may not be sufficiently offset by the reduced processing time per fold.\nConversely, for the ranger model, the utilization of parallelization demonstrates a clear advantage over the sequential version, as evidenced in the right section of Figure 2. This finding underscores that for more computationally intensive models like ranger, which have longer individual training times, the benefits of parallel processing significantly overcome the initial overhead of worker setup. This differentiation highlights the importance of considering the complexity and inherent processing time of models when deciding to implement parallelization strategies in these frameworks.\n\n\n\n\n\n\nTable 5: Average runtime in milliseconds of mlr3 with future and rpart depending on the resampling strategy.\n\n\n\n\n\n\nResampling\nLQ\nMedian\nUQ\n\n\n\n\ncv3\n625\n655\n703\n\n\ncv6\n738\n771\n817\n\n\ncv9\n831\n875\n923\n\n\nrcv100\n8620\n9043\n9532\n\n\n\n\n\n\n\n\n\n\n\n\nTable 6: Average runtime in milliseconds of mlr3 with future and ranger depending on the resampling strategy.\n\n\n\n\n\n\nResampling\nLQ\nMedian\nUQ\n\n\n\n\ncv3\n836\n884\n943\n\n\ncv6\n1200\n1249\n1314\n\n\ncv9\n1577\n1634\n1706\n\n\nrcv100\n32047\n32483\n33022\n\n\n\n\n\n\n\n\n\n\nWhen paired with doFuture, tidymodels exhibits significantly slower runtime compared to the mlr3 package utilizing future (Table 7 and Table 8). We observed that tidymodels exports more data to the parallel workers, which notably exceeds that of mlr3. This substantial difference in data export could plausibly account for the observed slower runtime when using tidymodels on small tasks.\n\n\n\n\n\n\nTable 7: Average runtime in milliseconds of tidymodels with doFuture and rpart depending on the resampling strategy.\n\n\n\n\n\n\nResampling\nLQ\nMedian\nUQ\n\n\n\n\ncv3\n2778\n2817\n3019\n\n\ncv6\n2808\n2856\n3033\n\n\ncv9\n2935\n2975\n3170\n\n\nrcv100\n9154\n9302\n9489\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8: Average runtime in milliseconds of tidymodels with doFuture and ranger depending on the resampling strategy.\n\n\n\n\n\n\nResampling\nLQ\nMedian\nUQ\n\n\n\n\ncv3\n2982\n3046\n3234\n\n\ncv6\n3282\n3366\n3543\n\n\ncv9\n3568\n3695\n3869\n\n\nrcv100\n27546\n27843\n28166\n\n\n\n\n\n\n\n\n\n\nThe utilization of the doParallel package demonstrates a notable improvement in handling smaller resampling tasks. In these scenarios, the resampling process consistently outperforms the mlr3 framework in terms of speed. However, it’s important to note that even with this enhanced performance, the doParallel package does not always surpass the efficiency of the sequential version, especially when working with the rpart model. This specific observation is illustrated in the left section of Figure 2.\n\n\n\n\n\n\nTable 9: Average runtime in milliseconds of tidymodels with doParallel and rpart depending on the resampling strategy.\n\n\n\n\n\n\nResampling\nLQ\nMedian\nUQ\n\n\n\n\ncv3\n557\n649\n863\n\n\ncv6\n602\n714\n910\n\n\ncv9\n661\n772\n968\n\n\nrcv100\n10609\n10820\n11071\n\n\n\n\n\n\n\n\n\n\n\n\nTable 10: Average runtime in milliseconds of tidymodels with doParallel and ranger depending on the resampling strategy.\n\n\n\n\n\n\nResampling\nLQ\nMedian\nUQ\n\n\n\n\ncv3\n684\n756\n948\n\n\ncv6\n1007\n1099\n1272\n\n\ncv9\n1360\n1461\n1625\n\n\nrcv100\n31205\n31486\n31793\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Average runtime, measured in milliseconds, for cross-validations using rpart (displayed on the left) and ranger (on the right). The comparison encompasses variations across different frameworks, the number of folds in the cross-validation, and the implementation of parallelization.\n\n\n\n\n\nIn the context of repeated cross-validation, our findings underscore the efficacy of parallelization (Figure 3). Across all frameworks tested, the adoption of parallel processing techniques yields a significant increase in speed. This enhancement is particularly noticeable in larger resampling tasks, where the demands on computational resources are more substantial.\nInterestingly, within these more extensive resampling scenarios, the doFuture package emerges as a more efficient option compared to doParallel. This distinction is important, as it highlights the relative strengths of different parallelization packages under varying workload conditions. While doParallel shows proficiency in smaller tasks, doFuture demonstrates its capability to handle larger, more complex resampling processes with greater speed and efficiency.\n\n\n\n\n\n\n\n\nFigure 3: Average runtime, measured in seconds, of a 100 times repeated 3-fold cross-validation using rpart (displayed on the left) and ranger (on the right). The comparison encompasses variations across different frameworks and the implementation of parallelization."
  },
  {
    "objectID": "gallery/technical/2023-10-30-tidymodels/index.html#tune-sequential",
    "href": "gallery/technical/2023-10-30-tidymodels/index.html#tune-sequential",
    "title": "Analyzing the Runtime Performance of tidymodels and mlr3",
    "section": "Tune Sequential",
    "text": "Tune Sequential\nWe then shift our focus to assessing the runtime performance of the tuning functions. In this phase, the tidymodels package is utilized to evaluate a predefined grid, comprising a specific set of hyperparameter configurations. To ensure a balanced and comparable analysis, we employ the \"design_points\" tuner from the mlr3tuning package. This approach allows us to evaluate the same grid within the mlr3 framework, maintaining consistency across both platforms. The grid used for this comparison contains 200 hyperparameter configurations each, for both the rpart and ranger models. This approach helps us to understand how each framework handles the optimization of model hyperparameters, a key aspect of building effective and efficient machine learning models.\n# tidymodels\ntm_mod = decision_tree(\n  cost_complexity = tune()) %&gt;%\n  set_engine(\"rpart\",\n    xval = 0) %&gt;%\n  set_mode(\"classification\")\n\ntm_design = data.table(\n  cost_complexity = seq(0.1, 0.2, length.out = 200))\n\n# mlr3\nlearner = lrn(\"classif.rpart\",\n  xval = 0,\n  cp = to_tune())\n\nmlr3_design = data.table(\n  cp = seq(0.1, 0.2, length.out = 200))\n# tidymodels\ntm_mod = rand_forest(\n  trees = tune()) %&gt;%\n  set_engine(\"ranger\",\n    num.threads = 1L,\n    seed = 1) %&gt;%\n  set_mode(\"classification\")\n\ntm_design = data.table(\n  trees = seq(1000, 1199))\n\n# mlr3\nlearner = lrn(\"classif.ranger\",\n  num.trees = to_tune(1, 10000),\n  num.threads = 1L,\n  seed = 1,\n  verbose = FALSE,\n  predict_type = \"prob\")\n\nmlr3_design = data.table(\n  num.trees = seq(1000, 1199))\n\n\n\n\n\nWe measure the runtime of the tune functions within each framework. Both the tidymodels and mlr3 frameworks are tasked with identifying the optimal hyperparameter configuration.\n\n# tidymodels tune\ntune::tune_grid(\n  tm_wf,\n  resamples = resamples,\n  grid = design,\n  metrics = metrics)\n\n# mlr3 tune\ntuner = tnr(\"design_points\", design = design, batch_size = nrow(design))\nmlr3tuning::tune(\n  tuner = tuner,\n  task = task,\n  learner = learner,\n  resampling = resampling,\n  measures = measure,\n  store_benchmark_result = FALSE)\n\nIn our sequential tuning tests, mlr3 demonstrates a notable advantage in terms of speed. This finding is clearly evidenced in our results, as shown in Table Table 11 for the rpart model and Table Table 12 for the ranger model. The faster performance of mlr3 in these sequential runs highlights its efficiency in handling the tuning process without parallelization.\n\n\n\n\n\n\nTable 11: Average runtime in seconds of tuning 200 points of rpart depending on the framework.\n\n\n\n\n\n\nFramework\nLQ\nMedian\nUQ\n\n\n\n\nmlr3\n27\n27\n28\n\n\ntidymodels\n37\n37\n39\n\n\n\n\n\n\n\n\n\n\n\n\nTable 12: Average runtime in seconds of tuning 200 points of ranger depending on the framework.\n\n\n\n\n\n\nFramework\nLQ\nMedian\nUQ\n\n\n\n\nmlr3\n167\n171\n175\n\n\ntidymodels\n194\n195\n196"
  },
  {
    "objectID": "gallery/technical/2023-10-30-tidymodels/index.html#tune-parallel",
    "href": "gallery/technical/2023-10-30-tidymodels/index.html#tune-parallel",
    "title": "Analyzing the Runtime Performance of tidymodels and mlr3",
    "section": "Tune Parallel",
    "text": "Tune Parallel\nConcluding our analysis, we proceed to evaluate the runtime performance of the tune functions, this time implementing parallelization to enhance efficiency. For these runs, parallelization is executed on 3 cores.\nIn the case of mlr3, we opt for the largest possible chunk size. This strategic choice means that all points within the tuning grid are sent to the workers in a single batch, effectively minimizing the overhead typically associated with parallelization. This approach is crucial in reducing the time spent in distributing tasks across multiple cores, thereby streamlining the tuning process. On the other hand, the tidymodels package also operates with the same chunk size, but this setting is determined and managed internally within the framework.\nBy conducting these parallelization tests, we aim to provide a deeper understanding of how each framework handles the distribution and management of computational tasks during the tuning process, particularly in a parallel computing environment. This final set of measurements is important in painting a complete picture of the runtime performance of the tune functions across both tidymodels and mlr3 under different operational settings.\n\noptions(\"mlr3.exec_chunk_size\" = 200)\n\nOur analysis of the parallelized tuning functions reveals that the runtimes for mlr3 and tidymodels are remarkably similar. However, subtle differences emerge upon closer inspection. For instance, the mlr3 package exhibits a slightly faster performance when tuning the rpart model, as indicated in Table 13. In contrast, it falls marginally behind tidymodels in tuning the ranger model, as shown in Table 14.\nInterestingly, when considering the specific context of a 3-fold cross-validation, the doParallel package outperforms doFuture in terms of speed, as demonstrated in Figure 4. This outcome suggests that the choice of parallelization package can have a significant impact on tuning efficiency, particularly in scenarios with a smaller number of folds.\nA key takeaway from our study is the clear benefit of enabling parallelization, regardless of the chosen framework-backend combination. Activating parallelization consistently enhances performance, making it a highly recommended strategy for tuning machine learning models, especially in tasks involving extensive hyperparameter exploration or larger datasets. This conclusion underscores the value of parallel processing in modern machine learning workflows, offering a practical solution for accelerating model tuning across various computational settings.\n\n\n\n\n\n\nTable 13: Average runtime in seconds of tuning 200 points of rpart depending on the framework.\n\n\n\n\n\n\nFramework\nBackend\nLQ\nMedian\nUQ\n\n\n\n\nmlr3\nfuture\n11\n12\n12\n\n\ntidymodels\ndoFuture\n17\n17\n17\n\n\ntidymodels\ndoParallel\n13\n13\n13\n\n\n\n\n\n\n\n\n\n\n\n\nTable 14: Average runtime in seconds of tuning 200 points of ranger depending on the framework.\n\n\n\n\n\n\nFramework\nBackend\nLQ\nMedian\nUQ\n\n\n\n\nmlr3\nfuture\n54\n55\n55\n\n\ntidymodels\ndoFuture\n58\n58\n59\n\n\ntidymodels\ndoParallel\n54\n54\n55\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Average runtime, measured in seconds, of a tuning 200 hyperparameter configurations of rpart (displayed on the left) and ranger (on the right). The comparison encompasses variations across different frameworks and the implementation of parallelization."
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "",
    "text": "Here are some interesting reads regarding BART:\n\nThe first BART paper (Chipman, George, and McCulloch 2010).\nThe first implementation of BART for survival data (Bonato et al. 2011). This includes fully parametric AFT and Weibull models and the semi-parametric CoxPH regression model.\nThe first non-parametric implementation of BART for survival data (R. A. Sparapani et al. 2016)\nBART R package tutorial (R. Sparapani, Spanbauer, and McCulloch 2021)\n\nWe incorporated the survival BART model in mlr3extralearners and in this tutorial we will demonstrate how we can use packages like mlr3, mlr3proba and distr6 to more easily manipulate the output predictions to assess model convergence, validate our model (via several survival metrics), as well as perform model interpretation via PDPs (Partial Dependence Plots)."
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html#intro",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html#intro",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "",
    "text": "Here are some interesting reads regarding BART:\n\nThe first BART paper (Chipman, George, and McCulloch 2010).\nThe first implementation of BART for survival data (Bonato et al. 2011). This includes fully parametric AFT and Weibull models and the semi-parametric CoxPH regression model.\nThe first non-parametric implementation of BART for survival data (R. A. Sparapani et al. 2016)\nBART R package tutorial (R. Sparapani, Spanbauer, and McCulloch 2021)\n\nWe incorporated the survival BART model in mlr3extralearners and in this tutorial we will demonstrate how we can use packages like mlr3, mlr3proba and distr6 to more easily manipulate the output predictions to assess model convergence, validate our model (via several survival metrics), as well as perform model interpretation via PDPs (Partial Dependence Plots)."
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html#libraries",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html#libraries",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "Libraries",
    "text": "Libraries\n\nlibrary(mlr3extralearners)\nlibrary(mlr3pipelines)\nlibrary(mlr3proba)\nlibrary(distr6)\nlibrary(BART) # 2.9.4\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)"
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html#data",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html#data",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "Data",
    "text": "Data\nWe will use the Lung Cancer Dataset. We convert the time variable from days to months to ease the computational burden:\n\ntask_lung = tsk('lung')\n\nd = task_lung$data()\n# in case we want to select specific columns to keep\n# d = d[ ,colnames(d) %in% c(\"time\", \"status\", \"age\", \"sex\", \"ph.karno\"), with = FALSE]\nd$time = ceiling(d$time/30.44)\ntask_lung = as_task_surv(d, time = 'time', event = 'status', id = 'lung')\ntask_lung$label = \"Lung Cancer\"\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe original BART implementation supports categorical features (factors). This results in different importance scores per each dummy level which doesn’t work well with mlr3. So features of type factor or character are not allowed and we leave it to the user to encode them as they please.\nThe original BART implementation supports features with missing values. This is totally fine with mlr3 as well! In this example, we impute the features to show good ML practice.\n\n\n\nIn our lung dataset, we encode the sex feature and perform model-based imputation with the rpart regression learner:\n\npo_encode = po('encode', method = 'treatment')\npo_impute = po('imputelearner', lrn('regr.rpart'))\npre = po_encode %&gt;&gt;% po_impute\ntask = pre$train(task_lung)[[1]]\ntask\n\n&lt;TaskSurv:lung&gt; (168 x 9): Lung Cancer\n* Target: time, status\n* Properties: -\n* Features (7):\n  - int (6): age, meal.cal, pat.karno, ph.ecog, ph.karno, wt.loss\n  - dbl (1): sex\n\n\nNo missing values in our data:\n\ntask$missings()\n\n     time    status       age  meal.cal pat.karno   ph.ecog  ph.karno   wt.loss       sex \n        0         0         0         0         0         0         0         0         0 \n\n\nWe partition the data to train and test sets:\n\nset.seed(42)\npart = partition(task, ratio = 0.9)"
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html#train-and-test",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html#train-and-test",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "Train and Test",
    "text": "Train and Test\nWe train the BART model and predict on the test set:\n\n# default `ndpost` value: 1000. We reduce it to 50 to speed up calculations in this tutorial\nlearner = lrn(\"surv.bart\", nskip = 250, ndpost = 50, keepevery = 10, mc.cores = 10)\nlearner$train(task, row_ids = part$train)\np = learner$predict(task, row_ids = part$test)\np\n\nSee more details about BART’s parameters on the online documentation.\n\ndistr\nWhat kind of object is the predicted distr?\n\np$distr\n\nArrdist(17x30x50) \n\n\n\n\n\n\n\n\nTipArrdist dimensions:\n\n\n\n\nPatients (observations)\nTime points (months)\nNumber of posterior draws\n\n\n\nActually the $distr is an active R6 field - this means that some computation is required to create it. What the prediction object actually stores internally is a 3d survival array (can be used directly with no performance overhead):\n\ndim(p$data$distr)\n\n[1] 17 30 50\n\n\nThis is a more easy-to-understand and manipulate form of the full posterior survival matrix prediction from the BART package ((R. Sparapani, Spanbauer, and McCulloch 2021), pages 34-35).\n\n\n\n\n\n\nWarning\n\n\n\nThough we have optimized with C++ code the way the Arrdist object is constructed, calling the $distr field can be computationally taxing if the product of the sizes of the 3 dimensions above exceeds ~1 million. In our case, \\(23 \\times 31 \\times 50 = 35650\\) so the conversion to an Arrdist via $distr will certainly not create performance issues.\n\n\nAn example using the internal prediction data: get all the posterior probabilities of the 3rd patient in the test set, at 12 months (1 year):\n\np$data$distr[3, 12, ]\n\n [1] 0.16682639 0.52846967 0.51057138 0.23143445 0.24949314 0.23818569 0.28702429 0.39494813 0.68549420 0.38617165\n[11] 0.18528656 0.18365059 0.58371359 0.34762244 0.30502214 0.20621051 0.73117066 0.37176522 0.44852366 0.48714300\n[21] 0.13227459 0.36120922 0.04110908 0.08701728 0.26405672 0.13075270 0.33063856 0.71372303 0.28818814 0.43886058\n[31] 0.16152343 0.11132334 0.09516388 0.26125352 0.16578149 0.33411388 0.32607511 0.43967968 0.25908991 0.21763759\n[41] 0.47965258 0.50324377 0.13649939 0.40579983 0.29247802 0.09903617 0.56096633 0.26853225 0.20253541 0.42479851\n\n\nWorking with the $distr interface and Arrdist objects is very efficient as we will see later for predicting survival estimates.\n\n\n\n\n\n\nTip\n\n\n\nIn survival analysis, \\(S(t) = 1 - F(t)\\), where \\(S(t)\\) the survival function and \\(F(t)\\) the cumulative distribution function (cdf). The latter can be interpreted as risk or probability of death up to time \\(t\\).\nWe can verify the above from the prediction object:\n\nsurv_array = 1 - distr6::gprm(p$distr, \"cdf\") # 3d array\nall_equal(p$data$distr, surv_array)\n\nWarning: `all_equal()` was deprecated in dplyr 1.1.0.\nℹ Please use `all.equal()` instead.\nℹ And manually order the rows/cols as needed\n\n\n`y` must be a data frame.\n\n\n\n\n\n\ncrank\ncrank is the expected mortality (Sonabend, Bender, and Vollmer 2022) which is the sum of the predicted cumulative hazard function (as is done in random survival forest models). Higher values denote larger risk. To calculate crank, we need a survival matrix. So we have to choose which 3rd dimension we should use from the predicted survival array. This is what the which.curve parameter of the learner does:\n\nlearner$param_set$get_values()$which.curve\n\n[1] 0.5\n\n\nThe default value (\\(0.5\\) quantile) is the median survival probability. It could be any other quantile (e.g. \\(0.25\\)). Other possible values for which.curve are mean or a number denoting the exact posterior draw to extract (e.g. the last one, which.curve = 50)."
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html#feature-importance",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html#feature-importance",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "Feature importance",
    "text": "Feature importance\nDefault score is the observed count of each feature in the trees (so the higher the score, the more important the feature):\n\nlearner$param_set$values$importance\n\n[1] \"count\"\n\nlearner$importance()\n\n      sex   wt.loss  ph.karno  meal.cal pat.karno       age   ph.ecog \n     8.94      8.22      8.16      7.74      6.44      6.28      6.28"
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html#mcmc-diagnostics",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html#mcmc-diagnostics",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "MCMC Diagnostics",
    "text": "MCMC Diagnostics\nBART uses internally MCMC (Markov Chain Monte Carlo) to sample from the posterior survival distribution. We need to check that MCMC has converged, meaning that the chains have reached a stationary distribution that approximates the true posterior survival distribution (otherwise the predictions may be inaccurate, misleading and unreliable).\nWe use Geweke’s convergence diagnostic test as it is implemented in the BART R package. We choose 10 random patients from the train set to evaluate the MCMC convergence.\n\n# predictions on the train set\np_train = learner$predict(task, row_ids = part$train)\n\n# choose 10 patients from the train set randomly and make a list\nids = as.list(sample(length(part$train), 10))\n\nz_list = lapply(ids, function(id) {\n  # matrix with columns =&gt; time points and rows =&gt; posterior draws\n  post_surv = 1 - t(distr6::gprm(p_train$distr[id], \"cdf\")[1,,])\n  BART::gewekediag(post_surv)$z # get the z-scores\n})\n\n# plot the z scores vs time for all patients\ndplyr::bind_rows(z_list) %&gt;%\n  tidyr::pivot_longer(cols = everything()) %&gt;%\n  mutate(name = as.numeric(name)) %&gt;%\n  ggplot(aes(x = name, y = value)) +\n  geom_point() +\n  labs(x = \"Time (months)\", y = \"Z-scores\") +\n  # add critical values for a = 0.05\n  geom_hline(yintercept = 1.96, linetype = 'dashed', color = \"red\") +\n  geom_hline(yintercept = -1.96, linetype = 'dashed', color = \"red\") +\n  theme_bw(base_size = 14)\n\n\n\n\nGeweke plot for MCMC diagnostics. Z-scores for the difference in the mean survival prediction between the first 10% and last 50% part of a Markov chain. The predictions are taken from 10 random patients in the train set. Red lines indicate the a = 0.05 critical line. Only a few z-scores exceed the 95% limits so we conclude that convergence has been attained."
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html#performance-test-set",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html#performance-test-set",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "Performance (test set)",
    "text": "Performance (test set)\nWe will use the following survival metrics:\n\nIntegrated Brier Score (requires a survival distribution prediction - distr)\nUno’s C-index (requires a continuous ranking score prediction - crank)\n\nFor the first measure we will use the ERV (Explained Residual Variation) version, which standardizes the score against a Kaplan-Meier (KM) baseline (Sonabend et al. 2022). This means that values close to \\(0\\) represent performance similar to a KM model, negative values denote worse performance than KM and \\(1\\) is the absolute best possible score.\n\nmeasures = list(\n  msr(\"surv.graf\", ERV = TRUE),\n  msr(\"surv.cindex\", weight_meth = \"G2\", id = \"surv.cindex.uno\")\n)\n\nfor (measure in measures) {\n  print(p$score(measure, task = task, train_set = part$train))\n}\n\nsurv.graf \n0.1607549 \nsurv.cindex.uno \n      0.7120706 \n\n\n\n\n\n\n\n\nNote\n\n\n\nAll metrics use by default the median survival distribution from the 3d array, no matter what is the which.curve argument during the learner’s construction."
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html#resampling",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html#resampling",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "Resampling",
    "text": "Resampling\nPerforming resampling with the BART learner is very easy using mlr3.\nWe first stratify the data by status, so that in each resampling the proportion of censored vs un-censored patients remains the same:\n\ntask$col_roles$stratum = 'status'\ntask$strata\n\n       N                row_id\n   &lt;int&gt;                &lt;list&gt;\n1:   121       1,2,4,5,6,7,...\n2:    47  3,26,49,52,62,64,...\n\n\n\nrr = resample(task, learner, resampling = rsmp(\"cv\", folds = 5), store_backends = TRUE)\n\nINFO  [14:22:15.342] [mlr3] Applying learner 'surv.bart' on task 'lung' (iter 1/5)\nINFO  [14:22:16.982] [mlr3] Applying learner 'surv.bart' on task 'lung' (iter 2/5)\nINFO  [14:22:18.456] [mlr3] Applying learner 'surv.bart' on task 'lung' (iter 3/5)\nINFO  [14:22:19.907] [mlr3] Applying learner 'surv.bart' on task 'lung' (iter 4/5)\nINFO  [14:22:21.351] [mlr3] Applying learner 'surv.bart' on task 'lung' (iter 5/5)\n\n\nNo errors or warnings:\n\nrr$errors\n\nEmpty data.table (0 rows and 2 cols): iteration,msg\n\nrr$warnings\n\nEmpty data.table (0 rows and 2 cols): iteration,msg\n\n\nPerformance in each fold:\n\nrr$score(measures)\n\n   task_id learner_id resampling_id iteration   surv.graf surv.cindex.uno\n    &lt;char&gt;     &lt;char&gt;        &lt;char&gt;     &lt;int&gt;       &lt;num&gt;           &lt;num&gt;\n1:    lung  surv.bart            cv         1  0.06674423       0.6270652\n2:    lung  surv.bart            cv         2 -0.06994762       0.5789767\n3:    lung  surv.bart            cv         3 -0.05637324       0.5880529\n4:    lung  surv.bart            cv         4  0.02718982       0.6126352\n5:    lung  surv.bart            cv         5 -0.08941180       0.5695597\nHidden columns: task, learner, resampling, prediction_test\n\n\nMean cross-validation performance:\n\nrr$aggregate(measures)\n\n      surv.graf surv.cindex.uno \n    -0.02435972      0.59525794"
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html#uncertainty-quantification-in-survival-prediction",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html#uncertainty-quantification-in-survival-prediction",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "Uncertainty Quantification in Survival Prediction",
    "text": "Uncertainty Quantification in Survival Prediction\nWe will choose two patients from the test set and plot their survival prediction posterior estimates.\nLet’s choose the patients with the worst and the best survival time:\n\ndeath_times = p$truth[,1]\nsort(death_times)\n\n [1]  2  4  6  6  6  7  8  9 10 10 10 10 13 15 15 19 22\n\nworst_indx = which(death_times == min(death_times))[1] # died first\nbest_indx  = which(death_times == max(death_times))[1] # died last\n\npatient_ids = c(worst_indx, best_indx)\npatient_ids # which patient IDs\n\n[1] 12  9\n\ndeath_times = death_times[patient_ids]\ndeath_times # 1st is worst, 2nd is best\n\n[1]  2 22\n\n\nSubset Arrdist to only the above 2 patients:\n\narrd = p$distr[patient_ids]\narrd\n\nArrdist(2x30x50) \n\n\nWe choose time points (in months) for the survival estimates:\n\nmonths = seq(1, 36) # 1 month - 3 years\n\nWe use the $distr interface and the $survival property to get survival probabilities from an Arrdist object as well as the quantile credible intervals (CIs). The median survival probabilities can be extracted as follows:\n\nmed = arrd$survival(months) # 'med' for median\n\ncolnames(med) = paste0(patient_ids, \"_med\")\nmed = as_tibble(med) %&gt;% add_column(month = months)\nhead(med)\n\n# A tibble: 6 × 3\n  `12_med` `9_med` month\n     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n1    0.857   0.974     1\n2    0.741   0.948     2\n3    0.630   0.923     3\n4    0.524   0.900     4\n5    0.446   0.872     5\n6    0.362   0.833     6\n\n\nWe can briefly verify model’s predictions: 1st patient survival probabilities on any month are lower (worst) compared to the 2nd patient.\nNote that subsetting an Arrdist (3d array) creates a Matdist (2d matrix), for example we can explicitly get the median survival probabilities:\n\nmatd_median = arrd[, 0.5] # median\nhead(matd_median$survival(months)) # same as with `arrd`\n\n       [,1]      [,2]\n1 0.8574541 0.9736144\n2 0.7409911 0.9479014\n3 0.6304261 0.9226185\n4 0.5236389 0.8996011\n5 0.4457717 0.8721516\n6 0.3620130 0.8329688\n\n\nUsing the mean posterior survival probabilities or the ones from the last posterior draw is also possible and can be done as follows:\n\nmatd_mean = arrd[, \"mean\"] # mean (if needed)\nhead(matd_mean$survival(months))\n\n       [,1]      [,2]\n1 0.8494062 0.9721130\n2 0.7246768 0.9445261\n3 0.6213767 0.9154688\n4 0.5246258 0.8865332\n5 0.4487428 0.8576749\n6 0.3644190 0.8155328\n\nmatd_50draw = arrd[, 50] # the 50th posterior draw\nhead(matd_50draw$survival(months))\n\n       [,1]      [,2]\n1 0.7671943 0.9865953\n2 0.5885871 0.9733703\n3 0.4515607 0.9603225\n4 0.3464348 0.9474497\n5 0.2657828 0.9347494\n6 0.1812379 0.9112550\n\n\nTo get the CIs we will subset the Arrdist using a quantile number (0-1), which extracts a Matdist based on the cdf. The survival function is 1 - cdf, so low and upper bounds are reversed:\n\nlow  = arrd[, 0.975]$survival(months) # 2.5% bound\nhigh = arrd[, 0.025]$survival(months) # 97.5% bound\ncolnames(low)  = paste0(patient_ids, \"_low\")\ncolnames(high) = paste0(patient_ids, \"_high\")\nlow  = as_tibble(low)\nhigh = as_tibble(high)\n\nThe median posterior survival probabilities for the two patient of interest and the corresponding CI bounds in a tidy format are:\n\nsurv_tbl =\n  bind_cols(low, med, high) %&gt;%\n  pivot_longer(cols = !month, values_to = \"surv\",\n    names_to = c(\"patient_id\", \".value\"), names_sep = \"_\") %&gt;%\n  relocate(patient_id)\nsurv_tbl\n\n# A tibble: 72 × 5\n   patient_id month    low   med  high\n   &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 12             1 0.639  0.857 0.962\n 2 9              1 0.929  0.974 0.994\n 3 12             2 0.398  0.741 0.925\n 4 9              2 0.864  0.948 0.988\n 5 12             3 0.230  0.630 0.889\n 6 9              3 0.803  0.923 0.980\n 7 12             4 0.133  0.524 0.828\n 8 9              4 0.761  0.900 0.976\n 9 12             5 0.0773 0.446 0.770\n10 9              5 0.722  0.872 0.971\n# ℹ 62 more rows\n\n\nWe draw survival curves with the uncertainty for the survival probability quantified:\n\nmy_colors = c(\"#E41A1C\", \"#4DAF4A\")\nnames(my_colors) = patient_ids\n\nsurv_tbl %&gt;%\n  ggplot(aes(x = month, y = med)) +\n  geom_step(aes(color = patient_id), linewidth = 1) +\n  xlab('Time (Months)') +\n  ylab('Survival Probability') +\n  geom_ribbon(aes(ymin = low, ymax = high, fill = patient_id),\n    alpha = 0.3, show.legend = F) +\n  geom_vline(xintercept = death_times[1], linetype = 'dashed', color = my_colors[1]) +\n  geom_vline(xintercept = death_times[2], linetype = 'dashed', color = my_colors[2]) +\n  theme_bw(base_size = 14) +\n  scale_color_manual(values = my_colors) +\n  scale_fill_manual(values = my_colors) +\n  guides(color = guide_legend(title = \"Patient ID\"))\n\n\n\n\nUncertainty quantification for the survival prediction of two patients in the test set using 95% credible intervals. The two vertical lines correspond to the reported time of death (in months) for the two patients."
  },
  {
    "objectID": "gallery/technical/2023-10-25-bart-survival/index.html#partial-dependence-plot",
    "href": "gallery/technical/2023-10-25-bart-survival/index.html#partial-dependence-plot",
    "title": "Survival modeling in mlr3 using Bayesian Additive Regression Trees (BART)",
    "section": "Partial Dependence Plot",
    "text": "Partial Dependence Plot\nWe will use a Partial Dependence Plot (PDP) (Friedman 2001) to visualize how much different are males vs females in terms of their average survival predictions across time.\n\n\n\n\n\n\nNote\n\n\n\nPDPs assume that features are independent. In our case we need to check that sex doesn’t correlate with any of the other features used for training the BART learner. Since sex is a categorical feature, we fit a linear model using as target variable every other feature in the data (\\(lm(feature \\sim sex)\\)) and conduct an ANOVA (ANalysis Of VAriance) to get the variance explained or \\(R^2\\). The square root of that value is the correlation measure we want.\n\n\n\n# code from https://christophm.github.io/interpretable-ml-book/ale.html\nmycor = function(cnames, data) {\n  x.num = data[, cnames[1], with = FALSE][[1]]\n  x.cat = data[, cnames[2], with = FALSE][[1]]\n  # R^2 = Cor(X, Y)^2 in simple linear regression\n  sqrt(summary(lm(x.num ~ x.cat))$r.squared)\n}\n\ncnames = c(\"sex\")\ncombs = expand.grid(y = setdiff(colnames(d), \"sex\"), x = cnames)\ncombs$cor = apply(combs, 1, mycor, data = task$data()) # use the train set\ncombs\n\n          y   x        cor\n1      time sex 0.11338256\n2    status sex 0.22105006\n3       age sex 0.12814230\n4  meal.cal sex 0.16813658\n5 pat.karno sex 0.07075345\n6   ph.ecog sex 0.01197613\n7  ph.karno sex 0.01512325\n8   wt.loss sex 0.17288403\n\n\nsex doesn’t correlate strongly with any other feature, so we can compute the PDP:\n\n# create two datasets: one with males and one with females\n# all other features remain the same (use train data, 205 patients)\nd = task$data(rows = part$train) # `rows = part$test` to use the test set\n\nd$sex = 1\ntask_males = as_task_surv(d, time = 'time', event = 'status', id = 'lung-males')\nd$sex = 0\ntask_females = as_task_surv(d, time = 'time', event = 'status', id = 'lung-females')\n\n# make predictions\np_males   = learner$predict(task_males)\np_females = learner$predict(task_females)\n\n# take the median posterior survival probability\nsurv_males   = p_males$distr$survival(months) # patients x times\nsurv_females = p_females$distr$survival(months) # patients x times\n\n# tidy up data: average and quantiles across patients\ndata_males =\n  apply(surv_males, 1, function(row) {\n    tibble(\n      low = quantile(row, probs = 0.025),\n      avg = mean(row),\n      high = quantile(row, probs = 0.975)\n    )\n  }) %&gt;%\n  bind_rows() %&gt;%\n  add_column(sex = 'male', month = months, .before = 1)\n\ndata_females =\n  apply(surv_females, 1, function(row) {\n    tibble(\n      low = quantile(row, probs = 0.025),\n      avg = mean(row),\n      high = quantile(row, probs = 0.975)\n    )\n  }) %&gt;%\n  bind_rows() %&gt;%\n  add_column(sex = 'female', month = months, .before = 1)\n\npdp_tbl = bind_rows(data_males, data_females)\npdp_tbl\n\n# A tibble: 72 × 5\n   sex   month    low   avg  high\n   &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 male      1 0.809  0.944 0.979\n 2 male      2 0.652  0.892 0.958\n 3 male      3 0.525  0.842 0.934\n 4 male      4 0.411  0.792 0.911\n 5 male      5 0.332  0.748 0.889\n 6 male      6 0.240  0.686 0.851\n 7 male      7 0.168  0.635 0.816\n 8 male      8 0.120  0.579 0.775\n 9 male      9 0.0777 0.523 0.728\n10 male     10 0.0504 0.467 0.687\n# ℹ 62 more rows\n\n\n\nmy_colors = c(\"#E41A1C\", \"#4DAF4A\")\nnames(my_colors) = c('male', 'female')\n\npdp_tbl %&gt;%\n  ggplot(aes(x = month, y = avg)) +\n  geom_step(aes(color = sex), linewidth = 1) +\n  xlab('Time (Months)') +\n  ylab('Survival Probability') +\n  geom_ribbon(aes(ymin = low, ymax = high, fill = sex), alpha = 0.2, show.legend = F) +\n  theme_bw(base_size = 14) +\n  scale_color_manual(values = my_colors) +\n  scale_fill_manual(values = my_colors)\n\n\n\n\nFriedman’s partial dependence function with 95% prediction intervals: males vs females. Females show on average larger survival estimates compared to men, across all time points. Overlapping shaded area represents men and women that have similar survival characteristics."
  },
  {
    "objectID": "gallery/technical/2023-02-27-land-cover-classification/index.html",
    "href": "gallery/technical/2023-02-27-land-cover-classification/index.html",
    "title": "Spatial Data in the mlr3 Ecosystem",
    "section": "",
    "text": "Scope\nWorking with spatial data in R requires a lot of data wrangling e.g. reading from different file formats, converting between spatial formats, creating tables from point layers, and predicting spatial raster images. The goal of mlr3spatial is to simplify these workflows within the mlr3 ecosystem. As a practical example, we will perform a land cover classification for the city of Leipzig, Germany. Figure 1 illustrates the typical workflow for this type of task: Load the training data, create a spatial task, train a learner with it, and predict the final raster image.\n\n\n\n\n\n\n%%{ init: { 'flowchart': { 'curve': 'bump' } } }%%\n\nflowchart LR\n    subgraph files[Files]\n    vector[Vector]\n    raster[Raster]\n    end\n    subgraph load[Load Data]\n    sf\n    terra\n    end\n    vector --&gt; sf\n    raster --&gt; terra\n    subgraph train_model[Train Model]\n    task[Task]\n    learner[Learner]\n    end\n    terra --&gt; prediction_raster\n    task --&gt; learner\n    sf --&gt; task\n    subgraph predict[Spatial Prediction]\n    prediction_raster[Raster Image]\n    end\n    learner --&gt; prediction_raster\n\n\n\n\nFigure 1: Spatial prediction workflow in mlr3spatial.\n\n\n\n\n\nWe assume that you are familiar with the mlr3 ecosystem and know the basic concepts of remote sensing. If not, we recommend reading the mlr3book first. If you are interested in spatial resampling, check out the book chapter on spatial analysis.\n\n\nLand Cover Classification\nLand cover is the physical material or vegetation that covers the surface of the earth, including both natural and human-made features. Understanding land cover patterns and changes over time is critical for addressing global environmental challenges, such as climate change, land degradation, and loss of biodiversity. Land cover classification is the process of assigning land cover classes to pixels in a raster image. With mlr3spatial, we can easily perform a land cover classification within the mlr3 ecosystem.\nBefore we can start the land cover classification, we need to load the necessary packages. The mlr3spatial package relies on terra for processing raster data and sf for vector data. These widely used packages read all common raster and vector formats. Additionally, the stars and raster packages are supported.\n\nlibrary(mlr3)\nlibrary(mlr3spatial)\nlibrary(terra, exclude = \"resample\")\nlibrary(sf)\n\nWe will work with a Sentinel-2 scene of the city of Leipzig which consists of 7 bands with a 10 or 20m spatial resolution and an NDVI band. The data is included in the mlr3spatial package. We use the terra::rast() to load the TIFF raster file.\n\nleipzig_raster = rast(system.file(\"extdata\", \"leipzig_raster.tif\", package = \"mlr3spatial\"))\nleipzig_raster\n\nclass       : SpatRaster \ndimensions  : 206, 154, 8  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : 731810, 733350, 5692030, 5694090  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 32N (EPSG:32632) \nsource      : leipzig_raster.tif \nnames       :  b02,  b03,  b04,  b06,  b07,  b08, ... \nmin values  :  846,  645,  366,  375,  401,  374, ... \nmax values  : 4705, 4880, 5451, 4330, 5162, 5749, ... \n\n\nThe training data is a GeoPackage point layer with land cover labels and spectral features. We load the file and create a simple feature point layer.\n\nleipzig_vector = read_sf(system.file(\"extdata\", \"leipzig_points.gpkg\", package = \"mlr3spatial\"), stringsAsFactors = TRUE)\nleipzig_vector\n\nSimple feature collection with 97 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 731930.5 ymin: 5692136 xmax: 733220.3 ymax: 5693968\nProjected CRS: WGS 84 / UTM zone 32N\n# A tibble: 97 × 10\n     b02   b03   b04   b06   b07   b08   b11    ndvi land_cover               geom\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;             &lt;POINT [m]&gt;\n 1   903   772   426  2998  4240  4029  1816  0.809  forest     (732480.1 5693957)\n 2  1270  1256  1081  1998  2493  2957  2073  0.465  urban      (732217.4 5692769)\n 3  1033   996   777  2117  2748  2799  1595  0.565  urban      (732737.2 5692469)\n 4   962   773   500   465   505   396   153 -0.116  water      (733169.3 5692777)\n 5  1576  1527  1626  1715  1745  1768  1980  0.0418 urban      (732202.2 5692644)\n 6  1125  1185   920  3058  3818  3758  2682  0.607  pasture      (732153 5693059)\n 7   880   746   424  2502  3500  3397  1469  0.778  forest     (731937.9 5693722)\n 8  1332  1251  1385  1663  1799  1640  1910  0.0843 urban      (732416.2 5692324)\n 9   940   741   475   452   515   400   139 -0.0857 water      (732933.7 5693344)\n10   902   802   454  2764  3821  3666  1567  0.780  forest     (732411.3 5693352)\n# ℹ 87 more rows\n\n\nWe plot both layers to get an overview of the data. The training points are located in the districts of Lindenau and Zentrum West.\n\n\nCode\nlibrary(ggplot2)\nlibrary(tidyterra, exclude = \"filter\")\n\nggplot() +\n  geom_spatraster_rgb(data = leipzig_raster, r = 3, g = 2, b = 1, max_col_value = 5451) +\n  geom_spatvector(data = leipzig_vector, aes(color = land_cover)) +\n  scale_color_viridis_d(name = \"Land cover\", labels = c(\"Forest\", \"Pastures\", \"Urban\", \"Water\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe as_task_classif_st() function directly creates a spatial task from the point layer. This makes it unnecessary to transform the point layer to a data.frame with coordinates. Spatial tasks additionally store the coordinates of the training points. The coordinates are useful when estimating the performance of the model with spatial resampling.\n\ntask = as_task_classif_st(leipzig_vector, target = \"land_cover\")\ntask\n\n&lt;TaskClassifST:leipzig_vector&gt; (97 x 9)\n* Target: land_cover\n* Properties: multiclass\n* Features (8):\n  - dbl (8): b02, b03, b04, b06, b07, b08, b11, ndvi\n* Coordinates:\n           X       Y\n       &lt;num&gt;   &lt;num&gt;\n 1: 732480.1 5693957\n 2: 732217.4 5692769\n 3: 732737.2 5692469\n 4: 733169.3 5692777\n 5: 732202.2 5692644\n---                 \n93: 733018.7 5692342\n94: 732551.4 5692887\n95: 732520.4 5692589\n96: 732542.2 5692204\n97: 732437.8 5692300\n\n\nNow we can train a model with the task. We use a simple decision tree learner from the rpart package. The \"classif_st\" task is a specialization of the \"classif\" task and therefore works with all \"classif\" learners.\n\nlearner = lrn(\"classif.rpart\")\nlearner$train(task)\n\nTo get a complete land cover classification of Leipzig, we have to predict on each pixel and return a raster image with these predictions. The $predict() method of the learner only works for tabular data. To predict a raster image, we use the predict_spatial() function.\n\n# predict land cover map\nland_cover = predict_spatial(leipzig_raster, learner)\n\nWarning in warn_deprecated(\"DataBackend$data_formats\"): DataBackend$data_formats is deprecated and will be removed in\nthe future.\n\n\n\n\nCode\nggplot() +\n  geom_spatraster(data = land_cover) +\n  scale_fill_viridis_d(name = \"Land cover\", labels = c(\"Forest\", \"Pastures\", \"Urban\", \"Water\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nWorking with spatial data in R is very easy with the mlr3spatial package. You can quickly train a model with a point layer and predict a raster image. The mlr3spatial package is still in development and we are looking forward to your feedback and contributions.\n\n\nSession Information\n\nsessioninfo::session_info(info = \"packages\")\n\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package        * version    date (UTC) lib source\n   backports        1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   checkmate        2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class            7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   classInt         0.4-10     2023-09-05 [1] CRAN (R 4.4.1)\n   cli              3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n P codetools        0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace       2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon           1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table     * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DBI              1.2.3      2024-06-02 [1] CRAN (R 4.4.1)\n   digest           0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   dplyr            1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   e1071            1.7-14     2023-12-06 [1] CRAN (R 4.4.1)\n   evaluate         0.24.0     2024-06-10 [1] CRAN (R 4.4.1)\n   fansi            1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   farver           2.1.2      2024-05-13 [1] CRAN (R 4.4.1)\n   fastmap          1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   future           1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   generics         0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2        * 3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals          0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue             1.7.0      2024-01-09 [1] CRAN (R 4.4.1)\n   gtable           0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools        0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets      1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite         1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n P KernSmooth       2.23-24    2024-05-17 [?] CRAN (R 4.4.0)\n   knitr            1.48       2024-07-07 [1] CRAN (R 4.4.1)\n   lgr              0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle        1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv          0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr         2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n   mlr3           * 0.21.0     2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3misc         0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3spatial    * 0.5.0      2024-03-09 [1] CRAN (R 4.4.1)\n   mlr3website    * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   munsell          0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n   palmerpenguins   0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox          1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly       1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar           1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig        2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   proxy            0.4-27     2022-06-09 [1] CRAN (R 4.4.1)\n   purrr            1.0.2      2023-08-10 [1] CRAN (R 4.4.1)\n   R6               2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp             1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   renv             1.0.3      2023-09-19 [1] CRAN (R 4.4.1)\n   rlang            1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown        2.28       2024-08-17 [1] CRAN (R 4.4.1)\n P rpart            4.1.23     2023-12-05 [?] CRAN (R 4.4.0)\n   scales           1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo      1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   sf             * 1.0-18     2024-10-11 [1] CRAN (R 4.4.1)\n   stringi          1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   terra          * 1.7-83     2024-10-14 [1] CRAN (R 4.4.1)\n   tibble           3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyr            1.3.1      2024-01-24 [1] CRAN (R 4.4.1)\n   tidyselect       1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   tidyterra      * 0.6.1      2024-06-08 [1] CRAN (R 4.4.1)\n   units            0.8-5      2023-11-28 [1] CRAN (R 4.4.1)\n   utf8             1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid             1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs            0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   viridisLite      0.4.2      2023-05-02 [1] CRAN (R 4.4.1)\n   withr            3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun             0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml             2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "gallery/basic/2020-02-25-remove-correlated-features/index.html",
    "href": "gallery/basic/2020-02-25-remove-correlated-features/index.html",
    "title": "Select Uncorrelated Features",
    "section": "",
    "text": "The following example describes a situation where we aim to remove correlated features. This in essence means, that we drop features until no features have a correlation higher than a given cutoff. This is often useful when we for example want to use linear models."
  },
  {
    "objectID": "gallery/basic/2020-02-25-remove-correlated-features/index.html#prerequisites",
    "href": "gallery/basic/2020-02-25-remove-correlated-features/index.html#prerequisites",
    "title": "Select Uncorrelated Features",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis tutorial assumes familiarity with the basics of mlr3pipelines. Consult the mlr3book if some aspects are not fully understandable. Additionally, we compare different cutoff values via tuning using the mlr3tuning package. Again, the mlr3book has an intro to mlr3tuning and paradox.\nThe example describes a very involved use-case, where the behavior of PipeOpSelect is manipulated via a trafo on it’s ParamSet"
  },
  {
    "objectID": "gallery/basic/2020-02-25-remove-correlated-features/index.html#getting-started",
    "href": "gallery/basic/2020-02-25-remove-correlated-features/index.html#getting-started",
    "title": "Select Uncorrelated Features",
    "section": "Getting started",
    "text": "Getting started\nWe load the mlr3verse package which pulls in the most important packages for this example.\n\nlibrary(mlr3verse)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nThe basic pipeline looks as follows: We use PipeOpSelect to select a set of variables followed by a rpart learner.\n\ngraph_learner = po(\"select\") %&gt;&gt;% lrn(\"classif.rpart\")\n\nNow we get to the magic:\nWe want to use the function caret::findCorrelation() from the caret package in order to select uncorrelated variables. This function has a cutoff parameter, that specifies the maximum correlation allowed between variables. In order to expose this variable as a numeric parameter we can tune over we specify the following ParamSet:\n\nsearch_space = ps(cutoff = p_dbl(0, 1))\n\nWe define a function select_cutoff that takes as input a Task and returns a list of features we aim to keep.\nNow we use a trafo to transform the cutoff into a set of variables, which is what PipeOpSelect can work with. Note that we use x$cutoff = NULL in order to remove the temporary parameter we introduced, as PipeOpSelect does not know what to do with it.\n\nsearch_space$trafo = function(x, param_set) {\n  cutoff = x$cutoff\n  x$select.selector = function(task) {\n    fn = task$feature_names\n    data = task$data(cols = fn)\n    drop = caret::findCorrelation(cor(data), cutoff = cutoff, exact = TRUE, names = TRUE)\n    setdiff(fn, drop)\n  }\n  x$cutoff = NULL\n  x\n}\n\nIf you are not sure, you understand the trafo concept, consult the mlr3book. It has a section on the trafo concept.\nNow we tune over different values for cutoff.\n\ninstance = tune(\n  tuner = tnr(\"grid_search\"),\n  task = tsk(\"iris\"),\n  learner = graph_learner,\n  resampling = rsmp(\"cv\", folds = 3L),\n  measure = msr(\"classif.ce\"),\n  search_space = search_space,\n  # don't need the following line for optimization, this is for\n  # demonstration that different features were selected\n  store_models = TRUE)\n\nIn order to demonstrate that different cutoff values result in different features being selected, we can run the following to inspect the trained models. Note this inspects only the trained models of the first CV fold of each evaluated model. The features being excluded depends on the training data seen by the pipeline and may be different in different folds, even at the same cutoff value.\n\nas.data.table(instance$archive)[\n  order(cutoff),\n  list(cutoff, classif.ce,\n    featurenames = lapply(resample_result, function(x) {\n      x$learners[[1]]$model$classif.rpart$train_task$feature_names\n    }\n  ))]\n\n       cutoff classif.ce                                      featurenames\n 1: 0.0000000 0.28666667                                      Sepal.Length\n 2: 0.1111111 0.28666667                                      Sepal.Length\n 3: 0.2222222 0.28666667                                      Sepal.Length\n 4: 0.3333333 0.27333333                          Sepal.Length,Sepal.Width\n 5: 0.4444444 0.27333333                          Sepal.Length,Sepal.Width\n 6: 0.5555556 0.27333333                          Sepal.Length,Sepal.Width\n 7: 0.6666667 0.27333333                          Sepal.Length,Sepal.Width\n 8: 0.7777778 0.27333333                          Sepal.Length,Sepal.Width\n 9: 0.8888889 0.04000000              Petal.Width,Sepal.Length,Sepal.Width\n10: 1.0000000 0.06666667 Petal.Length,Petal.Width,Sepal.Length,Sepal.Width\n\n\nVoila, we created our own PipeOp, that uses very advanced knowledge of mlr3pipelines and paradox in only few lines of code."
  },
  {
    "objectID": "gallery/basic/2020-05-04-moneyball/index.html",
    "href": "gallery/basic/2020-05-04-moneyball/index.html",
    "title": "mlr3 and OpenML - Moneyball Use Case",
    "section": "",
    "text": "This use case shows how to easily work with datasets available via OpenML into an mlr3 workflow.\nThe following operations are illustrated:"
  },
  {
    "objectID": "gallery/basic/2020-05-04-moneyball/index.html#prerequisites",
    "href": "gallery/basic/2020-05-04-moneyball/index.html#prerequisites",
    "title": "mlr3 and OpenML - Moneyball Use Case",
    "section": "Prerequisites",
    "text": "Prerequisites\n\n# various functions of the mlr3 ecosystem\nlibrary(\"mlr3verse\")\n# about a dozen reasonable learners\nlibrary(\"mlr3learners\")\n# retrieving the data\nlibrary(\"OpenML\")\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/basic/2020-05-04-moneyball/index.html#retrieving-the-data-from-openml",
    "href": "gallery/basic/2020-05-04-moneyball/index.html#retrieving-the-data-from-openml",
    "title": "mlr3 and OpenML - Moneyball Use Case",
    "section": "Retrieving the data from OpenML",
    "text": "Retrieving the data from OpenML\nWe can use the OpenML package to retrieve data (and more) straight away. OpenML is is an inclusive movement to build an open, organized, online ecosystem for machine learning. Typically, you can retrieve the data with an data.id. The id can be found on OpenML. We choose the data set 41021. The related web page can be accessed here. This data set was uploaded by Joaquin Vanschoren.\n\noml_data = getOMLDataSet(data.id = 41021)\n\nDownloading from 'http://www.openml.org/api/v1/data/41021' to '/tmp/RtmpIRWjZy/cache/datasets/41021/description.xml'.\n\n\nDownloading from 'https://api.openml.org/data/v1/download/18626236/Moneyball.arff' to '/tmp/RtmpIRWjZy/cache/datasets/41021/dataset.arff'\n\n\nLoading required package: readr\n\n\nThe description indicates that the data set is associated with baseball or more precisely the story of Moneyball.\n\nhead(as.data.table(oml_data))\n\n\n\n\n\n\n\nHowever, the description within the OpenML object is not very detailed. The previously referenced page however states the following:\nIn the early 2000s, Billy Beane and Paul DePodesta worked for the Oakland Athletics. During their work there, they disrupted the game of baseball. They didn’t do it using a bat or glove, and they certainly didn’t do it by throwing money at the issue; in fact, money was the issue. They didn’t have enough of it, but they were still expected to keep up with teams that had more substantial endorsements. This is where Statistics came riding down the hillside on a white horse to save the day. This data set contains some of the information that was available to Beane and DePodesta in the early 2000s, and it can be used to better understand their methods.\nThis data set contains a set of variables that Beane and DePodesta emphasized in their work. They determined that statistics like on-base percentage (obp) and slugging percentage (slg) were very important when it came to scoring runs, however, they were largely undervalued by most scouts at the time. This translated to a gold mine for Beane and DePodesta. Since these players weren’t being looked at by other teams, they could recruit these players on a small budget. The variables are as follows:\n\nteam\nleague\nyear\nruns scored (rs)\nruns allowed (ra)\nwins (w)\non-base percentage (obp)\nslugging percentage (slg)\nbatting average (ba)\nplayoffs (binary)\nrankseason\nrankplayoffs\ngames played (g)\nopponent on-base percentage (oobp)\nopponent slugging percentage (oslg)\n\nWhile Beane and DePodesta defined most of these statistics and measures for individual players, this data set is on the team level.\nThese statistics seem very informative if you are into baseball. If baseball of rather obscure to you, simply take these features as given or give this article a quick read.\nFinally, note that the moneyball dataset is also included in the mlr3data package where you can get the preprocessed (integers properly encoded as such, etc.) data via:\n\ndata(\"moneyball\", package = \"mlr3data\")\nskimr::skim(moneyball)\n\n\nData summary\n\n\nName\nmoneyball\n\n\nNumber of rows\n1232\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n6\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nteam\n0\n1.0\nFALSE\n39\nBAL: 47, BOS: 47, CHC: 47, CHW: 47\n\n\nleague\n0\n1.0\nFALSE\n2\nAL: 616, NL: 616\n\n\nplayoffs\n0\n1.0\nFALSE\n2\n0: 988, 1: 244\n\n\nrankseason\n988\n0.2\nFALSE\n8\n2: 53, 1: 52, 3: 44, 4: 44\n\n\nrankplayoffs\n988\n0.2\nFALSE\n5\n3: 80, 4: 68, 1: 47, 2: 47\n\n\ng\n0\n1.0\nFALSE\n8\n162: 954, 161: 139, 163: 93, 160: 23\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n1988.96\n14.82\n1962.00\n1976.75\n1989.00\n2002.00\n2012.00\n▆▆▇▆▇\n\n\nrs\n0\n1.00\n715.08\n91.53\n463.00\n652.00\n711.00\n775.00\n1009.00\n▁▆▇▃▁\n\n\nra\n0\n1.00\n715.08\n93.08\n472.00\n649.75\n709.00\n774.25\n1103.00\n▂▇▆▂▁\n\n\nw\n0\n1.00\n80.90\n11.46\n40.00\n73.00\n81.00\n89.00\n116.00\n▁▃▇▆▁\n\n\nobp\n0\n1.00\n0.33\n0.02\n0.28\n0.32\n0.33\n0.34\n0.37\n▁▃▇▅▁\n\n\nslg\n0\n1.00\n0.40\n0.03\n0.30\n0.38\n0.40\n0.42\n0.49\n▁▅▇▅▁\n\n\nba\n0\n1.00\n0.26\n0.01\n0.21\n0.25\n0.26\n0.27\n0.29\n▁▂▇▆▂\n\n\noobp\n812\n0.34\n0.33\n0.02\n0.29\n0.32\n0.33\n0.34\n0.38\n▂▇▇▃▁\n\n\noslg\n812\n0.34\n0.42\n0.03\n0.35\n0.40\n0.42\n0.44\n0.50\n▁▆▇▅▁\n\n\n\n\n\nThe summary shows how this data we are dealing with looks like: Some data is missing, however, this has structural reasons. There are 39 teams with each maximally 47 years (1962 - 2012). For 988 cases the information on rankseason and rankplayoffs is missing. This is since these simply did not reach the playoffs and hence have no reported rank.\n\nsummary(moneyball[moneyball$playoffs == 0, c(\"rankseason\", \"rankplayoffs\")])\n\n   rankseason  rankplayoffs\n 1      :  0   1   :  0    \n 2      :  0   2   :  0    \n 3      :  0   3   :  0    \n 4      :  0   4   :  0    \n 5      :  0   5   :  0    \n (Other):  0   NA's:988    \n NA's   :988               \n\n\nOn the other hand, oobp and oslg have \\(812\\) missing values. It seems as if these measures were not available before \\(1998\\).\n\nlibrary(ggplot2)\nlibrary(naniar)\n\nggplot(moneyball, aes(x = year, y = oobp)) +\n  geom_miss_point()\n\n\n\n\n\n\n\n\nWe seem to have a missing data problem. Typically, in this case, we have three options: They are:\n\nComplete case analysis: Exclude all observation with missing values.\nComplete feature analysis: Exclude all features with missing values.\nMissing value imputation: Use a model to “guess” the missing values (based on the underlying distribution of the data.\n\nUsually, missing value imputation is preferred over the first two. However, in machine learning, one can try out all options and see which performs best for the underlying problem. For now, we limit ourselves to a rather simple imputation technique, imputation by randomly sampling from the univariate distribution. Note that this does not take the multivariate distribution into account properly and that there are more elaborate approaches. We only aim to impute oobp and oslg. For the other missing (categorical) features, we simply add a new level which indicates that information is missing (i.e. all missing values belong to).\nIt is important to note that in this case here the vast majority of information on the features is missing. In this case, imputation is performed to not throw away the existing information of the features.\nmlr3 has some solutions for that within the mlr3pipelines package. We start with an easy PipeOp which only performs numeric imputation.\n\nimp_num = po(\"imputehist\", affect_columns = selector_type(c(\"integer\", \"numeric\")))\n\nNext, we append the second imputation job for factors.\n\nimp_fct = po(\"imputeoor\", affect_columns = selector_type(\"factor\"))\ngraph = imp_num %&gt;&gt;% imp_fct\ngraph$plot(html = FALSE)"
  },
  {
    "objectID": "gallery/basic/2020-05-04-moneyball/index.html#creating-tasks-and-learners",
    "href": "gallery/basic/2020-05-04-moneyball/index.html#creating-tasks-and-learners",
    "title": "mlr3 and OpenML - Moneyball Use Case",
    "section": "Creating tasks and learners",
    "text": "Creating tasks and learners\nThe fact that there is missing data does not affect the task definition. The task determines what is the problem to be solved by machine learning. We want to explain the runs scored (rs). rs is an important measure as a run is equivalent to a ‘point’ scored in other sports. Naturally, the aim of a coach should be to maximise runs scored and minimise runs allowed. As runs scored and runs allowed are both legitimate targets we ignore the runs allowed here. The task is defined by:\n\n# creates a `mlr3` task from scratch, from a data.frame\n# 'target' names the column in the dataset we want to learn to predict\ntask = as_task_regr(moneyball, target = \"rs\")\ntask$missings()\n\n\n\n\n\n\n\nThe $missings() method indicates what we already knew: our missing values. Missing values are not always a problem. Some learners can deal with them pretty well. However, we want to use a random forest for our task.\n\n# creates a learner\ntest_learner = lrn(\"regr.ranger\")\n\n# displays the properties\ntest_learner$properties\n\n[1] \"hotstart_backward\" \"importance\"        \"oob_error\"         \"weights\"          \n\n\nTypically, in mlr3 the $properties field would tell us whether missing values are a problem to this learner or not. As it is not listed here, the random forest cannot deal with missing values.\nAs we aim to use imputation beforehand, we incorporate it into the learner. Our selected learner is going to be a random forest from the ranger package.\nOne can allow the embedding of the preprocessing (imputation) into a learner by creating a PipeOpLearner. This special Learner can be put into a graph together with the imputer.\n\n# convert learner to pipeop learner and set hyperparameter\npipeop_learner = po(lrn(\"regr.ranger\"), num.trees = 1000, importance = \"permutation\")\n\n# add pipeop learner to graph and create graph learner\ngraph_learner = as_learner(graph %&gt;&gt;% pipeop_learner)\n\nThe final graph looks like the following:\n\ngraph_learner$graph$plot(html = FALSE)"
  },
  {
    "objectID": "gallery/basic/2020-05-04-moneyball/index.html#train-and-predict",
    "href": "gallery/basic/2020-05-04-moneyball/index.html#train-and-predict",
    "title": "mlr3 and OpenML - Moneyball Use Case",
    "section": "Train and predict",
    "text": "Train and predict\nTo get a feeling of how our model performs we simply train the Learner on a subset of the data and predict the hold-out data.\n\n# defines the training and testing data; 95% is used for training\ntrain_set = sample(task$nrow, 0.95 * task$nrow)\ntest_set = setdiff(seq_len(task$nrow), train_set)\n\n# train learner on subset of task\ngraph_learner$train(task, row_ids = train_set)\n\n# predict using held out observations\nprediction = graph_learner$predict(task, row_ids = test_set)\n\nhead(as.data.table(prediction))\n\n\n\n\n\n\n\nViewing the predicted values it seems like the model predicts reasonable values that are fairly close to the truth."
  },
  {
    "objectID": "gallery/basic/2020-05-04-moneyball/index.html#evaluation-resampling",
    "href": "gallery/basic/2020-05-04-moneyball/index.html#evaluation-resampling",
    "title": "mlr3 and OpenML - Moneyball Use Case",
    "section": "Evaluation & Resampling",
    "text": "Evaluation & Resampling\nWhile the prediction indicated that the model is doing what it is supposed to, we want to have a more systematic understanding of the model performance. That means we want to know by how much our model is away from the truth on average. Cross-validation investigates this question. In mlr3 10-fold cross-validation is constructed as follows:\n\ncv10 = rsmp(\"cv\", folds = 10)\nrr = resample(task, graph_learner, cv10)\n\nWe choose some of the performance measures provided by:\n\nas.data.table(mlr_measures)\n\n\n\n\n\n\n\nWe choose the mean absolute error and the mean squared error.\n\nrr$score(msrs(c(\"regr.mae\", \"regr.mse\")))\n\n\n\n\n\n\n\nWe can also compute now by how much our model was on average wrong when predicting the runs scored.\n\nrr$aggregate(msr(\"regr.mae\"))\n\nregr.mae \n19.39997 \n\n\nThat seems not too bad. Considering that on average approximately 715 runs per team per season have been scored.\n\nmean(moneyball$rs)\n\n[1] 715.082"
  },
  {
    "objectID": "gallery/basic/2020-05-04-moneyball/index.html#performance-boost-of-imputation",
    "href": "gallery/basic/2020-05-04-moneyball/index.html#performance-boost-of-imputation",
    "title": "mlr3 and OpenML - Moneyball Use Case",
    "section": "Performance boost of imputation",
    "text": "Performance boost of imputation\nTo assess if imputation was beneficial, we can compare our current learner with a learner which ignores the missing features. Normally, one would set up a benchmark for this. However, we want to keep things short in this use case. Thus, we only set up the alternative learner (with identical hyperparameters) and compare the 10-fold cross-validated mean absolute error.\nAs we are mostly interested in the numeric imputation we leave the remaining graph as it is.\n\nimpute_oor = po(\"imputeoor\", affect_columns = selector_type(\"factor\"))\n\nSubsequently, we create a pipeline with PipeOpSelect.\n\nfeature_names = colnames(moneyball)[!sapply(moneyball, anyNA)]\nfeature_names = c(feature_names[feature_names %in% task$feature_names],\n  \"rankseason\", \"rankplayoffs\")\nselect_na = po(\"select\", selector = selector_name(feature_names))\n\ngraph_2 = impute_oor %&gt;&gt;% select_na\ngraph_2$plot(html = FALSE)\n\n\n\n\n\n\n\n\nNow we complete the learner and apply resampling as before.\n\ngraph_learner_2 = as_learner(graph_2 %&gt;&gt;% pipeop_learner)\nrr_2 = resample(task, graph_learner_2, cv10)\nrr_2$aggregate(msr(\"regr.mae\"))\n\nregr.mae \n19.03752 \n\n\nSurprisingly, the performance seems to be approximately the same. That means that the imputed features seem not very helpful. We can use the variable.importance of the random forest.\n\nsort(graph_learner$model$regr.ranger$model$variable.importance, decreasing = TRUE)\n\n\n\n\n\n\n\nWe see that according to this the left out oobp and oslg seem to have solely rudimentary explanatory power. This may be because there were simply too many instances or because the features are themselves not very powerful."
  },
  {
    "objectID": "gallery/basic/2020-05-04-moneyball/index.html#conclusion",
    "href": "gallery/basic/2020-05-04-moneyball/index.html#conclusion",
    "title": "mlr3 and OpenML - Moneyball Use Case",
    "section": "Conclusion",
    "text": "Conclusion\nSo, to sum up, what we have learned: We can access very cool data straight away with the OpenML package. (We are working on a better direct implementation into mlr3 at the moment.) We can work with missing data very well in mlr3. Nevertheless, we discovered that sometimes imputation does not lead to the intended goals. We also learned how to use some PipeOps from the mlr3pipelines package.\nBut most importantly, we found a way to predict the runs scored of MLB teams.\nIf you want to know more, read the mlr3book and the documentation of the mentioned packages."
  },
  {
    "objectID": "gallery/basic/2020-03-30-stratification-blocking/index.html",
    "href": "gallery/basic/2020-03-30-stratification-blocking/index.html",
    "title": "Resampling - Stratified, Blocked and Predefined",
    "section": "",
    "text": "Intro\nWhen evaluating machine learning algorithms through resampling, it is preferable that each train/test partition will be a representative subset of the whole data set. This post covers three ways to achieve such reliable resampling procedures:\n\nStratified resampling for classification problems where each train/test split maintains the target class distribution of the original data set.\nBlock resampling where a grouping factor determines which observations should be together in train/test splits.\nCustom resampling using predefined and manually created folds for the train/test splits.\n\n\n\nPrerequisites\nWe load the most important packages for this post.\n\nlibrary(mlr3verse)\nlibrary(mlbench)\nlibrary(data.table)\n\nWe initialize the random number generator with a fixed seed for reproducibility.\n\nset.seed(7832)\n\n\n\nStratified resampling\nIn classification tasks, the ratio of the target class distribution should be similar in each train/test split, which is achieved by stratification. This is particularly useful in the case of imbalanced classes and small data sets.\nStratification can also be performed with respect to explanatory categorical variables to ensure that all subgroups are represented in all training and test sets.\nIn mlr3, each Task has a slot $col_roles. This slot shows general roles certain features will have throughout different stages of the machine learning process. At least, the $col_roles slot shows which variables will be used as features and as the target. However, the $col_roles slot can be more diverse and some variables might even serve multiple roles. We can specify the variable used for stratification in task$col_roles$stratum. This will be illustrated in the following example using the german_credit data:\n\ntask_gc = tsk(\"german_credit\")\ntask_gc$col_roles\n\n$feature\n [1] \"age\"                     \"amount\"                  \"credit_history\"          \"duration\"               \n [5] \"employment_duration\"     \"foreign_worker\"          \"housing\"                 \"installment_rate\"       \n [9] \"job\"                     \"number_credits\"          \"other_debtors\"           \"other_installment_plans\"\n[13] \"people_liable\"           \"personal_status_sex\"     \"present_residence\"       \"property\"               \n[17] \"purpose\"                 \"savings\"                 \"status\"                  \"telephone\"              \n\n$target\n[1] \"credit_risk\"\n\n$name\ncharacter(0)\n\n$order\ncharacter(0)\n\n$stratum\ncharacter(0)\n\n$group\ncharacter(0)\n\n$weight\ncharacter(0)\n\n\nWe use the target feature called credit_risk to specify stratification with respect to the target variable:\n\ntask_gc$col_roles$stratum = \"credit_risk\"\n# alternatively task_gc$col_roles$stratum = task_gc$col_roles$target\n\nAfter the specification of task$col_roles$stratum, the active binding task$strata will show the number of observations in each group and the corresponding row id’s:\n\ntask_gc$strata\n\n     N                row_id\n1: 700       1,3,4,6,7,8,...\n2: 300  2, 5,10,11,12,14,...\n\n\nSpecify 3-fold cross validation and instantiate the resampling on the task:\n\ncv3 = rsmp(\"cv\", folds = 3)\ncv3$instantiate(task_gc)\ncv3$instance\n\n      row_id fold\n   1:      7    1\n   2:      8    1\n   3:      9    1\n   4:     17    1\n   5:     22    1\n  ---            \n 996:    959    3\n 997:    967    3\n 998:    980    3\n 999:    984    3\n1000:    999    3\n\n\nCheck if the target class distribution is similar in each fold:\n\ndt = merge(cv3$instance, task_gc$data()[, row_id := .I], by = \"row_id\")\ndt[, .(class_ratio = sum(credit_risk == \"bad\") /\n  sum(credit_risk == \"good\")), by = fold]\n\n   fold class_ratio\n1:    2   0.4291845\n2:    3   0.4291845\n3:    1   0.4273504\n\n\nAnd compare it with the target class distribution from the whole data set:\n\ndt[, .(class_ratio = sum(credit_risk == \"bad\") / sum(credit_risk == \"good\"))]\n\n   class_ratio\n1:   0.4285714\n\n\nNote that the variable used for stratification does not necessarily have to be the target class. In fact, multiple categorical features can be used for stratification to maintain their frequency distribution in each fold:\n\ntask_gc$col_roles$stratum = c(\"housing\", \"telephone\")\ntask_gc$strata\n\n     N                      row_id\n1: 280        1,13,20,21,26,30,...\n2: 433        2, 3, 7, 9,10,14,...\n3:  47   4,  5, 45, 76,134,192,...\n4:  61        6,19,37,55,63,69,...\n5:  63   8, 48, 60, 72, 96,100,...\n6: 116       11,12,15,22,23,28,...\n\n\nTo illustrate if stratification based on multiple categorical features works, we need to instantiate the CV folds again as we changed the features used for stratification:\n\ncv3$instantiate(task_gc)\ncv3$instance\n\n      row_id fold\n   1:     13    1\n   2:     21    1\n   3:     31    1\n   4:     33    1\n   5:     43    1\n  ---            \n 996:    945    3\n 997:    973    3\n 998:    974    3\n 999:    986    3\n1000:    993    3\n\n\nAgain, we check the relative frequency of observations in each group (combination of housing and telephone) across all folds:\n\ndt = merge(cv3$instance, task_gc$data()[, row_id := .I], by = \"row_id\")\ndt = dt[, .(freq = .N), by = list(fold, housing, telephone)]\ndt = dcast(dt, housing + telephone ~ fold)\n\nUsing 'freq' as value column. Use 'value.var' to override\n\ndt[, c(3:5) := lapply(.SD, function(x) x / sum(x)), .SDcols = 3:5]\ndt\n\n    housing                 telephone          1          2          3\n1: for free                        no 0.11607143 0.11711712 0.11480363\n2: for free yes (under customer name) 0.06250000 0.06306306 0.06344411\n3:     rent                        no 0.43154762 0.43243243 0.43504532\n4:     rent yes (under customer name) 0.27976190 0.27927928 0.28096677\n5:      own                        no 0.04761905 0.04804805 0.04531722\n6:      own yes (under customer name) 0.06250000 0.06006006 0.06042296\n\n\nAnd compare it with the relative frequency from the whole data set:\n\ntask_gc$data()[, .(freq = .N / max(.I)),\n  by = list(housing, telephone)\n][order(housing, telephone), ]\n\n    housing                 telephone       freq\n1: for free                        no 0.11681772\n2: for free yes (under customer name) 0.06415479\n3:     rent                        no 0.43300000\n4:     rent yes (under customer name) 0.28084253\n5:      own                        no 0.04895833\n6:      own yes (under customer name) 0.06106106\n\n\nIt is evident that in each fold, the combination of housing and telephone have similar frequencies that also coincide with the frequencies from the whole data set.\n\n\nBlock resampling\nAn additional concern when specifying resampling is respecting the natural grouping of the data. Blocking refers to the situation where subsets of observations belong together and must not be separated during resampling. Hence, for one train/test set pair the entire block is either in the training set or in the test set.\nThe following example is based on the BreastCancer data set from the mlbench package:\n\ndata(BreastCancer, package = \"mlbench\")\ntask_bc = as_task_classif(BreastCancer, target = \"Class\", positive = \"malignant\")\n\nIn the BreastCancer data set, for example, several observations have the same “Id” (Sample code number) which implies these are samples taken from the same patient at different times.\n\n# Let's count how many observation actually have the same Id more than once\nsum(table(BreastCancer$Id) &gt; 1)\n\n[1] 46\n\n\nThere are 46 Id’s with more than one observation (row).\nThe model trained on this data set will be used to predict cancer status of new patients. Hence, we have to make sure that each Id occurs exactly in one fold, so that all observations with the same Id should be either used for training or for evaluating the model. This way, we get less biased performance estimates via k-fold cross validation. The following example will illustrate block cross validation which can be achieved by specifying a blocking factor in the task$col_roles$group slot:\n\n# Use Id column as block factor\ntask_bc$col_roles$group = \"Id\"\n# Remove Id from feature\n# task_bc$col_roles$feature = setdiff(task_bc$col_roles$feature, \"Id\")\ncv5 = rsmp(\"cv\", folds = 5)\nset.seed(123)\ncv5$instantiate(task_bc)\ncv5$instance\n\n      row_id fold\n  1: 1016277    1\n  2: 1044572    1\n  3: 1049815    1\n  4: 1050718    1\n  5: 1054590    1\n ---             \n641: 1369821    5\n642: 1371026    5\n643: 1371920    5\n644:  714039    5\n645:  841769    5\n\n\nIn this case, the row_id column of the cv5$instance slot refers to values of the grouping variable “Id”. Additionally, the number of rows of the cv5$instance is the same as the number of unique groups:\n\nall(cv5$instance$row_id %in% BreastCancer$Id)\n\n[1] TRUE\n\nnrow(cv5$instance) == length(unique(BreastCancer$Id))\n\n[1] TRUE\n\n\nIf the specified blocking groups are respected, each Id appears only in exactly one fold. To inspect if blocking was successful when generating the folds we count how often each Id appears in a specific fold and print the Ids that appear in more than one fold:\n\ndt = merge(task_bc$data(), cv5$instance, by.x = \"Id\", by.y = \"row_id\")\ndt = dt[, .(unique_folds = length(unique(fold))), by = Id]\ndt[unique_folds &gt; 1, ]\n\nEmpty data.table (0 rows and 2 cols): Id,unique_folds\n\n\nAs expected, the table is empty as there are no Id’s present in more than one fold.\n\n\nResampling with predefined folds\nIn some use cases, it might be necessary to use predefined folds. When using k-fold cross validation without repetition this can be achieved by manually creating a feature used to denote folds and assigning it to the task$col_roles$group slot. First, we create a vector that contains 5 predefined folds:\n\nfolds = sample(rep(1:5, length.out = nrow(BreastCancer)),\n  size = nrow(BreastCancer),\n  replace = F\n)\nhead(folds, 20)\n\n [1] 2 2 4 1 5 2 5 3 1 5 4 3 3 4 5 3 3 5 2 4\n\ntable(folds)\n\nfolds\n  1   2   3   4   5 \n140 140 140 140 139 \n\n\nThis vector is now added to the data set and will be used as grouping factor just as when defining block resampling:\n\ntask_bc = TaskClassif$new(\n  id = \"BreastCancer\",\n  backend = data.frame(BreastCancer, foldIds = as.factor(folds)),\n  target = \"Class\",\n  positive = \"malignant\"\n)\ntask_bc$col_roles$group = \"foldIds\"\n# Remove \"foldIds\" from features\n# task_bc$col_roles$feature = setdiff(task_bc$col_roles$feature, \"foldIds\")\n\nWe now instantiate a 5-fold CV that will respect the predefined folds:\n\ncv5 = rsmp(\"cv\", folds = 5)\ncv5$instantiate(task_bc)\ncv5$instance\n\n   row_id fold\n1:      1    1\n2:      2    2\n3:      3    3\n4:      4    4\n5:      5    5\n\n\nSince we have only five predefined folds, the cv5$instance data table has five rows and shows which of our foldIds values (contained in the row_id column) will belong to which instantiated fold. To check if the predefined groups are respected, we count how often each foldIds appears in a specific fold:\n\ndt = merge(task_bc$data(), cv5$instance, by.x = \"foldIds\", by.y = \"row_id\")\ndt[, .(unique_folds = length(unique(fold))), by = foldIds]\n\n   foldIds unique_folds\n1:       1            1\n2:       2            1\n3:       3            1\n4:       4            1\n5:       5            1\n\n\nThere are five groups and each foldIds appears only in exactly one fold. This means that each instantiated fold corresponds to one of the predefined folds.\nThe previous example does not cover how to perform repeated k-fold CV or time series CV with predefined indices. This is possible via the mlr_resamplings_custom to which a list of predefined train and test indices can be assigned. In the following example, a custom resampling is created using indices created by caret::createMultiFolds():\n\ntask_gc = tsk(\"german_credit\")\ntrain_ind = caret::createMultiFolds(task_gc$truth(), k = 5, times = 10)\ntest_ind = lapply(train_ind, function(x) setdiff(1:task_gc$nrow, x))\nrc = rsmp(\"custom\")\nrc$instantiate(task_gc, train_ind, test_ind)\n\nWe now check if the instantiated custom resampling contains the intended folds:\n\n# check it for the first fold\nall.equal(train_ind[[1]], rc$train_set(1))\n\n[1] TRUE\n\n# check it for all folds\nunlist(lapply(1:rc$iters, function(i) all.equal(train_ind[[i]], rc$train_set(i))))\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[24] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[47] TRUE TRUE TRUE TRUE\n\n\n\n\nConclusions\nThis post shows how to control the resampling process when using mlr3 in order to account for data specificities."
  },
  {
    "objectID": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html",
    "href": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html",
    "title": "Feature Engineering of Date-Time Variables",
    "section": "",
    "text": "In this tutorial, we demonstrate how mlr3pipelines can be used to easily engineer features based on date-time variables. Relying on the Bike Sharing Dataset and the ranger learner we compare the root mean square error (RMSE) of a random forest using the original features (baseline), to the RMSE of a random forest using newly engineered features on top of the original ones."
  },
  {
    "objectID": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#motivation",
    "href": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#motivation",
    "title": "Feature Engineering of Date-Time Variables",
    "section": "Motivation",
    "text": "Motivation\nA single date-time variable (i.e., a POSIXct column) contains plenty of information ranging from year, month, day, hour, minute and second to other features such as week of the year, or day of the week. Moreover, most of these features are of cyclical nature, i.e., the eleventh and twelfth hour of a day are one hour apart, but so are the 23rd hour and midnight of the other day (see also this blog post and fastai for more information).\nNot respecting this cyclical nature results in treating hours on a linear continuum. One way to handle a cyclical feature \\(\\mathbf{x}\\) is to compute the sine and cosine transformation of \\(\\frac{2 \\pi \\mathbf{x}}{\\mathbf{x}_{\\text{max}}}\\), with \\(\\mathbf{x}_{\\text{max}} = 24\\) for hours and \\(60\\) for minutes and seconds.\nThis results in a two-dimensional representation of the feature:\n\n\n\n\n\n\n\n\n\nmlr3pipelines provides the PipeOpDateFeatures pipeline which can be used to automatically engineer features based on POSIXct columns, including handling of cyclical features.\nThis is useful as most learners naturally cannot handle dates and POSIXct variables and therefore require conversion prior to training."
  },
  {
    "objectID": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#prerequisites",
    "href": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#prerequisites",
    "title": "Feature Engineering of Date-Time Variables",
    "section": "Prerequisites",
    "text": "Prerequisites\nWe load the mlr3verse package which pulls in the most important packages for this example. The mlr3learners package loads additional learners.\n\nlibrary(mlr3verse)\nlibrary(mlr3learners)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#bike-sharing",
    "href": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#bike-sharing",
    "title": "Feature Engineering of Date-Time Variables",
    "section": "Bike Sharing",
    "text": "Bike Sharing\nThe Bike Sharing Dataset contains the hourly count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information. The dataset can be downloaded from the UCI Machine Learning Repository. After reading in the data, we fix some factor levels, and convert some data types:\nThe Bike Sharing Dataset contains the hourly count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information. We load the data set from the mlr3data package.\n\ndata(\"bike_sharing\", package = \"mlr3data\")\n\nOur goal will be to predict the total number of rented bikes on a given day: cnt.\n\nskimr::skim(bike_sharing)\n\n\nData summary\n\n\nName\nbike_sharing\n\n\nNumber of rows\n17379\n\n\nNumber of columns\n14\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n2\n\n\nlogical\n2\n\n\nnumeric\n9\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ndate\n0\n1\n10\n10\n0\n731\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nseason\n0\n1\nFALSE\n4\nsum: 4496, spr: 4409, win: 4242, fal: 4232\n\n\nweather\n0\n1\nFALSE\n4\n1: 11413, 2: 4544, 3: 1419, 4: 3\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\nholiday\n0\n1\n0.03\nFAL: 16879, TRU: 500\n\n\nworking_day\n0\n1\n0.68\nTRU: 11865, FAL: 5514\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n0.50\n0.50\n0.00\n0.00\n1.00\n1.00\n1.00\n▇▁▁▁▇\n\n\nmonth\n0\n1\n6.54\n3.44\n1.00\n4.00\n7.00\n10.00\n12.00\n▇▆▆▅▇\n\n\nhour\n0\n1\n11.55\n6.91\n0.00\n6.00\n12.00\n18.00\n23.00\n▇▇▆▇▇\n\n\nweekday\n0\n1\n3.00\n2.01\n0.00\n1.00\n3.00\n5.00\n6.00\n▇▃▃▃▇\n\n\ntemperature\n0\n1\n0.50\n0.19\n0.02\n0.34\n0.50\n0.66\n1.00\n▂▇▇▇▁\n\n\napparent_temperature\n0\n1\n0.48\n0.17\n0.00\n0.33\n0.48\n0.62\n1.00\n▁▆▇▆▁\n\n\nhumidity\n0\n1\n0.63\n0.19\n0.00\n0.48\n0.63\n0.78\n1.00\n▁▃▇▇▆\n\n\nwindspeed\n0\n1\n0.19\n0.12\n0.00\n0.10\n0.19\n0.25\n0.85\n▇▆▂▁▁\n\n\ncount\n0\n1\n189.46\n181.39\n1.00\n40.00\n142.00\n281.00\n977.00\n▇▃▁▁▁\n\n\n\n\n\nThe original dataset does not contain a POSIXct column, but we can easily generate one based on the other variables available (note that as no information regarding minutes and seconds is available, we set them to :00:00):\n\nbike_sharing$date = as.POSIXct(paste0(bike_sharing$date, \" \", bike_sharing$hour, \":00:00\"),\n  tz = \"GMT\", format = \"%Y-%m-%d %H:%M:%S\")"
  },
  {
    "objectID": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#baseline-random-forest",
    "href": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#baseline-random-forest",
    "title": "Feature Engineering of Date-Time Variables",
    "section": "Baseline Random Forest",
    "text": "Baseline Random Forest\nWe construct a new regression task and keep a holdout set.\n\ntask = as_task_regr(bike_sharing, target = \"count\")\n\nvalidation_set = sample(seq_len(task$nrow), size = 0.3 * task$nrow)\n\ntask$set_row_roles(validation_set, roles = \"holdout\")\n\nTo estimate the performance on unseen data, we will use a 3-fold cross-validation. Note that this involves validating on past data, which is usually bad practice but should suffice for this example:\n\ncv3 = rsmp(\"cv\", folds = 3)\n\nTo obtain reliable estimates on how well our model generalizes to the future, we would have to split our training and test sets according to the date variable.\nAs our baseline model, we use a random forest, ranger learner. For the baseline, we dropdate, our new POSIXct variable which we will only use later.\n\nlearner_ranger = lrn(\"regr.ranger\")\ntask_ranger = task$clone()\ntask_ranger$select(setdiff(task$feature_names, c(\"date\")))\n\nWe can then use resample() with 3-fold cross-validation:\n\nrr_ranger = resample(task_ranger, learner = learner_ranger, resampling = cv3)\n\nrr_ranger$score(msr(\"regr.mse\"))[, .(iteration, task_id, learner_id, resampling_id, regr.mse)]\n\n   iteration      task_id  learner_id resampling_id regr.mse\n1:         1 bike_sharing regr.ranger            cv 4543.904\n2:         2 bike_sharing regr.ranger            cv 4276.996\n3:         3 bike_sharing regr.ranger            cv 4767.763\n\n\nWe calculate the average RMSE.\n\nrr_ranger$aggregate()\n\nregr.mse \n4529.554 \n\n\nWe now want to improve our baseline model by using newly engineered features based on the date POSIXct column."
  },
  {
    "objectID": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#pipeopdatefeatures",
    "href": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#pipeopdatefeatures",
    "title": "Feature Engineering of Date-Time Variables",
    "section": "PipeOpDateFeatures",
    "text": "PipeOpDateFeatures\nTo engineer new features we use PipeOpDateFeatures. This pipeline automatically dispatches on POSIXct columns of the data and by default adds plenty of new date-time related features. Here, we want to add all except for minute and second, because this information is not available. As we additionally want to use cyclical versions of the features we set cyclic = TRUE:\n\npipeop_date = po(\"datefeatures\", cyclic = TRUE, minute = FALSE, second = FALSE)\n\nTraining this pipeline will result in simply adding the new features (and removing the original POSIXct feature(s) used for the feature engineering, see also the keep_date_var parameter). In our task, we can now drop the features, yr, mnth, hr, and weekday, because our pipeline will generate these anyways:\n\ntask_ex = task$clone()\ntask_ex$select(setdiff(task$feature_names,\n  c(\"instant\", \"dteday\", \"yr\", \"mnth\", \"hr\", \"weekday\", \"casual\", \"registered\")))\n\npipeop_date$train(list(task_ex))\n\n$output\n&lt;TaskRegr:bike_sharing&gt; (12166 x 32)\n* Target: count\n* Properties: -\n* Features (31):\n  - dbl (23): apparent_temperature, date.day_of_month, date.day_of_month_cos, date.day_of_month_sin,\n    date.day_of_week, date.day_of_week_cos, date.day_of_week_sin, date.day_of_year, date.day_of_year_cos,\n    date.day_of_year_sin, date.hour, date.hour_cos, date.hour_sin, date.month, date.month_cos,\n    date.month_sin, date.week_of_year, date.week_of_year_cos, date.week_of_year_sin, date.year, humidity,\n    temperature, windspeed\n  - lgl (3): date.is_day, holiday, working_day\n  - int (3): hour, month, year\n  - fct (2): season, weather\n\n\nNote that it may be useful to familiarize yourself with PipeOpRemoveConstants which can be used after the feature engineering to remove features that are constant. PipeOpDateFeatures does not do this step automatically.\nTo combine this feature engineering step with a random forest, ranger learner, we now construct a GraphLearner."
  },
  {
    "objectID": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#using-the-new-features-in-a-graphlearner",
    "href": "gallery/basic/2020-05-02-feature-engineering-of-date-time-variables/index.html#using-the-new-features-in-a-graphlearner",
    "title": "Feature Engineering of Date-Time Variables",
    "section": "Using the New Features in a GraphLearner",
    "text": "Using the New Features in a GraphLearner\nWe create a GraphLearner consisting of the PipeOpDateFeatures pipeline and a ranger learner. This GraphLearner then behaves like any other Learner:\n\ngraph = po(\"datefeatures\", cyclic = TRUE, minute = FALSE, second = FALSE) %&gt;&gt;%\n  lrn(\"regr.ranger\")\n\ngraph_learner = as_learner(graph)\n\nplot(graph, html = FALSE)\n\n\n\n\n\n\n\n\nUsing resample() with 3-fold cross-validation on the task yields:\n\ntask_graph_learner = task$clone()\ntask_graph_learner$select(setdiff(task$feature_names,\n  c(\"instant\", \"dteday\", \"yr\", \"mnth\", \"hr\", \"weekday\", \"casual\", \"registered\")))\n\nrr_graph_learner = resample(task_graph_learner, learner = graph_learner, resampling = cv3)\n\nrr_graph_learner$score(msr(\"regr.mse\"))\n\n        task_id               learner_id resampling_id iteration regr.mse\n1: bike_sharing datefeatures.regr.ranger            cv         1 2521.642\n2: bike_sharing datefeatures.regr.ranger            cv         2 2229.746\n3: bike_sharing datefeatures.regr.ranger            cv         3 2254.390\nHidden columns: task, learner, resampling, prediction\n\n\n\n\n\n\n\n\nWe calculate the average RMSE.\n\nrr_graph_learner$aggregate()\n\nregr.mse \n2335.259 \n\n\nand therefore improved by almost 94%!\nFinally, we fit our GraphLearner on the complete training set and predict on the validation set:\n\ntask$select(setdiff(task$feature_names, c(\"year\", \"month\", \"hour\", \"weekday\")))\n\ngraph_learner$train(task)\n\nprediction = graph_learner$predict(task, row_ids = task$row_roles$validation)\n\nWhere we can obtain the RMSE on the held-out validation data.\n\nprediction$score(msr(\"regr.mse\"))\n\nregr.mse \n460.0281"
  },
  {
    "objectID": "gallery/basic/2020-08-14-comparison-of-decision-boundaries/index.html",
    "href": "gallery/basic/2020-08-14-comparison-of-decision-boundaries/index.html",
    "title": "Comparison of Decision Boundaries of Classification Learners",
    "section": "",
    "text": "The visualization of decision boundaries helps to understand what the pros and cons of individual classification learners are. This posts demonstrates how to create such plots.\nWe load the mlr3 package.\nlibrary(\"mlr3\")\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/basic/2020-08-14-comparison-of-decision-boundaries/index.html#artificial-data-sets",
    "href": "gallery/basic/2020-08-14-comparison-of-decision-boundaries/index.html#artificial-data-sets",
    "title": "Comparison of Decision Boundaries of Classification Learners",
    "section": "Artificial Data Sets",
    "text": "Artificial Data Sets\nThe three artificial data sets are generated by task generators (implemented in mlr3):\n\nN &lt;- 200\ntasks &lt;- list(\n  tgen(\"xor\")$generate(N),\n  tgen(\"moons\")$generate(N),\n  tgen(\"circle\")$generate(N)\n)\n\n\nXOR\nPoints are distributed on a 2-dimensional cube with corners \\((\\pm 1, \\pm 1)\\). Class is \"red\" if \\(x\\) and \\(y\\) have the same sign, and \"black\" otherwise.\n\nplot(tgen(\"xor\"))\n\n\n\n\n\n\n\n\n\n\nCircle\nTwo circles with same center but different radii. Points in the smaller circle are \"black\", points only in the larger circle are \"red\".\n\nplot(tgen(\"circle\"))\n\n\n\n\n\n\n\n\n\n\nMoons\nTwo interleaving half circles (“moons”).\n\nplot(tgen(\"moons\"))"
  },
  {
    "objectID": "gallery/basic/2020-08-14-comparison-of-decision-boundaries/index.html#learners",
    "href": "gallery/basic/2020-08-14-comparison-of-decision-boundaries/index.html#learners",
    "title": "Comparison of Decision Boundaries of Classification Learners",
    "section": "Learners",
    "text": "Learners\nWe consider the following learners:\n\nlibrary(\"mlr3learners\")\n\nlearners &lt;- list(\n  # k-nearest neighbours classifier\n  lrn(\"classif.kknn\", id = \"kkn\", predict_type = \"prob\", k = 3),\n\n  # linear svm\n  lrn(\"classif.svm\", id = \"lin. svm\", predict_type = \"prob\", kernel = \"linear\"),\n\n  # radial-basis function svm\n  lrn(\"classif.svm\",\n    id = \"rbf svm\", predict_type = \"prob\", kernel = \"radial\",\n    gamma = 2, cost = 1, type = \"C-classification\"\n  ),\n\n  # naive bayes\n  lrn(\"classif.naive_bayes\", id = \"naive bayes\", predict_type = \"prob\"),\n\n  # single decision tree\n  lrn(\"classif.rpart\", id = \"tree\", predict_type = \"prob\", cp = 0, maxdepth = 5),\n\n  # random forest\n  lrn(\"classif.ranger\", id = \"random forest\", predict_type = \"prob\")\n)\n\nThe hyperparameters are chosen in a way that the decision boundaries look “typical” for the respective classifier. Of course, with different hyperparameters, results may look very different."
  },
  {
    "objectID": "gallery/basic/2020-08-14-comparison-of-decision-boundaries/index.html#fitting-the-models",
    "href": "gallery/basic/2020-08-14-comparison-of-decision-boundaries/index.html#fitting-the-models",
    "title": "Comparison of Decision Boundaries of Classification Learners",
    "section": "Fitting the Models",
    "text": "Fitting the Models\nTo apply each learner on each task, we first build an exhaustive grid design of experiments with benchmark_grid() and then pass it to benchmark() to do the actual work. A simple holdout resampling is used here:\n\ndesign &lt;- benchmark_grid(\n  tasks = tasks,\n  learners = learners,\n  resamplings = rsmp(\"holdout\")\n)\n\nbmr &lt;- benchmark(design, store_models = TRUE)\n\nA quick look into the performance values:\n\nperf &lt;- bmr$aggregate(msr(\"classif.acc\"))[, c(\"task_id\", \"learner_id\", \"classif.acc\")]\nperf\n\n       task_id    learner_id classif.acc\n 1:    xor_200           kkn   0.9402985\n 2:    xor_200      lin. svm   0.5223881\n 3:    xor_200       rbf svm   0.9701493\n 4:    xor_200   naive bayes   0.4328358\n 5:    xor_200          tree   0.9402985\n 6:    xor_200 random forest   1.0000000\n 7:  moons_200           kkn   1.0000000\n 8:  moons_200      lin. svm   0.8805970\n 9:  moons_200       rbf svm   1.0000000\n10:  moons_200   naive bayes   0.8955224\n11:  moons_200          tree   0.8955224\n12:  moons_200 random forest   0.9552239\n13: circle_200           kkn   0.8805970\n14: circle_200      lin. svm   0.4925373\n15: circle_200       rbf svm   0.8955224\n16: circle_200   naive bayes   0.7014925\n17: circle_200          tree   0.7462687\n18: circle_200 random forest   0.7761194"
  },
  {
    "objectID": "gallery/basic/2020-08-14-comparison-of-decision-boundaries/index.html#plotting",
    "href": "gallery/basic/2020-08-14-comparison-of-decision-boundaries/index.html#plotting",
    "title": "Comparison of Decision Boundaries of Classification Learners",
    "section": "Plotting",
    "text": "Plotting\nTo generate the plots, we iterate over the individual ResampleResult objects stored in the BenchmarkResult, and in each iteration we store the plot of the learner prediction generated by the mlr3viz package.\n\nlibrary(\"mlr3viz\")\n\nn &lt;- bmr$n_resample_results\nplots &lt;- vector(\"list\", n)\nfor (i in seq_len(n)) {\n  rr &lt;- bmr$resample_result(i)\n  plots[[i]] &lt;- autoplot(rr, type = \"prediction\")\n}\n\nWe now have a list of plots. Each one can be printed individually:\n\nprint(plots[[1]])\n\n\n\n\n\n\n\n\nNote that only observations from the test data is plotted as points.\nTo get a nice annotated overview, we arranged all plots together in a single pdf file. The number in the upper right is the respective accuracy on the test set.\n\npdf(file = \"plot_learner_prediction.pdf\", width = 20, height = 6)\nntasks &lt;- length(tasks)\nnlearners &lt;- length(learners)\nm &lt;- msr(\"classif.acc\")\n\n# for each plot\nfor (i in seq_along(plots)) {\n  plots[[i]] &lt;- plots[[i]] +\n    # remove legend\n    ggplot2::theme(legend.position = \"none\") +\n    # remove labs\n    ggplot2::xlab(\"\") + ggplot2::ylab(\"\") +\n    # add accuracy score as annotation\n    ggplot2::annotate(\"text\",\n      label = sprintf(\"%.2f\", bmr$resample_result(i)$aggregate(m)),\n      x = Inf, y = Inf, vjust = 2, hjust = 1.5\n    )\n}\n\n# for each plot of the first column\nfor (i in seq_len(ntasks)) {\n  ii &lt;- (i - 1) * nlearners + 1L\n  plots[[ii]] &lt;- plots[[ii]] + ggplot2::ylab(sub(\"_[0-9]+$\", \"\", tasks[[i]]$id))\n}\n\n# for each plot of the first row\nfor (i in seq_len(nlearners)) {\n  plots[[i]] &lt;- plots[[i]] + ggplot2::ggtitle(learners[[i]]$id)\n}\n\ngridExtra::grid.arrange(grobs = plots, nrow = length(tasks))\ndev.off()\n\nAs you can see, the decision boundaries look very different. Some are linear, others are parallel to the axis, and yet others are highly non-linear. The boundaries are partly very smooth with a slow transition of probabilities, others are very abrupt. All these properties are important during model selection, and should be considered for your problem at hand."
  },
  {
    "objectID": "gallery/basic/2020-01-31-encode-factors-for-xgboost/index.html",
    "href": "gallery/basic/2020-01-31-encode-factors-for-xgboost/index.html",
    "title": "Encode Factor Levels for xgboost",
    "section": "",
    "text": "The package xgboost unfortunately does not support handling of categorical features. Therefore, it is required to manually convert factor columns to numerical dummy features. We show how to use mlr3pipelines to augment the xgboost learner with an automatic factor encoding.\nWe load the mlr3verse package which pulls in the most important packages for this example.\nlibrary(mlr3verse)\n\nLoading required package: mlr3\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")"
  },
  {
    "objectID": "gallery/basic/2020-01-31-encode-factors-for-xgboost/index.html#construct-the-base-objects",
    "href": "gallery/basic/2020-01-31-encode-factors-for-xgboost/index.html#construct-the-base-objects",
    "title": "Encode Factor Levels for xgboost",
    "section": "Construct the Base Objects",
    "text": "Construct the Base Objects\nFirst, we take an example task with factors (german_credit) and create the xgboost learner:\n\nlibrary(mlr3learners)\n\ntask = tsk(\"german_credit\")\nprint(task)\n\n&lt;TaskClassif:german_credit&gt; (1000 x 21): German Credit\n* Target: credit_risk\n* Properties: twoclass\n* Features (20):\n  - fct (14): credit_history, employment_duration, foreign_worker, housing, job, other_debtors,\n    other_installment_plans, people_liable, personal_status_sex, property, purpose, savings, status,\n    telephone\n  - int (3): age, amount, duration\n  - ord (3): installment_rate, number_credits, present_residence\n\nlearner = lrn(\"classif.xgboost\", nrounds = 100)\nprint(learner)\n\n&lt;LearnerClassifXgboost:classif.xgboost&gt;\n* Model: -\n* Parameters: nrounds=100, nthread=1, verbose=0, early_stopping_set=none\n* Packages: mlr3, mlr3learners, xgboost\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric\n* Properties: hotstart_forward, importance, missings, multiclass, twoclass, weights\n\n\nWe now compare the feature types of the task and the supported feature types:\n\nunique(task$feature_types$type)\n\n[1] \"integer\" \"factor\"  \"ordered\"\n\nlearner$feature_types\n\n[1] \"logical\" \"integer\" \"numeric\"\n\nsetdiff(task$feature_types$type, learner$feature_types)\n\n[1] \"factor\"  \"ordered\"\n\n\nIn this example, we have to convert factors and ordered factors to numeric columns to apply the xgboost learner. Because xgboost is based on decision trees (at least in its default settings), it is perfectly fine to convert the ordered factors to integer. Unordered factors must still be encoded though."
  },
  {
    "objectID": "gallery/basic/2020-01-31-encode-factors-for-xgboost/index.html#construct-pipeline",
    "href": "gallery/basic/2020-01-31-encode-factors-for-xgboost/index.html#construct-pipeline",
    "title": "Encode Factor Levels for xgboost",
    "section": "Construct Pipeline",
    "text": "Construct Pipeline\nFinally, we construct a linear pipeline consisting of\n\nthe factor encoder fencoder,\nthe ordered factor converter ord_to_int, and\nthe xgboost base learner.\n\n\ngraph = fencoder %&gt;&gt;% ord_to_int %&gt;&gt;% learner\nprint(graph)\n\nGraph with 3 PipeOps:\n              ID         State        sccssors prdcssors\n          encode        &lt;list&gt;        colapply          \n        colapply        &lt;list&gt; classif.xgboost    encode\n classif.xgboost &lt;&lt;UNTRAINED&gt;&gt;                  colapply\n\n\nThe pipeline is wrapped in a GraphLearner so that it behaves like a regular learner:\n\ngraph_learner = as_learner(graph)\n\nWe can now apply the new learner on the task, here with a 3-fold cross validation:\n\nrr = resample(task, graph_learner, rsmp(\"cv\", folds = 3))\nrr$aggregate()\n\nclassif.ce \n 0.2620435 \n\n\nSuccess! We augmented xgboost with handling of factors and ordered factors. If we combine this learner with a tuner from mlr3tuning, we get a universal and competitive learner."
  },
  {
    "objectID": "gallery/pipelines/2020-04-18-regression-chains/index.html",
    "href": "gallery/pipelines/2020-04-18-regression-chains/index.html",
    "title": "Regression Chains",
    "section": "",
    "text": "In this tutorial we demonstrate how to use mlr3pipelines to handle multi-target regression by arranging regression models as a chain, i.e., creating a linear sequence of regression models.\n\nRegression Chains\nIn a simple regression chain, regression models are arranged in a linear sequence. Here, the first model will use the input to predict a single output and the second model will use the input and the prediction output of the first model to make its own prediction and so on. For more details, see e.g. Spyromitros-Xioufis et al. (2016).\n\n\nBefore you start\nThe following sections describe an approach towards working with tasks that have multiple targets. E.g., in the example below, we have three target variables \\(y_{1}\\) to \\(y_{3}\\). This type of Task can be created via the mlr3multioutput package (currently under development) in the future. mlr3multioutput will also offer simple chaining approaches as pre-built pipelines (so called ppls). The current goal of this post is to show how such modeling steps can be written as a relatively small amount of pipeline steps and how such steps can be put together. Writing pipelines with such steps allows for great flexibility in modeling more complicated scenarios such as the ones described below.\n\n\nPrerequisites\nWe load the mlr3verse package which pulls in the most important packages for this example.\n\nlibrary(mlr3verse)\n\nLoading required package: mlr3\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n\n\n\nData\nIn the following, we rely on some toy data. We simulate 100 responses to three target variables, \\(y_{1}\\), \\(y_{2}\\), and \\(y_{3}\\) following a multivariate normal distribution with a mean and covariance matrix of:\n\nlibrary(data.table)\nlibrary(mvtnorm)\nset.seed(2409)\nn = 100\n(mean &lt;- c(y1 = 1, y2 = 2, y3 = 3))\n\ny1 y2 y3 \n 1  2  3 \n\n(sigma &lt;- matrix(c(1, -0.5, 0.25, -0.5, 1, -0.25, 0.25, -0.25, 1),\n  nrow = 3, ncol = 3, byrow = TRUE\n))\n\n      [,1]  [,2]  [,3]\n[1,]  1.00 -0.50  0.25\n[2,] -0.50  1.00 -0.25\n[3,]  0.25 -0.25  1.00\n\nY = rmvnorm(n, mean = mean, sigma = sigma)\n\nThe feature variables \\(x_{1}\\), and \\(x_{2}\\) are simulated as follows: \\(x_{1}\\) is simply given by \\(y_{1}\\) and an independent normally distributed error term and \\(x_{2}\\) is given by \\(y_{2}\\) and an independent normally distributed error term.\n\nx1 = Y[, 1] + rnorm(n, sd = 0.1)\nx2 = Y[, 2] + rnorm(n, sd = 0.1)\n\nThe final data is given as:\n\ndata = as.data.table(cbind(Y, x1, x2))\nstr(data)\n\nClasses 'data.table' and 'data.frame':  100 obs. of  5 variables:\n $ y1: num  0.681 1.836 0.355 1.783 0.974 ...\n $ y2: num  2.33 1.735 3.126 0.691 1.573 ...\n $ y3: num  3.19 3.14 2.74 4.31 2.77 ...\n $ x1: num  0.788 1.754 0.174 1.844 1.05 ...\n $ x2: num  2.336 1.665 2.967 0.651 1.634 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\nThis simulates a situation where we have multiple target variables that are correlated with each other, such that predicting them along with each other can improve the resulting prediction model. As a real-world example for such a situation, consider e.g. hospital data, where time spent in the ICU (not known a priori) heavily influences the cost incurred by a patient’s treatment.\n\n\n3D Visualization of the Data\nIf you feel confident to already have a good feeling of the data, feel free to skip this section. If not, you can use the rgl package to play around with the following four 3D plots with either the feature variables or \\(y_{1}\\) and \\(y_{2}\\) on the x- and y-axis and the target variables on the respective z-axes:\n\nlibrary(rgl)\ncolfun = colorRampPalette(c(\"#161B1D\", \"#ADD8E6\"))\n\n\nsetorder(data, y1)\nplot3d(data$x1, data$x2, data$y1,\n  xlab = \"x1\", ylab = \"x2\", zlab = \"y1\",\n  type = \"s\", radius = 0.1, col = colfun(n)\n)\n\n\nsetorder(data, y2)\nplot3d(data$x1, data$x2, data$y2,\n  xlab = \"x1\", ylab = \"x2\", zlab = \"y2\",\n  type = \"s\", radius = 0.1, col = colfun(n)\n)\n\n\nsetorder(data, y3)\nplot3d(data$x1, data$x2, data$y3,\n  xlab = \"x1\", ylab = \"x2\", zlab = \"y3\",\n  type = \"s\", radius = 0.1, col = colfun(n)\n)\n\n\nsetorder(data, y3)\nplot3d(data$y1, data$y2, data$y3,\n  xlab = \"y1\", ylab = \"y2\", zlab = \"y3\",\n  type = \"s\", radius = 0.1, col = colfun(n)\n)\n\n\n\nBuilding the Pipeline\nIn our regression chain, the first model will predict \\(y_{1}\\). Therefore, we initialize our Task with respect to this target:\n\ntask = as_task_regr(data, id = \"multiregression\", target = \"y1\")\n\nAs Learners we will use simple linear regression models. Our pipeline building the regression chain then has to do the following:\n\nUse the input to predict \\(y_{1}\\) within the first learner (i.e., \\(y_{1} \\sim x_{1} + x_{2}\\)).\nCombine the input with the prediction of \\(y_{1}\\), \\(\\hat{y_{1}}\\) and use this to predict \\(y_{2}\\) within the second learner (i.e., \\(y_{2} \\sim x_{1} + x_{2} + \\hat{y_{1}}\\)).\nCombine the input with the prediction of \\(y_{2}\\) and use this to predict \\(y_{3}\\) within the final third learner (i.e., \\(y_{3} \\sim x_{1} + x_{2} + \\hat{y_{1}} + \\hat{y_{2}}\\)).\n\nTo combine predictions of a Learner with the previous input, we rely on PipeOpLearnerCV and PipeOpNOP arranged in parallel via gunion() combined via PipeOpFeatureUnion. To drop the respective remaining target variables as features, we rely on PipeOpColRoles. The first step of predicting \\(y_{1}\\) looks like the following:\n\nstep1 = po(\"copy\", outnum = 2, id = \"copy1\") %&gt;&gt;%\n  gunion(list(\n    po(\"colroles\",\n      id = \"drop_y2_y3\",\n      new_role = list(y2 = character(), y3 = character())\n    ) %&gt;&gt;%\n      po(\"learner_cv\", learner = lrn(\"regr.lm\"), id = \"y1_learner\"),\n    po(\"nop\", id = \"nop1\")\n  )) %&gt;&gt;%\n  po(\"featureunion\", id = \"union1\")\nstep1$plot(html = FALSE)\n\n\n\n\n\n\n\n\nTraining using the input Task, shows us how the output and the $state look like:\n\nstep1_out = step1$train(task)[[1]]\nstep1_out\n\n&lt;TaskRegr:multiregression&gt; (100 x 6)\n* Target: y1\n* Properties: -\n* Features (5):\n  - dbl (5): x1, x2, y1_learner.response, y2, y3\n\nstep1$state\n\n$copy1\nlist()\n\n$drop_y2_y3\n$drop_y2_y3$dt_columns\n[1] \"x1\" \"x2\" \"y2\" \"y3\"\n\n$drop_y2_y3$affected_cols\n[1] \"x1\" \"x2\" \"y2\" \"y3\"\n\n$drop_y2_y3$intasklayout\n   id    type\n1: x1 numeric\n2: x2 numeric\n3: y2 numeric\n4: y3 numeric\n\n$drop_y2_y3$outtasklayout\n   id    type\n1: x1 numeric\n2: x2 numeric\n\n$drop_y2_y3$outtaskshell\nEmpty data.table (0 rows and 3 cols): y1,x1,x2\n\n\n$y1_learner\n$y1_learner$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n(Intercept)           x1           x2  \n   -0.03762      0.99851      0.01364  \n\n\n$y1_learner$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$y1_learner$train_time\n[1] 0.004\n\n$y1_learner$param_vals\nnamed list()\n\n$y1_learner$task_hash\n[1] \"933a5b384a9aaebf\"\n\n$y1_learner$data_prototype\nEmpty data.table (0 rows and 3 cols): y1,x1,x2\n\n$y1_learner$task_prototype\nEmpty data.table (0 rows and 3 cols): y1,x1,x2\n\n$y1_learner$mlr3_version\n[1] '0.16.1'\n\n$y1_learner$train_task\n&lt;TaskRegr:multiregression&gt; (100 x 3)\n* Target: y1\n* Properties: -\n* Features (2):\n  - dbl (2): x1, x2\n\n$y1_learner$affected_cols\n[1] \"x1\" \"x2\"\n\n$y1_learner$intasklayout\n   id    type\n1: x1 numeric\n2: x2 numeric\n\n$y1_learner$outtasklayout\n                    id    type\n1: y1_learner.response numeric\n\n$y1_learner$outtaskshell\nEmpty data.table (0 rows and 2 cols): y1,y1_learner.response\n\n\n$nop1\nlist()\n\n$union1\nlist()\n\n\nWithin the second step we then have to define \\(y_{2}\\) as the new target. This can be done using PipeOpUpdateTarget (note that PipeOpUpdateTarget currently is not exported but will be in a future version). By default, PipeOpUpdateTarget drops the original target from the feature set, here \\(y_{1}\\).\n\nmlr_pipeops$add(\"update_target\", mlr3pipelines:::PipeOpUpdateTarget)\n\n\nstep2 = po(\"update_target\",\n  id = \"y2_target\",\n  new_target_name = \"y2\"\n) %&gt;&gt;%\n  po(\"copy\", outnum = 2, id = \"copy2\") %&gt;&gt;%\n  gunion(list(\n    po(\"colroles\",\n      id = \"drop_y3\",\n      new_role = list(y3 = character())\n    ) %&gt;&gt;%\n      po(\"learner_cv\", learner = lrn(\"regr.lm\"), id = \"y2_learner\"),\n    po(\"nop\", id = \"nop2\")\n  )) %&gt;&gt;%\n  po(\"featureunion\", id = \"union2\")\n\nAgain, we can train to see how the output and $state look like, but now using the output of step1 as the input:\n\nstep2_out = step2$train(step1_out)[[1]]\nstep2_out\n\n&lt;TaskRegr:multiregression&gt; (100 x 6)\n* Target: y2\n* Properties: -\n* Features (5):\n  - dbl (5): x1, x2, y1_learner.response, y2_learner.response, y3\n\nstep2$state\n\n$y2_target\nlist()\n\n$copy2\nlist()\n\n$drop_y3\n$drop_y3$dt_columns\n[1] \"x1\"                  \"x2\"                  \"y1_learner.response\" \"y3\"                 \n\n$drop_y3$affected_cols\n[1] \"y1_learner.response\" \"x1\"                  \"x2\"                  \"y3\"                 \n\n$drop_y3$intasklayout\n                    id    type\n1:                  x1 numeric\n2:                  x2 numeric\n3: y1_learner.response numeric\n4:                  y3 numeric\n\n$drop_y3$outtasklayout\n                    id    type\n1:                  x1 numeric\n2:                  x2 numeric\n3: y1_learner.response numeric\n\n$drop_y3$outtaskshell\nEmpty data.table (0 rows and 4 cols): y2,y1_learner.response,x1,x2\n\n\n$y2_learner\n$y2_learner$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n        (Intercept)  y1_learner.response                   x1                   x2  \n            0.07135              0.22773             -0.25186              0.97877  \n\n\n$y2_learner$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$y2_learner$train_time\n[1] 0.01\n\n$y2_learner$param_vals\nnamed list()\n\n$y2_learner$task_hash\n[1] \"1bc5196bab655ff5\"\n\n$y2_learner$data_prototype\nEmpty data.table (0 rows and 4 cols): y2,y1_learner.response,x1,x2\n\n$y2_learner$task_prototype\nEmpty data.table (0 rows and 4 cols): y2,y1_learner.response,x1,x2\n\n$y2_learner$mlr3_version\n[1] '0.16.1'\n\n$y2_learner$train_task\n&lt;TaskRegr:multiregression&gt; (100 x 4)\n* Target: y2\n* Properties: -\n* Features (3):\n  - dbl (3): x1, x2, y1_learner.response\n\n$y2_learner$affected_cols\n[1] \"y1_learner.response\" \"x1\"                  \"x2\"                 \n\n$y2_learner$intasklayout\n                    id    type\n1:                  x1 numeric\n2:                  x2 numeric\n3: y1_learner.response numeric\n\n$y2_learner$outtasklayout\n                    id    type\n1: y2_learner.response numeric\n\n$y2_learner$outtaskshell\nEmpty data.table (0 rows and 2 cols): y2,y2_learner.response\n\n\n$nop2\nlist()\n\n$union2\nlist()\n\n\nIn the final third step we define \\(y_{3}\\) as the new target (again, PipeOpUpdateTarget drops the previous original target from the feature set, here \\(y_{2}\\)):\n\nstep3 = po(\"update_target\",\n  id = \"y3_target\",\n  new_target_name = \"y3\"\n) %&gt;&gt;%\n  po(\"learner\", learner = lrn(\"regr.lm\"), id = \"y3_learner\")\n\nUsing the output of step2 as input:\n\nstep3_out = step3$train(step2_out)[[1]]\nstep3_out\n\nNULL\n\nstep3$state\n\n$y3_target\nlist()\n\n$y3_learner\n$y3_learner$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n        (Intercept)  y2_learner.response  y1_learner.response                   x1                   x2  \n             2.6445               0.8155               3.8776              -3.5217              -0.7304  \n\n\n$y3_learner$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$y3_learner$train_time\n[1] 0.013\n\n$y3_learner$param_vals\nnamed list()\n\n$y3_learner$task_hash\n[1] \"24dbd64658d33d6d\"\n\n$y3_learner$data_prototype\nEmpty data.table (0 rows and 5 cols): y3,y2_learner.response,y1_learner.response,x1,x2\n\n$y3_learner$task_prototype\nEmpty data.table (0 rows and 5 cols): y3,y2_learner.response,y1_learner.response,x1,x2\n\n$y3_learner$mlr3_version\n[1] '0.16.1'\n\n$y3_learner$train_task\n&lt;TaskRegr:multiregression&gt; (100 x 5)\n* Target: y3\n* Properties: -\n* Features (4):\n  - dbl (4): x1, x2, y1_learner.response, y2_learner.response\n\n\nThe complete pipeline, more precisely Graph, looks like the following:\n\ngraph = step1 %&gt;&gt;% step2 %&gt;&gt;% step3\ngraph$plot(html = FALSE)\n\n\n\n\n\n\n\n\n\n\nEvaluating the Pipeline\nBy wrapping our Graph in a GraphLearner, we can perform 3-fold cross-validation and get an estimated average of the root-mean-square error (of course, in a real world setting splitting the data in a training and test set should have been done):\n\nlearner = as_learner(graph)\nrr = resample(task, learner, rsmp(\"cv\", folds = 3))\nrr$aggregate(msr(\"regr.mse\"))\n\n regr.mse \n0.7265587 \n\n\n\n\nPredicting with the Pipeline\nFor completeness, we also show how a prediction step without having any target variable data available would look like:\n\ndata_predict = as.data.table(cbind(x1, x2, y1 = NA, y2 = NA, y3 = NA))\nlearner$train(task)\nlearner$predict_newdata(data_predict)\n\n&lt;PredictionRegr&gt; for 100 observations:\n    row_ids truth response\n          1    NA 3.116960\n          2    NA 3.327345\n          3    NA 3.010821\n---                       \n         98    NA 3.462541\n         99    NA 3.020585\n        100    NA 3.664326\n\n\nNote that we have to initialize the Task with \\(y_{1}\\) as the target but the pipeline will automatically predict \\(y_{3}\\) in the final step as our final target, which was our ultimate goal here.\n\n\n\n\n\nReferences\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves, and Ioannis Vlahavas. 2016. “Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs.” Machine Learning 104 (1): 55–98. https://doi.org/10.1007/s10994-016-5546-z."
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-tuning-stacking/index.html",
    "href": "gallery/pipelines/2020-04-27-tuning-stacking/index.html",
    "title": "Tuning a Stacked Learner",
    "section": "",
    "text": "Multilevel stacking is an ensemble technique, where predictions of several learners are added as new features to extend the orginal data on different levels. On each level, the extended data is used to train a new level of learners. This can be repeated for several iterations until a final learner is trained. To avoid overfitting, it is advisable to use test set (out-of-bag) predictions in each level.\nIn this post, a multilevel stacking example will be created using mlr3pipelines and tuned using mlr3tuning . A similar example is available in the mlr3book. However, we additionally explain how to tune the hyperparameters of the whole ensemble and each underlying learner jointly.\nIn our stacking example, we proceed as follows:\n\nLevel 0: Based on the input data, we train three learners (rpart, glmnet and lda) on a sparser feature space obtained using different feature filter methods from mlr3filters to obtain slightly decorrelated predictions. The test set predictions of these learners are attached to the original data (used in level 0) and will serve as input for the learners in level 1.\nLevel 1: We transform this extended data using PCA, on which we then train additional three learners (rpart, glmnet and lda). The test set predictions of the level 1 learners are attached to input data used in level 1.\nFinally, we train a final ranger learner to the data extended by level 1. Note that the number of features selected by the feature filter method in level 0 and the number of principal components retained in level 1 will be jointly tuned with some other hyperparameters of the learners in each level."
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-tuning-stacking/index.html#intro",
    "href": "gallery/pipelines/2020-04-27-tuning-stacking/index.html#intro",
    "title": "Tuning a Stacked Learner",
    "section": "",
    "text": "Multilevel stacking is an ensemble technique, where predictions of several learners are added as new features to extend the orginal data on different levels. On each level, the extended data is used to train a new level of learners. This can be repeated for several iterations until a final learner is trained. To avoid overfitting, it is advisable to use test set (out-of-bag) predictions in each level.\nIn this post, a multilevel stacking example will be created using mlr3pipelines and tuned using mlr3tuning . A similar example is available in the mlr3book. However, we additionally explain how to tune the hyperparameters of the whole ensemble and each underlying learner jointly.\nIn our stacking example, we proceed as follows:\n\nLevel 0: Based on the input data, we train three learners (rpart, glmnet and lda) on a sparser feature space obtained using different feature filter methods from mlr3filters to obtain slightly decorrelated predictions. The test set predictions of these learners are attached to the original data (used in level 0) and will serve as input for the learners in level 1.\nLevel 1: We transform this extended data using PCA, on which we then train additional three learners (rpart, glmnet and lda). The test set predictions of the level 1 learners are attached to input data used in level 1.\nFinally, we train a final ranger learner to the data extended by level 1. Note that the number of features selected by the feature filter method in level 0 and the number of principal components retained in level 1 will be jointly tuned with some other hyperparameters of the learners in each level."
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-tuning-stacking/index.html#prerequisites",
    "href": "gallery/pipelines/2020-04-27-tuning-stacking/index.html#prerequisites",
    "title": "Tuning a Stacked Learner",
    "section": "Prerequisites",
    "text": "Prerequisites\nWe load the mlr3verse package which pulls in the most important packages for this example. The mlr3learners package loads additional learners.\n\nlibrary(mlr3verse)\nlibrary(mlr3learners)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nFor the stacking example, we use the sonar classification task:\n\ntask_sonar = tsk(\"sonar\")\ntask_sonar$col_roles$stratum = task_sonar$target_names # stratification"
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-tuning-stacking/index.html#pipeline-creation",
    "href": "gallery/pipelines/2020-04-27-tuning-stacking/index.html#pipeline-creation",
    "title": "Tuning a Stacked Learner",
    "section": "Pipeline creation",
    "text": "Pipeline creation\n\nLevel 0\nAs mentioned, the level 0 learners are rpart, glmnet and lda:\n\nlearner_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlearner_glmnet = lrn(\"classif.glmnet\", predict_type = \"prob\")\nlearner_lda = lrn(\"classif.lda\", predict_type = \"prob\")\n\nTo create the learner out-of-bag predictions, we use PipeOpLearnerCV:\n\ncv1_rpart = po(\"learner_cv\", learner_rpart, id = \"rprt_1\")\ncv1_glmnet = po(\"learner_cv\", learner_glmnet, id = \"glmnet_1\")\ncv1_lda = po(\"learner_cv\", learner_lda, id = \"lda_1\")\n\nA sparser representation of the input data in level 0 is obtained using the following filters:\n\nanova = po(\"filter\", flt(\"anova\"), id = \"filt1\")\nmrmr = po(\"filter\", flt(\"mrmr\"), id = \"filt2\")\nfind_cor = po(\"filter\", flt(\"find_correlation\"), id = \"filt3\")\n\nTo summarize these steps into level 0, we use the gunion() function. The out-of-bag predictions of all level 0 learners is attached using PipeOpFeatureUnion along with the original data passed via PipeOpNOP:\n\nlevel0 = gunion(list(\n  anova %&gt;&gt;% cv1_rpart,\n  mrmr %&gt;&gt;% cv1_glmnet,\n  find_cor %&gt;&gt;% cv1_lda,\n  po(\"nop\", id = \"nop1\"))) %&gt;&gt;%\n  po(\"featureunion\", id = \"union1\")\n\nWe can have a look at the graph from level 0:\n\nlevel0$plot(html = FALSE)\n\n\n\n\n\n\n\n\n\n\nLevel 1\nNow, we create the level 1 learners:\n\ncv2_rpart = po(\"learner_cv\", learner_rpart, id = \"rprt_2\")\ncv2_glmnet = po(\"learner_cv\", learner_glmnet, id = \"glmnet_2\")\ncv2_lda = po(\"learner_cv\", learner_lda, id = \"lda_2\")\n\nAll level 1 learners will use PipeOpPCA transformed data as input:\n\nlevel1 = level0 %&gt;&gt;%\n  po(\"copy\", 4) %&gt;&gt;%\n  gunion(list(\n    po(\"pca\", id = \"pca2_1\", param_vals = list(scale. = TRUE)) %&gt;&gt;% cv2_rpart,\n    po(\"pca\", id = \"pca2_2\", param_vals = list(scale. = TRUE)) %&gt;&gt;% cv2_glmnet,\n    po(\"pca\", id = \"pca2_3\", param_vals = list(scale. = TRUE)) %&gt;&gt;% cv2_lda,\n    po(\"nop\", id = \"nop2\"))) %&gt;&gt;%\n  po(\"featureunion\", id = \"union2\")\n\nWe can have a look at the graph from level 1:\n\nlevel1$plot(html = FALSE)\n\n\n\n\n\n\n\n\nThe out-of-bag predictions of the level 1 learners are attached to the input data from level 1 and a final ranger learner will be trained:\n\nranger_lrn = lrn(\"classif.ranger\", predict_type = \"prob\")\n\nensemble = level1 %&gt;&gt;% ranger_lrn\nensemble$plot(html = FALSE)\n\n\n\n\n\n\n\n\n\n\nDefining the tuning space\nIn order to tune the ensemble’s hyperparameter jointly, we define the search space using ParamSet from the paradox package:\n\nsearch_space_ensemble = ps(\n  filt1.filter.nfeat = p_int(5, 50),\n  filt2.filter.nfeat = p_int(5, 50),\n  filt3.filter.nfeat = p_int(5, 50),\n  pca2_1.rank. = p_int(3, 50),\n  pca2_2.rank. = p_int(3, 50),\n  pca2_3.rank. = p_int(3, 20),\n  rprt_1.cp = p_dbl(0.001, 0.1),\n  rprt_1.minbucket = p_int(1, 10),\n  glmnet_1.alpha = p_dbl(0, 1),\n  rprt_2.cp = p_dbl(0.001, 0.1),\n  rprt_2.minbucket = p_int(1, 10),\n  glmnet_2.alpha = p_dbl(0, 1),\n  classif.ranger.mtry = p_int(1, 10),\n  classif.ranger.sample.fraction = p_dbl(0.5, 1),\n  classif.ranger.num.trees = p_int(50, 200))\n\n\n\nPerformance comparison\nEven with a simple ensemble, there is quite a few things to setup. We compare the performance of the ensemble with a simple tuned ranger learner.\nTo proceed, we convert the ensemble pipeline as a GraphLearner:\n\nlearner_ensemble = as_learner(ensemble)\nlearner_ensemble$id = \"ensemble\"\nlearner_ensemble$predict_type = \"prob\"\n\nWe define the search space for the simple ranger learner:\n\nsearch_space_ranger = ps(\n  mtry = p_int(1, 10),\n  sample.fraction = p_dbl(0.5, 1),\n  num.trees = p_int(50, 200))\n\nFor performance comparison, we use the benchmark() function that requires a design incorporating a list of learners and a list of tasks. Here, we have two learners (the simple ranger learner and the ensemble) and one task. Since we want to tune the simple ranger learner as well as the whole ensemble learner, we need to create an AutoTuner for each learner to be compared. To do so, we need to define a resampling strategy for the tuning in the inner loop (we use 3-fold cross-validation) and for the final evaluation (outer loop) use use holdout validation:\n\ninner_resampling = rsmp(\"cv\", folds = 3)\n\n# AutoTuner for the ensemble learner\nat_1 = auto_tuner(\n  tuner = tnr(\"random_search\"),\n  learner = learner_ensemble,\n  resampling = inner_resampling,\n  measure = msr(\"classif.auc\"),\n  search_space = search_space_ensemble,\n  term_evals = 3) # to limit running time\n\n# AutoTuner for the simple ranger learner\nat_2 = auto_tuner(\n  tuner = tnr(\"random_search\"),\n  learner = ranger_lrn,\n  resampling = inner_resampling,\n  measure = msr(\"classif.auc\"),\n  search_space = search_space_ranger,\n  term_evals = 3) # to limit running time\n\n# Define the list of learners\nlearners = list(at_1, at_2)\n\n# For benchmarking, we use a simple holdout\nouter_resampling = rsmp(\"holdout\")\nouter_resampling$instantiate(task_sonar)\n\ndesign = benchmark_grid(\n  tasks = task_sonar,\n  learners = learners,\n  resamplings = outer_resampling\n)\n\n\nbmr = benchmark(design, store_models = TRUE)\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n\n\n\nbmr$aggregate(msr(\"classif.auc\"))[, .(nr, task_id, learner_id, resampling_id, iters, classif.auc)]\n\nFor a more reliable comparison, the number of evaluation of the random search should be increased."
  },
  {
    "objectID": "gallery/pipelines/2020-04-27-tuning-stacking/index.html#conclusion",
    "href": "gallery/pipelines/2020-04-27-tuning-stacking/index.html#conclusion",
    "title": "Tuning a Stacked Learner",
    "section": "Conclusion",
    "text": "Conclusion\nThis example shows the versatility of mlr3pipelines. By using more learners, varied representations of the data set as well as more levels, a powerful yet compute hungry pipeline can be created. It is important to note that care should be taken to avoid name clashes of pipeline objects."
  },
  {
    "objectID": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html",
    "href": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html",
    "title": "Pipelines, Selectors, Branches",
    "section": "",
    "text": "mlr3pipelines offers a very flexible way to create data preprocessing steps. This is achieved by a modular approach using PipeOps. For detailed overview check the mlr3book.\nRecommended prior readings:\n\nmlr3pipelines tutorial - german credit\nImpute missing variables .\n\nThis post covers:\n\nHow to apply different preprocessing steps on different features\nHow to branch different preprocessing steps, which allows to select the best performing path\nHow to tune the whole pipeline"
  },
  {
    "objectID": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#intro",
    "href": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#intro",
    "title": "Pipelines, Selectors, Branches",
    "section": "",
    "text": "mlr3pipelines offers a very flexible way to create data preprocessing steps. This is achieved by a modular approach using PipeOps. For detailed overview check the mlr3book.\nRecommended prior readings:\n\nmlr3pipelines tutorial - german credit\nImpute missing variables .\n\nThis post covers:\n\nHow to apply different preprocessing steps on different features\nHow to branch different preprocessing steps, which allows to select the best performing path\nHow to tune the whole pipeline"
  },
  {
    "objectID": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#prerequisites",
    "href": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#prerequisites",
    "title": "Pipelines, Selectors, Branches",
    "section": "Prerequisites",
    "text": "Prerequisites\nWe load the mlr3verse package which pulls in the most important packages for this example.\n\nlibrary(mlr3verse)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nThe Pima Indian Diabetes classification task will be used.\n\ntask_pima = tsk(\"pima\")\nskimr::skim(task_pima$data())\n\n\nData summary\n\n\nName\ntask_pima$data()\n\n\nNumber of rows\n768\n\n\nNumber of columns\n9\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndiabetes\n0\n1\nFALSE\n2\nneg: 500, pos: 268\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nage\n0\n1.00\n33.24\n11.76\n21.00\n24.00\n29.00\n41.00\n81.00\n▇▃▁▁▁\n\n\nglucose\n5\n0.99\n121.69\n30.54\n44.00\n99.00\n117.00\n141.00\n199.00\n▁▇▇▃▂\n\n\ninsulin\n374\n0.51\n155.55\n118.78\n14.00\n76.25\n125.00\n190.00\n846.00\n▇▂▁▁▁\n\n\nmass\n11\n0.99\n32.46\n6.92\n18.20\n27.50\n32.30\n36.60\n67.10\n▅▇▃▁▁\n\n\npedigree\n0\n1.00\n0.47\n0.33\n0.08\n0.24\n0.37\n0.63\n2.42\n▇▃▁▁▁\n\n\npregnant\n0\n1.00\n3.85\n3.37\n0.00\n1.00\n3.00\n6.00\n17.00\n▇▃▂▁▁\n\n\npressure\n35\n0.95\n72.41\n12.38\n24.00\n64.00\n72.00\n80.00\n122.00\n▁▃▇▂▁\n\n\ntriceps\n227\n0.70\n29.15\n10.48\n7.00\n22.00\n29.00\n36.00\n99.00\n▆▇▁▁▁"
  },
  {
    "objectID": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#selection-of-features-for-preprocessing-steps",
    "href": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#selection-of-features-for-preprocessing-steps",
    "title": "Pipelines, Selectors, Branches",
    "section": "Selection of features for preprocessing steps",
    "text": "Selection of features for preprocessing steps\nSeveral features of the pima task have missing values:\n\ntask_pima$missings()\n\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure  triceps \n       0        0        5      374       11        0        0       35      227 \n\n\nA common approach in such situations is to impute the missing values and to add a missing indicator column as explained in the Impute missing variables post. Suppose we want to use\n\nPipeOpImputeHist on features “glucose”, “mass” and “pressure” which have only few missing values and\nPipeOpImputeMedian on features “insulin” and “triceps” which have much more missing values.\n\nIn the following subsections, we show two approaches to implement this.\n\n1. Consider all features and apply the preprocessing step only to certain features\nUsing the affect_columns argument of a PipeOp to define the variables on which a PipeOp will operate with an appropriate Selector function:\n\n# imputes values based on histogram\nimputer_hist = po(\"imputehist\",\n  affect_columns = selector_name(c(\"glucose\", \"mass\", \"pressure\")))\n# imputes values using the median\nimputer_median = po(\"imputemedian\",\n  affect_columns = selector_name(c(\"insulin\", \"triceps\")))\n# adds an indicator column for each feature with missing values\nmiss_ind = po(\"missind\")\n\nWhen PipeOps are constructed this way, they will perform the specified preprocessing step on the appropriate features and pass all the input features to the subsequent steps:\n\n# no missings in \"glucose\", \"mass\" and \"pressure\"\nimputer_hist$train(list(task_pima))[[1]]$missings()\n\ndiabetes      age  insulin pedigree pregnant  triceps  glucose     mass pressure \n       0        0      374        0        0      227        0        0        0 \n\n# no missings in \"insulin\" and \"triceps\"\nimputer_median$train(list(task_pima))[[1]]$missings()\n\ndiabetes      age  glucose     mass pedigree pregnant pressure  insulin  triceps \n       0        0        5       11        0        0       35        0        0 \n\n\nWe construct a pipeline that combines imputer_hist and imputer_median. Here, imputer_hist will impute the features “glucose”, “mass” and “pressure”, and imputer_median will impute “insulin” and “triceps”. In each preprocessing step, all the input features are passed to the next step. In the end, we obtain a data set without missing values:\n\n# combine the two impuation methods\nimpute_graph = imputer_hist %&gt;&gt;% imputer_median\nimpute_graph$plot(html = FALSE)\n\n\n\n\n\n\n\nimpute_graph$train(task_pima)[[1]]$missings()\n\ndiabetes      age pedigree pregnant  glucose     mass pressure  insulin  triceps \n       0        0        0        0        0        0        0        0        0 \n\n\nThe PipeOpMissInd operator replaces features with missing values with a missing value indicator:\n\nmiss_ind$train(list(task_pima))[[1]]$data()\n\n     diabetes missing_glucose missing_insulin missing_mass missing_pressure missing_triceps\n  1:      pos         present         missing      present          present         present\n  2:      neg         present         missing      present          present         present\n  3:      pos         present         missing      present          present         missing\n  4:      neg         present         present      present          present         present\n  5:      pos         present         present      present          present         present\n ---                                                                                       \n764:      neg         present         present      present          present         present\n765:      neg         present         missing      present          present         present\n766:      neg         present         present      present          present         present\n767:      pos         present         missing      present          present         missing\n768:      neg         present         missing      present          present         present\n\n\nObviously, this step can not be applied to the already imputed data as there are no missing values. If we want to combine the previous two imputation steps with a third step that adds missing value indicators, we would need to PipeOpCopy the data two times and supply the first copy to impute_graph and the second copy to miss_ind using gunion(). Finally, the two outputs can be combined with PipeOpFeatureUnion:\n\nimpute_missind = po(\"copy\", 2) %&gt;&gt;%\n  gunion(list(impute_graph, miss_ind)) %&gt;&gt;%\n  po(\"featureunion\")\nimpute_missind$plot(html = FALSE)\n\n\n\n\n\n\n\n\n\nimpute_missind$train(task_pima)[[1]]$data()\n\n     diabetes age pedigree pregnant glucose mass pressure insulin triceps missing_glucose missing_insulin missing_mass\n  1:      pos  50    0.627        6     148 33.6       72     125      35         present         missing      present\n  2:      neg  31    0.351        1      85 26.6       66     125      29         present         missing      present\n  3:      pos  32    0.672        8     183 23.3       64     125      29         present         missing      present\n  4:      neg  21    0.167        1      89 28.1       66      94      23         present         present      present\n  5:      pos  33    2.288        0     137 43.1       40     168      35         present         present      present\n ---                                                                                                                  \n764:      neg  63    0.171       10     101 32.9       76     180      48         present         present      present\n765:      neg  27    0.340        2     122 36.8       70     125      27         present         missing      present\n766:      neg  30    0.245        5     121 26.2       72     112      23         present         present      present\n767:      pos  47    0.349        1     126 30.1       60     125      29         present         missing      present\n768:      neg  23    0.315        1      93 30.4       70     125      31         present         missing      present\n     missing_pressure missing_triceps\n  1:          present         present\n  2:          present         present\n  3:          present         missing\n  4:          present         present\n  5:          present         present\n ---                                 \n764:          present         present\n765:          present         present\n766:          present         present\n767:          present         missing\n768:          present         present\n\n\n\n\n2. Select the features for each preprocessing step and apply the preprocessing steps to this subset\nWe can use the PipeOpSelect to select the appropriate features and then apply the desired impute PipeOp on them:\n\nimputer_hist_2 = po(\"select\",\n  selector = selector_name(c(\"glucose\", \"mass\", \"pressure\")),\n  id = \"slct1\") %&gt;&gt;% # unique id so we can combine it in a pipeline with other select PipeOps\n  po(\"imputehist\")\n\nimputer_hist_2$plot(html = FALSE)\n\n\n\n\n\n\n\n\n\nimputer_hist_2$train(task_pima)[[1]]$data()\n\n     diabetes glucose mass pressure\n  1:      pos     148 33.6       72\n  2:      neg      85 26.6       66\n  3:      pos     183 23.3       64\n  4:      neg      89 28.1       66\n  5:      pos     137 43.1       40\n ---                               \n764:      neg     101 32.9       76\n765:      neg     122 36.8       70\n766:      neg     121 26.2       72\n767:      pos     126 30.1       60\n768:      neg      93 30.4       70\n\n\n\nimputer_median_2 =\n  po(\"select\", selector = selector_name(c(\"insulin\", \"triceps\")), id = \"slct2\") %&gt;&gt;%\n  po(\"imputemedian\")\n\nimputer_median_2$train(task_pima)[[1]]$data()\n\n     diabetes insulin triceps\n  1:      pos     125      35\n  2:      neg     125      29\n  3:      pos     125      29\n  4:      neg      94      23\n  5:      pos     168      35\n ---                         \n764:      neg     180      48\n765:      neg     125      27\n766:      neg     112      23\n767:      pos     125      29\n768:      neg     125      31\n\n\nTo reproduce the result of the fist example (1.), we need to copy the data four times and apply imputer_hist_2, imputer_median_2 and miss_ind on each of the three copies. The fourth copy is required to select the features without missing values and to append it to the final result. We can do this as follows:\n\nother_features = task_pima$feature_names[task_pima$missings()[-1] == 0]\n\nimputer_missind_2 = po(\"copy\", 4) %&gt;&gt;%\n  gunion(list(imputer_hist_2,\n    imputer_median_2,\n    miss_ind,\n    po(\"select\", selector = selector_name(other_features), id = \"slct3\"))) %&gt;&gt;%\n  po(\"featureunion\")\n\nimputer_missind_2$plot(html = FALSE)\n\n\n\n\n\n\n\n\n\nimputer_missind_2$train(task_pima)[[1]]$data()\n\n     diabetes glucose mass pressure insulin triceps missing_glucose missing_insulin missing_mass missing_pressure\n  1:      pos     148 33.6       72     125      35         present         missing      present          present\n  2:      neg      85 26.6       66     125      29         present         missing      present          present\n  3:      pos     183 23.3       64     125      29         present         missing      present          present\n  4:      neg      89 28.1       66      94      23         present         present      present          present\n  5:      pos     137 43.1       40     168      35         present         present      present          present\n ---                                                                                                             \n764:      neg     101 32.9       76     180      48         present         present      present          present\n765:      neg     122 36.8       70     125      27         present         missing      present          present\n766:      neg     121 26.2       72     112      23         present         present      present          present\n767:      pos     126 30.1       60     125      29         present         missing      present          present\n768:      neg      93 30.4       70     125      31         present         missing      present          present\n     missing_triceps age pedigree pregnant\n  1:         present  50    0.627        6\n  2:         present  31    0.351        1\n  3:         missing  32    0.672        8\n  4:         present  21    0.167        1\n  5:         present  33    2.288        0\n ---                                      \n764:         present  63    0.171       10\n765:         present  27    0.340        2\n766:         present  30    0.245        5\n767:         missing  47    0.349        1\n768:         present  23    0.315        1\n\n\nNote that when there is one input channel, it is automatically copied as many times as needed for the downstream PipeOps. In other words, the code above works also without po(\"copy\", 4):\n\nimputer_missind_3 = gunion(list(imputer_hist_2,\n  imputer_median_2,\n  miss_ind,\n  po(\"select\", selector = selector_name(other_features), id = \"slct3\"))) %&gt;&gt;%\n  po(\"featureunion\")\n\nimputer_missind_3$train(task_pima)[[1]]$data()\n\n     diabetes glucose mass pressure insulin triceps missing_glucose missing_insulin missing_mass missing_pressure\n  1:      pos     148 33.6       72     125      35         present         missing      present          present\n  2:      neg      85 26.6       66     125      29         present         missing      present          present\n  3:      pos     183 23.3       64     125      29         present         missing      present          present\n  4:      neg      89 28.1       66      94      23         present         present      present          present\n  5:      pos     137 43.1       40     168      35         present         present      present          present\n ---                                                                                                             \n764:      neg     101 32.9       76     180      48         present         present      present          present\n765:      neg     122 36.8       70     125      27         present         missing      present          present\n766:      neg     121 26.2       72     112      23         present         present      present          present\n767:      pos     126 30.1       60     125      29         present         missing      present          present\n768:      neg      93 30.4       70     125      31         present         missing      present          present\n     missing_triceps age pedigree pregnant\n  1:         present  50    0.627        6\n  2:         present  31    0.351        1\n  3:         missing  32    0.672        8\n  4:         present  21    0.167        1\n  5:         present  33    2.288        0\n ---                                      \n764:         present  63    0.171       10\n765:         present  27    0.340        2\n766:         present  30    0.245        5\n767:         missing  47    0.349        1\n768:         present  23    0.315        1\n\n\nUsually, po(\"copy\") is required when there are more than one input channels and multiple output channels, and their numbers do not match."
  },
  {
    "objectID": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#branching",
    "href": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#branching",
    "title": "Pipelines, Selectors, Branches",
    "section": "Branching",
    "text": "Branching\nWe can not know if the combination of a learner with this preprocessing graph will benefit from the imputation steps and the added missing value indicators. Maybe it would have been better to just use imputemedian on all the variables. We could investigate this assumption by adding an alternative path to the graph with the mentioned imputemedian. This is possible using the “branch” PipeOp:\n\nimputer_median_3 = po(\"imputemedian\", id = \"simple_median\") # add the id so it does not clash with `imputer_median`\n\nbranches = c(\"impute_missind\", \"simple_median\") # names of the branches\n\ngraph_branch = po(\"branch\", branches) %&gt;&gt;%\n  gunion(list(impute_missind, imputer_median_3)) %&gt;&gt;%\n  po(\"unbranch\")\n\ngraph_branch$plot(html = FALSE)"
  },
  {
    "objectID": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#tuning-the-pipeline",
    "href": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#tuning-the-pipeline",
    "title": "Pipelines, Selectors, Branches",
    "section": "Tuning the pipeline",
    "text": "Tuning the pipeline\nTo finalize the graph, we combine it with a rpart learner:\n\ngraph = graph_branch %&gt;&gt;%\n  lrn(\"classif.rpart\")\n\ngraph$plot(html = FALSE)\n\n\n\n\n\n\n\n\nTo define the parameters to be tuned, we first check the available ones in the graph:\n\nas.data.table(graph$param_set)[, .(id, class, lower, upper, nlevels)]\n\n                              id    class lower upper nlevels\n 1:             branch.selection ParamFct    NA    NA       2\n 2:    imputehist.affect_columns ParamUty    NA    NA     Inf\n 3:  imputemedian.affect_columns ParamUty    NA    NA     Inf\n 4:                missind.which ParamFct    NA    NA       2\n 5:                 missind.type ParamFct    NA    NA       4\n 6:       missind.affect_columns ParamUty    NA    NA     Inf\n 7: simple_median.affect_columns ParamUty    NA    NA     Inf\n 8:             classif.rpart.cp ParamDbl     0     1     Inf\n 9:     classif.rpart.keep_model ParamLgl    NA    NA       2\n10:     classif.rpart.maxcompete ParamInt     0   Inf     Inf\n11:       classif.rpart.maxdepth ParamInt     1    30      30\n12:   classif.rpart.maxsurrogate ParamInt     0   Inf     Inf\n13:      classif.rpart.minbucket ParamInt     1   Inf     Inf\n14:       classif.rpart.minsplit ParamInt     1   Inf     Inf\n15: classif.rpart.surrogatestyle ParamInt     0     1       2\n16:   classif.rpart.usesurrogate ParamInt     0     2       3\n17:           classif.rpart.xval ParamInt     0   Inf     Inf\n\n\nWe decide to jointly tune the \"branch.selection\", \"classif.rpart.cp\" and \"classif.rpart.minbucket\" hyperparameters:\n\nsearch_space = ps(\n  branch.selection = p_fct(c(\"impute_missind\", \"simple_median\")),\n  classif.rpart.cp = p_dbl(0.001, 0.1),\n  classif.rpart.minbucket = p_int(1, 10))\n\nIn order to tune the graph, it needs to be converted to a learner:\n\ngraph_learner = as_learner(graph)\n\ncv3 = rsmp(\"cv\", folds = 3)\n\ncv3$instantiate(task_pima) # to generate folds for cross validation\n\ninstance = tune(\n  tuner = tnr(\"random_search\"),\n  task = task_pima,\n  learner = graph_learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  search_space = search_space,\n  term_evals = 5)\n\nas.data.table(instance$archive, unnest = NULL, exclude_columns = c(\"x_domain\", \"uhash\", \"resample_result\"))\n\n   branch.selection classif.rpart.cp classif.rpart.minbucket classif.ce runtime_learners           timestamp batch_nr\n1:    simple_median       0.02172886                       2  0.2799479            2.774 2023-11-02 16:33:12        1\n2:   impute_missind       0.07525939                       1  0.2760417            2.701 2023-11-02 16:33:25        2\n3:   impute_missind       0.09207969                       3  0.2773438            1.031 2023-11-02 16:33:36        3\n4:   impute_missind       0.03984117                       6  0.2721354            2.184 2023-11-02 16:33:47        4\n5:   impute_missind       0.09872643                       7  0.2773438            2.507 2023-11-02 16:33:57        5\n   warnings errors\n1:        0      0\n2:        0      0\n3:        0      0\n4:        0      0\n5:        0      0\n\n\nThe best performance in this short tuned experiment was achieved with:\n\ninstance$result\n\n   branch.selection classif.rpart.cp classif.rpart.minbucket learner_param_vals  x_domain classif.ce\n1:   impute_missind       0.03984117                       6          &lt;list[9]&gt; &lt;list[3]&gt;  0.2721354"
  },
  {
    "objectID": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#conclusion",
    "href": "gallery/pipelines/2020-04-23-pipelines-selectors-branches/index.html#conclusion",
    "title": "Pipelines, Selectors, Branches",
    "section": "Conclusion",
    "text": "Conclusion\nThis post shows ways on how to specify features on which preprocessing steps are to be performed. In addition it shows how to create alternative paths in the learner graph. The preprocessing steps that can be used are not limited to imputation. Check the list of available PipeOp."
  },
  {
    "objectID": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html",
    "href": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html",
    "title": "Liver Patient Classification Based on Diagnostic Measures",
    "section": "",
    "text": "The following examples were created as part of the Introduction to Machine Learning Lecture at LMU Munich. The goal of the project was to create and compare one or several machine learning pipelines for the problem at hand together with exploratory analysis and an exposition of results. The posts were contributed to the mlr3gallery by the authors and edited for better legibility by the editor. We want to thank the authors for allowing us to publish their results. Note, that correctness of the results can not be guaranteed.\n\n\nThis tutorial assumes familiarity with the basics of mlr3tuning and mlr3pipelines. Consult the mlr3book if some aspects are not fully understandable. We load the most important packages for this example.\n\nlibrary(mlr3verse)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(DataExplorer)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nNote, that expensive calculations are pre-saved in rds files in this tutorial to save computational time.\nMachine learning (ML), a branch of both computer science and statistics, in conjunction with new computing technologies has been transforming research and industries across the board over the past decade. A prime example for this is the healthcare industry, where applications of ML, as well as artificial intelligence in general, have become more and more popular in recent years. One very frequently researched and applied use of ML in the medical field is the area of disease identification and diagnosis. ML technologies have shown potential in detecting anomalies and diseases through pattern recognition, even though an entirely digital diagnosis by a computer is probably still something for the far future. However, suitable and reliable models estimating the risk of diseases could help real doctors make quicker and better decisions today already. In this use case we examined machine learning algorithms and learners for the specific application of liver disease detection. The task is therefore a binary classification task to predict whether a patient has liver disease or not based on some common diagnostic measurements. This report is organized as follows. Section 1 introduces the data and section 2 provides more in-depth data exploration. Section 3 presents learners and their hyperparameter tuning while section 4, dealing with model fitting and benchmarking, presents results and conclusions."
  },
  {
    "objectID": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html#prerequisites",
    "href": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html#prerequisites",
    "title": "Liver Patient Classification Based on Diagnostic Measures",
    "section": "",
    "text": "This tutorial assumes familiarity with the basics of mlr3tuning and mlr3pipelines. Consult the mlr3book if some aspects are not fully understandable. We load the most important packages for this example.\n\nlibrary(mlr3verse)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(DataExplorer)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nNote, that expensive calculations are pre-saved in rds files in this tutorial to save computational time.\nMachine learning (ML), a branch of both computer science and statistics, in conjunction with new computing technologies has been transforming research and industries across the board over the past decade. A prime example for this is the healthcare industry, where applications of ML, as well as artificial intelligence in general, have become more and more popular in recent years. One very frequently researched and applied use of ML in the medical field is the area of disease identification and diagnosis. ML technologies have shown potential in detecting anomalies and diseases through pattern recognition, even though an entirely digital diagnosis by a computer is probably still something for the far future. However, suitable and reliable models estimating the risk of diseases could help real doctors make quicker and better decisions today already. In this use case we examined machine learning algorithms and learners for the specific application of liver disease detection. The task is therefore a binary classification task to predict whether a patient has liver disease or not based on some common diagnostic measurements. This report is organized as follows. Section 1 introduces the data and section 2 provides more in-depth data exploration. Section 3 presents learners and their hyperparameter tuning while section 4, dealing with model fitting and benchmarking, presents results and conclusions."
  },
  {
    "objectID": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html#univariate-distribution",
    "href": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html#univariate-distribution",
    "title": "Liver Patient Classification Based on Diagnostic Measures",
    "section": "Univariate distribution",
    "text": "Univariate distribution\nNext, we looked into the univariate distribution of each of the variables. We began with the target and the only discrete feature, gender, which are both binary.\n\n\n\n\n\n\n\n\n\nThe distribution of the target variable is quite imbalanced, as the barplot shows: the number of patients with and without liver disease equals 416 and 167, respectively. The underrepresentation of a class, in our case those without liver disease, might worsen the performance of ML models. In order to examine this, we additionally fitted the models on a dataset where we randomly over-sampled the minority class, resulting in a perfectly balanced dataset. Furthermore, we applied stratified sampling to ensure the proportion of the classes is maintained during cross-validation.\nThe only discrete feature gender is quite imbalanced, too. As one can see in the next section, this proportion is also observed within each target class. Prior to that, we looked into the distributions of the metric features.\n\n\n\n\n\n\n\n\n\nStrikingly, some of the metric features are extremely right-skewed and contain several extreme values. To reduce the impact of outliers and since some models assume normality of features, we log-transformed these variables."
  },
  {
    "objectID": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html#features-by-class",
    "href": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html#features-by-class",
    "title": "Liver Patient Classification Based on Diagnostic Measures",
    "section": "Features by class",
    "text": "Features by class\nTo picture the relationship between the target and the features, we analysed the distributions of the features by class. First, we examined the discrete feature gender.\n\n\n\n\n\n\n\n\n\nThe percentage of males in the “disease” class is slightly higher, but overall the difference is small. Besides that, the gender imbalance can be observed in both classes, as we mentioned before. To see the differences in metric features, we compare the following boxplots, where right-skewed features are not log-transformed yet.\n\n\n\n\n\n\n\n\n\nExcept for the total amount of protein, for each feature we obtain differences between the median values of the two classes. Notably, in the case of strongly right-skewed features the “disease” class contains far more extreme values than the “no disease” class, which is probably because of its larger size. This effect is weakened by log-transforming such features, as can be seen in the boxplots below. Moreover, the dispersion in the class “disease” is greater for these features, as the length of the boxes indicates. Overall, the features seem to be correlated to the target, so it makes sense to use them for this task and model their relationship with the target.\n\n\n\n\n\n\n\n\n\nNote, that the same result can be achieved more easily by using PipeOpMutate from mlr3pipelines. This PipeOp provides a smooth implementation to scale numeric features for mlr3 tasks."
  },
  {
    "objectID": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html#correlation",
    "href": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html#correlation",
    "title": "Liver Patient Classification Based on Diagnostic Measures",
    "section": "Correlation",
    "text": "Correlation\nAs we mentioned in the description of the data, there are features that are indirectly measured by another one. This suggests that they are highly correlated. Some of the models we want to compare assume independent features or have problems with multicollinearity. Therefore, we checked for correlations between features.\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nWarning in cor(data, use = method[1], method = method[2]): the standard deviation is zero\n\n\n\n\n\n\n\n\n\nFor four of the pairs we obtained a very high correlation coefficient. Looking at these features, it is clear they affect each other. As the complexity of the model should be minimized and due to multicollinearity concerns, we decided to take only one of each pair. When deciding on which features to keep, we chose those that are more specific and relevant regarding liver disease. Therefore, we chose albumin over the ratio between albumin and globulin and also over the total amount of protein. The same argument applies to using the amount of direct bilirubin instead of the total amount of bilirubin. Regarding aspartate transaminase and alanine transaminase, it was not clear which one to use, especially since we have no given real world implementation for the task and no medical training. Since we did not notice any fundamental differences in the data for these two features, we arbitrarily chose aspartate transaminase."
  },
  {
    "objectID": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html#final-dataset",
    "href": "gallery/pipelines/2020-09-11-liver-patient-classification/index.html#final-dataset",
    "title": "Liver Patient Classification Based on Diagnostic Measures",
    "section": "Final Dataset",
    "text": "Final Dataset\n\n## Reducing, transforming and scaling dataset\nilpd = ilpd %&gt;%\n  select(-total_bilirubin, -alanine_transaminase, -total_protein,\n         -albumin_globulin_ratio) %&gt;%\n  mutate(\n    # Recode gender\n    gender = as.numeric(ifelse(gender == \"Female\", 1, 0)),\n     # Remove labels for class\n    diseased = factor(ifelse(diseased == \"yes\", 1, 0)),\n     # Log for features with skewed distributions\n    alkaline_phosphatase = log(alkaline_phosphatase),\n    aspartate_transaminase = log(aspartate_transaminase),\n    direct_bilirubin = log(direct_bilirubin)\n  )\n\npo_scale = po(\"scale\")\npo_scale$param_set$values$affect_columns =\n  selector_name(c(\"age\", \"direct_bilirubin\", \"alkaline_phosphatase\",\n  \"aspartate_transaminase\", \"albumin\"))\n\nLastly, we standardized all metric features, as different ranges and units might weigh features. This is especially important for the k-NN model. The following table shows the final dataset and the transformations we applied. Note: Different from log or other transformation, scaling depends on the data themselves. Scaling data before data are split leads to data leakage, were information of train and test set are shared. As Data Leakage causes higher performance, scaling should always be applied in each data split induced by the ML workflow separately. Therefore we strongly recommend the usage of PipeOpScale in such cases.\n\n\n\nVariable\nTransformation\n\n\n\n\nage\nscaled\n\n\nalbumin\nscaled\n\n\nalkaline_phosphatase\nscaled and log-transformed\n\n\naspartate_transaminase\nscaled and log-transformed\n\n\ndirect_bilirubin\nscaled and log-transformed\n\n\ndiseased\nnone\n\n\ngender\nnone"
  },
  {
    "objectID": "terminators.html",
    "href": "terminators.html",
    "title": "Terminators",
    "section": "",
    "text": "Stop tuning when a performance level is reached.\n\nlibrary(mlr3verse)\n\n# load terminator and set performance level\nterminator = trm(\"perf_reached\", level = 0.25)\n\n# load tuner\ntuner = tnr(\"random_search\", batch_size = 10)\n\n# retrieve task\ntask = tsk(\"pima\")\n\n# load learner and set search space\nlearner = lts(lrn(\"classif.rpart\"))\n\n# set instance\ninstance = ti(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = terminator\n)\n\n# hyperparameter tuning on the pima data set\ntuner$optimize(instance)\n\n# best performing hyperparameter configuration\ninstance$result\n\n# fit final model on complete data set\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n\nprint(learner)"
  },
  {
    "objectID": "terminators.html#example-usage",
    "href": "terminators.html#example-usage",
    "title": "Terminators",
    "section": "",
    "text": "Stop tuning when a performance level is reached.\n\nlibrary(mlr3verse)\n\n# load terminator and set performance level\nterminator = trm(\"perf_reached\", level = 0.25)\n\n# load tuner\ntuner = tnr(\"random_search\", batch_size = 10)\n\n# retrieve task\ntask = tsk(\"pima\")\n\n# load learner and set search space\nlearner = lts(lrn(\"classif.rpart\"))\n\n# set instance\ninstance = ti(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = terminator\n)\n\n# hyperparameter tuning on the pima data set\ntuner$optimize(instance)\n\n# best performing hyperparameter configuration\ninstance$result\n\n# fit final model on complete data set\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n\nprint(learner)"
  },
  {
    "objectID": "gallery-all-pipelines.html",
    "href": "gallery-all-pipelines.html",
    "title": "Build Pipelines",
    "section": "",
    "text": "Build Pipelines\n\n\n  \n    \n      Tuning a Complex Graph\n      Tune a preprocessing pipeline and multiple tuners at once.\n\n      2021-02-03 - Lennart Schneider\n    \n  \n    \n      Liver Patient Classification Based on Diagnostic Measures\n      Tune and benchmark pipelines.\n\n      2020-09-11 - Julian Lange, Jae-Eun Nam, Viet Tran, Simon Wiegrebe, Henri Funk (Editor)\n    \n  \n    \n      Target Transformations via Pipelines\n      Transform the target variable.\n\n      2020-06-15 - Lennart Schneider\n    \n  \n    \n      Tuning a Stacked Learner\n      Tune a multilevel stacking model.\n\n      2020-04-27 - Milan Dragicevic, Giuseppe Casalicchio\n    \n  \n    \n      A Pipeline for the Titanic Data Set - Advanced\n      Create new features and impute missing values with a pipeline.\n\n      2020-04-27 - Florian Pfisterer\n    \n  \n    \n      Pipelines, Selectors, Branches\n      Build a preprocessing pipeline with branching.\n\n      2020-04-23 - Milan Dragicevic, Giuseppe Casalicchio\n    \n  \n    \n      Regression Chains\n      Handle multi-target regression with regression chains.\n\n      2020-04-18 - Lennart Schneider\n    \n  \n    \n      A Pipeline for the Titanic Data Set - Basics\n      Build a graph.\n\n      2020-03-12 - Florian Pfisterer\n    \n  \n    \n      Tuning Over Multiple Learners\n      Tune over multiple learners for a single task.\n\n      2020-02-01 - Jakob Richter, Bernd Bischl\n    \n  \n\nNo matching items"
  },
  {
    "objectID": "gallery-all-exercises.html",
    "href": "gallery-all-exercises.html",
    "title": "Exercise Collection for Practice and Learning",
    "section": "",
    "text": "Exercise Collection for Practice and Learning\n\n\n  \n    Introduction\n    \n      \n        \n          Train Predict Evaluate Basics Solution\n          Introduction to German Credit dataset and classification. Train predict and evaluate a logistic regression learner with hold-out split.\n\n          2025-04-23 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Resampling Solution\n          Use 5-fold cross validation to evaluate logistic regression and knn learner on german credit set.\n\n          2025-04-24 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Benchmarking Solution\n          Hyperparameter tuning and benchmarking on german credit task.\n\n          2025-04-25 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Tree Methods Solution\n          Use, plot and benchmark classification tree and random forest on german credit set.\n\n          2025-04-30 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n    \n  \n\n  \n    Tuning\n    \n      \n        \n          Advanced Resampling with Custom Measure Solution\n          Use stratified resampling to evaluate the german credit set and blocking for BreastCancer set. Define custom measures in mlr3 and use them to evaluate a model on the mtcars task.\n\n          2025-05-07 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Tuning\n          Optimize hyperparameters for k-NN and SVM classifier on german credit set.\n\n          2025-05-14 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Nested Resampling\n          Estimate the generalization error of a k-NN model on german credit set via nested resampling.\n\n          2025-05-15 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Deep dive into Bayesian Optimization\n          Use Bayesian optimization (BO) using `bbotk` and `mlr3mbo` for general black box optimization problems, and more specifically, hyperparameter optimization (HPO).\n\n          2025-05-16 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n    \n  \n\n  \n    Feature Preprocessing\n    \n      \n        \n          Encoding and Scaling\n          Create a pipeline to do feature preprocessing (one-hot-encoding, Yeo-Johnson transformation) for the german credit task.\n\n          2025-05-21 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Impact of Encoding\n          Construct pipelines for benchmark experiments on kc_housing set.\n\n          2025-05-22 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Xgboost\n          Optimize hyperparameters of xgboost for german credit task.\n\n          2025-05-23 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n    \n  \n\n  \n    Advanced Feature Preprocessing\n    \n      \n        \n          Filter\n          Use filters in a mlr3 pipeline\n\n          2025-05-28 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Feature Selection\n          Select features from the german credit set and evaluate model performance.\n\n          2025-05-29 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Filters\n          Use pipelines for efficient pre-processing and model training on a the kc_housing task.\n\n          2025-05-30 - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n    \n  \n\n  \n    Performance Evaluation\n    \n      \n        \n          Benchmarking Hypothesis\n          Benchmark models in multiple scenarios, using hypothesis tests as an additional diagnostic tool to make the benchmark more rigorous.\n\n           - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n    \n  \n\n  \n    Advanced Performance Evaluation\n    \n      \n        \n          Calibration with probably\n          Learn the basics of `tidymodels` for supervised learning, assess if a model is well-calibrated, and calibrate it with `probably`.\n\n           - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Calibration with mlr3 V2\n          Learn the basics of `tidymodels` for supervised learning, assess if a model is well-calibrated, and calibrate it with `mlr3`.\n\n           - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Calibration with mlr3\n          Learn the basics of `tidymodels` for supervised learning, assess if a model is well-calibrated, and calibrate it with `mlr3`.\n\n           - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n    \n  \n\n  \n    Ensembles Stacking\n    \n      \n        \n          Model Averaging\n          Learn how to do ensembling and model averaging with `mlr3pipelines` and optimizing weights with `bbotk`.\n\n           - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Model Averaging\n          Do ensembling and model averaging on german credit set.\n\n           - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n        \n          Greedy Ensemble Selection and Stacking\n          Implement greedy ensemble selection and stacking on german credit set.\n\n           - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n    \n  \n\n  \n    Parallelization\n    \n      \n        \n          Parallelization\n          Set up a large scale benchmark experiment with parallelization\n\n           - Giuseppe Casalicchio, Essential Data Science Training GmbH\n        \n      \n    \n  \n\n  \n    Imputation\n    \n      \n        \n          Imputation\n          Learn the basics of imputation (i.e. filling in missing data) with `mlr3pipelines`.\n\n           - Fiona Ewald, Essential Data Science Training GmbH\n        \n      \n        \n          Imputation Homework\n          Learn the basics of imputation (i.e. filling in missing data) with `mlr3pipelines`.\n\n           - Fiona Ewald, Essential Data Science Training GmbH\n        \n      \n    \n  \n\n  \n    Imbalanced\n    \n      \n        \n          Imabalanced ROC-Analysis threshold Tuning\n          Train a classifier on German Credit set and tune the output of a probabilistic model with ROC threshold analysis.\n\n           - Fiona Ewald, Essential Data Science Training GmbH\n        \n      \n        \n          Pipeline Imbalanced Classification\n          Learn how to deal with imbalanced classification problems.\n\n           - Fiona Ewald, Essential Data Science Training GmbH\n        \n      \n    \n  \n\nNo matching items"
  },
  {
    "objectID": "resamplings.html",
    "href": "resamplings.html",
    "title": "Resamplings",
    "section": "",
    "text": "Resamplings split the observations multiple times into two sets: training and test. The former is used to fit the model, the latter is used to evaluate the predictions. The Resampling objects provide an abstraction for this procedure while respecting stratification as well as grouping/blocking if this is required by the Task.\nIf only a single split is required (i.e., a holdout split), the partition() function provides a single split into training and test set.\n\n\n\n\n\n\n\n\nFit a Random Forest on the Wisconsin Breast Cancer Data Set using a 3-fold cross validation.\n\nlibrary(\"mlr3verse\")\n\nLoading required package: mlr3\n\n# retrieve the task\ntask = tsk(\"breast_cancer\")\n\n# retrieve a learner\nlearner = lrn(\"classif.ranger\")\n\n# retrieve resampling strategy\nresampling = rsmp(\"cv\", folds = 3)\n\n# perform resampling\nrr = resample(task, learner, resampling)\nrr\n\n\n── &lt;ResampleResult&gt; with 3 resampling iterations ───────────────────────────────\n       task_id     learner_id resampling_id iteration     prediction_test\n breast_cancer classif.ranger            cv         1 &lt;PredictionClassif&gt;\n breast_cancer classif.ranger            cv         2 &lt;PredictionClassif&gt;\n breast_cancer classif.ranger            cv         3 &lt;PredictionClassif&gt;\n warnings errors\n        0      0\n        0      0\n        0      0"
  },
  {
    "objectID": "resamplings.html#example-usage",
    "href": "resamplings.html#example-usage",
    "title": "Resamplings",
    "section": "",
    "text": "Fit a Random Forest on the Wisconsin Breast Cancer Data Set using a 3-fold cross validation.\n\nlibrary(\"mlr3verse\")\n\nLoading required package: mlr3\n\n# retrieve the task\ntask = tsk(\"breast_cancer\")\n\n# retrieve a learner\nlearner = lrn(\"classif.ranger\")\n\n# retrieve resampling strategy\nresampling = rsmp(\"cv\", folds = 3)\n\n# perform resampling\nrr = resample(task, learner, resampling)\nrr\n\n\n── &lt;ResampleResult&gt; with 3 resampling iterations ───────────────────────────────\n       task_id     learner_id resampling_id iteration     prediction_test\n breast_cancer classif.ranger            cv         1 &lt;PredictionClassif&gt;\n breast_cancer classif.ranger            cv         2 &lt;PredictionClassif&gt;\n breast_cancer classif.ranger            cv         3 &lt;PredictionClassif&gt;\n warnings errors\n        0      0\n        0      0\n        0      0"
  },
  {
    "objectID": "learners.html",
    "href": "learners.html",
    "title": "Learners",
    "section": "",
    "text": "To keep the dependencies on other packages reasonable, the base package mlr3 only ships with with regression and classification trees from the rpart package and some learners for debugging. A subjective selection of implementations for essential ML algorithms can be found in mlr3learners package. Survival learners are provided by mlr3proba, cluster learners via mlr3cluster. Additional learners, including some learners which are not yet to be considered stable or which are not available on CRAN, are connected via the mlr3extralearners package. For neural networks, see the mlr3torch extension.\n\n\n\n\n\n\n\n\nFit a classification tree on the Wisconsin Breast Cancer Data Set and predict on left-out observations.\n\nlibrary(\"mlr3verse\")\n\nRegistered S3 methods overwritten by 'mlr3viz':\n  method                    from     \n  autoplot.LearnerSurvCoxPH mlr3proba\n  plot.LearnerSurvCoxPH     mlr3proba\n\n# retrieve the task\ntask = tsk(\"breast_cancer\")\n\n# split into two partitions\nsplit = partition(task)\n\n# retrieve a learner\nlearner = lrn(\"classif.rpart\", keep_model = TRUE, predict_type = \"prob\")\n\n# fit decision tree\nlearner$train(task, split$train)\n\n# access learned model\nlearner$model\n\nn= 458 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 458 161 benign (0.351528384 0.648471616)  \n   2) bare_nuclei=3,4,5,6,7,8,9,10 171  23 malignant (0.865497076 0.134502924)  \n     4) cell_shape=3,4,5,6,7,8,9,10 149   7 malignant (0.953020134 0.046979866) *\n     5) cell_shape=1,2 22   6 benign (0.272727273 0.727272727)  \n      10) cl_thickness=4,5,6,7,8,9,10 8   2 malignant (0.750000000 0.250000000) *\n      11) cl_thickness=1,2,3 14   0 benign (0.000000000 1.000000000) *\n   3) bare_nuclei=1,2 287  13 benign (0.045296167 0.954703833)  \n     6) cell_size=4,5,6,7,8,9,10 14   2 malignant (0.857142857 0.142857143) *\n     7) cell_size=1,2,3 273   1 benign (0.003663004 0.996336996) *\n\n# predict on data frame with new data\npredictions = learner$predict_newdata(task$data(split$test))\n\n# predict on subset of the task\npredictions = learner$predict(task, split$test)\n\n# inspect predictions\npredictions\n\n\n── &lt;PredictionClassif&gt; for 225 observations: ───────────────────────────────────\n row_ids     truth  response prob.malignant prob.benign\n       5    benign    benign    0.003663004  0.99633700\n       9    benign    benign    0.003663004  0.99633700\n      14    benign    benign    0.000000000  1.00000000\n     ---       ---       ---            ---         ---\n     671    benign    benign    0.003663004  0.99633700\n     677    benign    benign    0.003663004  0.99633700\n     681 malignant malignant    0.953020134  0.04697987\n\npredictions$score(msr(\"classif.auc\"))\n\nclassif.auc \n  0.9780656 \n\nautoplot(predictions, type = \"roc\")\n\nWarning in ggplot2::fortify(object, raw_curves = raw_curves, reduce_points = reduce_points): Arguments in `...` must be used.\n✖ Problematic argument:\n• raw_curves = raw_curves\nℹ Did you misspell an argument name?"
  },
  {
    "objectID": "learners.html#example-usage",
    "href": "learners.html#example-usage",
    "title": "Learners",
    "section": "",
    "text": "Fit a classification tree on the Wisconsin Breast Cancer Data Set and predict on left-out observations.\n\nlibrary(\"mlr3verse\")\n\nRegistered S3 methods overwritten by 'mlr3viz':\n  method                    from     \n  autoplot.LearnerSurvCoxPH mlr3proba\n  plot.LearnerSurvCoxPH     mlr3proba\n\n# retrieve the task\ntask = tsk(\"breast_cancer\")\n\n# split into two partitions\nsplit = partition(task)\n\n# retrieve a learner\nlearner = lrn(\"classif.rpart\", keep_model = TRUE, predict_type = \"prob\")\n\n# fit decision tree\nlearner$train(task, split$train)\n\n# access learned model\nlearner$model\n\nn= 458 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 458 161 benign (0.351528384 0.648471616)  \n   2) bare_nuclei=3,4,5,6,7,8,9,10 171  23 malignant (0.865497076 0.134502924)  \n     4) cell_shape=3,4,5,6,7,8,9,10 149   7 malignant (0.953020134 0.046979866) *\n     5) cell_shape=1,2 22   6 benign (0.272727273 0.727272727)  \n      10) cl_thickness=4,5,6,7,8,9,10 8   2 malignant (0.750000000 0.250000000) *\n      11) cl_thickness=1,2,3 14   0 benign (0.000000000 1.000000000) *\n   3) bare_nuclei=1,2 287  13 benign (0.045296167 0.954703833)  \n     6) cell_size=4,5,6,7,8,9,10 14   2 malignant (0.857142857 0.142857143) *\n     7) cell_size=1,2,3 273   1 benign (0.003663004 0.996336996) *\n\n# predict on data frame with new data\npredictions = learner$predict_newdata(task$data(split$test))\n\n# predict on subset of the task\npredictions = learner$predict(task, split$test)\n\n# inspect predictions\npredictions\n\n\n── &lt;PredictionClassif&gt; for 225 observations: ───────────────────────────────────\n row_ids     truth  response prob.malignant prob.benign\n       5    benign    benign    0.003663004  0.99633700\n       9    benign    benign    0.003663004  0.99633700\n      14    benign    benign    0.000000000  1.00000000\n     ---       ---       ---            ---         ---\n     671    benign    benign    0.003663004  0.99633700\n     677    benign    benign    0.003663004  0.99633700\n     681 malignant malignant    0.953020134  0.04697987\n\npredictions$score(msr(\"classif.auc\"))\n\nclassif.auc \n  0.9780656 \n\nautoplot(predictions, type = \"roc\")\n\nWarning in ggplot2::fortify(object, raw_curves = raw_curves, reduce_points = reduce_points): Arguments in `...` must be used.\n✖ Problematic argument:\n• raw_curves = raw_curves\nℹ Did you misspell an argument name?"
  },
  {
    "objectID": "blogroll.html",
    "href": "blogroll.html",
    "title": "Blogroll",
    "section": "",
    "text": "https://www.r-bloggers.com/"
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Support",
    "section": "",
    "text": "Questions regarding the software can be asked on Stack Overflow, where you can use the mlr3 tag."
  },
  {
    "objectID": "support.html#stackoverflow",
    "href": "support.html#stackoverflow",
    "title": "Support",
    "section": "",
    "text": "Questions regarding the software can be asked on Stack Overflow, where you can use the mlr3 tag."
  },
  {
    "objectID": "support.html#github",
    "href": "support.html#github",
    "title": "Support",
    "section": "GitHub",
    "text": "GitHub\nFor bug reports, suggestions, or feature requests please raise an issue in the corresponding GitHub repository. Your issue will be addressed sooner if you provide a reproducible example - e.g. using the reprex R package - and if you open it in the correct repository."
  },
  {
    "objectID": "support.html#mattermost",
    "href": "support.html#mattermost",
    "title": "Support",
    "section": "Mattermost",
    "text": "Mattermost\nYou can join our public Mattermost channel."
  },
  {
    "objectID": "gallery-all-optimization.html",
    "href": "gallery-all-optimization.html",
    "title": "Optimize Models",
    "section": "",
    "text": "Optimize Models\n\n\n  \n    \n      Recursive Feature Elimination on the Sonar Data Set\n      Utilize the built-in feature importance of models.\n\n      2023-02-07 - Marc Becker\n    \n  \n    \n      Shadow Variable Search on the Pima Indian Diabetes Data Set\n      Run a feature selection with permutated features.\n\n      2023-02-01 - Marc Becker, Sebastian Fischer\n    \n  \n    \n      Default Hyperparameter Configuration\n      Run the default hyperparameter configuration of learners as a baseline.\n\n      2023-01-31 - Marc Becker\n    \n  \n    \n      Hyperband Series - Data Set Subsampling\n      Optimize the hyperparameters of a Support Vector Machine with Hyperband.\n\n      2023-01-16 - Marc Becker, Sebastian Fischer\n    \n  \n    \n      Hotstarting\n      Resume the training of learners.\n\n      2023-01-16 - Marc Becker, Sebastian Fischer\n    \n  \n    \n      Hyperband Series - Iterative Training\n      Optimize the hyperparameters of an XGBoost model with Hyperband.\n\n      2023-01-15 - Marc Becker, Sebastian Fischer\n    \n  \n    \n      Practical Tuning Series - Tuning and Parallel Processing\n      Run various jobs in mlr3 in parallel.\n\n      2021-03-12 - Marc Becker, Theresa Ullmann, Michel Lang, Bernd Bischl, Jakob Richter, Martin Binder\n    \n  \n    \n      Practical Tuning Series - Build an Automated Machine Learning System\n      Implement a simple automated machine learning system.\n\n      2021-03-11 - Marc Becker, Theresa Ullmann, Michel Lang, Bernd Bischl, Jakob Richter, Martin Binder\n    \n  \n    \n      Practical Tuning Series - Tune a Preprocessing Pipeline\n      Build a simple preprocessing pipeline and tune it.\n\n      2021-03-10 - Marc Becker, Theresa Ullmann, Michel Lang, Bernd Bischl, Jakob Richter, Martin Binder\n    \n  \n    \n      Practical Tuning Series - Tune a Support Vector Machine\n      Optimize the hyperparameters of a support vector machine.\n\n      2021-03-09 - Marc Becker, Theresa Ullmann, Michel Lang, Bernd Bischl, Jakob Richter, Martin Binder\n    \n  \n    \n      Integer Hyperparameters in Tuners for Real-valued Search Spaces\n      Optimize integer hyperparameters with tuners that can only propose real numbers.\n\n      2021-01-19 - Marc Becker\n    \n  \n    \n      Threshold Tuning for Classification Tasks\n      Adjust the probability thresholds of classes.\n\n      2020-10-14 - Florian Pfisterer\n    \n  \n\nNo matching items"
  },
  {
    "objectID": "tuners.html",
    "href": "tuners.html",
    "title": "Tuners",
    "section": "",
    "text": "Popular black-box optimization techniques are implemented in the bbotk package. The corresponding connectors to for tuning hyperparameters of learners or pipelines reside as Tuner objects in package mlr3tuning. Additionally, packages mlr3hyperband and mlr3mbo provide some modern and sophisticated approaches.\nAll tuners operator on box-constrained tuning spaces which have to be defined by the user. Some popular spaces from literature are readily available as tuning spaces.\n\n\n\n\n\n\n\n\nTune the hyperparameters of a classification tree on the Palmer Penguins data set with random search.\n\nlibrary(mlr3verse)\n\n# retrieve task\ntask = tsk(\"penguins\")\n\n# load learner and set search space\nlearner = lrn(\"classif.rpart\",\n  cp = to_tune(1e-04, 1e-1, logscale = TRUE),\n  minsplit = to_tune(2, 128, logscale = TRUE)\n)\n\n# load tuner and set batch size\ntuner = tnr(\"random_search\", batch_size = 10)\n\n# hyperparameter tuning on the palmer penguins data set\ninstance = tune(\n  tuner = tuner,\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  term_evals = 50\n)\n\n# best performing hyperparameter configuration\ninstance$result\n\n          cp  minsplit learner_param_vals  x_domain classif.ce\n       &lt;num&gt;     &lt;num&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n1: -7.488084 0.7934714          &lt;list[3]&gt; &lt;list[2]&gt; 0.05217391\n\n# surface plot\nautoplot(instance, type = \"surface\")\n\n\n\n\n\n\n\n# fit final model on complete data set\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n\nprint(learner)\n\n\n── &lt;LearnerClassifRpart&gt; (classif.rpart): Classification Tree ──────────────────\n• Model: rpart\n• Parameters: cp=0.0005597, minsplit=2, xval=0\n• Packages: mlr3 and rpart\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, factor, and ordered\n• Encapsulation: none (fallback: -)\n• Properties: importance, missings, multiclass, selected_features, twoclass,\nand weights\n• Other settings: use_weights = 'use'"
  },
  {
    "objectID": "tuners.html#example-usage",
    "href": "tuners.html#example-usage",
    "title": "Tuners",
    "section": "",
    "text": "Tune the hyperparameters of a classification tree on the Palmer Penguins data set with random search.\n\nlibrary(mlr3verse)\n\n# retrieve task\ntask = tsk(\"penguins\")\n\n# load learner and set search space\nlearner = lrn(\"classif.rpart\",\n  cp = to_tune(1e-04, 1e-1, logscale = TRUE),\n  minsplit = to_tune(2, 128, logscale = TRUE)\n)\n\n# load tuner and set batch size\ntuner = tnr(\"random_search\", batch_size = 10)\n\n# hyperparameter tuning on the palmer penguins data set\ninstance = tune(\n  tuner = tuner,\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  term_evals = 50\n)\n\n# best performing hyperparameter configuration\ninstance$result\n\n          cp  minsplit learner_param_vals  x_domain classif.ce\n       &lt;num&gt;     &lt;num&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n1: -7.488084 0.7934714          &lt;list[3]&gt; &lt;list[2]&gt; 0.05217391\n\n# surface plot\nautoplot(instance, type = \"surface\")\n\n\n\n\n\n\n\n# fit final model on complete data set\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n\nprint(learner)\n\n\n── &lt;LearnerClassifRpart&gt; (classif.rpart): Classification Tree ──────────────────\n• Model: rpart\n• Parameters: cp=0.0005597, minsplit=2, xval=0\n• Packages: mlr3 and rpart\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, factor, and ordered\n• Encapsulation: none (fallback: -)\n• Properties: importance, missings, multiclass, selected_features, twoclass,\nand weights\n• Other settings: use_weights = 'use'"
  },
  {
    "objectID": "tuning_spaces.html",
    "href": "tuning_spaces.html",
    "title": "Tuning Spaces",
    "section": "",
    "text": "The package mlr3tuningspaces ships with some predefined tuning spaces for hyperparameter optimization. See the respective manual page for the article from which they were extracted.\n\n\n\n\n\n\n\n\nLoad a tuning space for the classification tree learner from the Bischl et al. (2021) article.\n\nlibrary(mlr3verse)\n\n# load learner and set search space\nlearner = lts(lrn(\"classif.rpart\"))\n\n# retrieve task\ntask = tsk(\"pima\")\n\n# load tuner and set batch size\ntuner = tnr(\"random_search\", batch_size = 10)\n\n# hyperparameter tuning on the pima data set\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 5, batch_size = 25),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n)\n\n# best performing hyperparameter configuration\ninstance$result\n\n         cp minbucket minsplit learner_param_vals  x_domain classif.ce\n      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n1: -9.21034  2.087194 4.859812          &lt;list[4]&gt; &lt;list[3]&gt;  0.2304688\n\n# fit final model on complete data set\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n\nprint(learner)\n\n\n── &lt;LearnerClassifRpart&gt; (classif.rpart): Classification Tree ──────────────────\n• Model: rpart\n• Parameters: cp=0.0001, minbucket=8, minsplit=128, xval=0\n• Packages: mlr3 and rpart\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, factor, and ordered\n• Encapsulation: none (fallback: -)\n• Properties: importance, missings, multiclass, selected_features, twoclass,\nand weights\n• Other settings: use_weights = 'use'"
  },
  {
    "objectID": "tuning_spaces.html#example-usage",
    "href": "tuning_spaces.html#example-usage",
    "title": "Tuning Spaces",
    "section": "",
    "text": "Load a tuning space for the classification tree learner from the Bischl et al. (2021) article.\n\nlibrary(mlr3verse)\n\n# load learner and set search space\nlearner = lts(lrn(\"classif.rpart\"))\n\n# retrieve task\ntask = tsk(\"pima\")\n\n# load tuner and set batch size\ntuner = tnr(\"random_search\", batch_size = 10)\n\n# hyperparameter tuning on the pima data set\ninstance = tune(\n  tuner = tnr(\"grid_search\", resolution = 5, batch_size = 25),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n)\n\n# best performing hyperparameter configuration\ninstance$result\n\n         cp minbucket minsplit learner_param_vals  x_domain classif.ce\n      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;             &lt;list&gt;    &lt;list&gt;      &lt;num&gt;\n1: -9.21034  2.087194 4.859812          &lt;list[4]&gt; &lt;list[3]&gt;  0.2304688\n\n# fit final model on complete data set\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n\nprint(learner)\n\n\n── &lt;LearnerClassifRpart&gt; (classif.rpart): Classification Tree ──────────────────\n• Model: rpart\n• Parameters: cp=0.0001, minbucket=8, minsplit=128, xval=0\n• Packages: mlr3 and rpart\n• Predict Types: [response] and prob\n• Feature Types: logical, integer, numeric, factor, and ordered\n• Encapsulation: none (fallback: -)\n• Properties: importance, missings, multiclass, selected_features, twoclass,\nand weights\n• Other settings: use_weights = 'use'"
  }
]