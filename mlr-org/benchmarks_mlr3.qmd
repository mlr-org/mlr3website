---
title: "mlr3 Runtime and Memory Benchmarks"
sidebar: false
toc: true
cache: false
---

{{< include _setup.qmd >}}

```{r}
#| include: false
library(data.table)
library(ggplot2)
library(gt)
library(DBI)

con = dbConnect(RSQLite::SQLite(), here::here("mlr-org/benchmark_results.db"))
snapshot = setDT(dbReadTable(con, "mlr3_snapshots"))
snapshot[, mlr3 := factor(mlr3)]
snapshot[, paradox := factor(paradox)]

plot_runtime = function(data) {
  ggplot(data, aes(x = mlr3, y = median_runtime)) +
  geom_col(group = 1, fill = "#008080") +
  geom_errorbar(aes(ymin = median_runtime - mad_runtime, ymax = median_runtime + mad_runtime), width = 0.5, position = position_dodge(0.9)) +
  geom_hline(aes(yintercept = model_time * evals / 1000), linetype = "dashed") +
  facet_wrap(~evals, scales = "free_y", labeller = labeller(evals = function(value) sprintf("%s Resampling Iterations", value))) +
  labs(x = "mlr3Version", y = "Runtime [s]") +
  theme_minimal(base_size = 7) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

plot_overhead = function(data) {
  data = copy(data)
  data[, overhead := median_runtime - total_model_time ]
  ggplot(data, aes(x = mlr3, y = overhead)) +
  geom_col(group = 1, fill = "#008080") +
  geom_errorbar(aes(ymin = overhead - mad_runtime, ymax = overhead + mad_runtime), width = 0.5, position = position_dodge(0.9)) +
  #geom_hline(aes(yintercept = model_time * evals / 1000), linetype = "dashed") +
  facet_wrap(~evals, scales = "free_y", labeller = labeller(evals = function(value) sprintf("%s Resampling Iterations", value))) +
  labs(x = "mlr3 Version", y = "Overhead [s]") +
  theme_minimal(base_size = 7) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

plot_memory = function(data) {
  ggplot(data, aes(x = mlr3, y = median_memory)) +
  geom_col(group = 1, fill = "#ff6347") +
  geom_errorbar(aes(ymin = median_memory - mad_memory, ymax = median_memory + mad_memory), width = 0.5, position = position_dodge(0.9)) +
  geom_hline(aes(yintercept = 131), linetype = "dashed") +
  facet_wrap(~evals, scales = "free_y", labeller = labeller(evals = function(value) sprintf("%s Resampling Iterations", value))) +
  labs(x = "mlr3 Version", y = "Memory [MB]") +
  theme_minimal(base_size = 7) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

create_table = function(data) {
  data = data[, -c("mad_memory", "mad_runtime")]

  data_1000 = data[task == "data_1000", -"task"]
  data_10000 = data[task == "data_10000", -c("task", "k", "total_model_time")]
  data = merge(data_1000, data_10000, by = c("mlr3", "paradox", "model_time", "evals"), suffixes = c("", "_10000"))

  setcolorder(data, c("mlr3", "paradox", "model_time", "evals", "total_model_time", "median_runtime", "median_runtime_10000", "k", "median_memory", "median_memory_10000"))

  data %>%
    gt() %>%
    cols_label(
      mlr3 = "mlr3 Version",
      paradox = "paradox Version",
      model_time = "Model Time [ms]",
      evals = "Resampling Iterations",
      total_model_time = "Total Model Time [s]",
      median_runtime = "Median Runtime [s]",
      median_runtime_10000 = "Median Runtime 10,000 [s]",
      k = "K",
      median_memory = "Median Memory [MB]",
      median_memory_10000 = "Median Memory 10,000 [s]") %>%
    fmt_number(columns = c("k", "median_runtime", "median_runtime_10000"), n_sigfig = 2) %>%
    fmt_number(columns = c("median_memory", "median_memory_10000"), decimals = 0) %>%
    tab_style(
      style = list(
        cell_fill(color = "crimson"),
        cell_text(weight = "bold")
      ),
      locations = cells_body(
        columns = "k",
        rows = k > 3
      )
    )  %>%
    tab_row_group(
      label = "1000 Resampling Iterations",
      rows = evals == 1000
    ) %>%
    tab_row_group(
      label = "100 Resampling Iterations",
      rows = evals == 100
    ) %>%
    tab_row_group(
      label = "10 Resampling Iterations",
      rows = evals == 10
    )
  }
```

The mlr3 package comes with `resample()` and `benchmark()` functions to evaluate models.
The runtime and memory usage of these functions are compared for different mlr3 versions.
The experiments vary the training time of the models and the number of resampling iterations.
The training time of the models is set to 1 ms, 10 ms, 100 ms, and 1 s.

# Summary of Latest mlr3 Version

Models with a training time of 1 second and 100 ms have almost no overhead in `resample()` and `benchmark()`.
For models with a training time of 10 ms, the runtime is doubled by the overhead.
The runtime is 10 times larger than the model training for models with a training time of 1 ms.

In section X we analyzed the runtime performance when training models in parallel with `future::multisession` on 10 cores.
In this scenario the overhead includes the time to start the parallel workers and the time to collect the results.
For 1 seconds training time it is beneficial to train the models in parallel even for 10 resampling iterations.
For 100 ms training time the parallelization overhead is too large for 10 resampling iterations.
The models with a training time of 10 ms only benefit from parallelization when more than 1000 resampling iterations are run.
For models with a training time of 1 ms the parallelization overhead is too large for all scenarios.
The sequential execution is faster than the parallel execution.

The results of `resample()` and `benchmark()` are very similar.
 As the same code is used internally, this was to be expected.






# Resampling

```{r}
#| include: false
data_memory = setDT(dbReadTable(con, "mlr3_resample_memory"))[, list(task, evals, renv_project, median_memory, mad_memory)]
data_memory[, renv_project := gsub("mlr3/default/snapshots/", "", renv_project)]
data_memory = data_memory [snapshot, on = "renv_project"]
setorderv(data_memory, c("task", "evals", "mlr3"), order = c(1, 1, 1))
data_memory = data_memory[, -c("renv_project")]

data_runtime = setDT(dbReadTable(con, "mlr3_resample_runtime"))[, list(model_time, task, evals, renv_project, median_runtime, mad_runtime, k)]
data_runtime[, renv_project := gsub("mlr3/default/snapshots/", "", renv_project)]
data_runtime = data_runtime[snapshot, on = "renv_project"]
setorderv(data_runtime, c("task", "model_time", "evals", "mlr3"), order = c(1, 1, 1, 1))
data_runtime = data_runtime[, -c("renv_project")]
data_runtime[, median_runtime := median_runtime / 1000]
data_runtime[, mad_runtime := mad_runtime / 1000]
data_runtime[, total_model_time := model_time * evals / 1000]

data_runtime = merge(data_runtime, data_memory, by = c("task", "evals", "mlr3", "paradox"), sort = FALSE)
```

The runtime and memory usage of the `resample()` function is compared for different mlr3 versions.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset.
The resampling iterations (`evals`) are set to 1000, 100, and 10.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.sleep",
  sleep_train = model_time / 2,
  sleep_predict = model_time / 2)

resampling = rsmp("subsampling", repeats = evals)

resample(task, learner, resampling)
```

## Model Time 1000 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `resample()` with models trained for 1 s depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 1000 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `resample()` with models trained for 1 s depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 1000])
```

## Model Time 100 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `resample()` with models trained for 100 ms depending on the mlr3 version.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 100 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `resample()` with models trained for 100 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 100])
```

## Model Time 10 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `resample()` with models trained for 10 ms depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 10 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `resample()` with models trained for 10 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 10])
```

## Model Time 1 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `resample()` with models trained for 1 ms depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 1 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `resample()` with models trained for 1 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 1])
```

## Memory

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `resample()` depending on the mlr3 version.
#|  Error bars represent the median absolute deviation of the memory usage.
plot_memory(data_memory[task == "data_1000"])
```

# Benchmark

```{r}
#| include: false

data_memory = setDT(dbReadTable(con, "mlr3_benchmark_memory"))[, list(task, evals, renv_project, median_memory, mad_memory)]
data_memory[, renv_project := gsub("mlr3/default/snapshots/", "", renv_project)]
data_memory = data_memory [snapshot, on = "renv_project"]
setorderv(data_memory, c("task", "evals", "mlr3"), order = c(1, 1, 1))
data_memory = data_memory[, -c("renv_project")]

data_runtime = setDT(dbReadTable(con, "mlr3_benchmark_runtime"))[, list(model_time, task, evals, renv_project, median_runtime, mad_runtime, k)]
data_runtime[, renv_project := gsub("mlr3/default/snapshots/", "", renv_project)]
data_runtime = data_runtime[snapshot, on = "renv_project"]
setorderv(data_runtime, c("task", "model_time", "evals", "mlr3"), order = c(1, 1, 1, 1))
data_runtime = data_runtime[, -c("renv_project")]
data_runtime[, median_runtime := median_runtime / 1000]
data_runtime[, mad_runtime := mad_runtime / 1000]
data_runtime[, total_model_time := model_time * evals / 1000]

data_runtime = merge(data_runtime, data_memory, by = c("task", "evals", "mlr3", "paradox"), sort = FALSE)
```

The runtime and memory usage of the `benchmark()` function is compared for different mlr3 versions.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset.
The resampling iterations (`evals`) are set to 1000, 100, and 10.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.sleep",
  sleep_train = model_time / 2,
  sleep_predict = model_time / 2)

resampling = rsmp("subsampling", repeats = instance$evals / 5)

design = benchmark_grid(task, replicate(5, learner), resampling)

benchmark(design)
```

## Model Time 1000 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `benchmark()` with models trained for 1 s depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 1000 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `benchmark()` with models trained for 1 s depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 1000])
```

## Model Time 100 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `benchmark()` with models trained for 100 ms depending on the mlr3 version.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 100 & task == "data_1000"])
```


```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `benchmark()` with models trained for 100 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 100])
```

## Model Time 10 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `benchmark()` with models trained for 10 ms depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 10 & task == "data_1000"])
```

´
```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `benchmark()` with models trained for 10 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 10])
```

## Model Time 1 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `benchmark()` with models trained for 1 ms depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 1 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `benchmark()` with models trained for 1 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 1])
```

## Memory

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `benchmark()` with models trained for 100 ms depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the memory usage.
plot_memory(data_memory[task == "data_1000"])
```

# Encapsulation

```{r}
#| include: false

# data_memory = setDT(dbReadTable(con, "mlr3_resample_memory"))[, list(task, evals, renv_project, median_memory, mad_memory)]
# data_memory[, renv_project := gsub("mlr3/default/snapshots/", "", renv_project)]
# data_memory = data_memory [snapshot, on = "renv_project"]
# setorderv(data_memory, c("task", "evals", "mlr3"), order = c(1, 1, 1))
# data_memory = data_memory[, -c("renv_project")]

data_runtime = setDT(dbReadTable(con, "mlr3_encapsulation_runtime"))[, list(task, method, renv_project, median_runtime, mad_runtime)]
data_runtime[, renv_project := gsub("mlr3/default/snapshots/", "", renv_project)]
data_runtime = data_runtime[snapshot, on = "renv_project"]
setorderv(data_runtime, c("task", "method", "mlr3"), order = c(1, 1, 1))
data_runtime = data_runtime[, -c("renv_project")]
data_runtime[, median_runtime := median_runtime / 1000]
data_runtime[, mad_runtime := mad_runtime / 1000]

# data_runtime = merge(data_runtime, data_memory, by = c("task", "evals", "mlr3", "paradox"), sort = FALSE)
```

```{r}
#| echo: false
ggplot(data_runtime[task == "data_1000"], aes(x = mlr3, y = median_runtime)) +
  geom_col(group = 1, fill = "#008080") +
  geom_errorbar(aes(ymin = median_runtime - mad_runtime, ymax = median_runtime + mad_runtime), width = 0.5, position = position_dodge(0.9)) +
  facet_wrap(~method, scales = "free_y", labeller = labeller(evals = function(value) sprintf("%s Method", value))) +
  labs(x = "mlr3Version", y = "Runtime [s]") +
  theme_minimal(base_size = 7) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Resampling Parallel

```{r}
plot_runtime = function(data) {
  ggplot(data, aes(x = mlr3, y = median_runtime)) +
  geom_col(group = 1, fill = "#008080") +
  geom_errorbar(aes(ymin = median_runtime - mad_runtime, ymax = median_runtime + mad_runtime), width = 0.5, position = position_dodge(0.9)) +
  geom_hline(aes(yintercept = model_time * evals / 10 / 1000), linetype = "dashed") +
  facet_wrap(~evals, scales = "free_y", labeller = labeller(evals = function(value) sprintf("%s Resampling Iterations", value))) +
  labs(x = "mlr3Version", y = "Runtime [s]") +
  theme_minimal(base_size = 7) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
}
```

```{r}
#| include: false
data_memory = setDT(dbReadTable(con, "mlr3_resample_parallel_memory"))[, list(task, evals, renv_project, median_memory, mad_memory)]
data_memory[, renv_project := gsub("mlr3/default/snapshots/", "", renv_project)]
data_memory = data_memory [snapshot, on = "renv_project"]
setorderv(data_memory, c("task", "evals", "mlr3"), order = c(1, 1, 1))
data_memory = data_memory[, -c("renv_project")]

data_runtime = setDT(dbReadTable(con, "mlr3_resample_parallel_runtime"))[, list(model_time, task, evals, renv_project, median_runtime, mad_runtime, k)]
data_runtime[, renv_project := gsub("mlr3/default/snapshots/", "", renv_project)]
data_runtime = data_runtime[snapshot, on = "renv_project"]
setorderv(data_runtime, c("task", "model_time", "evals", "mlr3"), order = c(1, 1, 1, 1))
data_runtime = data_runtime[, -c("renv_project")]
data_runtime[, median_runtime := median_runtime / 1000]
data_runtime[, mad_runtime := mad_runtime / 1000]
data_runtime[, total_model_time := model_time * evals / 1000]

data_runtime = merge(data_runtime, data_memory, by = c("task", "evals", "mlr3", "paradox"), sort = FALSE)
```

The runtime and memory usage of the `resample()` function is compared for different mlr3 versions.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset.
The resampling iterations (`evals`) are set to 1000, 100, and 10.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.sleep",
  sleep_train = model_time / 2,
  sleep_predict = model_time / 2)

resampling = rsmp("subsampling", repeats = evals)

future::plan("multisession", workers = 10)
options("mlr3.exec_chunk_size" = evals / 10)

resample(task, learner, resampling)
```

## Model Time 1000 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `resample()` with models trained for 1 s depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 1000 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `resample()` with models trained for 1 s depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 1000])
```

## Model Time 100 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `resample()` with models trained for 100 ms depending on the mlr3 version.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 100 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `resample()` with models trained for 100 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 100])
```

## Model Time 10 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `resample()` with models trained for 10 ms depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 10 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `resample()` with models trained for 10 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 10])
```

## Model Time 1 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `resample()` with models trained for 1 ms depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 1 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `resample()` with models trained for 1 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 1])
```

## Memory

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `resample()` depending on the mlr3 version.
#|  Error bars represent the median absolute deviation of the memory usage.
plot_memory(data_memory[task == "data_1000"])
```

# Benchmark Parallel

```{r}
#| include: false

data_memory = setDT(dbReadTable(con, "mlr3_benchmark_parallel_memory"))[, list(task, evals, renv_project, median_memory, mad_memory)]
data_memory[, renv_project := gsub("mlr3/default/snapshots/", "", renv_project)]
data_memory = data_memory [snapshot, on = "renv_project"]
setorderv(data_memory, c("task", "evals", "mlr3"), order = c(1, 1, 1))
data_memory = data_memory[, -c("renv_project")]

data_runtime = setDT(dbReadTable(con, "mlr3_benchmark_parallel_runtime"))[, list(model_time, task, evals, renv_project, median_runtime, mad_runtime, k)]
data_runtime[, renv_project := gsub("mlr3/default/snapshots/", "", renv_project)]
data_runtime = data_runtime[snapshot, on = "renv_project"]
setorderv(data_runtime, c("task", "model_time", "evals", "mlr3"), order = c(1, 1, 1, 1))
data_runtime = data_runtime[, -c("renv_project")]
data_runtime[, median_runtime := median_runtime / 1000]
data_runtime[, mad_runtime := mad_runtime / 1000]
data_runtime[, total_model_time := model_time * evals / 1000]

data_runtime = merge(data_runtime, data_memory, by = c("task", "evals", "mlr3", "paradox"), sort = FALSE)
```

The runtime and memory usage of the `benchmark()` function is compared for different mlr3 versions.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset.
The resampling iterations (`evals`) are set to 1000, 100, and 10.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.sleep",
  sleep_train = model_time / 2,
  sleep_predict = model_time / 2)

resampling = rsmp("subsampling", repeats = instance$evals / 5)

design = benchmark_grid(task, replicate(5, learner), resampling)

future::plan("multisession", workers = 10)
options("mlr3.exec_chunk_size" = evals / 10)

benchmark(design)
```

## Model Time 1000 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `benchmark()` with models trained for 1 s depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 1000 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `benchmark()` with models trained for 1 s depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 1000])
```

## Model Time 100 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `benchmark()` with models trained for 100 ms depending on the mlr3 version.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 100 & task == "data_1000"])
```


```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `benchmark()` with models trained for 100 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 100])
```

## Model Time 10 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `benchmark()` with models trained for 10 ms depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 10 & task == "data_1000"])
```

´
```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `benchmark()` with models trained for 10 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 10])
```

## Model Time 1 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `benchmark()` with models trained for 1 ms depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the runtime.
plot_runtime(data_runtime[model_time == 1 & task == "data_1000"])
```

```{r}
#| echo: false
#| column: body-outset
#| tbl-cap: |
#|  Runtime and memory usage of `benchmark()` with models trained for 1 ms depending on the mlr3 version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
#|  The table includes runtime and memory usage for tasks of size 1000 and 10,000.
create_table(data_runtime[model_time == 1])
```

## Memory

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `benchmark()` with models trained for 100 ms depending on the mlr3 version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute devidation of the memory usage.
plot_memory(data_memory[task == "data_1000"])
```



# Version Histroy

## mlr3

**mlr3 0.20.3**

* refactor: Optimize runtime of fixing factor levels.
* refactor: Optimize runtime of setting row roles.
* refactor: Optimize runtime of marshalling.
* refactor: Optimize runtime of `Task$col_info`

**mlr3 0.18.0**

* Skip unnecessary clone of learner's state in `resample()`.

**mlr3 0.17.1**

* Remove `data_prototype` when resampling from `learner$state` to reduce memory consumption.
* Optimize runtime of `resample()` and `benchmark()` by reducing the number of hashing operations.
* Reduce number of threads used by data.table and BLAS to 1 when running resample() or benchmark() in parallel.

**mlr3 0.17.0**

* Speed up resampling by removing unnecessary calls to `packageVersion()`.
* The `design` of `benchmark()` can now include parameter settings.

## paradox

**paradox 1.0.1**

* Performance improvements.

**paradox 1.0.0**

* Removed Param objects. ParamSet now uses a data.table internally; individual parameters are more like Domain objects now.
