---
sidebar: false
toc: false
title: Frequently Asked Questions
---

{{< include _setup.qmd >}}

* [Why is there only the rpart learner?](#learner)
* [How can I use parallelization?](#parallelization)
* [Why is the parallelization with the future package slow?](#parallelization-slow)
* [Why is the parallelization of tuning slow?](#tuning-slow)
* [Why are the CPUs on my system not fully utilized when using parallelization?](#parallelization-cpu)
* [How can I use time constraints when tuning?](#time-constraints)

## Why is there only the rpart learner? {#learner}

The base package `r ref_pkg("mlr3")` ships only with regression and classification trees from the `r ref_pkg("rpart")` package and some learners for debugging.
A selection of popular learners can be found in the extension package `r ref_pkg("mlr3learners")`.
Survival learners are provided by `r ref_pkg("mlr3proba")`, cluster learners via `r ref_pkg("mlr3cluster")`.
Additional learners can be found in the extension packages `r ref_pkg("mlr3extralearners")`.
If your favorite learner is missing, please open a learner [request](https://github.com/mlr-org/mlr3extralearners).
An overview of all learners can be found on our [website](learners.html).

## How can I use parallelization? {#parallelization}

Parallelization is supported when training learners, resampling, tuning and predicting.
We recommend reading the section about [Parallelization](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-parallelization) in the `mlr3book`.

## Why is the parallelization with the future package slow? {#parallelization-slow}

Starting and terminating workers as well as possible communication between workers comes at a price in the form of additionally required runtime which is called [parallelization overhead](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-parallelization).
This overhead strongly varies between parallelization backends and must be carefully weighed against the runtime of the sequential execution to determine if parallelization is worth the effort.
When resampling or tuning a fast-fitting learner, it helps to chunk multiple resampling iterations into a single computational job.
The option [`mlr3.exec_chunk_bins`](https://mlr3.mlr-org.com/reference/mlr3-package.html#package-options) determines the number of chunks to split the resampling iterations into.
For example, when running a benchmark with 100 resampling iterations, `options("mlr3.exec_chunk_bins" = 4)` creates 4 computational jobs with 25 resampling iterations each.
This reduces the parallelization overhead and speeds up the execution.

## Why is the parallelization of tuning slow? {#tuning-slow}

Tuning can also suffer from the [parallelization overhead](#parallelization-slow) described above.
Additionally, the batch size of the tuner can have a large impact on the runtime.
Setting an optimal batch size is explained in the section [Parallelization of Tuning](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-parallel-tuning) of the `mlr3book`.

## Why are the CPUs on my system not fully utilized when using parallelization? {#parallelization-cpu}

If there are few jobs with dissimilar runtimes, the system may end up waiting for the last chunk to finish, while other resources are idle.
This is referred to as [synchronization overhead](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-parallelization).
When minimizing the synchronization overhead, a too large chunk size can lead to a situation where the last chunk takes much longer than the others.
This can be avoided by setting `mlr3.exec_chunk_bins` to a smaller value than the number of cores available on the system.

## How can I use time constraints when tuning?

Time constraints can be set for individual learners, tuning processes, and nested resampling.
The gallery post [Time constraints in the mlr3 ecosystem](gallery/technical/2023-12-21-time-constraints/) provides an overview of the different options.
