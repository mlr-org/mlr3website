@Article{mlr3,
  title = {{mlr3}: A modern object-oriented machine learning framework in {R}},
  author = {Michel Lang and Martin Binder and Jakob Richter and Patrick Schratz and Florian Pfisterer and Stefan Coors and Quay Au and Giuseppe Casalicchio and Lars Kotthoff and Bernd Bischl},
  journal = {Journal of Open Source Software},
  year = {2019},
  month = {dec},
  doi = {10.21105/joss.01903},
  url = {https://joss.theoj.org/papers/10.21105/joss.01903},
}

@Article{mlr3proba,
  title = {{mlr3proba}: An {R} Package for Machine Learning in Survival Analysis},
  author = {Raphael Sonabend and Franz J Kir√°ly and Andreas Bender and Bernd Bischl and Michel Lang},
  journal = {Bioinformatics},
  month = {02},
  year = {2021},
  doi = {10.1093/bioinformatics/btab039},
  issn = {1367-4803},
}

@Article{mlr3pipelines,
  title = {{mlr3pipelines} - Flexible Machine Learning Pipelines in {R}},
  author = {Martin Binder and Florian Pfisterer and Michel Lang and Lennart Schneider and Lars Kotthoff and Bernd Bischl},
  journal = {Journal of Machine Learning Research},
  year = {2021},
  volume = {22},
  number = {184},
  pages = {1-7},
  url = {https://jmlr.org/papers/v22/21-0281.html},
}

@Article{future,
  author = {Henrik Bengtsson},
  title = {A Unifying Framework for Parallel and Distributed Processing in {R} using Futures},
  year = {2021},
  journal = {The R Journal},
  doi = {10.32614/RJ-2021-048},
  pages = {208--227},
  volume = {13},
  number = {2},
}

@Article{checkmate,
  title = {{checkmate}: Fast Argument Checks for Defensive {R} Programming},
  author = {Michel Lang},
  journal = {The R Journal},
  year = {2017},
  doi = {10.32614/RJ-2017-028},
  pages = {437--445},
  volume = {9},
  number = {1},
}

@Article{batchtools,
  title = {batchtools: Tools for {R} to work on batch systems},
  author = {Michel Lang and Bernd Bischl and Dirk Surmann},
  journal = {The Journal of Open Source Software},
  year = {2017},
  month = {feb},
  number = {10},
  doi = {10.21105/joss.00135},
}

@article{pargent2023tutorial,
author = {Florian Pargent and Ramona Schoedel and Clemens Stachl},
title ={Best Practices in Supervised Machine Learning: A Tutorial for Psychologists},
journal = {Advances in Methods and Practices in Psychological Science},
volume = {6},
number = {3},
pages = {25152459231162559},
year = {2023},
doi = {10.1177/25152459231162559},

URL = { 
        https://doi.org/10.1177/25152459231162559
    
},
eprint = { 
        https://doi.org/10.1177/25152459231162559
    
}
,
    abstract = { Supervised machine learning (ML) is becoming an influential analytical method in psychology and other social sciences. However, theoretical ML concepts and predictive-modeling techniques are not yet widely taught in psychology programs. This tutorial is intended to provide an intuitive but thorough primer and introduction to supervised ML for psychologists in four consecutive modules. After introducing the basic terminology and mindset of supervised ML, in Module 1, we cover how to use resampling methods to evaluate the performance of ML models (bias-variance trade-off, performance measures, k-fold cross-validation). In Module 2, we introduce the nonlinear random forest, a type of ML model that is particularly user-friendly and well suited to predicting psychological outcomes. Module 3 is about performing empirical benchmark experiments (comparing the performance of several ML models on multiple data sets). Finally, in Module 4, we discuss the interpretation of ML models, including permutation variable importance measures, effect plots (partial-dependence plots, individual conditional-expectation profiles), and the concept of model fairness. Throughout the tutorial, intuitive descriptions of theoretical concepts are provided, with as few mathematical formulas as possible, and followed by code examples using the mlr3 and companion packages in R. Key practical-analysis steps are demonstrated on the publicly available PhoneStudy data set (N = 624), which includes more than 1,800 variables from smartphone sensing to predict Big Five personality trait scores. The article contains a checklist to be used as a reminder of important elements when performing, reporting, or reviewing ML analyses in psychology. Additional examples and more advanced concepts are demonstrated in online materials (https://osf.io/9273g/). }
}


@article{bischl_hyperparameter_2021,
	title = {Hyperparameter {Optimization}: {Foundations}, {Algorithms}, {Best} {Practices} and {Open} {Challenges}},
	shorttitle = {Hyperparameter {Optimization}},
	url = {http://arxiv.org/abs/2107.05847},
	abstract = {Most machine learning algorithms are configured by one or several hyperparameters that must be carefully chosen and often considerably impact performance. To avoid a time consuming and unreproducible manual trial-and-error process to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods, e.g., based on resampling error estimation for supervised machine learning, can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods such as grid or random search, evolutionary algorithms, Bayesian optimization, Hyperband and racing. It gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with ML pipelines, runtime improvements, and parallelization.},
	urldate = {2021-11-09},
	journal = {arXiv:2107.05847 [cs, stat]},
	author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	month = jul,
	year = {2021}
}
