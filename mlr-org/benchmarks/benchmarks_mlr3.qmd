---
title: "mlr3 - Runtime and Memory Benchmarks"
sidebar: false
toc: true
cache: false
lazy-cache: false
format:
  html:
    fig-width: 12
    fig-height: 9
---

{{< include ../_setup.qmd >}}

```{r}
#| include: false
library(data.table)
library(ggplot2)
library(gt)
library(DBI)

con = dbConnect(RSQLite::SQLite(), here::here("mlr-org/benchmarks/results_lrz.db"))
snapshot = setDT(dbReadTable(con, "mlr3_snapshots"))
snapshot[, mlr3 := factor(mlr3)]
snapshot[, paradox := factor(paradox)]

jobs_runtime = setDT(dbReadTable(con, "mlr3_runtime_jobs"))
jobs_runtime[, args := map(args, function(args) unserialize(args))]

results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("mlr3_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "mlr3"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime$function_name)

walk(c("train", "predict", "predict_newdata", "predict_newdata_fast", "encapsulate"), function(function_name) {
  data = results_runtime[[function_name]]
  data[, k_1 := (median_runtime + 1) / 1]
  data[, k_10 := (median_runtime + 10) / 10]
  data[, k_100 := (median_runtime + 100) / 100]
  data[, k_1000 := (median_runtime + 1000) / 1000]
})

walk(c("resample", "benchmark", "resample_score", "resample_aggregate"), function(function_name) {
  data = results_runtime[[function_name]]
  data[, k_1 := (median_runtime + 1 * evals) / (1 * evals)]
  data[, k_10 := (median_runtime + 10 * evals) / (10 * evals)]
  data[, k_100 := (median_runtime + 100 * evals) / (100 * evals)]
  data[, k_1000 := (median_runtime + 1000 * evals) / (1000 * evals)]
})

jobs_memory = setDT(dbReadTable(con, "mlr3_memory_jobs"))
jobs_memory[, args := map(args, function(args) unserialize(args))]

results_memory = set_names(pmap(jobs_memory, function(function_name, args, ...) {
  arg_names = names(args)
  data_memory = setDT(dbReadTable(con, sprintf("mlr3_memory_%s", function_name)))[, c(arg_names, "renv_project", "median_memory", "mad_memory"), with = FALSE]
  data_memory = data_memory[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_memory, c(arg_names, "mlr3"), order = c(rep(1, length(arg_names)), -1))
  data_memory[, -c("renv_project")]
}), jobs_memory$function_name)

# merge memory and runtime data
results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  data_runtime = results_runtime[[function_name]]
  data_memory = results_memory[[function_name]]
  data_runtime[data_memory, on = c(names(args), "mlr3", "paradox")]
}), jobs_runtime$function_name)

jobs_runtime_parallel = setDT(dbReadTable(con, "mlr3_runtime_parallel_jobs"))
jobs_runtime_parallel[, args := map(args, function(args) unserialize(args))]

results_runtime_parallel = set_names(pmap(jobs_runtime_parallel, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("mlr3_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "mlr3"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime_parallel$function_name)


results_runtime_parallel = set_names(pmap(list(c("resample", "benchmark"), c("resample_parallel", "benchmark_parallel")), function(function_name, function_name_parallel) {

  data = copy(results_runtime[[function_name]])

  data[, total_runtime_1 := median_runtime + evals * 1]
  data[, total_runtime_10 := median_runtime + evals * 10]
  data[, total_runtime_100 := median_runtime + evals * 100]
  data[, total_runtime_1000 := median_runtime + evals * 1000]

  data_parallel = copy(results_runtime_parallel[[function_name_parallel]])

  data_parallel[, total_runtime_parallel_1 := median_runtime + evals * 1 / 10]
  data_parallel[, total_runtime_parallel_10 := median_runtime + evals * 10 / 10]
  data_parallel[, total_runtime_parallel_100 := median_runtime + evals * 100 / 10]
  data_parallel[, total_runtime_parallel_1000 := median_runtime + evals * 1000 / 100]

  tmp = cbind(data[, list(total_runtime_1, total_runtime_10, total_runtime_100, total_runtime_1000)],
            data_parallel[, list(total_runtime_parallel_1, total_runtime_parallel_10, total_runtime_parallel_100, total_runtime_parallel_1000)])

  tmp[, pk_1 := total_runtime_1 / total_runtime_parallel_1]
  tmp[, pk_10 := total_runtime_10 / total_runtime_parallel_10]
  tmp[, pk_100 := total_runtime_100 / total_runtime_parallel_100]
  tmp[, pk_1000 := total_runtime_1000 / total_runtime_parallel_1000]

  tmp[pk_1 < 1, pk_1 := NA]
  tmp[pk_10 < 1, pk_10 := NA]
  tmp[pk_100 < 1, pk_100 := NA]
  tmp[pk_1000 < 1, pk_1000 := NA]

  cbind(results_runtime[[function_name]], tmp[, list(pk_1, pk_10, pk_100, pk_1000)])
}), c("resample", "benchmark"))

results_runtime[["resample"]] = results_runtime_parallel[["resample"]]
results_runtime[["benchmark"]] = results_runtime_parallel[["benchmark"]]
```


# Scope

This report analyzes the runtime and memory usage of `mlr3` across the four most recent package versions.
It focuses on the learner methods `$train()` and `$predict()` and on the evaluation functions `resample()` and `benchmark()`.
The benchmarks quantify the runtime overhead introduced by `mlr3` and the memory usage.
Overhead is reported relative to the training time of the underlying models.
The study varies dataset size and the number of resampling iterations.
All experiments also assess the effect of parallelization on runtime and memory.
The impact of encapsulation is examined by comparing alternative encapsulation methods.

Given the size of the `mlr3` ecosystem, performance bottlenecks can arise at multiple stages.
This report helps users assess whether observed runtimes fall within expected ranges.
Substantial anomalies in runtime or memory should be reported by opening a GitHub issue.
Benchmarks are executed on a high‑performance cluster optimized for multi‑core throughput rather than single‑core speed.
Consequently, single‑core runtimes may be faster on a modern local machine.

# Summary of Latest mlr3 Version

The benchmarks are comprehensive, so we summarize the results for the latest `mlr3` version.
The runtime overhead of `mlr3` must be interpreted relative to model training and prediction times.
For instance, if `ranger::ranger()` takes 100 ms to train and `lrn("classif.ranger")$train()` takes 110 ms, the overhead is 10%.
If the same model requires 1 s to train, the overhead is 1%.
The overhead is shown relative to the training time of the models with the factors `k_1`, `k_10`, `k_100`, and `k_1000`.
The subscript denotes the model’s training time in milliseconds.
The factors `pk_1`, `pk_10`, `pk_100`, and `pk_1000` report the speedup of parallel over sequential execution.

We first consider `$train()`.
For models with training times of 1000 ms and 100 ms, the overhead is minimal.
When training takes 10 ms, runtime approximately doubles.
For 1 ms models, overhead is roughly ten times the bare model training time.

The overhead of `$predict()` is comparable to `$train()`, and dataset size has only a minor effect.
`$predict_newdata()` converts `newdata` to a task and then predicts, which roughly doubles the overhead relative to `$predict()`.
The recently introduced `$predict_newdata_fast()` is substantially faster than `$predict_newdata()`.
For models with 10 ms prediction time, the overhead is about 10%.
For models with 1 ms prediction time, the overhead is about 50%.

The overhead of `resample()` and `benchmark()` is small for 1000 ms and 100 ms models.
For 10 ms models, the total runtime is approximately twice the bare training time.
For 1 ms models, the total runtime is approximately ten times the bare training time.
An empty R session consumes 131 MB of memory.
Resampling with 10 iterations uses approximately 164 MB, increasing to about 225 MB for 1000 iterations.
Memory usage for `benchmark()` is comparable to `resample()`.

`mlr3` parallelizes over resampling iterations via the `future` package.
Parallel execution adds overhead due to worker initialization.
We therefore compare parallel and sequential runtimes.
For 1 s models, parallel `resample()` and `benchmark()` reduce total runtime.
For 100 ms models, parallelization is advantageous primarily for 100 or 1000 iterations.
For 10 ms and 1 ms models, parallel execution overtakes sequential execution mainly at 1000 iterations.
Memory grows with the number of cores because each core launches a separate R session.
Using 10 cores results in a total memory footprint of approximately 1.2 GB.

Encapsulation captures and logs conditions such as messages, warnings, and errors without interrupting control flow.
Encapsulation via `callr` introduces approximately 1 s of additional runtime per model training.
Encapsulation via `evaluate` adds negligible runtime overhead.

# Train {#train}

```{r}
#| include: false

# gt table for experiments that depend on the task size
table_task = function(data) {
  data = data[, -c("mad_runtime", "mad_memory", "paradox")]
  data[, task := as.integer(gsub("data_", "", task))]
  setcolorder(data, c("mlr3", "task", "median_runtime", "k_1000", "k_100", "k_10", "k_1", "median_memory"))

  data %>%
    gt() %>%
    cols_label(
      mlr3 = "mlr3",
      task = "Task Size",
      median_runtime = "Runtime",
      k_1 = html('k<sub>1</sub>'),
      k_10 = html('k<sub>10</sub>'),
      k_100 = html('k<sub>100</sub>'),
      k_1000 = html('k<sub>1000</sub>'),
      median_memory = "Memory") %>%
    cols_units(
      median_runtime = "ms",
      median_memory = "mb"
    ) %>%
    fmt_number(columns = c("median_runtime", "median_memory"), decimals = 0, sep_mark = "") %>%
    fmt_number(columns = c("k_1", "k_10", "k_100", "k_1000"), n_sigfig = 2) %>%
    data_color(
      columns = c("k_1", "k_10", "k_100", "k_1000"),
      fn = scales::col_numeric(
        palette = c("#2ecc71", "white"),
        domain = c(1, 3),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
    tab_row_group(
      label = "10000 Observations",
      rows = task == 10000
    ) %>%
    tab_row_group(
      label = "1000 Observations",
      rows = task == 1000
    ) %>%
    tab_row_group(
      label = "100 Observations",
      rows = task == 100
    ) %>%
    tab_row_group(
      label = "10 Observations",
      rows = task == 10
    ) %>%
    tab_row_group(
      label = "1 Observation",
      rows = task == 1
    )
  }
```

The runtime and memory usage of `$train()` are measured for different mlr3 versions.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")

learner$train(task)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `$train()` by mlr3 version and dataset size.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
#|   The table includes runtime and memory usage for tasks of size 10, 100, 1,000 and 10,000.
table_task(results_runtime$train)
```

# Predict {#predict}

The runtime of `$predict()` is measured across `mlr3` versions.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")

learner$train(task)

learner$predict(task)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `$predict()` by mlr3 version and dataset size.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
#|   The table includes runtime and memory usage for tasks of size 10, 100, 1,000 and 10,000.
table_task(results_runtime$predict)
```

# Predict Newdata {#predict-newdata}

The runtime of `$predict_newdata()` is measured across `mlr3` versions.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")

learner$train(task)

learner$predict_newdata(newdata)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `$predict_newdata()` by mlr3 version and dataset size.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
#|   The table includes runtime and memory usage for tasks of size 1, 10, 100, 1,000 and 10,000.
table_task(results_runtime$predict_newdata)
```

# Predict Newdata Fast {#predict-newdata-fast}

The runtime of `$predict_newdata_fast()` is measured across `mlr3` versions.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")

learner$train(task)

learner$predict_newdata_fast(task)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `$predict_newdata_fast()` by mlr3 version and dataset size.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
#|   The table includes runtime and memory usage for tasks of size 1, 10, 100, 1,000 and 10,000.
table_task(results_runtime$predict_newdata_fast)
```

# Resampling {#resampling}

```{r}
#| include: false

# creates a gt table for experiments that depend on the task size and the number of resampling iterations
table_evals = function(data, task = "data_1000") {
  .task = task
  data = data[list(.task), , on = "task"]
  data = data[, -c("mad_runtime", "mad_memory", "task", "paradox")]
  setcolorder(data, c("mlr3", "evals", "median_runtime", "k_1000", "k_100", "k_10", "k_1", "median_memory", "pk_1000", "pk_100", "pk_10", "pk_1"))

  data %>%
    gt() %>%
    cols_label(
      mlr3 = "mlr3",
      evals = "Resampling Iterations",
      median_runtime = "Runtime",
      k_1 = html('k<sub>1</sub>'),
      k_10 = html('k<sub>10</sub>'),
      k_100 = html('k<sub>100</sub>'),
      k_1000 = html('k<sub>1000</sub>'),
      pk_1 = html('pk<sub>1</sub>'),
      pk_10 = html('pk<sub>10</sub>'),
      pk_100 = html('pk<sub>100</sub>'),
      pk_1000 = html('pk<sub>1000</sub>'),
      median_memory = "Memory") %>%
     cols_units(
      median_runtime = "s",
      median_memory = "mb"
    ) %>%
    sub_missing(
      columns = everything(),
      rows = everything(),
      missing_text = "---"
    ) %>%
    fmt_number(columns = c("median_runtime", "median_memory"), decimals = 0, sep_mark = "") %>%
    fmt_number(columns = c("k_1", "k_10", "k_100", "k_1000", "pk_1", "pk_10", "pk_100", "pk_1000"), n_sigfig = 2) %>%
    data_color(
      columns = c("k_1", "k_10", "k_100", "k_1000"),
      fn = scales::col_numeric(
        palette = c("#2ecc71", "white"),
        domain = c(1, 3),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
     data_color(
      columns = c("pk_1", "pk_10", "pk_100", "pk_1000"),
      fn = scales::col_numeric(
        palette = c("white", "#A689E1"),
        domain = c(1, max(data$pk_1000, na.rm = TRUE)),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
    tab_row_group(
      label = "1000 Resampling Iterations",
      rows = evals == 1000
    ) %>%
    tab_row_group(
      label = "100 Resampling Iterations",
      rows = evals == 100
    ) %>%
    tab_row_group(
      label = "10 Resampling Iterations",
      rows = evals == 10
    )
  }
```

The runtime and memory usage of `resample()` are measured across `mlr3` versions.
The number of resampling iterations (`evals`) is set to 1000, 100, and 10.
We also measure the runtime of `resample()` with `future::multisession` parallelization on 10 cores.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")

resampling = rsmp("subsampling", repeats = evals)

resample(task, learner, resampling)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `resample()` by mlr3 version and resampling iterations on the spam dataset with 1,000 observations.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
#|   The pk factors indicate how many times faster the parallel runtime is compared to the sequential runtime.
#|   No pk factor is shown when the parallel runtime is slower than the sequential runtime.
table_evals(results_runtime$resample, task = "data_1000")
```

# Benchmark {#benchmark}

The runtime and memory usage of `benchmark()` are measured across `mlr3` versions.
The number of resampling iterations (`evals`) is set to 1000, 100, and 10.
We also measure the runtime of `benchmark()` with `future::multisession` parallelization on 10 cores.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")
resampling = rsmp("subsampling", repeats = evals / 5)

design = benchmark_grid(task, replicate(5, learner), resampling)

benchmark(design)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `benchmark()` by mlr3 version and resampling iterations on the spam dataset with 1,000 observations.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
#|   The pk factors indicate how many times faster the parallel runtime is compared to the sequential runtime.
#|   No pk factor is shown when the parallel runtime is slower than the sequential runtime.
table_evals(results_runtime$benchmark, task = "data_1000")
```

# Encapsulation {#encapsulation}

The runtime and memory usage of `$train()` are measured for different encapsulation methods and `mlr3` versions.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")
learner$encapsulate(method, fallback = lrn("classif.featureless"))

learner$train(task)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `$train()` by mlr3 version and encapsulation method.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
data = copy(results_runtime$encapsulate)
data = data[list("data_1000"), , on = "task"]
data = data[, -c("mad_runtime", "mad_memory", "task", "paradox")]
setcolorder(data, c("mlr3", "method", "median_runtime", "k_1000", "k_100", "k_10", "k_1", "median_memory"))
setorderv(data, c("method", "mlr3"), order = c(1, 1))

data %>%
  gt() %>%
  cols_label(
    mlr3 = "mlr3",
    method = "Method",
    median_runtime = "Runtime",
    k_1 = html('k<sub>1</sub>'),
    k_10 = html('k<sub>10</sub>'),
    k_100 = html('k<sub>100</sub>'),
    k_1000 = html('k<sub>1000</sub>'),
    median_memory = "Memory") %>%
     cols_units(
      median_runtime = "s",
      median_memory = "mb"
    ) %>%
  fmt_number(columns = c("median_runtime", "median_memory"), decimals = 0, sep_mark = "") %>%
  fmt_number(columns = c("k_1", "k_10", "k_100", "k_1000"), n_sigfig = 2) %>%
  tab_row_group(
    label = "No Encapsulation",
    rows = method == "none"
  ) %>%
  tab_row_group(
    label = "Evaluate",
    rows = method == "evaluate"
  ) %>%
  tab_row_group(
    label = "Callr",
    rows = method == "callr"
  )
```

