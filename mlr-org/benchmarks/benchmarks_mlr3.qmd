---
title: "mlr3 - Runtime and Memory Benchmarks"
sidebar: false
toc: true
cache: false
lazy-cache: false
format:
  html:
    fig-width: 12
    fig-height: 9
---

{{< include ../_setup.qmd >}}

```{r}
#| include: false
library(data.table)
library(ggplot2)
library(gt)
library(DBI)

con = dbConnect(RSQLite::SQLite(), here::here("mlr-org/benchmarks/results_lrz.db"))
snapshot = setDT(dbReadTable(con, "mlr3_snapshots"))
snapshot[, mlr3 := factor(mlr3)]
snapshot[, paradox := factor(paradox)]

jobs_runtime = setDT(dbReadTable(con, "mlr3_runtime_jobs"))
jobs_runtime[, args := map(args, function(args) unserialize(args))]

results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("mlr3_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "mlr3"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime$function_name)

walk(c("train", "predict", "predict_newdata", "predict_newdata_fast", "encapsulate"), function(function_name) {
  data = results_runtime[[function_name]]
  data[, k_1 := (median_runtime + 1) / 1]
  data[, k_10 := (median_runtime + 10) / 10]
  data[, k_100 := (median_runtime + 100) / 100]
  data[, k_1000 := (median_runtime + 1000) / 1000]
})

walk(c("resample", "benchmark", "resample_score", "resample_aggregate"), function(function_name) {
  data = results_runtime[[function_name]]
  data[, k_1 := (median_runtime + 1 * evals) / (1 * evals)]
  data[, k_10 := (median_runtime + 10 * evals) / (10 * evals)]
  data[, k_100 := (median_runtime + 100 * evals) / (100 * evals)]
  data[, k_1000 := (median_runtime + 1000 * evals) / (1000 * evals)]
})

jobs_memory = setDT(dbReadTable(con, "mlr3_memory_jobs"))
jobs_memory[, args := map(args, function(args) unserialize(args))]

results_memory = set_names(pmap(jobs_memory, function(function_name, args, ...) {
  arg_names = names(args)
  data_memory = setDT(dbReadTable(con, sprintf("mlr3_memory_%s", function_name)))[, c(arg_names, "renv_project", "median_memory", "mad_memory"), with = FALSE]
  data_memory = data_memory[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_memory, c(arg_names, "mlr3"), order = c(rep(1, length(arg_names)), -1))
  data_memory[, -c("renv_project")]
}), jobs_memory$function_name)

# merge memory and runtime data
results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  data_runtime = results_runtime[[function_name]]
  data_memory = results_memory[[function_name]]
  data_runtime[data_memory, on = c(names(args), "mlr3", "paradox")]
}), jobs_runtime$function_name)


jobs_object_size = setDT(dbReadTable(con, "mlr3_object_size_jobs"))
jobs_object_size[, args := map(args, function(args) unserialize(args))]

results_object_size = set_names(pmap(jobs_object_size, function(function_name, args, ...) {
  arg_names = names(args)
  data_object_size = setDT(dbReadTable(con, sprintf("mlr3_object_size_%s", function_name)))[, c(arg_names, "renv_project", "median_size", "median_unserialized_size"), with = FALSE]
  data_object_size = data_object_size[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_object_size, c(arg_names, "mlr3"), order = c(rep(1, length(arg_names)), -1))
  data_object_size = data_object_size[, -c("renv_project")]
  data_object_size[, median_size := median_size / 1e6]
  data_object_size[, median_unserialized_size := median_unserialized_size / 1e6]
}), jobs_object_size$function_name)
```

# Scope

This report analyzes the runtime and memory usage of the mlr3 package across different versions.
It focuses on the `$train()` and `$predict()` methods of the learner and the `resample()` and `benchmark()` functions used for model evaluation.
The benchmarks vary the training time of the models, the number of resampling iterations, and the size of the dataset.
Additionally, the experiments are conducted in parallel to assess the impact of parallelization on both runtime and memory usage.
The overhead introduced by encapsulation is examined by comparing different encapsulation methods.
Furthermore, the report evaluates the object sizes of various mlr3 objects.

Given the extensive package ecosystem of mlr3, performance bottlenecks can occur at multiple stages.
This report aims to help users determine whether the runtime of their workflows falls within expected ranges.
If significant runtime or memory anomalies are observed, users are encouraged to report them by opening a GitHub issue.

Benchmarks are conducted on a high-performance cluster optimized for multi-core performance rather than single-core speed.
Consequently, runtimes may be faster on a local machine.
The benchmarks for `$train()` and `$predict()` were conducted on a different, slower cluster.

# Summary of Latest mlr3 Version

The benchmarks are comprehensive; therefore, we present a summary of the results for the latest mlr3 version.
The runtime of mlr3 should always be considered relative to the training and prediction time of the underlying models.
For example, if the training step of a random forest model `ranger::ranger()` takes 100 ms and the `$train()` method of `lrn("classif.ranger")` takes 110 ms, the overhead is 10%.
When the same model takes 1 second to train, the overhead introduced by mlr3 is only 1%.
Instead of using real models, we simulate the training and prediction time for models by sleeping for 1, 10, 100, and 1000 ms.

We start by measuring the runtime of the `$train()` method of the learner.
For models with a training time of 1000 and 100 ms, the overhead introduced by mlr3 is minimal.
Models with a training time of 10 ms take 2 times longer to train in mlr3.
For models with a training time of 1 ms, the overhead is approximately 10 times larger than the actual model training time.
The overhead of `$predict()` is similar to `$train()` and the size of the dataset being predicted plays only a minor role.
The `$predict_newdata()` method converts the data to a task and then predicts on it which doubles the overhead of the `$predict()` method.
The recently introduced `$predict_newdata_fast()` method is much faster than `$predict_newdata()`.
For models with a prediction time of 10 ms, the overhead is around 10%.
For models with a prediction time of 1 ms, the overhead is around 50%.

The overhead of `resample()` and `benchmark()` is also very low for models with a training time of 1000 and 100 ms.
However, for models with a training time of 10 ms, the overhead approximately doubles the runtime.
In cases where the training time is only 1 ms, the overhead results in the runtime being ten times larger than the actual model training time.
Running an empty R session consumes 131 MB of memory.
Resampling with 10 iterations consumes approximately 164 MB, increasing to around 225 MB when performing 1,000 resampling iterations.
Memory usage for benchmarking is comparable to that of resampling.

mlr3 utilizes the `future` package to enable parallelization over resampling iterations.
However, running `resample()` and `benchmark()` in parallel introduces overhead due to the initiation of worker processes.
Therefore, we compare the runtime of parallel execution with that of sequential execution.
For models with a 1-second training time, using `resample()` and `benchmark()` in parallel reduces runtime.
For models with a 100 ms training time, parallel execution is only advantageous when performing 100 or 1,000 resampling iterations.
For models with 10 ms and 1 ms training times, sequential execution only becomes slower than parallel execution when running 1,000 resampling iterations.
Memory usage increases significantly with the number of cores since each core initiates a separate R session.
Utilizing 10 cores results in a total memory usage of around 1.2 GB.

Encapsulation ensures that signaled conditions (e.g., messages, warnings and errors) are intercepted and that all conditions raised during the training or prediction step are logged into the learner without interrupting the program flow.
The encapsulation of the `$train()` method introduces a runtime overhead of ~1 seconds per model training when using the `callr` package.
In contrast, the encapsulation of the `evaluate` package adds negligible overhead to the runtime.

When saving and loading mlr3 objects, their size can become significantly larger than expected.
This issue is inherent to R6 objects due to data duplication during serialization and deserialization.
The latest versions of mlr3 have implemented various strategies to mitigate this problem, substantially reducing the previously observed large increases in object size.

# Train {#train}

```{r}
#| include: false

# creates a gt table for experiments that depend on the task size
create_table_task = function(data) {
  data = data[, -c("mad_runtime", "mad_memory")]
  data[, task := as.integer(gsub("data_", "", task))]
  setcolorder(data, c("mlr3", "paradox", "task", "median_runtime", "k_1000", "k_100", "k_10", "k_1", "median_memory"))

  data %>%
    gt() %>%
    cols_label(
      mlr3 = "mlr3 Version",
      paradox = "paradox Version",
      task = "Task Size",
      median_runtime = "Runtime [ms]",
      k_1 = "k1",
      k_10 = "k10",
      k_100 = "k100",
      k_1000 = "k1000",
      median_memory = "Memory [MB]") %>%
    fmt_number(columns = c("median_runtime", "median_memory"), decimals = 0, sep_mark = "") %>%
    fmt_number(columns = c("k_1", "k_10", "k_100", "k_1000"), n_sigfig = 2) %>%
    data_color(
      columns = c("k_1", "k_10", "k_100", "k_1000"),
      fn = scales::col_numeric(
        palette = c("#2ecc71", "white"),
        domain = c(1, 3),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
    tab_row_group(
      label = "10000 Observations",
      rows = task == 10000
    ) %>%
    tab_row_group(
      label = "1000 Observations",
      rows = task == 1000
    ) %>%
    tab_row_group(
      label = "100 Observations",
      rows = task == 100
    ) %>%
    tab_row_group(
      label = "10 Observations",
      rows = task == 10
    ) %>%
    tab_row_group(
      label = "1 Observation",
      rows = task == 1
    )
  }

# creates a gt table for experiments that depend on the task size and the number of resampling iterations
create_table_task_evals = function(data, task = "data_1000") {
  .task = task
  data = data[list(.task), , on = "task"]
  data = data[, -c("mad_runtime", "mad_memory", "task")]
  setcolorder(data, c("mlr3", "paradox", "evals", "median_runtime", "k_1000", "k_100", "k_10", "k_1", "median_memory"))

  data %>%
    gt() %>%
    cols_label(
      mlr3 = "mlr3 Version",
      paradox = "paradox Version",
      evals = "Resampling Iterations",
      median_runtime = "Runtime [ms]",
      k_1 = "k1",
      k_10 = "k10",
      k_100 = "k100",
      k_1000 = "k1000",
      median_memory = "Memory [MB]") %>%
    fmt_number(columns = c("median_runtime", "k_1", "k_10", "k_100", "k_1000", "median_memory"), decimals = 0, sep_mark = "") %>%
    data_color(
      columns = c("k_1", "k_10", "k_100", "k_1000"),
      fn = scales::col_numeric(
        palette = c("#2ecc71", "white"),
        domain = c(1, 3),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
    tab_row_group(
      label = "1000 Resampling Iterations",
      rows = evals == 1000
    ) %>%
    tab_row_group(
      label = "100 Resampling Iterations",
      rows = evals == 100
    ) %>%
    tab_row_group(
      label = "10 Resampling Iterations",
      rows = evals == 10
    )
  }

# creates a gt table for experiments that depend on the task size and the number of resampling iterations
create_table_task_methods = function(data, task = "data_1000") {
  .task = task
  data = data[list(.task), , on = "task"]
  data = data[, -c("mad_runtime", "mad_memory", "task")]
  setcolorder(data, c("mlr3", "paradox", "method", "median_runtime", "k_1000", "k_100", "k_10", "k_1", "median_memory"))
  #setorderv(data, c("method", "mlr3", "paradox"), order = c(1, 1, 1))

  data %>%
    gt() %>%
    cols_label(
      mlr3 = "mlr3 Version",
      paradox = "paradox Version",
      method = "Method",
      median_runtime = "Runtime [ms]",
      k_1 = "k1",
      k_10 = "k10",
      k_100 = "k100",
      k_1000 = "k1000",
      median_memory = "Memory [MB]") %>%
    fmt_number(columns = c("median_runtime", "k_1", "k_10", "k_100", "k_1000", "median_memory"), decimals = 0, sep_mark = "") %>%
    data_color(
      columns = c("k_1", "k_10", "k_100", "k_1000"),
      fn = scales::col_numeric(
        palette = c("#2ecc71", "white"),
        domain = c(1, 3),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
    tab_row_group(
      label = "No Encapsulation",
      rows = method == "none"
    ) %>%
    tab_row_group(
      label = "Evaluate",
      rows = method == "evaluate"
    ) %>%
    tab_row_group(
      label = "Callr",
      rows = method == "callr"
    )
  }

create_table_object_size_task = function(data) {
  data[, task := as.integer(gsub("data_", "", task))]
  setcolorder(data, c("mlr3", "paradox", "task", "median_size", "median_unserialized_size"))

  data %>%
    gt() %>%
    cols_label(
      mlr3 = "mlr3 Version",
      paradox = "paradox Version",
      task = "Task Size",
      median_size = "In Memory [MB]",
      median_unserialized_size = "Serialized [MB]") %>%
    fmt_number(columns = c("median_size", "median_unserialized_size"), n_sigfig = 2) %>%
    tab_row_group(
      label = "10000 Observations",
      rows = task == 10000
    ) %>%
    tab_row_group(
      label = "1000 Observations",
      rows = task == 1000
    )
}

create_table_object_size = function(data) {
  setcolorder(data, c("mlr3", "paradox", "median_size", "median_unserialized_size"))

  data %>%
    gt() %>%
    cols_label(
      mlr3 = "mlr3 Version",
      paradox = "paradox Version",
      median_size = "In Memory [MB]",
      median_unserialized_size = "Serialized [MB]") %>%
    fmt_number(columns = c("median_size", "median_unserialized_size"), n_sigfig = 2)
}

create_table_object_size_task_evals = function(data, task = "data_1000") {
  .task = task
  data = data[list(.task), , on = "task"]
  data = data[, -c("task"), with = FALSE]
  setcolorder(data, c("mlr3", "paradox", "evals", "median_size", "median_unserialized_size"))

  data %>%
    gt() %>%
    cols_label(
      mlr3 = "mlr3 Version",
      paradox = "paradox Version",
      evals = "Resampling Iterations",
      median_size = "In Memory [MB]",
      median_unserialized_size = "Serialized [MB]") %>%
    fmt_number(columns = c("median_size", "median_unserialized_size"), n_sigfig = 2) %>%
    tab_row_group(
      label = "1000 Resampling Iterations",
      rows = evals == 1000
    ) %>%
    tab_row_group(
      label = "100 Resampling Iterations",
      rows = evals == 100
    ) %>%
    tab_row_group(
      label = "10 Resampling Iterations",
      rows = evals == 10
    )
}
```

The runtime and memory usage of the `$train()` method is measured for different mlr3 versions.
The train step is performed for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset with 1000 observations.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")

learner$train(task)
```

```{r}
#| echo: false
#| warning: false
create_table_task(results_runtime$train)
```

# Predict {#predict}

The runtime of the `$predict()` method is measured for different mlr3 versions.
The predict step is performed for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset with different amounts of observations (10, 100, 1000, and 10000).

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")

learner$train(task)

learner$predict(task)
```

```{r}
#| echo: false
#| warning: false
create_table_task(results_runtime$predict)
```

# Predict Newdata {#predict-newdata}

The runtime of the `$predict_newdata()` method is measured for different mlr3 versions.
The predict step is performed for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset with different amounts of observations (1, 10, 100, 1000, and 10000).

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")

learner$train(task)

learner$predict_newdata(newdata)
```

```{r}
#| echo: false
#| warning: false
create_table_task(results_runtime$predict_newdata)
```

# Predict Newdata Fast {#predict-newdata-fast}

The runtime of the `$predict_newdata_fast()` method is measured for different mlr3 versions.
The predict step is performed for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset with different amounts of observations (1, 10, 100, 1000, and 10000).

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")

learner$train(task)

learner$predict_newdata_fast(task)
```

```{r}
#| echo: false
#| warning: false
create_table_task(results_runtime$predict_newdata_fast)
```

# Resampling {#resampling}

The runtime and memory usage of the `resample()` function is measured for different mlr3 versions.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset with 1000 observations.
The resampling iterations (`evals`) are set to 1000, 100, and 10.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")

resampling = rsmp("subsampling", repeats = evals)

resample(task, learner, resampling)
```

```{r}
#| echo: false
#| warning: false
create_table_task_evals(results_runtime$resample, task = "data_1000")
```


# Benchmark {#benchmark}

The runtime and memory usage of the `benchmark()` function is measured for different mlr3 versions.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset with 1000 and 10,000 observations.
The resampling iterations (`evals`) are set to 1000, 100, and 10.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")
resampling = rsmp("subsampling", repeats = evals / 5)

design = benchmark_grid(task, replicate(5, learner), resampling)

benchmark(design)
```

```{r}
#| echo: false
#| warning: false
create_table_task_evals(results_runtime$benchmark, task = "data_1000")
```

# Encapsulation {#encapsulation}

The runtime and memory usage of the `$train()` method is measured for different encapsulation methods and mlr3 versions.

```{r}
#| eval: false
task = tsk("spam")
learner = lrn("classif.featureless")
learner$encapsulate(method, fallback = lrn("classif.featureless"))

learner$train(task)
```

```{r}
#| echo: false
#| warning: false
create_table_task_methods(results_runtime$encapsulate)
```

# Object Size {#object-size}

The size of different mlr3 objects is compared for different mlr3 versions.
The size is measured in memory and after calling `serialize()` and `unserialize()`.

## Task

```{r}
#| eval: false
task = tsk("spam_1000")
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Memory usage of a `Task` object depending on the mlr3 version.
#|  The size is measured in memory and after calling `serialize()` and `unserialize()`.
create_table_object_size_task(results_object_size$task)
```

## Learner

```{r}
#| eval: false
learner = lrn("classif.rpart")
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Memory usage of a `lrn("classif.rpart")`object depending on the mlr3 version.
#|  The size is measured in memory and after calling `serialize()` and `unserialize()`.
create_table_object_size(results_object_size$learner_initialize)
```

```{r}
#| eval: false
learner$param_set
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Memory usage of a `lrn("classif.rpart")$param_set`object depending on the mlr3 version.
#|  The size is measured in memory and after calling `serialize()` and `unserialize()`.
create_table_object_size(results_object_size$learner_param_set)
```

```{r}
#| eval: false
learner$train(task)
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Memory usage of a trained `lrn("classif.rpart")`object depending on the mlr3 version.
#|  The size is measured in memory and after calling `serialize()` and `unserialize()`.
create_table_object_size_task(results_object_size$learner_train)
```

```{r}
#| eval: false
learner$model
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Memory usage of a model in a `lrn("classif.rpart")`object depending on the mlr3 version.
#|  The size is measured in memory and after calling `serialize()` and `unserialize()`.
create_table_object_size_task(results_object_size$learner_model)
```

## Prediction

```{r}
#| eval: false
pred = learner$predict(task)
```

`lobstr::obj_size()` seems to misreport the size of `pred$data$response`.
Thus the objects size appears smaller after unserializing.

```{r}
#| echo: false
#| tbl-cap: |
#|  Memory usage of a `Prediction` object depending on the mlr3 version.
#|  The size is measured in memory and after calling `serialize()` and `unserialize()`.
create_table_object_size_task(results_object_size$prediction)
```

## Resampling

```{r}
#| eval: false
resampling = rsmp("subsampling", repeats = evals)
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Memory usage of a `Resampling` object depending on the mlr3 version.
#|  The size is measured in memory and after calling `serialize()` and `unserialize()`.
create_table_object_size_task_evals(results_object_size$resampling, task = "data_1000")
```

```{r}
#| eval: false
resampling$instantiate(task)
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Memory usage of a instantiated `Resampling` object depending on the mlr3 version.
#|  The size is measured in memory and after calling `serialize()` and `unserialize()`.
#create_table_object_size_task_evals(results_object_size$resampling_instantiated, task = "data_1000")
```

## Resample Result

```{r}
#| eval: false
rr = resample(task, learner, resampling)
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Memory usage of a `ResampleResult` object depending on the mlr3 version.
#|  The size is measured in memory and after calling `serialize()` and `unserialize()`.
create_table_object_size_task_evals(results_object_size$resample_result, task = "data_1000")
```


```{r}
#| eval: false
rr = resample(task, learner, resampling, store_models = TRUE)
```


```{r}
#| echo: false
#| tbl-cap: |
#|  Memory usage of a `ResamplingResult` object with models depending on the mlr3 version.
#|  The size is measured in memory and after calling `serialize()` and `unserialize()`.
create_table_object_size_task_evals(results_object_size$resample_result_store_models, task = "data_1000")
```


# Version History

This section provides an overview of the changes in the mlr3 and paradox packages regarding memory usage and runtime.

## mlr3

**mlr3 0.21.0**

* Optimize the runtime of fixing factor levels.
* Optimize the runtime of setting row roles.
* Optimize the runtime of marshalling.
* Optimize the runtime of `Task$col_info`.

**mlr3 0.18.0**

* Skip unnecessary clone of learner's state in `resample()`.

**mlr3 0.17.1**

* Remove `data_prototype` when resampling from `learner$state` to reduce memory consumption.
* Optimize runtime of `resample()` and `benchmark()` by reducing the number of hashing operations.
* Reduce number of threads used by data.table and BLAS to 1 when running `resample()` or `benchmark()` in parallel.

**mlr3 0.17.0**

* Speed up resampling by removing unnecessary calls to `packageVersion()`.
* The `design` of `benchmark()` can now include parameter settings.

## paradox

**paradox 1.0.1**

* Performance improvements.

**paradox 1.0.0**

* Removed Param objects. ParamSet now uses a data.table internally; individual parameters are more like Domain objects now.
