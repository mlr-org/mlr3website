---
title: "mlr3fselect - Runtime and Memory Benchmarks"
sidebar: false
toc: true
cache: false
lazy-cache: false
format:
  html:
    fig-width: 12
    fig-height: 9
---

{{< include ../_setup.qmd >}}

```{r}
#| include: false
library(data.table)
library(ggplot2)
library(gt)
library(DBI)

con = dbConnect(RSQLite::SQLite(), here::here("mlr-org/benchmarks/results_lrz.db"))
snapshot = setDT(dbReadTable(con, "mlr3fselect_snapshots"))
snapshot[, mlr3fselect := factor(mlr3fselect)]

jobs_runtime = setDT(dbReadTable(con, "mlr3fselect_runtime_jobs"))
jobs_runtime[, args := map(args, function(args) unserialize(args))]

results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("mlr3fselect_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "mlr3fselect"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime$function_name)


walk(c("fselect", "fselect_nested"), function(function_name) {
  evals = 1000L
  data = results_runtime[[function_name]]
  data[, k_1 := (median_runtime + 1 * evals) / (1 * evals)]
  data[, k_10 := (median_runtime + 10 * evals) / (10 * evals)]
  data[, k_100 := (median_runtime + 100 * evals) / (100 * evals)]
  data[, k_1000 := (median_runtime + 1000 * evals) / (1000 * evals)]
})

jobs_memory = setDT(dbReadTable(con, "mlr3fselect_runtime_jobs"))
jobs_memory[, args := map(args, function(args) unserialize(args))]

results_memory = set_names(pmap(jobs_memory, function(function_name, args, ...) {
  arg_names = names(args)
  data_memory = setDT(dbReadTable(con, sprintf("mlr3fselect_memory_%s", function_name)))[, c(arg_names, "renv_project", "median_memory", "mad_memory"), with = FALSE]
  data_memory = data_memory[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_memory, c(arg_names, "mlr3fselect"), order = c(rep(1, length(arg_names)), -1))
  data_memory[, -c("renv_project")]
}), jobs_memory$function_name)

# merge memory and runtime data
results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  data_runtime = results_runtime[[function_name]]
  data_memory = results_memory[[function_name]]
  data_runtime[data_memory, on = c(names(args), "mlr3fselect", "bbotk", "mlr3", "paradox")]
}), jobs_runtime$function_name)

# parallel runtime
jobs_runtime_parallel = setDT(dbReadTable(con, "mlr3fselect_runtime_parallel_jobs"))
jobs_runtime_parallel[, args := map(args, function(args) unserialize(args))]

results_runtime_parallel = set_names(pmap(jobs_runtime_parallel, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("mlr3fselect_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "mlr3fselect"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime_parallel$function_name)


results_runtime_parallel = set_names(pmap(list(c("fselect", "fselect_nested"), c("fselect_parallel", "fselect_nested_parallel")), function(function_name, function_name_parallel) {

  data = copy(results_runtime[[function_name]])
  evals = 1000L

  data[, total_runtime_1 := median_runtime + evals * 1]
  data[, total_runtime_10 := median_runtime + evals * 10]
  data[, total_runtime_100 := median_runtime + evals * 100]
  data[, total_runtime_1000 := median_runtime + evals * 1000]

  data_parallel = copy(results_runtime_parallel[[function_name_parallel]])

  data_parallel[, total_runtime_parallel_1 := median_runtime + evals * 1 / 10]
  data_parallel[, total_runtime_parallel_10 := median_runtime + evals * 10 / 10]
  data_parallel[, total_runtime_parallel_100 := median_runtime + evals * 100 / 10]
  data_parallel[, total_runtime_parallel_1000 := median_runtime + evals * 1000 / 100]

  tmp = cbind(data[, list(total_runtime_1, total_runtime_10, total_runtime_100, total_runtime_1000)],
            data_parallel[, list(total_runtime_parallel_1, total_runtime_parallel_10, total_runtime_parallel_100, total_runtime_parallel_1000)])

  tmp[, pk_1 := total_runtime_1 / total_runtime_parallel_1]
  tmp[, pk_10 := total_runtime_10 / total_runtime_parallel_10]
  tmp[, pk_100 := total_runtime_100 / total_runtime_parallel_100]
  tmp[, pk_1000 := total_runtime_1000 / total_runtime_parallel_1000]

  tmp[pk_1 < 1, pk_1 := NA]
  tmp[pk_10 < 1, pk_10 := NA]
  tmp[pk_100 < 1, pk_100 := NA]
  tmp[pk_1000 < 1, pk_1000 := NA]

  cbind(results_runtime[[function_name]], tmp[, list(pk_1, pk_10, pk_100, pk_1000)])
}), c("fselect", "fselect_nested"))

results_runtime[["fselect"]] = results_runtime_parallel[["fselect"]]
results_runtime[["fselect_nested"]] = results_runtime_parallel[["fselect_nested"]]

# creates a gt table for experiments that depend on the task size and the number of resampling iterations
table_task = function(data) {
  data = copy(data)
  data[, task := as.integer(gsub("data_", "", task))]
  data = data[, -c("mad_runtime", "mad_memory")]
  setcolorder(data, c("mlr3fselect", "bbotk", "mlr3", "paradox", "task", "median_runtime", "k_1000", "k_100", "k_10", "k_1", "median_memory"))
  data[, median_runtime := median_runtime / 1000]
  set(data, j = "bbotk", value = NULL)
  set(data, j = "mlr3", value = NULL)
  set(data, j = "paradox", value = NULL)

 data %>%
    gt() %>%
    cols_label(
      mlr3fselect = "mlr3fselect",
      task = "Task Size",
      median_runtime = "Overhead",
      k_1 = html('k<sub>1</sub>'),
      k_10 = html('k<sub>10</sub>'),
      k_100 = html('k<sub>100</sub>'),
      k_1000 = html('k<sub>1000</sub>'),
      pk_1 = html('pk<sub>1</sub>'),
      pk_10 = html('pk<sub>10</sub>'),
      pk_100 = html('pk<sub>100</sub>'),
      pk_1000 = html('pk<sub>1000</sub>'),
      median_memory = "Memory") %>%
    cols_units(
      median_runtime = "s",
      median_memory = "mb"
    ) %>%
    sub_missing(
      columns = everything(),
      rows = everything(),
      missing_text = "---"
    ) %>%
    fmt_number(columns = c("median_runtime", "median_memory"), decimals = 0, sep_mark = "") %>%
    fmt_number(columns = c("k_1", "k_10", "k_100", "k_1000", "pk_1", "pk_10", "pk_100", "pk_1000"), n_sigfig = 2) %>%
    data_color(
      columns = c("k_1", "k_10", "k_100", "k_1000"),
      fn = scales::col_numeric(
        palette = c("#2ecc71", "white"),
        domain = c(1, 3),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
     data_color(
      columns = c("pk_1", "pk_10", "pk_100", "pk_1000"),
      fn = scales::col_numeric(
        palette = c("white", "#A689E1"),
        domain = c(1, max(data$pk_1000, na.rm = TRUE)),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
    tab_row_group(
      label = "10000 Observations",
      rows = task == 10000
    ) %>%
    tab_row_group(
      label = "1000 Observations",
      rows = task == 1000
    )
}
```


# Scope

This report analyzes runtime and memory usage of `mlr3fselect` across recent versions.
It evaluates `fselect()` and `fselect_nested()` in sequential and parallel execution.
Given the size of the `mlr3` ecosystem, performance bottlenecks can arise at multiple stages.
This report enables users to assess whether observed runtimes and memory footprints are within expected ranges.
Substantial anomalies should be reported via a GitHub issue.
Benchmarks are executed on a high‑performance cluster optimized for multi‑core throughput rather than single‑core speed.
Runtimes on modern local machines may therefore differ.

# Summary of Latest mlr3fselect Version

The benchmarks are comprehensive, so we summarize results for the latest `mlr3fselect` version.
We measure runtime and memory for random search with 1,000 resampling iterations on the spam dataset with 1,000 and 10,000 instances.
Nested resampling uses 10 outer iterations and the same random search in the inner loop with a holdout resampling.
Overhead introduced by `fselect()` and `fselect_nested()` must be interpreted relative to the model training time.
For 1 s training time, overhead is minimal.
For 100 ms training time, overhead is approximately 20%.
For 10 ms training time, overhead approximately doubles to triples total runtime.
For 1 ms training time, total runtime is about 16 to 20 times the bare model training time.
Memory usage for `fselect()` and `fselect_nested()` ranges between 450 MB and 550 MB.
An empty R session consumes 131 MB.
`mlr3fselect` parallelizes over resampling iterations using the `future` package.
Parallel execution adds overhead from worker initialization, so we compare parallel and sequential runtimes.
Parallel `fselect()` reduces total runtime for all training times.
Memory increases with core count because each worker is a separate R session.
Using 10 cores requires around 1.8 GB.
`fselect_nested()` parallelizes over the outer resampling loop.
Across all training times, the parallel version is faster than the sequential version.
Total memory usage is approximately 3.3 GB.

# Feature Selection {#fselect}

We measure runtime and memory usage of `fselect()` across `mlr3fselect` versions.
Random search is used with `batch_size = 1000`.
Models are trained on the spam dataset with 1,000 and 10,000 instances.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.rpart")

fselect(
  fselector = fs("random_search", batch_size = 1000),
  task = task,
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000),
  store_benchmark_result = FALSE,
  store_models = FALSE
)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `fselect()` by `mlr3fselect` version and task size.
#|   The k factors indicate how many times longer total runtime is than the model training time.
#|   The subscripts denote reference training times in milliseconds; for example, k100 corresponds to 100 ms.
#|   A green background marks cases where total runtime is less than three times the model training time.
#|   The pk factors report the speedup of parallel relative to sequential execution.
#|   The pk factor is omitted when parallel execution is slower than sequential execution.
table_task(results_runtime$fselect)
```

# Nested Feature Selection {#fselect-nested}

We measure runtime and memory usage of `fselect_nested()` across `mlr3fselect` versions.
The outer resampling performs 10 iterations, and the inner random search evaluates 1,000 feature subsets.
Models are trained on the spam dataset with 1,000 and 10,000 instances.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.rpart")

fselect_nested(
  fselector = fs("random_search", batch_size = 1000),
  task = task,
  learner = learner,
  inner_resampling = rsmp("holdout"),
  outer_resampling = rsmp("subsampling", repeats = 10),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000),
  store_fselect_instance = FALSE,
  store_benchmark_result = FALSE,
  store_models = FALSE
)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `fselect_nested()` by `mlr3fselect` version and task size.
#|   The k factors indicate how many times longer total runtime is than the model training time.
#|   The subscripts denote reference training times in milliseconds; for example, k100 corresponds to 100 ms.
#|   A green background marks cases where total runtime is less than three times the model training time.
#|   The pk factors report the speedup of parallel relative to sequential execution.
#|   The pk factor is omitted when parallel execution is slower than sequential execution.
table_task(results_runtime$fselect_nested)
```


