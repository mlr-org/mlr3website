---
title: "mlr3fselect - Runtime and Memory Benchmarks"
sidebar: false
toc: true
cache: false
lazy-cache: false
format:
  html:
    fig-width: 12
    fig-height: 9
---

{{< include ../_setup.qmd >}}

```{r}
#| include: false
library(data.table)
library(ggplot2)
library(gt)
library(DBI)

con = dbConnect(RSQLite::SQLite(), here::here("mlr-org/benchmarks/results_lrz.db"))
snapshot = setDT(dbReadTable(con, "mlr3fselect_snapshots"))
snapshot[, mlr3fselect := factor(mlr3fselect)]

jobs_runtime = setDT(dbReadTable(con, "mlr3fselect_runtime_jobs"))
jobs_runtime[, args := map(args, function(args) unserialize(args))]

results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("mlr3fselect_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "mlr3fselect"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime$function_name)


walk(c("fselect", "fselect_nested"), function(function_name) {
  evals = 1000L
  data = results_runtime[[function_name]]
  data[, k_1 := (median_runtime + 1 * evals) / (1 * evals)]
  data[, k_10 := (median_runtime + 10 * evals) / (10 * evals)]
  data[, k_100 := (median_runtime + 100 * evals) / (100 * evals)]
  data[, k_1000 := (median_runtime + 1000 * evals) / (1000 * evals)]
})

jobs_memory = setDT(dbReadTable(con, "mlr3fselect_runtime_jobs"))
jobs_memory[, args := map(args, function(args) unserialize(args))]

results_memory = set_names(pmap(jobs_memory, function(function_name, args, ...) {
  arg_names = names(args)
  data_memory = setDT(dbReadTable(con, sprintf("mlr3fselect_memory_%s", function_name)))[, c(arg_names, "renv_project", "median_memory", "mad_memory"), with = FALSE]
  data_memory = data_memory[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_memory, c(arg_names, "mlr3fselect"), order = c(rep(1, length(arg_names)), -1))
  data_memory[, -c("renv_project")]
}), jobs_memory$function_name)

# merge memory and runtime data
results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  data_runtime = results_runtime[[function_name]]
  data_memory = results_memory[[function_name]]
  data_runtime[data_memory, on = c(names(args), "mlr3fselect", "bbotk", "mlr3", "paradox")]
}), jobs_runtime$function_name)

# parallel runtime
jobs_runtime_parallel = setDT(dbReadTable(con, "mlr3fselect_runtime_parallel_jobs"))
jobs_runtime_parallel[, args := map(args, function(args) unserialize(args))]

results_runtime_parallel = set_names(pmap(jobs_runtime_parallel, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("mlr3fselect_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "mlr3fselect"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime_parallel$function_name)


results_runtime_parallel = set_names(pmap(list(c("fselect", "fselect_nested"), c("fselect_parallel", "fselect_nested_parallel")), function(function_name, function_name_parallel) {

  data = copy(results_runtime[[function_name]])
  evals = 1000L

  data[, total_runtime_1 := median_runtime + evals * 1]
  data[, total_runtime_10 := median_runtime + evals * 10]
  data[, total_runtime_100 := median_runtime + evals * 100]
  data[, total_runtime_1000 := median_runtime + evals * 1000]

  data_parallel = copy(results_runtime_parallel[[function_name_parallel]])

  data_parallel[, total_runtime_parallel_1 := median_runtime + evals * 1 / 10]
  data_parallel[, total_runtime_parallel_10 := median_runtime + evals * 10 / 10]
  data_parallel[, total_runtime_parallel_100 := median_runtime + evals * 100 / 10]
  data_parallel[, total_runtime_parallel_1000 := median_runtime + evals * 1000 / 100]

  tmp = cbind(data[, list(total_runtime_1, total_runtime_10, total_runtime_100, total_runtime_1000)],
            data_parallel[, list(total_runtime_parallel_1, total_runtime_parallel_10, total_runtime_parallel_100, total_runtime_parallel_1000)])

  tmp[, pk_1 := total_runtime_1 / total_runtime_parallel_1]
  tmp[, pk_10 := total_runtime_10 / total_runtime_parallel_10]
  tmp[, pk_100 := total_runtime_100 / total_runtime_parallel_100]
  tmp[, pk_1000 := total_runtime_1000 / total_runtime_parallel_1000]

  tmp[pk_1 < 1, pk_1 := NA]
  tmp[pk_10 < 1, pk_10 := NA]
  tmp[pk_100 < 1, pk_100 := NA]
  tmp[pk_1000 < 1, pk_1000 := NA]

  cbind(results_runtime[[function_name]], tmp[, list(pk_1, pk_10, pk_100, pk_1000)])
}), c("fselect", "fselect_nested"))

results_runtime[["fselect"]] = results_runtime_parallel[["fselect"]]
results_runtime[["fselect_nested"]] = results_runtime_parallel[["fselect_nested"]]

# creates a gt table for experiments that depend on the task size and the number of resampling iterations
create_table_task = function(data) {
  data = copy(data)
  data[, task := as.integer(gsub("data_", "", task))]
  data = data[, -c("mad_runtime", "mad_memory")]
  setcolorder(data, c("mlr3fselect", "bbotk", "mlr3", "paradox", "task", "median_runtime", "k_1000", "k_100", "k_10", "k_1", "median_memory"))
  data[, median_runtime := median_runtime / 1000]
  set(data, j = "bbotk", value = NULL)
  set(data, j = "mlr3", value = NULL)
  set(data, j = "paradox", value = NULL)

 data %>%
    gt() %>%
    cols_label(
      mlr3fselect = "mlr3fselect Version",
      task = "Task Size",
      median_runtime = "Overhead",
      k_1 = html('k<sub>1</sub>'),
      k_10 = html('k<sub>10</sub>'),
      k_100 = html('k<sub>100</sub>'),
      k_1000 = html('k<sub>1000</sub>'),
      pk_1 = html('pk<sub>1</sub>'),
      pk_10 = html('pk<sub>10</sub>'),
      pk_100 = html('pk<sub>100</sub>'),
      pk_1000 = html('pk<sub>1000</sub>'),
      median_memory = "Memory") %>%
     cols_units(
      median_runtime = "s",
      median_memory = "mb"
    ) %>%
    fmt_number(columns = c("median_runtime", "median_memory"), decimals = 0, sep_mark = "") %>%
    fmt_number(columns = c("k_1", "k_10", "k_100", "k_1000", "pk_1", "pk_10", "pk_100", "pk_1000"), n_sigfig = 2) %>%
    data_color(
      columns = c("k_1", "k_10", "k_100", "k_1000"),
      fn = scales::col_numeric(
        palette = c("#2ecc71", "white"),
        domain = c(1, 3),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
     data_color(
      columns = c("pk_1", "pk_10", "pk_100", "pk_1000"),
      fn = scales::col_numeric(
        palette = c("white", "#2ecc71"),
        domain = c(1, max(data$pk_1000, na.rm = TRUE)),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
    tab_row_group(
      label = "10000 Observations",
      rows = task == 10000
    ) %>%
    tab_row_group(
      label = "1000 Observations",
      rows = task == 1000
    )
}
```


# Scope

This report analyzes the runtime and memory usage of the `mlr3fselect` package across different versions.
The benchmarks include the `fselect()` and `fselect_nested()` functions both in sequential and parallel mode.
The benchmarks vary the training time of the models and the size of the dataset.

Given the extensive package ecosystem of mlr3, performance bottlenecks can occur at multiple stages.
This report aims to help users determine whether the runtime of their workflows falls within expected ranges.
If significant runtime or memory anomalies are observed, users are encouraged to report them by opening a GitHub issue.

Benchmarks are conducted on a high-performance cluster optimized for multi-core performance rather than single-core speed.
Consequently, runtimes may be faster on a local machine.

# Summary of Latest mlr3fselect Version

The benchmarks are comprehensive; therefore, we present a summary of the results for the latest `mlr3fselect` version.
We measure the runtime and memory usage of a random search with 1000 resampling iterations on the spam dataset with 1000 and 10,000 instances.
The nested resampling is conducted with 10 outer resampling iterations and uses the same random search for the inner resampling loop.
The overhead introduced by `fselect()` and `fselect_nested()` should always be considered relative to the training time of the models.
For models with longer training times, such as 1 second, the overhead is minimal.
For models with a training time of 100 ms, the overhead is approximately 20%.
For models with a training time of 10 ms, the overhead approximately doubles or triples the runtime.
In cases where the training time is only 1 ms, the overhead results in the runtime being 16 to 20 times larger than the actual model training time.
The memory usage of `fselect()` and `fselect_nested()` is between 450 MB and 550 MB.
Running an empty R session consumes 131 MB of memory.

`mlr3fselect` utilizes the `future` package to enable parallelization over resampling iterations.
However, running `fselect()` and `fselect_nested()` in parallel introduces overhead due to the initiation of worker processes.
Therefore, we compare the runtime of parallel execution with that of sequential execution.
For models with a 1-second, 100 ms, and 10 ms training time, using `fselect()`  in parallel reduces runtime.
For models 1 ms training times, sequential execution becomes slower than parallel execution.
Memory usage increases significantly with the number of cores since each core initiates a separate R session.
Utilizing 10 cores results in a total memory usage of around 1.8 GB.
The `fselect_nested()` functions parallelize over the outer resampling loop.
For all training times, the parallel version is faster than the sequential version.
The memory usage is around 3.3 GB.

# Feature Selection {#fselect}

The runtime and memory usage of the `fselect()` function is measured for different mlr3fselect versions.
A random search is used with a batch size of 1000.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset with 1000 and 10,000 instances.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.rpart")

fselect(
  fselector = fs("random_search", batch_size = 1000),
  task = task,
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000),
  store_benchmark_result = FALSE,
  store_models = FALSE
)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `fselect()` by mlr3fselect version and dataset size.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
create_table_task(results_runtime$fselect)
```

# Nested Feature Selection {#fselect-nested}

The runtime and memory usage of the `fselect_nested()` function is measured for different mlr3fselect versions.
The outer resampling has 10 iterations and the inner random search evaluates 1000 feature subsets in total.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset with 1000 and 10,000 instances.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.rpart")

fselect_nested(
  fselector = fs("random_search", batch_size = 1000),
  task = task,
  learner = learner,
  inner_resampling = rsmp("holdout"),
  outer_resampling = rsmp("subsampling", repeats = 10),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000),
  store_fselect_instance = FALSE,
  store_benchmark_result = FALSE,
  store_models = FALSE
)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `fselect_nested()` by mlr3fselect version and dataset size.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
create_table_task(results_runtime$fselect_nested)
```


