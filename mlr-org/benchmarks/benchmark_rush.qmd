---
title: "mlr3 - Runtime and Memory Benchmarks"
sidebar: false
toc: true
cache: false
lazy-cache: false
format:
  html:
    fig-width: 12
    fig-height: 9
---

{{< include ../_setup.qmd >}}

```{r}
#| include: false
library(data.table)
library(ggplot2)
library(gt)
library(DBI)

con = dbConnect(RSQLite::SQLite(), here::here("mlr-org/benchmarks/results_lrz.db"))
snapshot = setDT(dbReadTable(con, "rush_snapshots"))
snapshot[, rush := factor(rush)]

jobs_runtime = setDT(dbReadTable(con, "rush_runtime_jobs"))
jobs_runtime[, args := map(args, function(args) unserialize(args))]

results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("rush_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "rush"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime$function_name)
```

# Scope

This report analyzes the runtime and memory usage of `rush` across the most recent package versions.
It focuses on the core methods `$push_running_tasks()`, `$push_results()` and `$fetch_finished_tasks()`.
This report helps users assess whether observed runtimes fall within expected ranges.
Substantial anomalies in runtime should be reported by opening a GitHub issue.
Benchmarks are executed on a high‑performance cluster optimized for multi‑core throughput rather than single‑core speed.
Consequently, single‑core runtimes may be faster on a modern local machine.

# Summary of Latest mlr3 Version

The benchmarks are comprehensive, so we summarize the results for the latest `mlr3` version.
The runtime overhead of `mlr3` must be interpreted relative to model training and prediction times.
For instance, if `ranger::ranger()` takes 100 ms to train and `lrn("classif.ranger")$train()` takes 110 milliseconds, the overhead is 10%.
If the same model requires 1 second to train, the overhead is 1%.
The overhead is shown relative to the training time of the models with the factors `k_1`, `k_10`, `k_100`, and `k_1000`.
The subscript denotes the model’s training time in milliseconds.
The factors `pk_1`, `pk_10`, `pk_100`, and `pk_1000` report the speedup of parallel over sequential execution.

We first consider `$train()`.
For models with training times of 1000 ms and 100 ms, the overhead is minimal.
When training takes 10 ms, runtime approximately doubles.
For 1 ms models, overhead is roughly ten times the bare model training time.

The overhead of `$predict()` is comparable to `$train()`, and dataset size has only a minor effect.
`$predict_newdata()` converts `newdata` to a task and then predicts, which roughly doubles the overhead relative to `$predict()`.
The recently introduced `$predict_newdata_fast()` is substantially faster than `$predict_newdata()`.
For models with 10 ms prediction time, the overhead is about 10%.
For models with 1 ms prediction time, the overhead is about 50%.

The overhead of `resample()` and `benchmark()` is small for 1000 ms and 100 ms models.
For 10 ms models, the total runtime is approximately twice the bare training time.
For 1 ms models, the total runtime is approximately ten times the bare training time.
An empty R session consumes 131 MB of memory.
Resampling with 10 iterations uses approximately 164 MB, increasing to about 225 MB for 1000 iterations.
Memory usage for `benchmark()` is comparable to `resample()`.

`mlr3` parallelizes over resampling iterations via the `future` package.
Parallel execution adds overhead due to worker initialization.
We therefore compare parallel and sequential runtimes.
For 1 s models, parallel `resample()` and `benchmark()` reduce total runtime.
For 100 ms models, parallelization is advantageous primarily for 100 or 1000 iterations.
For 10 ms and 1 ms models, parallel execution overtakes sequential execution mainly at 1000 iterations.
Memory grows with the number of cores because each core launches a separate R session.
Using 10 cores results in a total memory footprint of approximately 1.2 GB.

Encapsulation captures and logs conditions such as messages, warnings, and errors without interrupting control flow.
Encapsulation via `callr` introduces approximately 1 s of additional runtime per model training.
Encapsulation via `evaluate` adds negligible runtime overhead.

# Push Running Tasks {#push-running-tasks}

```{r}
#| include: false

# gt table for experiments that depend on the task size
table_task = function(data) {
  data = data[, -c("mad_runtime")]
  setcolorder(data, c("rush", "n_tasks", "median_runtime"))

  data %>%
    gt() %>%
    cols_label(
      rush = "rush",
      n_tasks = "Number of Tasks",
      median_runtime = "Runtime",
    ) %>%
    cols_units(
      median_runtime = "ms"
    ) %>%
    fmt_number(columns = c("median_runtime"), decimals = 2, sep_mark = "")
  }
```

The runtime of `$push_running_tasks()`.

```{r}
#| eval: false
xss = list(list(x1 = 1))
keys = rush$push_running_tasks(xss)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime of `$push_running_tasks()` by rush version.
table_task(results_runtime$push_running_tasks)
```

# Push Results {#push-results}

The runtime of `$push_results()`.

```{r}
#| eval: false
yss = list(list(y = 1))
rush$push_results(keys, yss)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime of `$push_results()` by rush version.
table_task(results_runtime$push_results)
```

# Fetch Finished Tasks {#fetch-finished-tasks}

The runtime of `$fetch_finished_tasks()`.

```{r}
#| eval: false
rush$fetch_finished_tasks()
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime of `$fetch_finished_tasks()` by number of tasks and rush version.
table_task(results_runtime$fetch_finished_tasks)
```

# Fetch Finished Tasks with Cache {#fetch-finished-tasks-with-cache}

```{r}
#| include: false
table_fetch = function(data) {
  data = data[, -c("mad_runtime", "n_tasks")]
  setcolorder(data, c("rush", "cache_size", "median_runtime"))

  data %>%
    gt() %>%
    cols_label(
      rush = "rush",
      cache_size = "Cache Size",
      median_runtime = "Runtime",
    ) %>%
    cols_units(
      median_runtime = "ms"
    ) %>%
    fmt_number(columns = c("median_runtime"), decimals = 2, sep_mark = "")
  }
```

The runtime of `$fetch_finished_tasks()` when one new task is fetched and the other tasks are cached.

```{r}
#| eval: false
rush$fetch_finished_tasks()
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime of `$fetch_finished_tasks()` with cache by cache size and rush version.
table_fetch(results_runtime$fetch_cached_tasks)
```
