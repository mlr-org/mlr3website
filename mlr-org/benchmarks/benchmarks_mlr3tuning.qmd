---
title: "mlr3tuning - Runtime and Memory Benchmarks"
sidebar: false
toc: true
cache: false
lazy-cache: false
format:
  html:
    fig-width: 12
    fig-height: 9
---

{{< include ../_setup.qmd >}}

```{r}
#| include: false
library(data.table)
library(ggplot2)
library(gt)
library(DBI)

con = dbConnect(RSQLite::SQLite(), here::here("mlr-org/benchmarks/results_lrz.db"))
snapshot = setDT(dbReadTable(con, "mlr3tuning_snapshots"))
snapshot[, mlr3 := factor(mlr3)]
snapshot[, paradox := factor(paradox)]

jobs_runtime = setDT(dbReadTable(con, "mlr3tuning_runtime_jobs"))
jobs_runtime[, args := map(args, function(args) unserialize(args))]

results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("mlr3tuning_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "mlr3"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime$function_name)


walk(c("tune", "tune_nested"), function(function_name) {
  evals = 1000L
  data = results_runtime[[function_name]]
  data[, k_1 := (median_runtime + 1 * evals) / (1 * evals)]
  data[, k_10 := (median_runtime + 10 * evals) / (10 * evals)]
  data[, k_100 := (median_runtime + 100 * evals) / (100 * evals)]
  data[, k_1000 := (median_runtime + 1000 * evals) / (1000 * evals)]
})

# jobs_memory = setDT(dbReadTable(con, "mlr3tuning_memory_jobs"))
# jobs_memory[, args := map(args, function(args) unserialize(args))]

# results_memory = set_names(pmap(jobs_memory, function(function_name, args, ...) {
#   arg_names = names(args)
#   data_memory = setDT(dbReadTable(con, sprintf("mlr3tuning_memory_%s", function_name)))[, c(arg_names, "renv_project", "median_memory", "mad_memory"), with = FALSE]
#   data_memory = data_memory[snapshot, on = "renv_project", nomatch = 0]
#   setorderv(data_memory, c(arg_names, "mlr3"), order = c(rep(1, length(arg_names)), -1))
#   data_memory[, -c("renv_project")]
# }), jobs_memory$function_name)

# # merge memory and runtime data
# results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
#   data_runtime = results_runtime[[function_name]]
#   data_memory = results_memory[[function_name]]
#   data_runtime[data_memory, on = c(names(args), "mlr3", "paradox")]
# }), jobs_runtime$function_name)


# jobs_object_size = setDT(dbReadTable(con, "mlr3_object_size_jobs"))
# jobs_object_size[, args := map(args, function(args) unserialize(args))]

# results_object_size = set_names(pmap(jobs_object_size, function(function_name, args, ...) {
#   arg_names = names(args)
#   data_object_size = setDT(dbReadTable(con, sprintf("mlr3_object_size_%s", function_name)))[, c(arg_names, "renv_project", "median_size", "median_unserialized_size"), with = FALSE]
#   data_object_size = data_object_size[snapshot, on = "renv_project", nomatch = 0]
#   setorderv(data_object_size, c(arg_names, "mlr3"), order = c(rep(1, length(arg_names)), -1))
#   data_object_size = data_object_size[, -c("renv_project")]
#   data_object_size[, median_size := median_size / 1e6]
#   data_object_size[, median_unserialized_size := median_unserialized_size / 1e6]
# }), jobs_object_size$function_name)

# creates a gt table for experiments that depend on the task size and the number of resampling iterations
create_table_task = function(data) {
  data[, task := as.integer(gsub("data_", "", task))]
  data[, median_memory := 100L] # remove!
  data = data[, -c("mad_runtime", "mad_memory")]
  setcolorder(data, c("mlr3tuning", "bbotk", "mlr3", "paradox", "task", "median_runtime", "k_1000", "k_100", "k_10", "k_1", "median_memory"))

 data %>%
    gt() %>%
    cols_label(
      mlr3tuning = "mlr3tuning Version",
      bbotk = "bbotk Version",
      mlr3 = "mlr3 Version",
      paradox = "paradox Version",
      task = "Task Size",
      median_runtime = "Runtime [ms]",
      k_1 = "k1",
      k_10 = "k10",
      k_100 = "k100",
      k_1000 = "k1000",
      median_memory = "Memory [MB]") %>%
    fmt_number(columns = c("median_runtime", "median_memory"), decimals = 0, sep_mark = "") %>%
    fmt_number(columns = c("k_1", "k_10", "k_100", "k_1000"), n_sigfig = 2) %>%
    data_color(
      columns = c("k_1", "k_10", "k_100", "k_1000"),
      fn = scales::col_numeric(
        palette = c("#2ecc71", "white"),
        domain = c(1, 3),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
    tab_row_group(
      label = "10000 Observations",
      rows = task == 10000
    ) %>%
    tab_row_group(
      label = "1000 Observations",
      rows = task == 1000
    )
}


# library(data.table)
# library(ggplot2)
# library(gt)
# library(DBI)

# con = dbConnect(RSQLite::SQLite(), here::here("mlr-org/benchmarks/results.db"))
# snapshot = setDT(dbReadTable(con, "mlr3tuning_snapshots"))
# snapshot[, mlr3 := factor(mlr3)]
# snapshot[, paradox := factor(paradox)]
# snapshot[, mlr3tuning := factor(mlr3tuning)]
# snapshot[, bbotk := factor(bbotk)]

# plot_runtime = function(data) {
#   ggplot(data, aes(x = mlr3tuning, y = median_runtime)) +
#   geom_col(group = 1, fill = "#008080") +
#   geom_errorbar(aes(ymin = pmax(median_runtime - mad_runtime, 0), ymax = median_runtime + mad_runtime), width = 0.5, position = position_dodge(0.9)) +
#   geom_hline(aes(yintercept = total_model_time), linetype = "dashed") +
#   labs(x = "mlr3Version", y = "Runtime [s]") +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
# }

# plot_memory = function(data) {
#   ggplot(data, aes(x = mlr3tuning, y = median_memory)) +
#   geom_col(group = 1, fill = "#ff6347") +
#   geom_errorbar(aes(ymin = median_memory - mad_memory, ymax = median_memory + mad_memory), width = 0.5, position = position_dodge(0.9)) +
#   geom_hline(aes(yintercept = 131), linetype = "dashed") +
#   #facet_wrap(~scales = "free_y", labeller = labeller(evals = function(value) sprintf("%s Resampling Iterations", value))) +
#   labs(x = "mlr3 Version", y = "Memory [MB]") +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
# }

# create_table = function(data) {
#   data = data[, -c("mad_memory", "mad_runtime")]

#   data_1000 = data[task == "data_1000", -"task"]
#   data_10000 = data[task == "data_10000", -c("task", "k", "total_model_time")]
#   data = merge(data_1000, data_10000, by = c("mlr3tuning", "bbotk", "mlr3", "paradox", "model_time"), suffixes = c("", "_10000"))

#   setcolorder(data, c("mlr3tuning", "bbotk", "mlr3", "paradox", "model_time", "total_model_time", "median_runtime", "median_runtime_10000", "k", "median_memory", "median_memory_10000"))

#   data %>%
#     gt() %>%
#     cols_label(
#       mlr3tuning = "mlr3tuning Version",
#       bbotk = "bbotk Version",
#       mlr3 = "mlr3 Version",
#       paradox = "paradox Version",
#       model_time = "Model Time [ms]",
#       total_model_time = "Total Model Time [s]",
#       median_runtime = "Median Runtime [s]",
#       median_runtime_10000 = "Median Runtime 10,000 [s]",
#       k = "K",
#       median_memory = "Median Memory [MB]",
#       median_memory_10000 = "Median Memory 10,000 [s]") %>%
#     fmt_number(columns = c("k", "median_runtime", "median_runtime_10000"), n_sigfig = 2) %>%
#     fmt_number(columns = c("median_memory", "median_memory_10000"), decimals = 0) %>%
#     tab_style(
#       style = list(
#         cell_fill(color = "crimson"),
#         cell_text(weight = "bold")
#       ),
#       locations = cells_body(
#         columns = "k",
#         rows = k > 3
#       )
#     )
#   }
```

# Scope

This report analyzes the runtime and memory usage of the `mlr3tuning` package across different versions.
The benchmarks include the `tune()` and `tune_nested()` functions both in sequential and parallel mode.
The benchmarks vary the training time of the models and the size of the dataset.

Given the extensive package ecosystem of mlr3, performance bottlenecks can occur at multiple stages.
This report aims to help users determine whether the runtime of their workflows falls within expected ranges.
If significant runtime or memory anomalies are observed, users are encouraged to report them by opening a GitHub issue.

Benchmarks are conducted on a high-performance cluster optimized for multi-core performance rather than single-core speed.
Consequently, runtimes may be faster on a local machine.

# Summary of Latest mlr3tuning Version

The benchmarks are comprehensive; therefore, we present a summary of the results for the latest `mlr3tuning` version.
We measure the runtime and memory usage of a random search with 1000 resampling iterations on the spam dataset with 1000 and 10,000 instances.
The nested resampling is conducted with 10 outer resampling iterations and uses the same random search for the inner resampling loop.
The overhead introduced by `tune()` and `tune_nested()` should always be considered relative to the training time of the models.
For models with longer training times, such as 1 second, the overhead is minimal.
For models with a training time of 100 ms, the overhead is approximately 20%.
For models with a training time of 10 ms, the overhead approximately doubles or triples the runtime.
In cases where the training time is only 1 ms, the overhead results in the runtime being 15 to 25 times larger than the actual model training time.
The memory usage of `tune()` and `tune_nested()` is between 370 MB and 670 MB.
Running an empty R session consumes 131 MB of memory.

`mlr3tuning` utilizes the `future` package to enable parallelization over resampling iterations.
However, running `tune()` and `tune_nested()` in parallel introduces overhead due to the initiation of worker processes.
Therefore, we compare the runtime of parallel execution with that of sequential execution.
For models with a 1-second, 100 ms, and 10 ms training time, using `tune()`  in parallel reduces runtime.
For models 1 ms training times, sequential execution becomes slower than parallel execution.
Memory usage increases significantly with the number of cores since each core initiates a separate R session.
Utilizing 10 cores results in a total memory usage of around 1.5 GB.
The `tune_nested()` functions parallelize over the outer resampling loop.
For all training times, the parallel version is faster than the sequential version.
The memory usage is around 3.6 GB.

# Tune {#tune}

The runtime and memory usage of the `tune()` function is measured for different mlr3tuning versions.
A random search is used with a batch size of 1000.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset with 1000 and 10,000 instances.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.rpart",
  cp = to_tune(0, 1))

tune(
  tune = tnr("random_search", batch_size = 1000),
  task = task,
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000),
  store_benchmark_result = FALSE,
  store_models = FALSE
)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `tune()` by mlr3tuning version and dataset size.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
create_table_task(results_runtime$tune)
```

# Nested Tuning {#tune-nested}

The runtime and memory usage of the `tune_nested()` function is measured for different mlr3tuning versions.
The outer resampling has 10 iterations and the inner random search evaluates 1000 configurations in total.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset with 1000 and 10,000 instances.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.rpart",
  cp = to_tune(0, 1))

tune_nested(
  tuner = tnr("random_search", batch_size = 1000),
  task = task,
  learner = learner,
  inner_resampling = rsmp("holdout"),
  outer_resampling = rsmp("subsampling", repeats = 10),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000),
  store_tune_instance = FALSE,
  store_benchmark_result = FALSE,
  store_models = FALSE
)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `tune_nested()` by mlr3tuning version and dataset size.
#|   The k factors indicate how many times longer the total runtime is compared to the model training time.
#|   The numbers represent the model training time itself e.g. k100 are models trained for 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
create_table_task(results_runtime$tune_nested)
```
