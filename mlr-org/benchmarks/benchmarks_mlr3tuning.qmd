---
title: "mlr3tuning - Runtime and Memory Benchmarks"
sidebar: false
toc: true
cache: false
lazy-cache: false
format:
  html:
    fig-width: 12
    fig-height: 9
---

{{< include ../_setup.qmd >}}

```{r}
#| include: false
library(data.table)
library(ggplot2)
library(gt)
library(DBI)

con = dbConnect(RSQLite::SQLite(), here::here("mlr-org/benchmarks/results_lrz.db"))
snapshot = setDT(dbReadTable(con, "mlr3tuning_snapshots"))
snapshot[, mlr3tuning := factor(mlr3tuning)]

jobs_runtime = setDT(dbReadTable(con, "mlr3tuning_runtime_jobs"))
jobs_runtime[, args := map(args, function(args) unserialize(args))]

results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("mlr3tuning_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "mlr3tuning"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime$function_name)


walk(c("tune", "tune_nested"), function(function_name) {
  evals = 1000L
  data = results_runtime[[function_name]]
  data[, k_1 := (median_runtime + 1 * evals) / (1 * evals)]
  data[, k_10 := (median_runtime + 10 * evals) / (10 * evals)]
  data[, k_100 := (median_runtime + 100 * evals) / (100 * evals)]
  data[, k_1000 := (median_runtime + 1000 * evals) / (1000 * evals)]
})

jobs_memory = setDT(dbReadTable(con, "mlr3tuning_runtime_jobs"))
jobs_memory[, args := map(args, function(args) unserialize(args))]

results_memory = set_names(pmap(jobs_memory, function(function_name, args, ...) {
  arg_names = names(args)
  data_memory = setDT(dbReadTable(con, sprintf("mlr3tuning_memory_%s", function_name)))[, c(arg_names, "renv_project", "median_memory", "mad_memory"), with = FALSE]
  data_memory = data_memory[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_memory, c(arg_names, "mlr3tuning"), order = c(rep(1, length(arg_names)), -1))
  data_memory[, -c("renv_project")]
}), jobs_memory$function_name)

# merge memory and runtime data
results_runtime = set_names(pmap(jobs_runtime, function(function_name, args, ...) {
  data_runtime = results_runtime[[function_name]]
  data_memory = results_memory[[function_name]]
  data_runtime[data_memory, on = c(names(args), "mlr3tuning", "bbotk", "mlr3", "paradox")]
}), jobs_runtime$function_name)

# parallel runtime
jobs_runtime_parallel = setDT(dbReadTable(con, "mlr3tuning_runtime_parallel_jobs"))
jobs_runtime_parallel[, args := map(args, function(args) unserialize(args))]

results_runtime_parallel = set_names(pmap(jobs_runtime_parallel, function(function_name, args, ...) {
  arg_names = names(args)
  data_runtime = setDT(dbReadTable(con, sprintf("mlr3tuning_runtime_%s", function_name)))[, c(arg_names, "renv_project", "median_runtime", "mad_runtime"), with = FALSE]
  data_runtime = data_runtime[snapshot, on = "renv_project", nomatch = 0]
  setorderv(data_runtime, c(arg_names, "mlr3tuning"), order = c(rep(1, length(arg_names)), -1))
  data_runtime[, -c("renv_project")]
}), jobs_runtime_parallel$function_name)


results_runtime_parallel = set_names(pmap(list(c("tune", "tune_nested"), c("tune_parallel", "tune_nested_parallel")), function(function_name, function_name_parallel) {

  data = copy(results_runtime[[function_name]])
  evals = 1000L

  data[, total_runtime_1 := median_runtime + evals * 1]
  data[, total_runtime_10 := median_runtime + evals * 10]
  data[, total_runtime_100 := median_runtime + evals * 100]
  data[, total_runtime_1000 := median_runtime + evals * 1000]

  data_parallel = copy(results_runtime_parallel[[function_name_parallel]])

  data_parallel[, total_runtime_parallel_1 := median_runtime + evals * 1 / 10]
  data_parallel[, total_runtime_parallel_10 := median_runtime + evals * 10 / 10]
  data_parallel[, total_runtime_parallel_100 := median_runtime + evals * 100 / 10]
  data_parallel[, total_runtime_parallel_1000 := median_runtime + evals * 1000 / 100]

  tmp = cbind(data[, list(total_runtime_1, total_runtime_10, total_runtime_100, total_runtime_1000)],
            data_parallel[, list(total_runtime_parallel_1, total_runtime_parallel_10, total_runtime_parallel_100, total_runtime_parallel_1000)])

  tmp[, pk_1 := total_runtime_1 / total_runtime_parallel_1]
  tmp[, pk_10 := total_runtime_10 / total_runtime_parallel_10]
  tmp[, pk_100 := total_runtime_100 / total_runtime_parallel_100]
  tmp[, pk_1000 := total_runtime_1000 / total_runtime_parallel_1000]

  tmp[pk_1 < 1, pk_1 := NA]
  tmp[pk_10 < 1, pk_10 := NA]
  tmp[pk_100 < 1, pk_100 := NA]
  tmp[pk_1000 < 1, pk_1000 := NA]

  cbind(results_runtime[[function_name]], tmp[, list(pk_1, pk_10, pk_100, pk_1000)])
}), c("tune", "tune_nested"))

results_runtime[["tune"]] = results_runtime_parallel[["tune"]]
results_runtime[["tune_nested"]] = results_runtime_parallel[["tune_nested"]]

# creates a gt table for experiments that depend on the task size and the number of resampling iterations
table_task = function(data) {
  data = copy(data)
  data[, task := as.integer(gsub("data_", "", task))]
  data = data[, -c("mad_runtime", "mad_memory")]
  setcolorder(data, c("mlr3tuning", "bbotk", "mlr3", "paradox", "task", "median_runtime", "k_1000", "k_100", "k_10", "k_1", "median_memory"))
  data[, median_runtime := median_runtime / 1000]
  set(data, j = "bbotk", value = NULL)
  set(data, j = "mlr3", value = NULL)
  set(data, j = "paradox", value = NULL)

 data %>%
    gt() %>%
    cols_label(
      mlr3tuning = "mlr3tuning Version",
      task = "Task Size",
      median_runtime = "Overhead",
      k_1 = html('k<sub>1</sub>'),
      k_10 = html('k<sub>10</sub>'),
      k_100 = html('k<sub>100</sub>'),
      k_1000 = html('k<sub>1000</sub>'),
      pk_1 = html('pk<sub>1</sub>'),
      pk_10 = html('pk<sub>10</sub>'),
      pk_100 = html('pk<sub>100</sub>'),
      pk_1000 = html('pk<sub>1000</sub>'),
      median_memory = "Memory") %>%
     cols_units(
      median_runtime = "s",
      median_memory = "mb"
    ) %>%
    sub_missing(
      columns = everything(),
      rows = everything(),
      missing_text = "---"
    ) %>%
    fmt_number(columns = c("median_runtime", "median_memory"), decimals = 0, sep_mark = "") %>%
    fmt_number(columns = c("k_1", "k_10", "k_100", "k_1000", "pk_1", "pk_10", "pk_100", "pk_1000"), n_sigfig = 2) %>%
    data_color(
      columns = c("k_1", "k_10", "k_100", "k_1000"),
      fn = scales::col_numeric(
        palette = c("#2ecc71", "white"),
        domain = c(1, 3),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
     data_color(
      columns = c("pk_1", "pk_10", "pk_100", "pk_1000"),
      fn = scales::col_numeric(
        palette = c("white", "#A689E1"),
        domain = c(1, max(data$pk_1000, na.rm = TRUE)),
        na.color = "white"
      ),
      apply_to = "fill",
      autocolor_text = TRUE
    ) %>%
    tab_row_group(
      label = "10000 Observations",
      rows = task == 10000
    ) %>%
    tab_row_group(
      label = "1000 Observations",
      rows = task == 1000
    )
}
```

# Scope

This report analyzes the runtime and memory usage of `mlr3tuning` across versions.
It evaluates `tune()` and `tune_nested()` in sequential and parallel modes.
Given the size of the `mlr3` ecosystem, performance bottlenecks can arise at multiple stages.
This report helps users judge whether observed runtimes and memory footprints are within expected ranges.
Substantial anomalies should be reported via a GitHub issue.
Benchmarks are executed on a high‑performance cluster optimized for multi‑core throughput rather than single‑core speed.
Consequently, runtimes may be faster on a modern local machine.

# Summary of Latest mlr3tuning Version

The benchmarks are comprehensive, so we summarize results for the latest `mlr3tuning` version.
We measure runtime and memory for random search with 1,000 resampling iterations on the spam dataset with 1,000 and 10,000 instances.
Nested resampling uses 10 outer iterations and the same random search in the inner loop.
Overhead introduced by `tune()` and `tune_nested()` must be interpreted relative to model training time.
For 1 s training time, overhead is minimal.
For 100 ms training time, overhead is approximately 20%.
For 10 ms training time, overhead approximately doubles to triples total runtime.
For 1 ms training time, total runtime is about 15 to 25 times the bare model training time.
Memory usage for `tune()` and `tune_nested()` ranges between 370 MB and 670 MB.
An empty R session consumes 131 MB.
`mlr3tuning` parallelizes over resampling iterations using the `future` package.
Parallel execution adds overhead from worker initialization, so we compare parallel and sequential runtimes.
For all training times, parallel `tune()` reduces total runtime.
Memory increases with core count because each worker is a separate R session.
Using 10 cores requires around 1.5 GB.
`tune_nested()` parallelizes over the outer resampling loop.
Across all training times, the parallel version is faster than the sequential version.
Total memory usage is approximately 3.6 GB.

# Tune {#tune}

The runtime and memory usage of `tune()` are measured across `mlr3tuning` versions.
A random search is used with a batch size of 1,000.
Models are trained on the spam dataset with 1,000 and 10,000 instances.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.rpart",
  cp = to_tune(0, 1))

tune(
  tune = tnr("random_search", batch_size = 1000),
  task = task,
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000),
  store_benchmark_result = FALSE,
  store_models = FALSE
)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `tune()` by `mlr3tuning` version and task size.
#|   The k factors indicate how many times longer total runtime is than the model training time.
#|   The subscripts denote reference training times in milliseconds; for example, k100 corresponds to 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
#|   The pk factors report the speedup of parallel relative to sequential execution.
#|   The pk factor is omitted when parallel execution is slower than sequential execution.
table_task(results_runtime$tune)
```

# Nested Tuning {#tune-nested}

The runtime and memory usage of `tune_nested()` are measured across `mlr3tuning` versions.
The outer resampling performs 10 iterations, and the inner random search evaluates 1,000 feature subsets.
Models are trained on the spam dataset with 1,000 and 10,000 instances.

```{r}
#| eval: false
task = tsk("spam")

learner = lrn("classif.rpart",
  cp = to_tune(0, 1))

tune_nested(
  tuner = tnr("random_search", batch_size = 1000),
  task = task,
  learner = learner,
  inner_resampling = rsmp("holdout"),
  outer_resampling = rsmp("subsampling", repeats = 10),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 1000),
  store_tune_instance = FALSE,
  store_benchmark_result = FALSE,
  store_models = FALSE
)
```

```{r}
#| echo: false
#| warning: false
#| column: body-outset
#| tbl-cap: |
#|   Runtime and memory usage of `tune_nested()` by `mlr3tuning` version and task size.
#|   The k factors indicate how many times longer total runtime is than the model training time.
#|   The subscripts denote reference training times in milliseconds; for example, k100 corresponds to 100 ms.
#|   A green background highlights cases where the total runtime is less than three times the model training time.
#|   The pk factors report the speedup of parallel relative to sequential execution.
#|   The pk factor is omitted when parallel execution is slower than sequential execution.
table_task(results_runtime$tune_nested)
```
