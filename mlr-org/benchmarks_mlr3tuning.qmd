---
title: "Benchmarks mlr3tuning"
sidebar: false
toc: true
---

{{< include _setup.qmd >}}


```{r}
#| include: false
library(data.table)
library(ggplot2)
library(gt)

snapshot = mlr3misc::rowwise_table(
  ~renv_project,          ~mlr3tuning,  ~bbotk,   ~mlr3,      ~paradox,
  "snapshot_2022_12_22",  "0.17.2",     "0.7.2",  "0.14.1",   "0.11.0",
  "snapshot_2023_03_08",  "0.18.0",     "0.7.2",  "0.14.1",   "0.11.0",
  "snapshot_2023_06_26",  "0.19.0",     "0.7.2",  "0.16.1",   "0.11.1",
  "snapshot_2023_11_20",  "0.19.1",     "0.7.3",  "0.17.0",   "0.11.1",
  "snapshot_2023_11_28",  "0.19.2",     "0.7.3",  "0.17.0",   "0.11.1",
  "snapshot_2024_04_24",  "0.20.0",     "0.8.0",  "0.19.0",   "0.11.1",
  "snapshot_2024_07_05",  "1.0.0",      "1.0.0",  "0.20.0",   "1.0.1"
)

plot_runtime = function(data) {
  ggplot(data, aes(x = mlr3tuning, y = median_run_time)) +
  geom_col(group = 1, fill = "#008080") +
  geom_hline(aes(yintercept = total_model_time), linetype = "dashed") +
  geom_errorbar(aes(ymin = median_run_time - mad_run_time, ymax = median_run_time + mad_run_time), width = 0.5, position = position_dodge(0.9)) +
  facet_wrap(~evals, scales = "free_y", labeller = labeller(evals = function(value) sprintf("%s Resampling Iterations", value))) +
  labs(x = "mlr3tuning Version", y = "Runtime [s]") +
  theme_minimal(base_size = 7) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

plot_memory = function(data) {
  ggplot(data, aes(x = mlr3tuning, y = median_memory)) +
  geom_col(group = 1, fill = "#ff6347") +
  geom_errorbar(aes(ymin = median_memory - mad_memory, ymax = median_memory + mad_memory), width = 0.5, position = position_dodge(0.9)) +
  facet_wrap(~evals, scales = "free_y", labeller = labeller(evals = function(value) sprintf("%s Resampling Iterations", value))) +
  labs(x = "mlr3tuning Version", y = "Memory [MB]") +
  theme_minimal(base_size = 7) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

create_table = function(data) {
 data = data[, -c("mad_run_time", "mad_memory", "renv_project", "algorithm", "total_model_time")]

  data_1000 = data[task == "data_1000", -"task"]
  data_10000 = data[task == "data_10000", -c("task", "k")]
  data = merge(data_1000, data_10000, by = c("mlr3tuning", "bbotk", "mlr3", "paradox", "model_time", "evals"), suffixes = c("", "_10000"))

  setcolorder(data, c("mlr3tuning", "bbotk", "mlr3", "paradox", "model_time", "evals", "median_run_time", "median_run_time_10000", "k", "median_memory", "median_memory_10000"))

  data %>%
    gt() %>%
    cols_label(
      mlr3tuning = "mlr3tuning Version",
      mlr3 = "mlr3 Version",
      paradox = "paradox Version",
      model_time = "Model Time [ms]",
      evals = "Resampling Iterations",
      median_run_time = "Median Runtime [s]",
      median_run_time_10000 = "Median Runtime 10,000 [s]",
      k = "K",
      median_memory = "Median Memory [MB]",
      median_memory_10000 = "Median Memory 10,000 [MB]") %>%
    fmt_number(columns = c("k", "median_run_time", "median_run_time_10000"), n_sigfig = 2) %>%
    fmt_number(columns = c("median_memory", "median_memory_10000"), decimals = 0) %>%
    tab_style(
      style = list(
        cell_fill(color = "crimson"),
        cell_text(weight = "bold")
      ),
      locations = cells_body(
        columns = "k",
        rows = k > 3
      )
    )  %>%
    tab_row_group(
      label = "1000 Resampling Iterations",
      rows = evals == 1000
    ) %>%
    tab_row_group(
      label = "100 Resampling Iterations",
      rows = evals == 100
    ) %>%
    tab_row_group(
      label = "10 Resampling Iterations",
      rows = evals == 10
    )
  }
```

# Version History

**mlr3 0.18.0**

* Skip unnecessary clone of learner's state in `resample()`.

**mlr3 0.17.1**

* Remove `data_prototype` when resampling from `learner$state` to reduce memory consumption.
* Optimize Median runtimeof `resample()` and `benchmark()` by reducing the number of hashing operations.

**mlr3 0.17.0**

* Speed up resampling by removing unnecessary calls to `packageVersion()`.
* The `design` of `benchmark()` can now include parameter settings.

**paradox 1.0.1**

* Performance improvements.

# Hyperparameter Optimization

```{r}
#| include: false
data_memory = fread(here::here("mlr-org/benchmark_results/mlr3tuning_tune_memory.csv"))[, list(model_time, task, evals, renv_project, algorithm, median_memory, mad_memory)]
data_runtime = fread(here::here("mlr-org/benchmark_results/mlr3tuning_tune_runtime.csv"))[, list(model_time, task, evals, renv_project, algorithm, median_run_time, mad_run_time, k)]
data = merge(data_memory, data_runtime, by = c("model_time", "task", "evals", "renv_project", "algorithm"))

data[, renv_project := gsub("mlr3tuning/default/snapshots/", "", renv_project)]
data = data[snapshot, on = "renv_project"]
data[, mlr3tuning := factor(mlr3tuning, levels = c("0.17.2", "0.18.0", "0.19.0", "0.19.1", "0.19.2", "0.20.0", "1.0.0"))]
setorderv(data, c("algorithm", "model_time", "evals"), order = c(1, 1, 1))
data[, median_run_time := median_run_time / 1000] #ms to s
data[, mad_run_time := mad_run_time / 1000] #ms to s
data[, total_model_time := model_time * evals / 1000] #ms to s
tune_median_run_time = data[, list(model_time, task, evals, renv_project, median_run_time)]
```

The runtime and memory usage of the `tune()` function is compared for different mlr3tuning versions.
The function is used to perform hyperparameter optimization with a random search.
The models are trained for different amounts of time (1 ms, 10 ms, 100 ms, and 1000 ms) on the spam dataset.
The resampling iterations (`evals`) are set to 1000, 100, and 10.

```{r}
#| eval: false
learner = lrn("classif.sleep",
  sleep_train = model_time / 2,
  sleep_predict = model_time / 2)

instance = tune(
  tuner = fs("random_search", batch_size = evals),
  task = tsk("spam"),
  learner = learner,
  resampling = rsmp("holdout"),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = evals),
  store_benchmark_result = FALSE,
  store_models = FALSE
)
```

## Model Time 1000 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `tune()` with models trained for 1 s depending on the mlr3tuning version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute deviation of the runtime.
plot_runtime(data[algorithm == "tune_sequential" & model_time == 1000 & task == "data_1000"])
```

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `tune()` with models trained for 1 s depending on the mlr3tuning version.
#|  Error bars represent the median absolute deviation of the memory usage.
plot_memory(data[algorithm == "tune_sequential" & model_time == 1000 & task == "data_1000"])
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Runtime and memory usage of `tune()` with models trained for 1 s depending on the mlr3tuning version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
create_table(data[algorithm == "tune_sequential" & model_time == 1000])
```

## Model Time 100 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `tune()` with models trained for 100 ms depending on the mlr3tuning version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute deviation of the runtime.
plot_runtime(data[algorithm == "tune_sequential" & model_time == 100 & task == "data_1000"])
```

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `tune()` with models trained for 100 ms depending on the mlr3tuning version.
#|  Error bars represent the median absolute deviation of the memory usage.
plot_memory(data[algorithm == "tune_sequential" & model_time == 100 & task == "data_1000"])
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Runtime and memory usage of `tune()` with models trained for 100 ms depending on the mlr3tuning version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
create_table(data[algorithm == "tune_sequential" & model_time == 100])
```

## Model Time 10 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `tune()` with models trained for 10 ms depending on the mlr3tuning version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute deviation of the runtime.
plot_runtime(data[algorithm == "tune_sequential" & model_time == 10 & task == "data_1000"])
```

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `tune()` with models trained for 10 ms depending on the mlr3tuning version.
#|  Error bars represent the median absolute deviation of the memory usage.
plot_memory(data[algorithm == "tune_sequential" & model_time == 10 & task == "data_1000"])
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Runtime and memory usage of `tune()` with models trained for 10 ms depending on the mlr3tuning version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
create_table(data[algorithm == "tune_sequential" & model_time == 10])
```

## Model Time 1 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `tune()` with models trained for 1 ms depending on the mlr3tuning version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute deviation of the runtime.
plot_runtime(data[algorithm == "tune_sequential" & model_time == 1 & task == "data_1000"])
```

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `tune()` with models trained for 1 ms depending on the mlr3tuning version.
#|  Error bars represent the median absolute deviation of the memory usage.
plot_memory(data[algorithm == "tune_sequential" & model_time == 1 & task == "data_1000"])
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Runtime and memory usage of `tune()` with models trained for 1 ms depending on the mlr3tuning version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
create_table(data[algorithm == "tune_sequential" & model_time == 1])
```


# Nested Hyperparameter Optimization

```{r}
#| include: false

data_memory = fread(here::here("mlr-org/benchmark_results/mlr3tuning_tune_nested_memory.csv"))[, list(model_time, task, evals, renv_project, algorithm, median_memory, mad_memory)]
data_runtime = fread(here::here("mlr-org/benchmark_results/mlr3tuning_tune_nested_runtime.csv"))[, list(model_time, task, evals, renv_project, algorithm, median_run_time, mad_run_time, k)]
data = merge(data_memory, data_runtime, by = c("model_time", "task", "evals", "renv_project", "algorithm"))

data[, renv_project := gsub("mlr3tuning/default/snapshots/", "", renv_project)]
data = data[snapshot, on = "renv_project"]
data[, mlr3tuning := factor(mlr3tuning, levels = c("0.17.2", "0.18.0", "0.19.0", "0.19.1", "0.19.2", "0.20.0", "1.0.0"))]
setorderv(data, c("algorithm", "model_time", "evals"), order = c(1, 1, 1))
data[, median_run_time := median_run_time / 1000] #ms to s
data[, mad_run_time := mad_run_time / 1000] #ms to s
data[, total_model_time := model_time * evals * 10 / 1000] #ms to s

# calculate how much longer nested feature selection is than feature selection
data = merge(data, tune_median_run_time, by = c("model_time", "task", "evals", "renv_project"), suffixes = c("", "_tune"))
data[, m := median_run_time / median_run_time_tune]

create_table = function(data) {
 data = data[, -c("mad_run_time", "mad_memory", "renv_project", "algorithm", "total_model_time")]

  data_1000 = data[task == "data_1000", -"task"]
  data_10000 = data[task == "data_10000", -c("task", "k", "median_run_time_tune", "m")]
  data = merge(data_1000, data_10000, by = c("mlr3tuning", "bbotk", "mlr3", "paradox", "model_time", "evals"), suffixes = c("", "_10000"))

  setcolorder(data, c(
    "mlr3tuning",
    "bbotk",
    "mlr3",
    "paradox",
    "model_time",
    "evals",
    "median_run_time",
    "median_run_time_10000",
    "k",
    "median_run_time_tune",
    "m",
    "median_memory",
    "median_memory_10000"))

  data %>%
    gt() %>%
    cols_label(
      mlr3tuning = "mlr3tuning Version",
      mlr3 = "mlr3 Version",
      paradox = "paradox Version",
      model_time = "Model Time [ms]",
      evals = "Resampling Iterations",
      median_run_time = "Median Runtime [s]",
      median_run_time_10000 = "Median Runtime 10,000 [s]",
      k = "K",
      median_run_time_tune = "Median Runtime Tune [s]",
      m = "m",
      median_memory = "Median Memory [MB]",
      median_memory_10000 = "Median Memory 10,000 [MB]") %>%
    fmt_number(columns = c("k", "median_run_time", "median_run_time_10000", "m", "median_run_time_tune"), n_sigfig = 2) %>%
    fmt_number(columns = c("median_memory", "median_memory_10000"), decimals = 0) %>%
    tab_style(
      style = list(
        cell_fill(color = "crimson"),
        cell_text(weight = "bold")
      ),
      locations = cells_body(
        columns = "k",
        rows = k > 3
      )
    )  %>%
    tab_row_group(
      label = "1000 Resampling Iterations",
      rows = evals == 1000
    ) %>%
    tab_row_group(
      label = "100 Resampling Iterations",
      rows = evals == 100
    ) %>%
    tab_row_group(
      label = "10 Resampling Iterations",
      rows = evals == 10
    )
  }
```


## Model Time 1000 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `tune_nested()` with models trained for 1 s depending on the mlr3tuning version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute deviation of the runtime.
plot_runtime(data[algorithm == "tune_nested_sequential" & model_time == 1000 & task == "data_1000"])
```

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `tune_nested()` with models trained for 1 s depending on the mlr3tuning version.
#|  Error bars represent the median absolute deviation of the memory usage.
plot_memory(data[algorithm == "tune_nested_sequential" & model_time == 1000 & task == "data_1000"])
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Runtime and memory usage of `tune_nested()` with models trained for 1 s depending on the mlr3tuning version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
create_table(data[algorithm == "tune_nested_sequential" & model_time == 1000])
```

## Model Time 100 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `tune_nested()` with models trained for 100 ms depending on the mlr3tuning version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute deviation of the runtime.
plot_runtime(data[algorithm == "tune_nested_sequential" & model_time == 100 & task == "data_1000"])
```

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `tune_nested()` with models trained for 100 ms depending on the mlr3tuning version.
#|  Error bars represent the median absolute deviation of the memory usage.
plot_memory(data[algorithm == "tune_nested_sequential" & model_time == 100 & task == "data_1000"])
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Runtime and memory usage of `tune_nested()` with models trained for 100 ms depending on the mlr3tuning version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
create_table(data[algorithm == "tune_nested_sequential" & model_time == 100])
```

## Model Time 10 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `tune_nested()` with models trained for 10 ms depending on the mlr3tuning version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute deviation of the runtime.
plot_runtime(data[algorithm == "tune_nested_sequential" & model_time == 10 & task == "data_1000"])
```

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `tune_nested()` with models trained for 10 ms depending on the mlr3tuning version.
#|  Error bars represent the median absolute deviation of the memory usage.
plot_memory(data[algorithm == "tune_nested_sequential" & model_time == 10 & task == "data_1000"])
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Runtime and memory usage of `tune_nested()` with models trained for 10 ms depending on the mlr3tuning version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
create_table(data[algorithm == "tune_nested_sequential" & model_time == 10])
```

## Model Time 1 ms

```{r}
#| echo: false
#| fig-cap: |
#|  Median runtime of `tune_nested()` with models trained for 1 ms depending on the mlr3tuning version.
#|  The dashed line indicates the total training time of the models.
#|  Error bars represent the median absolute deviation of the runtime.
plot_runtime(data[algorithm == "tune_nested_sequential" & model_time == 1 & task == "data_1000"])
```

```{r}
#| echo: false
#| fig-cap: |
#|  Memory usage of `tune_nested()` with models trained for 1 ms depending on the mlr3tuning version.
#|  Error bars represent the median absolute deviation of the memory usage.
plot_memory(data[algorithm == "tune_nested_sequential" & model_time == 1 & task == "data_1000"])
```

```{r}
#| echo: false
#| tbl-cap: |
#|  Runtime and memory usage of `tune_nested()` with models trained for 1 ms depending on the mlr3tuning version.
#|  The K factor shows how much longer the runtime is than the model training.
#|  A red background indicates that the runtime is 3 times larger than the total training time of the models.
create_table(data[algorithm == "tune_nested_sequential" & model_time == 1])
```

# Object Size

```{r}
#| include: false

data = fread(here::here("mlr-org/benchmark_results/mlr3tuning_object_size.csv"))

data[, renv_project := gsub("mlr3tuning/default/snapshots/", "", renv_project)]
data = data[snapshot, on = "renv_project"]

data[, mlr3tuning := factor(mlr3tuning, levels = c("0.17.2", "0.18.0", "0.19.0", "0.19.1", "0.19.2", "0.20.0", "1.0.0"))]
#setorderv(data, c("algorithm", "evals"), order = c(1, 1))

data = data[, -c("renv_project")]
data_1000 = data[task == "data_1000", -"task"]
data_10000 = data[task == "data_10000", -"task"]
data = merge(data_1000, data_10000, by = c("mlr3tuning", "mlr3", "paradox", "evals", "object", "learner"), suffixes = c("", "_10000"))

plot_size = function(data) {
  data = melt(data, id.vars = c("mlr3tuning", "mlr3", "paradox", "evals", "object", "learner"), measure.vars = c("size", "serialized_size"), value.name = "size", variable.name = "type")

  ggplot(data, aes(x = mlr3tuning, y = size, fill= type, group = type)) +
    geom_col(position = "dodge") +
    labs(x = "mlr3tuning Version", y = "Size [MB]") +
    scale_fill_manual(labels = c("In Memory", "Serialized"), values = c("#008080", "#ff6347"), name = "Type") +
    theme_minimal(base_size = 9) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
}

plot_size_evals = function(data) {
  data = melt(data, id.vars = c("mlr3tuning", "mlr3", "paradox", "evals", "object", "learner"), measure.vars = c("size", "serialized_size"), value.name = "size", variable.name = "type")

  ggplot(data, aes(x = mlr3tuning, y = size, fill= type, group = type)) +
    geom_col(position = "dodge") +
    labs(x = "mlr3tuning Version", y = "Size [MB]") +
    scale_fill_manual(labels = c("In Memory", "Serialization"), values = c("#008080", "#ff6347"), name = "Type") +
    facet_wrap(~evals, scales = "free_y", labeller = labeller(evals = function(value) sprintf("%s Resampling Iterations", value))) +
    theme_minimal(base_size = 9) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position="bottom")
}
```

## Automated Tuning

Tuning without storing any additional information.

```{r}
#| eval: false
at = auto_tuner(
  tuner = fs("random_search", batch_size = instance$evals),
  learner = learner,
  resampling = resampling,
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = instance$evals),
  store_models = FALSE,
  store_benchmark_result = FALSE,
  store_tuning_instance = FALSE
)
```

```{r}
#| echo: false
plot_size(data[object == "at"])
```

```{r}
#| eval: false
at$train(task)
```

Since we do not store any information about the optimization, the size of the objects should not increase the number of evaluations.

```{r}
#| echo: false
plot_size_evals(data[object == "at$train"])
```

```{r}
#| eval: false
at$model
```

The model slot only saves the final model and should therefore not increase with the number of evaluations.

```{r}
#| echo: false
plot_size_evals(data[object == "at$model"])
```

## Automated Tuning with Instance

```{r}
#| eval: false
at = auto_tuner(
  tuner = fs("random_search", batch_size = instance$evals),
  learner = learner,
  resampling = resampling,
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = instance$evals),
  store_tuning_instance = TRUE,
  store_benchmark_result = FALSE,
  store_models = FALSE
)
```

```{r}
#| eval: false
at$train(task)
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_instance$train"])
```

```{r}
#| eval: false
at$tuning_instance
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_instance$tuning_instance"])
```

```{r}
#| eval: false
at$tuning_instance$objective
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_instance$objective"])
```

```{r}
#| eval: false
at$tuning_instance$archive
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_instance$archive"])
```

## Automated Tuning with Benchmark Result

```{r}
#| eval: false
at = auto_tuner(
  tuner = fs("random_search", batch_size = instance$evals),
  learner = learner,
  resampling = resampling,
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = instance$evals),
  store_tuning_instance = TRUE,
  store_benchmark_result = TRUE,
  store_models = FALSE
)
```

```{r}
#| eval: false
at$train(task)
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_bmr$train"])
```

```{r}
#| eval: false
at$tuning_instance
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_bmr$tuning_instance"])
```

```{r}
#| eval: false
at$tuning_instance$objective
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_bmr$objective"])
```

```{r}
#| eval: false
at$tuning_instance$archive
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_bmr$archive"])
```

```{r}
#| eval: false
at$tuning_instance$archive$benchmark_result
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_bmr$benchmark_result"])
```


## Automated Tuning with Models

```{r}
#| eval: false
at = auto_tuner(
  tuner = fs("random_search", batch_size = instance$evals),
  learner = learner,
  resampling = resampling,
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = instance$evals),
  store_tuning_instance = TRUE,
  store_benchmark_result = TRUE,
  store_models = TRUE
)
```

```{r}
#| eval: false
at$train(task)
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_models$train"])
```

```{r}
#| eval: false
at$tuning_instance
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_models$tuning_instance"])
```

```{r}
#| eval: false
at$tuning_instance$objective
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_models$objective"])
```

```{r}
#| eval: false
at$tuning_instance$archive
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_models$archive"])
```

```{r}
#| eval: false
at$tuning_instance$archive$benchmark_result
```

```{r}
#| echo: false
plot_size_evals(data[object == "at_models$benchmark_result"])
```
