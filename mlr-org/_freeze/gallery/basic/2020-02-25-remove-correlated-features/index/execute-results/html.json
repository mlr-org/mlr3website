{
  "hash": "a80bb5fc590e926107d5d7eeb31c2158",
  "result": {
    "markdown": "---\ntitle: Select Uncorrelated Features\ncategories:\n  - tuning\n  - mlr3pipelines\n  - filtering\n  - classification\nauthor:\n  - name: Martin Binder\n  - name: Florian Pfisterer\ndate: 02-25-2020\ndescription: |\n  Remove correlated features with a pipeline.\nimage: ../../images/logo_color.png\naliases:\n  - ../../../gallery/2020-02-25-remove-correlated-features/index.html\n---\n\n\n\n\nThe following example describes a situation where we aim to remove **correlated features**.\nThis in essence means, that we drop features until no features have a correlation higher than a given `cutoff`.\nThis is often useful when we for example want to use **linear models**.\n\n## Prerequisites\n\nThis tutorial assumes familiarity with the basics of [mlr3pipelines](https://mlr3pipelines.mlr-org.com).\nConsult the [mlr3book](https://mlr3book.mlr-org.com/pipelines.html) if some aspects are not  fully understandable.\nAdditionally, we compare different cutoff values via tuning using the [mlr3tuning](https://mlr3tuning.mlr-org.com) package.\nAgain, the [mlr3book](https://mlr3book.mlr-org.com/paradox.html) has an intro to [mlr3tuning](https://mlr3tuning.mlr-org.com) and [paradox](https://paradox.mlr-org.com).\n\nThe example describes a very involved use-case, where the behavior of [`PipeOpSelect`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_select.html) is manipulated via a **trafo** on it's [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)\n\n## Getting started\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com) package which pulls in the most important packages for this example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\nThe basic pipeline looks as follows:\nWe use [`PipeOpSelect`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_select.html) to select a set of variables followed by a [`rpart learner`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = po(\"select\") %>>% lrn(\"classif.rpart\")\n```\n:::\n\n\nNow we get to the magic:\n\nWe want to use the function [`caret::findCorrelation()`](https://www.rdocumentation.org/packages/caret/topics/findCorrelation) from the [caret](https://cran.r-project.org/package=caret) package in order to select uncorrelated variables.\nThis function has a `cutoff` parameter, that specifies the maximum correlation allowed between variables.\nIn order to expose this variable as a `numeric` parameter we can tune over we specify the following [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(cutoff = p_dbl(0, 1))\n```\n:::\n\n\nWe define a function `select_cutoff` that takes as input a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) and returns a list of features we aim to keep.\n\nNow we use a `trafo` to transform the `cutoff` into a set of variables, which is what [`PipeOpSelect`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_select.html) can work with.\nNote that we use `x$cutoff = NULL` in order to remove the temporary parameter we introduced, as [`PipeOpSelect`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_select.html) does not know what to do with it.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space$trafo = function(x, param_set) {\n  cutoff = x$cutoff\n  x$select.selector = function(task) {\n    fn = task$feature_names\n    data = task$data(cols = fn)\n    drop = caret::findCorrelation(cor(data), cutoff = cutoff, exact = TRUE, names = TRUE)\n    setdiff(fn, drop)\n  }\n  x$cutoff = NULL\n  x\n}\n```\n:::\n\n\nIf you are not sure, you understand the `trafo` concept, consult the [mlr3book](https://mlr3book.mlr-org.com/paradox.html).\nIt has a section on the `trafo` concept.\n\nNow we tune over different values for `cutoff`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  tuner = tnr(\"grid_search\"),\n  task = tsk(\"iris\"),\n  learner = graph_learner,\n  resampling = rsmp(\"cv\", folds = 3L),\n  measure = msr(\"classif.ce\"),\n  search_space = search_space,\n  # don't need the following line for optimization, this is for\n  # demonstration that different features were selected\n  store_models = TRUE)\n```\n:::\n\n\nIn order to demonstrate that different cutoff values result in different features being selected, we can run the following to inspect the trained models.\nNote this inspects only the trained models of the first CV fold of each evaluated model.\nThe features being excluded depends on the training data seen by the pipeline and may be different in different folds, even at the same cutoff value.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[\n  order(cutoff),\n  list(cutoff, classif.ce,\n    featurenames = lapply(resample_result, function(x) {\n      x$learners[[1]]$model$classif.rpart$train_task$feature_names\n    }\n  ))]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       cutoff classif.ce                                      featurenames\n 1: 0.0000000 0.28666667                                      Sepal.Length\n 2: 0.1111111 0.28666667                                      Sepal.Length\n 3: 0.2222222 0.28666667                                      Sepal.Length\n 4: 0.3333333 0.27333333                          Sepal.Length,Sepal.Width\n 5: 0.4444444 0.27333333                          Sepal.Length,Sepal.Width\n 6: 0.5555556 0.27333333                          Sepal.Length,Sepal.Width\n 7: 0.6666667 0.27333333                          Sepal.Length,Sepal.Width\n 8: 0.7777778 0.27333333                          Sepal.Length,Sepal.Width\n 9: 0.8888889 0.04000000              Petal.Width,Sepal.Length,Sepal.Width\n10: 1.0000000 0.06666667 Petal.Length,Petal.Width,Sepal.Length,Sepal.Width\n```\n:::\n:::\n\n\nVoila, we created our own [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html), that uses very advanced knowledge of [mlr3pipelines](https://mlr3pipelines.mlr-org.com) and [paradox](https://paradox.mlr-org.com) in only few lines of code.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}