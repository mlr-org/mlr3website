{
  "hash": "62b6bca15b91bb4212da43d9cabc5cc8",
  "result": {
    "markdown": "---\ntitle: Impute Missing Variables\ncategories:\n  - classification\n  - imputation\n  - mlr3pipelines\nauthor:\n  - name: Florian Pfisterer\ndate: 01-31-2020\ndescription: |\n  Augment a Random Forest with automatic imputation.\naliases:\n  - ../../../gallery/2020-01-30-impute-missing-levels/index.html\n---\n\n\n\n\n## Prerequisites\n\nThis tutorial assumes familiarity with the basics of [mlr3pipelines](https://mlr3pipelines.mlr-org.com).\nConsult the [mlr3book](https://mlr3book.mlr-org.com/pipelines.html) if some aspects are not  fully understandable.\nIt deals with the problem of missing data.\n\nThe random forest implementation in the package [ranger](https://cran.r-project.org/package=ranger) unfortunately does not support missing values.\nTherefore, it is required to impute missing features before passing the data to the learner.\n\nWe show how to use [mlr3pipelines](https://mlr3pipelines.mlr-org.com) to augment the [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) with automatic imputation.\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com) package which pulls in the most important packages for this example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: mlr3\n```\n:::\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n```\n:::\n\n\n## Construct the Base Objects\n\nFirst, we take an example task with missing values ([`pima`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html)) and create the [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3learners)\n\ntask = tsk(\"pima\")\nprint(task)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TaskClassif:pima> (768 x 9): Pima Indian Diabetes\n* Target: diabetes\n* Properties: twoclass\n* Features (8):\n  - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure, triceps\n```\n:::\n\n```{.r .cell-code}\nlearner = lrn(\"classif.ranger\")\nprint(learner)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<LearnerClassifRanger:classif.ranger>\n* Model: -\n* Parameters: num.threads=1\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error, twoclass, weights\n```\n:::\n:::\n\n\nWe can now inspect the task for missing values.\n`task$missings()` returns the count of missing values for each variable.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure  triceps \n       0        0        5      374       11        0        0       35      227 \n```\n:::\n:::\n\n\nAdditionally, we can see that the [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) can not handle missing values:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$properties\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"hotstart_backward\" \"importance\"        \"multiclass\"        \"oob_error\"         \"twoclass\"         \n[6] \"weights\"          \n```\n:::\n:::\n\n\nFor comparison, other learners, e.g. the [`rpart learner`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html) can handle missing values internally.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn(\"classif.rpart\")$properties\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"importance\"        \"missings\"          \"multiclass\"        \"selected_features\" \"twoclass\"         \n[6] \"weights\"          \n```\n:::\n:::\n\n\nBefore we dive deeper, we quickly try to visualize the columns with many missing values:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(task$clone()$select(c(\"insulin\", \"triceps\")), type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-01-30-impute-missing-levels-007-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Operators overview\n\nAn overview over implemented [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s for imputation can be obtained like so:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(mlr_pipeops)[tags %in% \"missings\", list(key)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              key\n1: imputeconstant\n2:     imputehist\n3:  imputelearner\n4:     imputemean\n5:   imputemedian\n6:     imputemode\n7:      imputeoor\n8:   imputesample\n```\n:::\n:::\n\n\n## Construct Operators\n\n[mlr3pipelines](https://mlr3pipelines.mlr-org.com) contains several imputation methods.\nWe focus on rather simple ones, and show how to impute missing values for `factor` features and\n`numeric` features respectively.\n\nSince our task only has numeric features, we do not need to deal with imputing factor levels,\nand can instead concentrate on imputing numeric values:\n\nWe do this in a two-step process:\n* We create new indicator columns, that tells us whether the value of a feature is \"missing\" or \"present\".\n  We achieve this using the  [`missind`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_missind.html) [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html).\n\n* Afterwards, we impute every missing value by sampling from the histogram of the respective column.\n  We achieve this using the  [`imputehist`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputehist.html) [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html).\n\nWe also have to make sure to apply the pipe operators in the correct order!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimp_missind = po(\"missind\")\nimp_num = po(\"imputehist\", affect_columns = selector_type(\"numeric\"))\n```\n:::\n\n\nIn order to better understand we can look at the results of every [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) separately.\n\nWe can manually trigger the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) to test the operator on our task:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_ext = imp_missind$train(list(task))[[1]]\ntask_ext$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     diabetes missing_glucose missing_insulin missing_mass missing_pressure missing_triceps\n  1:      pos         present         missing      present          present         present\n  2:      neg         present         missing      present          present         present\n  3:      pos         present         missing      present          present         missing\n  4:      neg         present         present      present          present         present\n  5:      pos         present         present      present          present         present\n ---                                                                                       \n764:      neg         present         present      present          present         present\n765:      neg         present         missing      present          present         present\n766:      neg         present         present      present          present         present\n767:      pos         present         missing      present          present         missing\n768:      neg         present         missing      present          present         present\n```\n:::\n:::\n\n\nFor [`imputehist`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputehist.html), we can do the same:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_ext = imp_num$train(list(task))[[1]]\ntask_ext$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     diabetes age pedigree pregnant glucose   insulin mass pressure   triceps\n  1:      pos  50    0.627        6     148 163.11747 33.6       72 35.000000\n  2:      neg  31    0.351        1      85 160.63628 26.6       66 29.000000\n  3:      pos  32    0.672        8     183 297.18282 23.3       64  8.204983\n  4:      neg  21    0.167        1      89  94.00000 28.1       66 23.000000\n  5:      pos  33    2.288        0     137 168.00000 43.1       40 35.000000\n ---                                                                         \n764:      neg  63    0.171       10     101 180.00000 32.9       76 48.000000\n765:      neg  27    0.340        2     122  83.69836 36.8       70 27.000000\n766:      neg  30    0.245        5     121 112.00000 26.2       72 23.000000\n767:      pos  47    0.349        1     126  68.49318 30.1       60 24.460702\n768:      neg  23    0.315        1      93  17.80534 30.4       70 31.000000\n```\n:::\n:::\n\n\nThis time we obtain the imputed data set without `missing` values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_ext$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndiabetes      age pedigree pregnant  glucose  insulin     mass pressure  triceps \n       0        0        0        0        0        0        0        0        0 \n```\n:::\n:::\n\n\n## Putting everything together\n\nNow we have to put all [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s  together in order to form a graph that handles imputation automatically.\n\nWe do this by creating a [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) that copies the data twice, processes each copy using the respective imputation method and afterwards unions the features.\nFor this we need the following two [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s :\n* [`copy`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_copy.html): Creates copies of the data.\n* [`featureunion`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_featureunion.html) Merges the two tasks together.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = po(\"copy\", 2) %>>%\n  gunion(list(imp_missind, imp_num)) %>>%\n  po(\"featureunion\")\n```\n:::\n\n\nas a last step we append the learner we planned on using:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = graph %>>% po(learner)\n```\n:::\n\n\nWe can now visualize the resulting graph:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-01-30-impute-missing-levels-015-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n## Resampling\n\nCorrect imputation is especially important when applying imputation to held-out data during the `predict` step.\nIf applied incorrectly, imputation could leak info from the test set, which potentially skews our performance estimates.\n[mlr3pipelines](https://mlr3pipelines.mlr-org.com) takes this complexity away from the user and handles correct imputation internally.\n\nBy wrapping this graph into a  [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html), we can now train resample the full graph, here with a 3-fold cross validation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(graph)\nrr = resample(task, graph_learner, rsmp(\"cv\", folds = 3))\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.ce \n 0.2330729 \n```\n:::\n:::\n\n\n## Missing values during prediction\n\nIn some cases, we have missing values only in the data we want to predict on.\nIn order to showcase this, we create a copy of the task with several more missing columns.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndt = task$data()\ndt[1:10, \"age\"] = NA\ndt[30:70, \"pedigree\"] = NA\ntask_2 = as_task_classif(dt, id = \"pima2\", target = \"diabetes\")\n```\n:::\n\n\nAnd now we learn on `task`, while trying to predict on `task_2`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner$train(task)\ngraph_learner$predict(task_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 768 observations:\n    row_ids truth response\n          1   pos      pos\n          2   neg      neg\n          3   pos      pos\n---                       \n        766   neg      neg\n        767   pos      pos\n        768   neg      neg\n```\n:::\n:::\n\n\n## Missing factor features\n\nFor `factor` features, the process works analogously.\nInstead of using [`imputehist`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputehist.html), we can for example use [`imputeoor`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputeoor.html).\nThis will simply replace every `NA` in each factor variable with a new value `missing`.\n\nA full graph might the look like this:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimp_missind = po(\"missind\", affect_columns = NULL, which = \"all\")\nimp_fct = po(\"imputeoor\", affect_columns = selector_type(\"factor\"))\ngraph = po(\"copy\", 2) %>>%\n  gunion(list(imp_missind, imp_num %>>% imp_fct)) %>>%\n  po(\"featureunion\")\n```\n:::\n\n\nNote that we specify the parameter `affect_columns = NULL` when initializing [`missind`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_missind.html), because we also want indicator columns for our `factor` features.\nBy default, `affect_columns` would be set to `selector_invert(selector_type(c(\"factor\", \"ordered\", \"character\")))`.\nWe also set the parameter `which` to `\"all\"` to add indicator columns for all features, regardless whether values were missing during training or not.\n\nIn order to test out our new graph, we again create a situation where our task has missing factor levels.\nAs the ([`pima`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html)) task does not have any factor levels, we use the\nfamous ([`boston_housing`](https://mlr3.mlr-org.com/reference/mlr_tasks_boston_housing.html)) task.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# task_bh_1 is the training data without missings\ntask_bh_1 = tsk(\"boston_housing\")\n\n# task_bh_2 is the prediction data with missings\ndt = task_bh_1$data()\ndt[1:10, chas := NA][20:30, rm := NA]\ntask_bh_2 = as_task_regr(dt, id = \"bh\", target = \"medv\")\n```\n:::\n\n\nNow we train on `task_bh_1` and predict on `task_bh_2`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(graph %>>% po(lrn(\"regr.ranger\")))\ngraph_learner$train(task_bh_1)\ngraph_learner$predict(task_bh_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionRegr> for 506 observations:\n    row_ids truth response\n          1  24.0 25.16204\n          2  21.6 22.21102\n          3  34.7 33.84124\n---                       \n        504  23.9 23.68916\n        505  22.0 22.20551\n        506  11.9 16.14491\n```\n:::\n:::\n\n\nSuccess! We learned how to deal with missing values in less than 10 minutes.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}