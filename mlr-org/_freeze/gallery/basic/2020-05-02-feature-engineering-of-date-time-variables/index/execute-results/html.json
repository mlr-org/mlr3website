{
  "hash": "c14e7d50f6dfda9397e4a7ae77962b4d",
  "result": {
    "markdown": "---\ntitle: Feature Engineering of Date-Time Variables\ncategories:\n  - feature engineering\n  - mlr3pipelines\n  - regression\nauthor:\n  - name: Lennart Schneider\ndate: 05-02-2020\ndescription: |\n  Engineer features using date-time variables.\naliases:\n  - ../../../gallery/2020-05-02-feature-engineering-of-date-time-variables/index.html\n---\n\n\n\n\nIn this tutorial, we demonstrate how [mlr3pipelines](https://mlr3pipelines.mlr-org.com) can be used to easily engineer features based on date-time variables.\nRelying on the [Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) and the [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html) we compare the root mean square error (RMSE) of a random forest using the original features (baseline), to the RMSE of a random forest using newly engineered features on top of the original ones.\n\n## Motivation\nA single date-time variable (i.e., a `POSIXct` column) contains plenty of information ranging from year, month, day, hour, minute and second to other features such as week of the year, or day of the week.\nMoreover, most of these features are of cyclical nature, i.e., the eleventh and twelfth hour of a day are one hour apart, but so are the 23rd hour and midnight of the other day (see also this [blog post](http://blog.davidkaleko.com/feature-engineering-cyclical-features.html) and [fastai](https://docs.fast.ai/tabular.transform.html#Treating-date-columns) for more information).\n\nNot respecting this cyclical nature results in treating hours on a linear continuum. One way to handle a cyclical feature $\\mathbf{x}$ is to compute the sine and cosine transformation of $\\frac{2 \\pi \\mathbf{x}}{\\mathbf{x}_{\\text{max}}}$, with $\\mathbf{x}_{\\text{max}} = 24$ for hours and $60$ for minutes and seconds.\n\nThis results in a two-dimensional representation of the feature:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/2020-05-02-feature-engineering-of-date-time-variables-001-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n[mlr3pipelines](https://mlr3pipelines.mlr-org.com) provides the [`PipeOpDateFeatures`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_datefeatures.html) pipeline which can be used to automatically engineer features based on `POSIXct` columns, including handling of cyclical features.\n\nThis is useful as most learners naturally cannot handle dates and `POSIXct` variables and therefore require conversion prior to training.\n\n## Prerequisites\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com) package which pulls in the most important packages for this example.\nThe [mlr3learners](https://mlr3learners.mlr-org.com) package loads additional [`learners`](https://mlr3.mlr-org.com/reference/Learner.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3learners)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n```\n:::\n\n\n## Bike Sharing\n\nThe [Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset) contains the hourly count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information.\nThe dataset can be downloaded from the UCI Machine Learning Repository.\nAfter reading in the data, we fix some factor levels, and convert some data types:\n\nThe [`Bike Sharing Dataset`](https://mlr3data.mlr-org.com/reference/bike_sharing.html) contains the hourly count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information.\nWe load the data set from the [mlr3data](https://mlr3data.mlr-org.com) package.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(\"bike_sharing\", package = \"mlr3data\")\n```\n:::\n\n\nOur goal will be to predict the total number of rented bikes on a given day: `cnt`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nskimr::skim(bike_sharing)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |             |\n|:------------------------|:------------|\n|Name                     |bike_sharing |\n|Number of rows           |17379        |\n|Number of columns        |14           |\n|Key                      |NULL         |\n|_______________________  |             |\n|Column type frequency:   |             |\n|character                |1            |\n|factor                   |2            |\n|logical                  |2            |\n|numeric                  |9            |\n|________________________ |             |\n|Group variables          |None         |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|date          |         0|             1|  10|  10|     0|      731|          0|\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                                 |\n|:-------------|---------:|-------------:|:-------|--------:|:------------------------------------------|\n|season        |         0|             1|FALSE   |        4|sum: 4496, spr: 4409, win: 4242, fal: 4232 |\n|weather       |         0|             1|FALSE   |        4|1: 11413, 2: 4544, 3: 1419, 4: 3           |\n\n\n**Variable type: logical**\n\n|skim_variable | n_missing| complete_rate| mean|count                 |\n|:-------------|---------:|-------------:|----:|:---------------------|\n|holiday       |         0|             1| 0.03|FAL: 16879, TRU: 500  |\n|working_day   |         0|             1| 0.68|TRU: 11865, FAL: 5514 |\n\n\n**Variable type: numeric**\n\n|skim_variable        | n_missing| complete_rate|   mean|     sd|   p0|   p25|    p50|    p75|   p100|hist  |\n|:--------------------|---------:|-------------:|------:|------:|----:|-----:|------:|------:|------:|:-----|\n|year                 |         0|             1|   0.50|   0.50| 0.00|  0.00|   1.00|   1.00|   1.00|▇▁▁▁▇ |\n|month                |         0|             1|   6.54|   3.44| 1.00|  4.00|   7.00|  10.00|  12.00|▇▆▆▅▇ |\n|hour                 |         0|             1|  11.55|   6.91| 0.00|  6.00|  12.00|  18.00|  23.00|▇▇▆▇▇ |\n|weekday              |         0|             1|   3.00|   2.01| 0.00|  1.00|   3.00|   5.00|   6.00|▇▃▃▃▇ |\n|temperature          |         0|             1|   0.50|   0.19| 0.02|  0.34|   0.50|   0.66|   1.00|▂▇▇▇▁ |\n|apparent_temperature |         0|             1|   0.48|   0.17| 0.00|  0.33|   0.48|   0.62|   1.00|▁▆▇▆▁ |\n|humidity             |         0|             1|   0.63|   0.19| 0.00|  0.48|   0.63|   0.78|   1.00|▁▃▇▇▆ |\n|windspeed            |         0|             1|   0.19|   0.12| 0.00|  0.10|   0.19|   0.25|   0.85|▇▆▂▁▁ |\n|count                |         0|             1| 189.46| 181.39| 1.00| 40.00| 142.00| 281.00| 977.00|▇▃▁▁▁ |\n:::\n:::\n\n\nThe original dataset does not contain a `POSIXct` column, but we can easily generate one based on the other variables available (note that as no information regarding minutes and seconds is available, we set them to `:00:00`):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbike_sharing$date = as.POSIXct(paste0(bike_sharing$date, \" \", bike_sharing$hour, \":00:00\"),\n  tz = \"GMT\", format = \"%Y-%m-%d %H:%M:%S\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/2020-05-02-feature-engineering-of-date-time-variables-007-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Baseline Random Forest\n\nWe construct a new regression task and keep a [holdout set](https://mlr3book.mlr-org.com/tasks.html#tasks-roles).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(bike_sharing, target = \"count\")\n\nvalidation_set = sample(seq_len(task$nrow), size = 0.3 * task$nrow)\n\ntask$set_row_roles(validation_set, roles = \"holdout\")\n```\n:::\n\n\nTo estimate the performance on unseen data, we will use a 3-fold cross-validation.\nNote that this involves validating on past data, which is usually bad practice but should suffice for this example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv3 = rsmp(\"cv\", folds = 3)\n```\n:::\n\n\nTo obtain reliable estimates on how well our model generalizes to the future, we would have to split our training and test sets according to the date variable.\n\nAs our baseline model, we use a random forest, [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html).\nFor the baseline, we drop`date`, our new `POSIXct` variable which we will only use later.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_ranger = lrn(\"regr.ranger\")\ntask_ranger = task$clone()\ntask_ranger$select(setdiff(task$feature_names, c(\"date\")))\n```\n:::\n\n\nWe can then use [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) with 3-fold cross-validation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr_ranger = resample(task_ranger, learner = learner_ranger, resampling = cv3)\n\nrr_ranger$score(msr(\"regr.mse\"))[, .(iteration, task_id, learner_id, resampling_id, regr.mse)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   iteration      task_id  learner_id resampling_id regr.mse\n1:         1 bike_sharing regr.ranger            cv 4543.904\n2:         2 bike_sharing regr.ranger            cv 4276.996\n3:         3 bike_sharing regr.ranger            cv 4767.763\n```\n:::\n:::\n\n\nWe calculate the average RMSE.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr_ranger$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nregr.mse \n4529.554 \n```\n:::\n:::\n\n\nWe now want to improve our baseline model by using newly engineered features based on the `date` `POSIXct` column.\n\n## PipeOpDateFeatures\n\nTo engineer new features we use [`PipeOpDateFeatures`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_datefeatures.html).\nThis pipeline automatically dispatches on `POSIXct` columns of the data and by default adds plenty of new date-time related features.\nHere, we want to add all except for `minute` and `second`, because this information is not available. As we additionally want to use cyclical versions of the features we set `cyclic = TRUE`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npipeop_date = po(\"datefeatures\", cyclic = TRUE, minute = FALSE, second = FALSE)\n```\n:::\n\n\nTraining this pipeline will result in simply adding the new features (and removing the original `POSIXct` feature(s) used for the feature engineering, see also the `keep_date_var` parameter).\nIn our task, we can now drop the features, `yr`, `mnth`, `hr`, and `weekday`, because our pipeline will generate these anyways:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_ex = task$clone()\ntask_ex$select(setdiff(task$feature_names,\n  c(\"instant\", \"dteday\", \"yr\", \"mnth\", \"hr\", \"weekday\", \"casual\", \"registered\")))\n\npipeop_date$train(list(task_ex))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$output\n<TaskRegr:bike_sharing> (12166 x 32)\n* Target: count\n* Properties: -\n* Features (31):\n  - dbl (23): apparent_temperature, date.day_of_month, date.day_of_month_cos, date.day_of_month_sin,\n    date.day_of_week, date.day_of_week_cos, date.day_of_week_sin, date.day_of_year, date.day_of_year_cos,\n    date.day_of_year_sin, date.hour, date.hour_cos, date.hour_sin, date.month, date.month_cos,\n    date.month_sin, date.week_of_year, date.week_of_year_cos, date.week_of_year_sin, date.year, humidity,\n    temperature, windspeed\n  - lgl (3): date.is_day, holiday, working_day\n  - int (3): hour, month, year\n  - fct (2): season, weather\n```\n:::\n:::\n\n\nNote that it may be useful to familiarize yourself with [`PipeOpRemoveConstants`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_removeconstants.html) which can be used after the feature engineering to remove features that are constant.\n[`PipeOpDateFeatures`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_datefeatures.html) does not do this step automatically.\n\nTo combine this feature engineering step with a random forest, ranger learner, we now construct a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html).\n\n## Using the New Features in a GraphLearner\nWe create a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) consisting of the [`PipeOpDateFeatures`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_datefeatures.html) pipeline and a ranger learner.\nThis [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) then behaves like any other [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = po(\"datefeatures\", cyclic = TRUE, minute = FALSE, second = FALSE) %>>%\n  lrn(\"regr.ranger\")\n\ngraph_learner = as_learner(graph)\n\nplot(graph, html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-05-02-feature-engineering-of-date-time-variables-015-1.png){fig-align='center' width=768}\n:::\n:::\n\n\nUsing [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) with 3-fold cross-validation on the task yields:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_graph_learner = task$clone()\ntask_graph_learner$select(setdiff(task$feature_names,\n  c(\"instant\", \"dteday\", \"yr\", \"mnth\", \"hr\", \"weekday\", \"casual\", \"registered\")))\n\nrr_graph_learner = resample(task_graph_learner, learner = graph_learner, resampling = cv3)\n\nrr_graph_learner$score(msr(\"regr.mse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             task      task_id            learner               learner_id         resampling resampling_id iteration\n1: <TaskRegr[47]> bike_sharing <GraphLearner[38]> datefeatures.regr.ranger <ResamplingCV[20]>            cv         1\n2: <TaskRegr[47]> bike_sharing <GraphLearner[38]> datefeatures.regr.ranger <ResamplingCV[20]>            cv         2\n3: <TaskRegr[47]> bike_sharing <GraphLearner[38]> datefeatures.regr.ranger <ResamplingCV[20]>            cv         3\n             prediction regr.mse\n1: <PredictionRegr[19]> 2521.642\n2: <PredictionRegr[19]> 2229.746\n3: <PredictionRegr[19]> 2254.390\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-2960216d507265efb81a\" style=\"width:100%;height:auto;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-2960216d507265efb81a\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\"],[1,2,3],[\"bike_sharing\",\"bike_sharing\",\"bike_sharing\"],[\"datefeatures.regr.ranger\",\"datefeatures.regr.ranger\",\"datefeatures.regr.ranger\"],[\"cv\",\"cv\",\"cv\"],[2521.64241110067,2229.74565783182,2254.3902377426]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>iteration<\\/th>\\n      <th>task_id<\\/th>\\n      <th>learner_id<\\/th>\\n      <th>resampling_id<\\/th>\\n      <th>regr.mse<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,5]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nWe calculate the average RMSE.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr_graph_learner$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nregr.mse \n2335.259 \n```\n:::\n:::\n\n\nand therefore improved by almost 94%!\n\nFinally, we fit our [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) on the complete training set and predict on the validation set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$select(setdiff(task$feature_names, c(\"year\", \"month\", \"hour\", \"weekday\")))\n\ngraph_learner$train(task)\n\nprediction = graph_learner$predict(task, row_ids = task$row_roles$validation)\n```\n:::\n\n\nWhere we can obtain the RMSE on the held-out validation data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction$score(msr(\"regr.mse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nregr.mse \n460.0281 \n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<link href=\"../../../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/datatables-binding-0.25/datatables.js\"></script>\n<script src=\"../../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../../../site_libs/dt-core-1.11.3/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../../../site_libs/dt-core-1.11.3/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/dt-core-1.11.3/js/jquery.dataTables.min.js\"></script>\n<link href=\"../../../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}