{
  "hash": "057011d930c82b61dcf24fb4354e7e1d",
  "result": {
    "markdown": "---\ntitle: \"Hotstarting\"\ndescription: |\n  Resume the training of learners.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\n  - name: Sebastian Fischer\n    url: https://github.com/sebffischer\ndate: 2023-01-16\nbibliography: bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n      datatable.print.trunc.cols: TRUE\n---\n\n\n\n\n\n\n# Scope\n\nHotstarting a learner resumes the training from an already fitted model.\nAn example would be to train an already fit XGBoost model for an additional 500 boosting iterations.\nIn mlr3, we call this process **Hotstarting**, where a learner has access to a cache of already trained models which is called a [`mlr3::HoststartStack`](https://mlr3.mlr-org.com/reference/HoststartStack.html)\nWe distinguish between forward and backward hotstarting.\nWe start this post with backward hotstarting and then talk about the less efficient forward hotstarting.\n\n# Backward Hotstarting\n\nIn this example, we optimize the hyperparameters of a random forest and use hotstarting to reduce the runtime.\nHotstarting a random forest backwards is very simple.\nThe model remains unchanged and only a subset of the trees is used for prediction i.e. a new model is not fitted.\nFor example, a random forest is trained with 1000 trees and a specific hyperparameter configuration.\nIf another random forest with 500 trees but with the same hyperparameter configuration has to be trained, the model with 1000 trees is copied and only 500 trees are used for prediction.\n\nWe load the [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) and set the search space from the @bischl_hyperparameter_2021 article.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n\nlearner = lrn(\"classif.ranger\",\n  mtry.ratio      = to_tune(0, 1),\n  replace         = to_tune(),\n  sample.fraction = to_tune(1e-1, 1),\n  num.trees       = to_tune(1, 2000)\n)\n```\n:::\n\n\nWe activate hotstarting with the `allow_hotstart` option.\nWhen running a grid search with hotstarting, the grid is sorted by the hot start parameter.\nThis means the models with 2000 trees are trained first.\nThe models with less than 2000 trees hot start on the 2000 trees models which allows the training to be completed immediately.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  method = tnr(\"grid_search\", resolution = 5, batch_size = 5),\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  allow_hotstart = TRUE\n)\n```\n:::\n\n\nFor comparison, we perform the same tuning without hotstarting.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance_2 = tune(\n  method = tnr(\"grid_search\", resolution = 5, batch_size = 5),\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  allow_hotstart = FALSE\n)\n```\n:::\n\n\nWe plot the time of completion of each batch (see @fig-time-batch).\nEach batch includes 5 configurations.\nWe can see that tuning with hotstarting is slower at first.\nAs soon as all models are fitted with 2000 trees, the tuning runs much faster and overtakes the tuning without hotstarting.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Time of completion of each batch with and without hotstarting.](index_files/figure-html/fig-time-batch-1.png){#fig-time-batch fig-align='center' width=672}\n:::\n:::\n\n\n# Forward Hotstarting\n\nForward hotstarting is currently only supported by XGBoost.\nHowever, we have observed that hotstarting only provides a speed advantage for very large datasets and models with more than 5000 boosting rounds.\nThe reason is that copying the models from the main process to the workers is a major bottleneck.\nThe parallelization package [future](https://cran.r-project.org/package=future) copies the models sequentially to the workers.\nConsequently, it takes a long time until the last worker can even start.\nMoreover, copying itself consumes a lot of time, and copying the model back from the worker blocks the main process again.\nDuring the development process, we overestimated the speed benefits of hotstarting and underestimated the overhead of parallelization.\nWe can therefore only advise against using forward hotstarting during tuning.\nIt is much more efficient to use the internal early-stopping mechanism of XGBoost.\nThis eliminates the need to copy models to the worker.\nSee the [gallery post](/gallery/optimization/2022-11-04-early-stopping-with-xgboost) on early stopping for an example.\nWe might improve the efficiency of the hotstarting mechanism in the future, if there are convincing use cases.\n\n# Manual Hotstarting\n\nNevertheless, forward hotstarting can be useful without parallelization.\nIf you have an already trained model and want to add more boosting iteration to it.\nIn this example, the `learner_5000` is the already trained model.\nWe create a new learner with the same hyperparameters but double the number of boosting iteration.\nTo activate hotstarting, we create a `HotstartStack` and copy it to the `$hotstart_stack` slot of the new learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"spam\")\n\nlearner_5000 = lrn(\"classif.xgboost\", nrounds = 5000, eta = 0.1)\nlearner_5000$train(task)\n\nlearner_10000 = lrn(\"classif.xgboost\", nrounds = 10000, eta = 0.1)\nlearner_10000$hotstart_stack = HotstartStack$new(learner_5000)\nlearner_10000$train(task)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nTraining the initial model took 59.885 seconds.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_5000$state$train_time\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 59.885\n```\n:::\n:::\n\n\nAdding 5000 boosting rounds took 46.837 seconds.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_10000$state$train_time - learner_5000$state$train_time\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 46.837\n```\n:::\n:::\n\n\nTraining the model from the beginning would have taken about two minutes.\nThis means, without parallelization, we get the expected speed advantage.\n\n# Conclusion\n\nWe have seen how mlr3 enables to reduce the training time, by building on a hotstart stack of already trained learners.\nOne has to be careful, however, when using forward hotstarting during tuning because of the high parallelization overhead that arises from copying the models between the processes.\nIf a model has an internal early stopping implementation, it should usually be relied upon instead of using the mlr3 hotstarting mechanism.\nHowever, manual forward hotstarting can be helpful in some situations when we do not want to train a large model from the beginning.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}