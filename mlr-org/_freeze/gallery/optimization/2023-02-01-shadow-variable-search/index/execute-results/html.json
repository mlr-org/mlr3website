{
  "hash": "7c275e225e018760745f959a3befbecc",
  "result": {
    "markdown": "---\ntitle: \"Shadow Variable Search on the Pima Indian Diabetes Data Set\"\ndescription: |\n  Run a feature selection with permutated features.\ncategories:\n  - feature selection\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-02-01\nbibliography: ../../bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n---\n\n\n\n\n\n\n# Scope\n\nFeature selection is the process of finding an optimal set of features to improve the performance, interpretability and robustness of machine learning algorithms.\nIn this article, we introduce the *Shadow Variable Search* algorithm which is a [wrapper method](https://mlr3book.mlr-org.com/feature-selection.html#fs-wrapper) for feature selection.\nWrapper methods iteratively add features to the model that optimize a performance measure.\nAs an example, we will search for the optimal set of features for a [`support vector machine`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.svm.html) on the [`Pima Indian Diabetes`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) data set.\nWe focus on the [`shadow variable search`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_shadow_variable_search.html) algorithm and the application with the [`mlr3fselect::fselect()`](https://mlr3fselect.mlr-org.com/reference/fselect.html) function.\nWe assume that you are already familiar with the basic building blocks of the mlr3 ecosystem.\nIf you are new to feature selection, we recommend reading the [feature selection chapter](https://mlr3book.mlr-org.com/feature-selection.html) of the mlr3book.\nSome knowledge about [mlr3pipelines](https://mlr3pipelines.mlr-org.com) is beneficial but not necessary to understand the example.\n\n# Shadow Variable Search\n\nAdding shadow variables to a data set is a commonly used method in machine learning [@wu_controlling_2007; @thomas_probing_2017].\nThe idea is to add permutated copies of the original features to the data set.\nThese permutated copies are called shadow variables or pseudovariables and the permutation makes them unrelated to the target variable.\nThe subsequent search is similar to the sequential forward selection algorithm.\nFeatures are added to the model one at a time based on the highest performance improvement.\nThe difference is that the termination criterion is the selection of a shadow variable.\nSelecting a shadow variable means that the best improvement is achieved by adding a feature that is unrelated to the target variable.\nConsequently, the variables not yet selected are also correlated to the target variable only by chance.\nTherefore, only the previously selected features have a true influence on the target variable.\n\nFeature selection algorithms are implemented with the [`mlr3fselect::FSelector`](https://mlr3fselect.mlr-org.com/reference/FSelector.html) class.\nA complete list of all fselectors is available on the [website](https://mlr-org.com/fselectors.html).\nWe load the [`shadow variable search`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_shadow_variable_search.html) with the [`mlr3fselect::fs()`](https://mlr3fselect.mlr-org.com/reference/fs.html) function.\nThe algorithm has no parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n\nfs(\"shadow_variable_search\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<FSelectorShadowVariableSearch>: Shadow Variable Search\n* Parameters: list()\n* Properties: single-crit\n* Packages: mlr3fselect\n```\n:::\n:::\n\n\n# Task and Learner\n\nThe objective of the [`Pima Indian Diabetes`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) data set is to predict whether a person has diabetes or not.\nThe data set includes 768 patients with 8 measurements.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\n```\n:::\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nlibrary(data.table)\n\ntask = tsk(\"pima\")\ndata = melt(as.data.table(task), id.vars = task$target_names, measure.vars = task$feature_names)\n\nggplot(data, aes(x = value, fill = diabetes)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ variable, ncol = 8, scales = \"free\") +\n  scale_fill_viridis_d(end = 0.8) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank())\n```\n\n::: {.cell-output-display}\n![Distribution of the features in the Pima Indian Diabetes data set.](index_files/figure-html/2023-02-01-shadow-variable-search-004-1.png){fig-align='center' width=1344}\n:::\n:::\n\n\nThe data set contains missing values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure  triceps \n       0        0        5      374       11        0        0       35      227 \n```\n:::\n:::\n\n\nSupport vector machines cannot handle missing values.\nWe impute the missing values with the [`histogram imputation`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputehist.html) method.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = po(\"imputehist\") %>>% lrn(\"classif.svm\", predict_type = \"prob\")\n```\n:::\n\n\n# Feature Selection\n\nWe can now start the feature selection directly with the [`fselect()`](https://mlr3fselect.mlr-org.com/reference/fselect.html) function.\nIn addition to the task, learner and feature selection algorithm, we have to select a [`resampling strategy`](https://mlr3.mlr-org.com/reference/Resampling.html) and [`performance measure`](https://mlr3.mlr-org.com/reference/Measure.html) to determine how the performance of a feature subset is evaluated.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = fselect(\n  method = fs(\"shadow_variable_search\"),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.auc\")\n)\n```\n:::\n\n\nThe [`fselect()`](https://mlr3fselect.mlr-org.com/reference/fselect.html) function returns a [`mlr3fselect::FSelectInstanceSingelCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingelCrit.html) that includes an archive with all evaluated feature sets.\nEach feature has a corresponding shadow variable.\nWe only show the variables age, glucose and insulin and their shadow variables here.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, .(age, glucose, insulin, permuted__age, permuted__glucose, permuted__insulin, classif.auc)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      age glucose insulin permuted__age permuted__glucose permuted__insulin classif.auc\n 1:  TRUE   FALSE   FALSE         FALSE             FALSE             FALSE   0.6437052\n 2: FALSE    TRUE   FALSE         FALSE             FALSE             FALSE   0.7598155\n 3: FALSE   FALSE    TRUE         FALSE             FALSE             FALSE   0.4900280\n 4: FALSE   FALSE   FALSE         FALSE             FALSE             FALSE   0.6424026\n 5: FALSE   FALSE   FALSE         FALSE             FALSE             FALSE   0.5690107\n---                                                                                    \n54:  TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8266713\n55:  TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8063568\n56:  TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8244232\n57:  TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8234605\n58:  TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8164784\n```\n:::\n:::\n\n\nThe result of the feature selection is stored in the instance.\nIt contains the last feature set before a shadow variable was selected.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    age glucose insulin mass pedigree pregnant pressure triceps                  features classif.auc\n1: TRUE    TRUE   FALSE TRUE     TRUE    FALSE    FALSE   FALSE age,glucose,mass,pedigree    0.835165\n```\n:::\n:::\n\n\n# Final Model\n\nThe learner we use to make predictions on new data is called the final model.\nThe final model is trained with the optimal feature set on the full data set.\nWe subset the task to the optimal feature set and train the learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$select(instance$result_feature_set)\nlearner$train(task)\n```\n:::\n\n\nThe trained model can now be used to predict new, external data.\n\n# Conclusion\n\nThe shadow variable search is a fast feature selection method that is easy to use.\nThe [`fselect()`](https://mlr3fselect.mlr-org.com/reference/fselect.html) function is a convenient way to apply the method to a data set.\nIf you want to know more about feature selection, we recommend having a look at our [book](https://mlr3book.mlr-org.com/feature-selection.html#introduction).\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}