{
  "hash": "f0480adf79aa9e461170baab76bac2ed",
  "result": {
    "markdown": "---\ntitle: \"Default Hyperparameter Configuration\"\ndescription: |\n  Run the default hyperparameter configuration of learners as a baseline.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-01-19\nbibliography: bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 10\n      datatable.print.class: FALSE\n      datatable.print.keys: FALSE\n      datatable.print.trunc.cols: TRUE\n---\n\n\n\n\n\n\n# Scope\n\nThe predictive performance of modern machine learning algorithms is highly dependent on the choice of their hyperparameter configuration.\nOptions for setting hyperparameters are tuning, manual selection by the user, and using the default configuration of the algorithm.\nThe default configurations are chosen to work with a wide range of data sets but they usually do not achieve the best predictive performance.\nWhen tuning a learner in mlr3, we can run the default configuration as a baseline.\nSeeing how well it performs will tell us whether tuning pays off.\nIf the optimized configurations perform worse, we could expand the search space or try a different optimization algorithm.\nOf course, it could also be that tuning on the given data set is simply not worth it.\n\n# Example\n\nWe tune the hyperparameters of the [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) on the [`spam`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html) data set.\nThe search space is taken from the @bischl_hyperparameter_2021 article.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/2023-01-19-default-configuration-002_6eb4451fbfdcb073de1ff165505c4d2e'}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n\nlearner = lrn(\"classif.ranger\",\n  mtry.ratio      = to_tune(0, 1),\n  replace         = to_tune(),\n  sample.fraction = to_tune(1e-1, 1),\n  num.trees       = to_tune(1, 2000)\n)\n```\n:::\n\n\nWhen creating the tuning instance, we set `evaluate_default = TRUE` to test the default hyperparameter configuration.\nThe default configuration is evaluated in the first batch of the tuning run.\nThe other batches contain randomly sampled hyperparameter configurations.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/2023-01-19-default-configuration-003_bd90937949505a9b6c668a53a6b6ec40'}\n\n```{.r .cell-code}\ninstance = tune(\n  method = tnr(\"random_search\", batch_size = 5),\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp (\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  term_evals = 51,\n  evaluate_default = TRUE\n)\n```\n:::\n\n\nThe default configuration is recorded in the first row of the archive.\nThe other rows contain the results of the random search.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/2023-01-19-default-configuration-004_8ab99e883bb74d9a49b61c001be5142c'}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, .(batch_nr, mtry.ratio, replace, sample.fraction, num.trees, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    batch_nr mtry.ratio replace sample.fraction num.trees classif.ce\n 1:        1 0.12280702    TRUE       1.0000000       500 0.04889179\n 2:        2 0.81757154   FALSE       0.8117389      1528 0.06518905\n 3:        2 0.90097848   FALSE       0.9188504       571 0.06975228\n 4:        2 0.65584252    TRUE       0.3145144       681 0.06323338\n 5:        2 0.40363652   FALSE       0.7508936      1807 0.05801825\n---                                                                 \n47:       11 0.71528316    TRUE       0.4398745      1394 0.06127771\n48:       11 0.19136788   FALSE       0.8293552       249 0.04889179\n49:       11 0.09430346   FALSE       0.6233559      1307 0.04889179\n50:       11 0.52643368   FALSE       0.5993606      1403 0.05997392\n51:       11 0.17115160    TRUE       0.3309041       114 0.05867014\n```\n:::\n:::\n\n\nWe plot the performances of the evaluated hyperparameter configurations.\nThe blue line connects the best configuration of each batch.\nWe see that the default configuration already performs well and the optimized configurations can not beat it.\n\n\n::: {.cell layout-align=\"center\" hash='index_cache/html/2023-01-19-default-configuration-005_7ee8ae647e7f1e424835a4ee10c4f0a1'}\n\n```{.r .cell-code}\nlibrary(mlr3viz)\n\nautoplot(instance, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2023-01-19-default-configuration-005-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n# Conlcusion\n\nThe time required to test the default configuration is negligible compared to the time required to run the hyperparameter optimization.\nIt gives us a valuable indication of whether our tuning is properly configured.\nRunning the default configuration as a baseline is a good practice that should be used in every tuning run.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}