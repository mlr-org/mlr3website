{
  "hash": "d8483524bce103ccc4bd4d5616a919c0",
  "result": {
    "markdown": "---\ntitle: \"Default Hyperparameter Values\"\ndescription: |\n  Test default hyperparameter values during tuning.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-01-19\nbibliography: bibliography.bib\n---\n\n\n\n\n\n\n# Scope\n\nThe predictive performance of modern machine learning algorithms is highly dependent on the choice of their hyperparameter configuration.\nOne option is to use the default configuration of the algorithm.\nThese configurations are often chosen to work well with a wide range of data sets.\n\n# Example\n\nWe tune the hyperparameters of the [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) on the [`spam`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html) data set.\nThe search space is taken from the @bischl_hyperparameter_2021 article.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: mlr3\n```\n:::\n\n```{.r .cell-code}\nlearner = lrn(\"classif.ranger\",\n  mtry.ratio      = to_tune(0, 1),\n  replace         = to_tune(),\n  sample.fraction = to_tune(1e-1, 1),\n  num.trees       = to_tune(1, 2000)\n)\n```\n:::\n\n\nWhen creating the tuning instance, we set `evaluate_default = TRUE` to test the default hyperparameter configuration.\nThe default configuration is evaluated in the first batch of the tuning run.\nThe other batches contain randomly sampled hyperparameter configurations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  method = tnr(\"random_search\", batch_size = 5),\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp (\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  term_evals = 51,\n  evaluate_default = TRUE\n)\n```\n:::\n\n\nThe default configuration is recorded in the first row of the archive.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    mtry.ratio replace sample.fraction num.trees classif.ce x_domain_mtry.ratio x_domain_replace\n 1: 0.12280702    TRUE       1.0000000       500 0.04758801          0.12280702             TRUE\n 2: 0.70905833   FALSE       0.3148552       649 0.06453716          0.70905833            FALSE\n 3: 0.21006278   FALSE       0.4777352       618 0.05019557          0.21006278            FALSE\n 4: 0.04606191   FALSE       0.9679785      1107 0.04693611          0.04606191            FALSE\n 5: 0.36221788   FALSE       0.3546781       439 0.05736636          0.36221788            FALSE\n 6: 0.68189375    TRUE       0.4455002      1667 0.06192960          0.68189375             TRUE\n 7: 0.85711748   FALSE       0.7799393      1087 0.05671447          0.85711748            FALSE\n 8: 0.54974197   FALSE       0.4149724      1295 0.05801825          0.54974197            FALSE\n 9: 0.28608463   FALSE       0.8428370       456 0.04758801          0.28608463            FALSE\n10: 0.72529218   FALSE       0.2087743      1405 0.06388527          0.72529218            FALSE\n11: 0.44712317    TRUE       0.4472030       479 0.05606258          0.44712317             TRUE\n12: 0.14153432   FALSE       0.8157925      1864 0.04628422          0.14153432            FALSE\n13: 0.69266067   FALSE       0.6179607       447 0.05541069          0.69266067            FALSE\n14: 0.86528631    TRUE       0.9317713      1795 0.05410691          0.86528631             TRUE\n15: 0.98888315   FALSE       0.4575985      1693 0.05801825          0.98888315            FALSE\n16: 0.54146760   FALSE       0.5284760       973 0.05541069          0.54146760            FALSE\n17: 0.69712719   FALSE       0.7245499       596 0.05084746          0.69712719            FALSE\n18: 0.34370812    TRUE       0.3024334       151 0.06192960          0.34370812             TRUE\n19: 0.90156498    TRUE       0.5553487      1279 0.05671447          0.90156498             TRUE\n20: 0.08226453    TRUE       0.2156797      1215 0.06192960          0.08226453             TRUE\n21: 0.21208158    TRUE       0.6268328        79 0.05345502          0.21208158             TRUE\n22: 0.02936639   FALSE       0.6701020       335 0.05410691          0.02936639            FALSE\n23: 0.93894315   FALSE       0.2160808      1223 0.06518905          0.93894315            FALSE\n24: 0.92240679   FALSE       0.7865939      1964 0.05932203          0.92240679            FALSE\n25: 0.06293614    TRUE       0.5469157      1848 0.05345502          0.06293614             TRUE\n26: 0.29824807    TRUE       0.2961051       166 0.06127771          0.29824807             TRUE\n27: 0.81061562    TRUE       0.5099749       957 0.05997392          0.81061562             TRUE\n28: 0.39450936   FALSE       0.6308459       977 0.04954368          0.39450936            FALSE\n29: 0.06744465   FALSE       0.1265084       525 0.06323338          0.06744465            FALSE\n30: 0.70798409   FALSE       0.6709937       777 0.05345502          0.70798409            FALSE\n31: 0.85861089   FALSE       0.7111372      1895 0.05541069          0.85861089            FALSE\n32: 0.61384581    TRUE       0.3698579       374 0.06127771          0.61384581             TRUE\n33: 0.10858668   FALSE       0.9265499      1447 0.04693611          0.10858668            FALSE\n34: 0.95340907   FALSE       0.6495661      1682 0.05801825          0.95340907            FALSE\n35: 0.08611250   FALSE       0.8237771      1040 0.04628422          0.08611250            FALSE\n36: 0.79713462   FALSE       0.7368857      1623 0.05475880          0.79713462            FALSE\n37: 0.09559774    TRUE       0.4889912      1592 0.05215124          0.09559774             TRUE\n38: 0.86161102    TRUE       0.3501982       615 0.06323338          0.86161102             TRUE\n39: 0.34872693   FALSE       0.1831345       910 0.06388527          0.34872693            FALSE\n40: 0.31909094    TRUE       0.6281793      1659 0.05410691          0.31909094             TRUE\n41: 0.02900647   FALSE       0.4960310       140 0.05475880          0.02900647            FALSE\n42: 0.68041018   FALSE       0.4697572       828 0.05606258          0.68041018            FALSE\n43: 0.30795272   FALSE       0.3654631      1221 0.05475880          0.30795272            FALSE\n44: 0.52629698    TRUE       0.9754535      1191 0.05475880          0.52629698             TRUE\n45: 0.51091588    TRUE       0.6577406      1111 0.05736636          0.51091588             TRUE\n46: 0.36119895   FALSE       0.5961844      1122 0.05280313          0.36119895            FALSE\n47: 0.16449091    TRUE       0.2877900       146 0.05736636          0.16449091             TRUE\n48: 0.64769130   FALSE       0.5539487      1716 0.05410691          0.64769130            FALSE\n49: 0.45014783    TRUE       0.4324327      1851 0.05736636          0.45014783             TRUE\n50: 0.31402221   FALSE       0.3303031       321 0.05541069          0.31402221            FALSE\n51: 0.71370439    TRUE       0.1829285       877 0.06388527          0.71370439             TRUE\n    mtry.ratio replace sample.fraction num.trees classif.ce x_domain_mtry.ratio x_domain_replace\n    x_domain_sample.fraction x_domain_num.trees runtime_learners           timestamp batch_nr warnings errors\n 1:                1.0000000                500            0.682 2023-01-17 13:47:37        1        0      0\n 2:                0.3148552                649            1.843 2023-01-17 13:47:44        2        0      0\n 3:                0.4777352                618            1.410 2023-01-17 13:47:44        2        0      0\n 4:                0.9679785               1107            1.502 2023-01-17 13:47:44        2        0      0\n 5:                0.3546781                439            1.263 2023-01-17 13:47:44        2        0      0\n 6:                0.4455002               1667            5.210 2023-01-17 13:47:44        2        0      0\n 7:                0.7799393               1087            6.499 2023-01-17 13:47:51        3        0      0\n 8:                0.4149724               1295            3.462 2023-01-17 13:47:51        3        0      0\n 9:                0.8428370                456            1.389 2023-01-17 13:47:51        3        0      0\n10:                0.2087743               1405            3.712 2023-01-17 13:47:51        3        0      0\n11:                0.4472030                479            1.749 2023-01-17 13:47:51        3        0      0\n12:                0.8157925               1864            3.771 2023-01-17 13:48:03        4        0      0\n13:                0.6179607                447            2.981 2023-01-17 13:48:03        4        0      0\n14:                0.9317713               1795           11.590 2023-01-17 13:48:03        4        0      0\n15:                0.4575985               1693            9.690 2023-01-17 13:48:03        4        0      0\n16:                0.5284760                973            4.595 2023-01-17 13:48:03        4        0      0\n17:                0.7245499                596            2.810 2023-01-17 13:48:10        5        0      0\n18:                0.3024334                151            0.242 2023-01-17 13:48:10        5        0      0\n19:                0.5553487               1279            5.761 2023-01-17 13:48:10        5        0      0\n20:                0.2156797               1215            0.809 2023-01-17 13:48:10        5        0      0\n21:                0.6268328                 79            0.242 2023-01-17 13:48:10        5        0      0\n22:                0.6701020                335            0.295 2023-01-17 13:48:24        6        0      0\n23:                0.2160808               1223            2.795 2023-01-17 13:48:24        6        0      0\n24:                0.7865939               1964           12.556 2023-01-17 13:48:24        6        0      0\n25:                0.5469157               1848            2.126 2023-01-17 13:48:24        6        0      0\n26:                0.2961051                166            0.357 2023-01-17 13:48:24        6        0      0\n27:                0.5099749                957            4.241 2023-01-17 13:48:35        7        0      0\n28:                0.6308459                977            3.028 2023-01-17 13:48:35        7        0      0\n29:                0.1265084                525            0.438 2023-01-17 13:48:35        7        0      0\n30:                0.6709937                777            4.696 2023-01-17 13:48:35        7        0      0\n31:                0.7111372               1895           10.359 2023-01-17 13:48:35        7        0      0\n32:                0.3698579                374            1.350 2023-01-17 13:48:47        8        0      0\n33:                0.9265499               1447            2.932 2023-01-17 13:48:47        8        0      0\n34:                0.6495661               1682           10.894 2023-01-17 13:48:47        8        0      0\n35:                0.8237771               1040            2.318 2023-01-17 13:48:47        8        0      0\n36:                0.7368857               1623           10.029 2023-01-17 13:48:47        8        0      0\n37:                0.4889912               1592            1.509 2023-01-17 13:48:52        9        0      0\n38:                0.3501982                615            2.358 2023-01-17 13:48:52        9        0      0\n39:                0.1831345                910            1.347 2023-01-17 13:48:52        9        0      0\n40:                0.6281793               1659            3.514 2023-01-17 13:48:52        9        0      0\n41:                0.4960310                140            0.176 2023-01-17 13:48:52        9        0      0\n42:                0.4697572                828            4.022 2023-01-17 13:49:00       10        0      0\n43:                0.3654631               1221            2.889 2023-01-17 13:49:00       10        0      0\n44:                0.9754535               1191            6.574 2023-01-17 13:49:00       10        0      0\n45:                0.6577406               1111            3.937 2023-01-17 13:49:00       10        0      0\n46:                0.5961844               1122            3.564 2023-01-17 13:49:00       10        0      0\n47:                0.2877900                146            0.166 2023-01-17 13:49:07       11        0      0\n48:                0.5539487               1716            6.764 2023-01-17 13:49:07       11        0      0\n49:                0.4324327               1851            3.873 2023-01-17 13:49:07       11        0      0\n50:                0.3303031                321            0.571 2023-01-17 13:49:07       11        0      0\n51:                0.1829285                877            1.510 2023-01-17 13:49:07       11        0      0\n    x_domain_sample.fraction x_domain_num.trees runtime_learners           timestamp batch_nr warnings errors\n         resample_result\n 1: <ResampleResult[21]>\n 2: <ResampleResult[21]>\n 3: <ResampleResult[21]>\n 4: <ResampleResult[21]>\n 5: <ResampleResult[21]>\n 6: <ResampleResult[21]>\n 7: <ResampleResult[21]>\n 8: <ResampleResult[21]>\n 9: <ResampleResult[21]>\n10: <ResampleResult[21]>\n11: <ResampleResult[21]>\n12: <ResampleResult[21]>\n13: <ResampleResult[21]>\n14: <ResampleResult[21]>\n15: <ResampleResult[21]>\n16: <ResampleResult[21]>\n17: <ResampleResult[21]>\n18: <ResampleResult[21]>\n19: <ResampleResult[21]>\n20: <ResampleResult[21]>\n21: <ResampleResult[21]>\n22: <ResampleResult[21]>\n23: <ResampleResult[21]>\n24: <ResampleResult[21]>\n25: <ResampleResult[21]>\n26: <ResampleResult[21]>\n27: <ResampleResult[21]>\n28: <ResampleResult[21]>\n29: <ResampleResult[21]>\n30: <ResampleResult[21]>\n31: <ResampleResult[21]>\n32: <ResampleResult[21]>\n33: <ResampleResult[21]>\n34: <ResampleResult[21]>\n35: <ResampleResult[21]>\n36: <ResampleResult[21]>\n37: <ResampleResult[21]>\n38: <ResampleResult[21]>\n39: <ResampleResult[21]>\n40: <ResampleResult[21]>\n41: <ResampleResult[21]>\n42: <ResampleResult[21]>\n43: <ResampleResult[21]>\n44: <ResampleResult[21]>\n45: <ResampleResult[21]>\n46: <ResampleResult[21]>\n47: <ResampleResult[21]>\n48: <ResampleResult[21]>\n49: <ResampleResult[21]>\n50: <ResampleResult[21]>\n51: <ResampleResult[21]>\n         resample_result\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3viz)\n\nautoplot(instance, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n@probst2019\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}