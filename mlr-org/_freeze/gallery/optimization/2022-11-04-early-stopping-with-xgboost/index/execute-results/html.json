{
  "hash": "e559860f97ab1d7b1cf93d25388115d4",
  "result": {
    "markdown": "---\ntitle: \"Early Stopping with XGBoost\"\ndescription: |\n  Simultaneously optimize hyperparameters and use early stopping.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\ndate: 2022-11-09\naliases:\n  - ../../../gallery/2022-11-04-early-stopping-with-xgboost/index.html\n---\n\n\n\n\n\n\n# Scope\n\nIn this post, we use early stopping to reduce overfitting when training an [`XGBoost model`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html).\nWe start with a short recap on early stopping and overfitting.\nAfter that, we use the early stopping mechanism of XGBoost and train a model on the [`Spam Classification`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html) data set.\nFinally we show how to simultaneously tune hyperparameters and use early stopping.\nThe reader should be familiar with [tuning](https://mlr3gallery.mlr-org.com/posts/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/) in the mlr3 ecosystem.\n\n# Early Stopping\n\nEarly stopping is a technique used to reduce overfitting when fitting a model in an iterative process.\nOverfitting occurs when a model fits increasingly to the training data but the performance on unseen data decreases.\nThis means the model's training error decreases, while its test performance deteriorates.\nWhen using early stopping, the performance is monitored on a test set, and the training stops when performance decreases in a specific number of iterations.\n\n# XGBoost with Early Stopping\n\nWe initialize the random number generator with a fixed seed for reproducibility.\nThe [mlr3verse](https://mlr3verse.mlr-org.com) package provides all functions required for this example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\n\nlibrary(mlr3verse)\n```\n:::\n\n\nWhen training an XGBoost model, we can use early stopping to find the optimal number of boosting rounds.\nThe [`partition()`](https://mlr3.mlr-org.com/reference/partition.html) function splits the observations of the task into two disjoint sets.\nWe use 80% of observations to train the model and the remaining 20% as the [test set](https://mlr3.mlr-org.com/reference/Task.html#active-bindings) to monitor the performance.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"spam\")\nsplit = partition(task, ratio = 0.8)\ntask$set_row_roles(split$test, \"test\")\n```\n:::\n\n\nThe `early_stopping_set` parameter controls which set is used to monitor the performance.\nAdditionally, we need to define the range in which the performance must increase with `early_stopping_rounds` and the maximum number of boosting rounds with `nrounds`.\nIn this example, the training is stopped when the classification error is not decreasing for 100 rounds or 1000 rounds are reached.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.xgboost\",\n  nrounds = 1000,\n  early_stopping_rounds = 100,\n  early_stopping_set = \"test\",\n  eval_metric = \"error\"\n)\n```\n:::\n\n\nWe train the learner with early stopping.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$train(task)\n```\n:::\n\n\nThe `$evaluation_log` of the model stores the performance scores on the training and test set.\nFigure 1 shows that the classification error on the training set decreases, whereas the error on the test set increases after 20 rounds.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nlibrary(data.table)\n\ndata = melt(\n  learner$model$evaluation_log,\n  id.vars = \"iter\",\n  variable.name = \"set\",\n  value.name = \"error\"\n)\n\nggplot(data, aes(x = iter, y = error, group = set)) +\n  geom_line(aes(color = set)) +\n  geom_vline(aes(xintercept = learner$model$best_iteration), color = \"grey\") +\n  scale_colour_manual(values=c(\"#f8766d\", \"#00b0f6\"), labels = c(\"Train\", \"Test\")) +\n  labs(x = \"Rounds\", y = \"Classification Error\", color = \"Set\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Comparison between train and test set classification error.](index_files/figure-html/2022-11-04-early-stopping-with-xgboost-006-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe slot `$best_iteration` contains the optimal number of boosting rounds.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$model$best_iteration\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 20\n```\n:::\n:::\n\n\nNote that, `learner$predict()` will use the model from the last iteration, not the best one.\nSee the next section on how to fit a model with the optimal number of boosting rounds and hyperparameter configuration.\n\n# Tuning\n\nIn this section, we want to tune the hyperparameters of an XGBoost model and find the optimal number of boosting rounds in one go.\nFor this, we need the [`early stopping callback`](https://mlr3tuning.mlr-org.com/reference/mlr3tuning.early_stopping.html) which handles early stopping during the tuning process.\nThe performance of a hyperparameter configuration is evaluated with a resampling strategy while tuning e.g. 3-fold cross-validation.\nIn each resampling iteration, a new XGBoost model is trained and early stopping is used to find the optimal number of boosting rounds.\nThis results in three different optimal numbers of boosting rounds for one hyperparameter configuration when applying 3-fold cross-validation.\nThe callback picks the maximum of the three values and writes it to the archive.\nIt uses the maximum value because the final model is fitted on the complete data set.\nNow let's start with a practical example.\n\nFirst, we load the XGBoost learner and set the early stopping parameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.xgboost\",\n  nrounds = 1000,\n  early_stopping_rounds = 100,\n  early_stopping_set = \"test\"\n)\n```\n:::\n\n\nNext, we load a predefined tuning space from the [mlr3tuningspaces](https://mlr3tuningspaces.mlr-org.com) package.\nThe tuning space includes the most commonly tuned parameters of XGBoost.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuning_space = lts(\"classif.xgboost.default\")\nas.data.table(tuning_space)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  id lower upper logscale\n1:               eta 1e-04     1     TRUE\n2:           nrounds 1e+00  5000    FALSE\n3:         max_depth 1e+00    20    FALSE\n4:  colsample_bytree 1e-01     1    FALSE\n5: colsample_bylevel 1e-01     1    FALSE\n6:            lambda 1e-03  1000     TRUE\n7:             alpha 1e-03  1000     TRUE\n8:         subsample 1e-01     1    FALSE\n```\n:::\n:::\n\n\nWe argument the learner with the tuning space.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lts(learner)\n```\n:::\n\n\nThe default tuning space contains the `nrounds` hyperparameter.\nWe have to overwrite it with an upper bound for early stopping.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$param_set$set_values(nrounds = 1000)\n```\n:::\n\n\nWe run a small batch of random hyperparameter configurations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  tuner = tnr(\"random_search\", batch_size = 2),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 4,\n  callbacks = clbk(\"mlr3tuning.early_stopping\")\n)\n```\n:::\n\n\nWe can see that the optimal number of boosting rounds (`max_nrounds`) strongly depends on the other hyperparameters.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, list(batch_nr, max_nrounds, eta, max_depth, colsample_bytree, colsample_bylevel, lambda, alpha, subsample)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   batch_nr max_nrounds       eta max_depth colsample_bytree colsample_bylevel     lambda     alpha subsample\n1:        1        1000 -4.129996        13        0.5466492         0.3051989 -0.9448420  2.535477 0.5454539\n2:        1         998 -3.338899         8        0.6193478         0.7392354 -2.2338126  1.793921 0.2294161\n3:        2        1000 -6.059897         5        0.6118892         0.5445475 -6.5698270  5.414224 0.9635730\n4:        2        1000 -4.528129         1        0.1094186         0.2526143 -0.7535105 -3.041829 0.5934376\n```\n:::\n:::\n\n\nIn the best hyperparameter configuration, the value of `nrounds` is replaced by `max_nrounds` and early stopping is deactivated.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result_learner_param_vals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$nrounds\n[1] 998\n\n$nthread\n[1] 1\n\n$verbose\n[1] 0\n\n$early_stopping_set\n[1] \"none\"\n\n$eta\n[1] 0.03547598\n\n$max_depth\n[1] 8\n\n$colsample_bytree\n[1] 0.6193478\n\n$colsample_bylevel\n[1] 0.7392354\n\n$lambda\n[1] 0.1071192\n\n$alpha\n[1] 6.012984\n\n$subsample\n[1] 0.2294161\n```\n:::\n:::\n\n\nFinally, fit the final model on the complete data set.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.xgboost\")\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n```\n:::\n\n\nThe trained model can now be used to make predictions on new data.\n\nWe can also use the [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) to get a tuned XGBoost model.\nNote that, early stopping is deactivated when the final model is fitted.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}