{
  "hash": "ddc82858eefdcaf10fa672f788894136",
  "result": {
    "markdown": "---\ntitle: \"Shadow Variable Search on the Pima Indian Diabetes Data Set\"\ndescription: |\n  Select features with a few lines of code.\ncategories:\n  - feature selection\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-01-23\nbibliography: ../../bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n---\n\n\n\n\n\n\n# Scope\n\nFeature selection is the process of finding an optimal set of features to improve the performance and robustness of machine learning algorithms.\nIn this article, we introduce the [`mlr3fselect::fselect()`](https://mlr3fselect.mlr-org.com/reference/fselect.html) function and the [`shadow variable search`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_shadow_variable_search.html) algorithm [@wu_controlling_2007] for a quick and easy feature selection.\nAs an example, we will search for the optimal set of features for a [`support vector machine`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.svm.html) on the [`Pima Indian Diabetes`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) data set.\nWe assume that you are already familiar with the basic building blocks of the mlr3 ecosystem.\nSome knowledge about [mlr3pipelines](https://mlr3pipelines.mlr-org.com) is beneficial but not necessary to understand the example.\n\n# Task and Learner\n\nThe objective of the [`Pima Indian Diabetes`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) data set is to predict whether a person has diabetes or not.\nThe data set includes 768 patients with 8 measurements.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n\ntask = tsk(\"pima\")\n```\n:::\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\ntask = tsk(\"pima\")\ndata = melt(as.data.table(task), id.vars = task$target_names, measure.vars = task$feature_names)\n\nggplot(data, aes(x = value, fill = diabetes)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ variable, ncol = 8, scales = \"free\") +\n  scale_fill_viridis_d(end = 0.8) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank())\n```\n\n::: {.cell-output-display}\n![Distribution of the features in the Pima Indian Diabetes data set.](index_files/figure-html/2023-01-23-shadow-variable-search-on-pima-003-1.png){fig-align='center' width=1344}\n:::\n:::\n\n\nThe data set contains missing values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure  triceps \n       0        0        5      374       11        0        0       35      227 \n```\n:::\n:::\n\n\nSupport vector machines cannot handle missing values.\nWe impute the missing values with the [`histogram imputation`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputehist.html) method.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = po(\"imputehist\") %>>% lrn(\"classif.svm\", predict_type = \"prob\")\n```\n:::\n\n\n# Feature Selection\n\nThe [`mlr3fselect::fselect()`](https://mlr3fselect.mlr-org.com/reference/fselect.html) function controls and executes the feature selection.\nThe function internally creates an [`fselect instance`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html) and executes the feature selection directly.\nThe `method` argument specifies the feature selection algorithm.\nWe use the wrapper method [`shadow variable search`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_shadow_variable_search.html).\nShadow variable search adds permutated copies to the data set for each feature.\nWhile optimizing, it adds features until a shadow variable is selected.\nThe result is the last feature set without a shadow variable.\nThe [`resampling strategy`](https://mlr3.mlr-org.com/reference/Resampling.html) and [`performance measure`](https://mlr3.mlr-org.com/reference/Measure.html) determine how the performance of a model is evaluated.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = fselect(\n  method = fs(\"shadow_variable_search\"),\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.auc\")\n)\n```\n:::\n\n\nThe [`mlr3fselect::fselect()`](https://mlr3fselect.mlr-org.com/reference/fselect.html) function returns an instance that includes an archive with all evaluated feature sets.\nEach feature has a corresponding shadow variable.\nWe only show the variables age, glucose and insulin and their shadow variables here.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, .(age, glucose, insulin, permuted__age, permuted__glucose, permuted__insulin, classif.auc)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      age glucose insulin permuted__age permuted__glucose permuted__insulin classif.auc\n 1:  TRUE   FALSE   FALSE         FALSE             FALSE             FALSE   0.6437052\n 2: FALSE    TRUE   FALSE         FALSE             FALSE             FALSE   0.7598155\n 3: FALSE   FALSE    TRUE         FALSE             FALSE             FALSE   0.4900280\n 4: FALSE   FALSE   FALSE         FALSE             FALSE             FALSE   0.6424026\n 5: FALSE   FALSE   FALSE         FALSE             FALSE             FALSE   0.5690107\n---                                                                                    \n54:  TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8266713\n55:  TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8063568\n56:  TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8244232\n57:  TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8234605\n58:  TRUE    TRUE   FALSE         FALSE             FALSE             FALSE   0.8164784\n```\n:::\n:::\n\n\nThe best configuration and the corresponding measured performance can be retrieved from the instance.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    age glucose insulin mass pedigree pregnant pressure triceps                  features classif.auc\n1: TRUE    TRUE   FALSE TRUE     TRUE    FALSE    FALSE   FALSE age,glucose,mass,pedigree    0.835165\n```\n:::\n:::\n\n\n# Final Model\n\nThe learner we use to make predictions on new data is called the final model.\nThe final model is trained with the optimal feature set on the full data set.\nWe subset the task to the optimal feature set and train the learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$select(instance$result_feature_set)\nlearner$train(task)\n```\n:::\n\n\nThe trained model can now be used to predict new, external data.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}