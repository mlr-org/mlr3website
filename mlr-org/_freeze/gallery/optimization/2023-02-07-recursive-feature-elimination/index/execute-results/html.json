{
  "hash": "efbd60083f1aa43de3aa636b0c8253fb",
  "result": {
    "markdown": "---\ntitle: \"Recursive Feature Elimination on the Sonar Data Set\"\ndescription: |\n  Utilize the built-in feature importance of models.\ncategories:\n  - feature selection\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-02-07\nbibliography: ../../bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n---\n\n\n\n\n\n\n# Scope\n\nFeature selection is the process of finding an optimal set of features to improve the performance, interpretability and robustness of machine learning algorithms.\nIn this article, we introduce the *Recursive Feature Elimination* algorithm which is a [wrapper method](https://mlr3book.mlr-org.com/feature-selection.html#fs-wrapper) for feature selection.\nWrapper methods iteratively add features to the model that optimize a performance measure.\nAs an example, we will search for the optimal set of features for a [`gradient boosting machine`](https://mlr3extralearners.mlr-org.com/reference/mlr_learners_classif.gbm.html) and [`support vector machine`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.svm.html) on the [`Sonar`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) data set.\nWe assume that you are already familiar with the basic building blocks of the [mlr3 ecosystem](https://mlr-org.com/ecosystem.html).\nIf you are new to feature selection, we recommend reading the [feature selection chapter](https://mlr3book.mlr-org.com/feature-selection.html) of the mlr3book first.\n\n# Recursive Feature Elimination\n\nRecursive Feature Elimination (RFE) is a widely used feature selection method in high-dimensional data sets.\nThe idea is to iteratively remove the weakest feature from a model until the desired number of features is reached.\nThe weakest feature is determined by the built-in feature importance method of the model.\nCurrently, RFE works with support vector machines, decision tree algorithms and gradient boosting machines.\nSupported learners are tagged with the `\"importance\"` property.\nFor a full list of supported learners, see the learner page on the [mlr-org website](https://mlr-org.com/learners.html)  and search for `\"importance\"`.\n\n@guyon_gene_2002 developed the RFE algorithm for support vector machines (SVM-RFE) to select informative genes in cancer classification.\nThe importance of the features is given by the weight vector of a linear support vector machine.\nThis method was later extended to other machine learning algorithms.\nThe only requirement is that the models can internally measure the feature importance.\nThe random forest algorithm offers multiple options for measuring feature importance.\nThe commonly used methods are the mean decrease in accuracy (MDA) and the mean decrease in impurity (MDI).\nThe MDA measures the decrease in accuracy for a feature if it was randomly permuted in the out-of-bag sample.\nThe MDI is the total reduction in node impurity when the feature is used for splitting.\nGradient boosting algorithms like [`XGBoost`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html), [`LightGBM`](https://mlr3extralearners.mlr-org.com/reference/mlr_learners_classif.lightgbm.html) and [`GBM`](https://mlr3extralearners.mlr-org.com/reference/mlr_learners_classif.gbm.html) use similar methods to measure the importance of the features.\n\n[mlr3fselect](https://mlr3fselect.mlr-org.com) is the feature selection package of the [mlr3 ecosystem](https://mlr-org.com/ecosystem.html).\nIt implements the [`RFE`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_rfe.html) algorithm.\nWe load all packages of the ecosystem with the [`mlr3verse`](https://mlr3verse.mlr-org.com/reference/mlr3verse-package.html) package.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\nWe retrieve the [`RFE`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_rfe.html) optimizer with the [`fs()`](https://mlr3fselect.mlr-org.com/reference/fs.html) function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptimizer = fs(\"rfe\", n_features = 1, feature_number = 1)\n```\n:::\n\n\nThe algorithm has multiple control parameters.\nThe optimizer stops when the number of features equals `n_features`.\nThe parameters `feature_number`, `feature_fraction` and `subset_size` determine the number of features that are removed in each iteration.\nThe `feature_number` option removes a fixed number of features in each iteration, whereas `feature_fraction` removes a fraction of the features.\nThe `subset_size` argument is a vector that specifies exactly how many features are removed in each iteration.\nThe parameters are mutually exclusive and the default is `feature_fraction = 0.5`.\nUsually, RFE fits a new model in each iteration and calculates the feature importance again.\nWe can deactivate this behavior by setting `recursive = FALSE`.\n\n# Task\n\nThe objective of the [`Sonar`](https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html) data set is to predict whether a sonar signal bounced off a metal cylinder or a rock.\nThe data set includes 60 numerical features (see @fig-features).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"sonar\")\n```\n:::\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nlibrary(data.table)\n\ndata = melt(as.data.table(task), id.vars = task$target_names, measure.vars = task$feature_names)\n\nggplot(data, aes(x = value, fill = Class)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ variable, ncol = 6, scales = \"free\") +\n  scale_fill_viridis_d(end = 0.8) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank())\n```\n\n::: {.cell-output-display}\n![Distribution of the features in the Sonar data set.](index_files/figure-html/fig-features-1.png){#fig-features fig-align='center' width=1344}\n:::\n:::\n\n\n# Gradient Boosting Machine\n\nWe start with the gradient-boosting learner and set the predict type to `\"prob\"` to obtain class probabilities.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.gbm\", distribution = \"bernoulli\", predict_type = \"prob\")\n```\n:::\n\n\nNow we define the feature selection problem by using the [`fsi()`](https://mlr3fselect.mlr-org.com/reference/fsi.html) function that constructs an [`FSelectInstanceSingleCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html).\nIn addition to the task and learner, we have to select a [`resampling strategy`](https://mlr3.mlr-org.com/reference/Resampling.html) and [`performance measure`](https://mlr3.mlr-org.com/reference/Measure.html) to determine how the performance of a feature subset is evaluated.\nWe pass the `\"none\"` terminator because the `n_features` parameter of the optimizer determines when the feature selection stops.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 6),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\"),\n  store_models = TRUE)\n```\n:::\n\n\nWe are now ready to start the recursive feature elimination.\nTo do this, we simply pass the instance to the `$optimize()` method of the optimizer.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptimizer$optimize(instance)\n```\n:::\n\n\nThe optimizer saves the best feature set and the corresponding estimated performance in `instance$result`.\n\n@fig-gbm shows the optimization path of the feature selection.\nWe observe that the performance increases first as the number of features decreases.\nAs soon as informative features are removed, the performance drops again.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(viridisLite)\nlibrary(mlr3misc)\n\ndata = as.data.table(instance$archive)\ndata[, n:= map_int(importance, length)]\n\nggplot(data, aes(x = n, y = classif.auc)) +\n  geom_line(\n    color = viridis(1, begin = 0.5),\n    linewidth = 1) +\n  geom_point(\n    fill = viridis(1, begin = 0.5),\n    shape = 21,\n    size = 3,\n    stroke = 0.5,\n    alpha = 0.8) +\n  xlab(\"Number of Features\") +\n  scale_x_reverse() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Optimization path of the feature selection.](index_files/figure-html/fig-gbm-1.png){#fig-gbm fig-align='center' width=672}\n:::\n:::\n\n\n# Support Vector Machine\n\nNow we will select the optimal feature set for a support vector machine.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"linear\", predict_type = \"prob\")\n```\n:::\n\n\nSince the feature importance can only be calculated with a linear SVM, the learner does not support the calculation at first.\nThis can be seen by the missing `\"importance\"` property.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$properties\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"multiclass\" \"twoclass\"  \n```\n:::\n:::\n\n\nUsing the `\"mlr3fselect.svm_rfe\"` callback however makes it possible to use a linear SVM with the [`RFE`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_rfe.html) optimizer.\nWe load the callback with the [`clbk()`](https://mlr3misc.mlr-org.com/reference/clbk.html) function and pass it to the `callback` argument of the [`fsi()`](https://mlr3fselect.mlr-org.com/reference/fsi.html) function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 6),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\"),\n  callback = clbk(\"mlr3fselect.svm_rfe\"),\n  store_models = TRUE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptimizer$optimize(instance)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(viridisLite)\nlibrary(mlr3misc)\n\ndata = as.data.table(instance$archive)\ndata[, n:= map_int(importance, length)]\n\nggplot(data, aes(x = n, y = classif.auc)) +\n  geom_line(\n    color = viridis(1, begin = 0.5),\n    linewidth = 1) +\n  geom_point(\n    fill = viridis(1, begin = 0.5),\n    shape = 21,\n    size = 3,\n    stroke = 0.5,\n    alpha = 0.8) +\n  xlab(\"Number of Features\") +\n  scale_x_reverse() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Optimization path of the feature selection.](index_files/figure-html/fig-svm-1.png){#fig-svm fig-align='center' width=672}\n:::\n:::\n\n\nFor datasets with a lot of features, it is more efficient to remove several features per iteration. We show an example where 25% of the features are removed in each iteration.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptimizer = fs(\"rfe\", n_features = 1, feature_fraction = 0.75)\n\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 6),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\"),\n  callback = clbk(\"mlr3fselect.svm_rfe\"),\n  store_models = TRUE)\n\noptimizer$optimize(instance)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(viridisLite)\nlibrary(mlr3misc)\n\ndata = as.data.table(instance$archive)\ndata[, n:= map_int(importance, length)]\n\nggplot(data, aes(x = n, y = classif.auc)) +\n  geom_line(\n    color = viridis(1, begin = 0.5),\n    linewidth = 1) +\n  geom_point(\n    fill = viridis(1, begin = 0.5),\n    shape = 21,\n    size = 3,\n    stroke = 0.5,\n    alpha = 0.8) +\n  xlab(\"Number of Features\") +\n  scale_x_reverse() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Optimization path of the feature selection.](index_files/figure-html/fig-svm-2-1.png){#fig-svm-2 fig-align='center' width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}