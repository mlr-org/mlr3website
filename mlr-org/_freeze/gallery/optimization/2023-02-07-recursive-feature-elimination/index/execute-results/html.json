{
  "hash": "479d004830842edc87bf71da3be60364",
  "result": {
    "markdown": "---\ntitle: \"Recursive Feature Elimination on the Sonar Data Set\"\ndescription: |\n  .\ncategories:\n  - feature selection\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-02-07\nbibliography: ../../bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n---\n\n\n\n\n\n\n# Scope\n\nFeature selection is the process of finding an optimal set of features to improve the performance, interpretability and robustness of machine learning algorithms.\nIn this article, we introduce the *Recursive Feature Elimination* algorithm which is a [wrapper method](https://mlr3book.mlr-org.com/feature-selection.html#fs-wrapper) for feature selection.\nWrapper methods iteratively add features to the model that optimize a performance measure.\nAs an example, we will search for the optimal set of features for a [`gradient boosting machine`](https://mlr3extralearners.mlr-org.com/reference/mlr_learners_classif.gbm.html) and [`support vector machine`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.svm.html) on the [`Sonar`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) data set.\nWe assume that you are already familiar with the basic building blocks of the [mlr3 ecosystem](https://mlr-org.com/ecosystem.html).\nIf you are new to feature selection, we recommend reading the [feature selection chapter](https://mlr3book.mlr-org.com/feature-selection.html) of the mlr3book first.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\n# Recursive Feature Elimination\n\nRecursive Feature Elimination (RFE) is a widely used feature selection method in high-dimensional data sets.\nThe idea is to iteratively remove the weakest feature from a model until the desired number of features is reached.\nThe weakest feature is determined by the built-in feature importance method of the model.\nCurrently, RFE works with support vector machines, decision tree algorithms and gradient boosting machines.\nSupported learners are tagged with the `\"importance\"` property.\nFor a full list of supported learners, see the learner page on the [mlr-org website](https://mlr-org.com/learners.html)  and search for `\"importance\"`.\n\n@guyon_gene_2002 developed the RFE algorithm for support vector machines (SVM-RFE) to select informative genes in cancer classification.\nThe importance of the features is given by the weight vector of a linear support vector machine.\nThis method was later extended to other machine learning algorithms.\nThe only requirement is that the models can internally measure the feature importance.\nThe random forest algorithm offers multiple options for measuring feature importance.\nThe commonly used methods are the mean decrease in accuracy (MDA) and the mean decrease in impurity (MDI).\nThe MDA measures the decrease in accuracy for a feature if it was randomly permuted in the out-of-bag sample.\nThe MDI is the total reduction in node impurity when the feature is used for splitting.\nGradient boosting algorithms like [`XGBoost`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html), [`LightGBM`](https://mlr3extralearners.mlr-org.com/reference/mlr_learners_classif.lightgbm.html) and [`GBM`](https://mlr3extralearners.mlr-org.com/reference/mlr_learners_classif.gbm.html) use similar methods to measure the importance of the features.\n\n[mlr3fselect](https://mlr3fselect.mlr-org.com) is the feature selection package of the [mlr3 ecosystem](https://mlr-org.com/ecosystem.html).\nIt implements the [`RFE`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_shadow_variable_search.html) algorithm.\nWe load all packages of the ecosystem with the [`mlr3verse`](https://mlr3verse.mlr-org.com/reference/mlr3verse-package.html) package.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\nWe retrieve the [`RFE`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_shadow_variable_search.html) optimizer with the [`fs()`](https://mlr3fselect.mlr-org.com/reference/fs.html) function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptimizer = fs(\"rfe\", n_features = 1, feature_number = 1)\n```\n:::\n\n\nThe algorithm has multiple control parameters.\nThe optimizer stops when the number of features equals `n_features`.\nThe parameters `feature_number`, `feature_fraction` and `subset_size` determine the number of features that are removed in each iteration.\nThe `feature_number` option removes a fixed number of features in each iteration, whereas `feature_fraction` removes a fraction of the features.\nThe `subset_size` argument is a vector that specifies exactly how many features are removed in each iteration.\nThe parameters are mutually exclusive and the default is `feature_fraction = 0.5`.\nUsually, RFE fits a new model in each iteration and calculates the feature importance again.\nWe can deactivate this behavior by setting `recursive = FALSE`.\n\n# Task\n\nThe objective of the [`Sonar`](https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html) data set is to predict whether a sonar signal bounced off a metal cylinder or a rock.\nThe data set includes 60 numerical features (see @fig-features).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"sonar\")\n```\n:::\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\nlibrary(data.table)\n\ndata = melt(as.data.table(task), id.vars = task$target_names, measure.vars = task$feature_names)\n\nggplot(data, aes(x = value, fill = Class)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(~ variable, ncol = 6, scales = \"free\") +\n  scale_fill_viridis_d(end = 0.8) +\n  theme_minimal() +\n  theme(axis.title.x = element_blank())\n```\n\n::: {.cell-output-display}\n![Distribution of the features in the Sonar data set.](index_files/figure-html/fig-features-1.png){#fig-features fig-align='center' width=1344}\n:::\n:::\n\n\n# Gradient Boosting Machine\n\nWe start with the gradient-boosting learner and set the predict type to `\"prob\"` to obtain class probabilities.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.gbm\", distribution = \"bernoulli\", predict_type = \"prob\")\n```\n:::\n\n\nNow we define the feature selection problem by using the [`fsi()`](https://mlr3fselect.mlr-org.com/reference/fsi.html) function that constructs an [`FSelectInstanceSingleCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html).\nIn addition to the task and learner, we have to select a [`resampling strategy`](https://mlr3.mlr-org.com/reference/Resampling.html) and [`performance measure`](https://mlr3.mlr-org.com/reference/Measure.html) to determine how the performance of a feature subset is evaluated.\nWe pass the `\"none\"` terminator because the `n_features` parameter of the optimizer determines when the feature selection stops.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 6),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\"),\n  store_models = TRUE)\n```\n:::\n\n\nWe are now ready to start the recursive feature elimination.\nTo do this, we simply pass the instance to the `$optimize()` method of the optimizer.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptimizer$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      V1  V10  V11  V12   V13   V14  V15  V16   V17   V18   V19    V2  V20  V21   V22  V23   V24   V25   V26  V27  V28\n1: FALSE TRUE TRUE TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE\n     V29    V3  V30  V31   V32   V33   V34   V35  V36  V37   V38  V39   V4   V40   V41   V42  V43   V44  V45  V46  V47\n1: FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE\n    V48  V49   V5   V50  V51  V52   V53  V54  V55   V56   V57   V58  V59    V6   V60    V7    V8   V9\n1: TRUE TRUE TRUE FALSE TRUE TRUE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE\n                      features classif.auc\n1: V10,V11,V12,V15,V16,V20,...   0.9304303\n```\n:::\n:::\n\n\nThe optimizer returns the best feature set and the corresponding estimated performance.\n\n@fig-optimization-path shows the optimization path of the feature selection.\nWe observe that the performance increases first as the number of features decreases.\nAs soon as informative features are removed, the performance drops again.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(viridisLite)\nlibrary(mlr3misc)\n\ndata = as.data.table(instance$archive)\ndata[, n:= map_int(importance, length)]\n\nggplot(data, aes(x = n, y = classif.auc)) +\n  geom_line(\n    color = viridis(1, begin = 0.5),\n    linewidth = 1) +\n  geom_point(\n    fill = viridis(1, begin = 0.5),\n    shape = 21,\n    size = 3,\n    stroke = 0.5,\n    alpha = 0.8) +\n  xlab(\"Number of Features\") +\n  scale_x_reverse() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Optimization path of the feature selection.](index_files/figure-html/fig-gbm-1.png){#fig-gbm fig-align='center' width=672}\n:::\n:::\n\n\n# Support Vector Machine\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"linear\", predict_type = \"prob\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 6),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\"),\n  callback = clbk(\"mlr3fselect.svm_rfe\"),\n  store_models = TRUE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptimizer$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     V1   V10   V11  V12   V13   V14   V15   V16   V17   V18   V19    V2   V20   V21   V22  V23   V24   V25   V26   V27\n1: TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE\n     V28   V29   V3  V30  V31  V32   V33   V34   V35   V36  V37   V38  V39   V4  V40   V41   V42   V43   V44  V45   V46\n1: FALSE FALSE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE\n     V47  V48  V49    V5  V50   V51  V52   V53   V54   V55   V56   V57   V58   V59    V6   V60   V7   V8   V9\n1: FALSE TRUE TRUE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE\n                    features classif.auc\n1: V1,V12,V23,V3,V30,V31,...   0.9195767\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3viz)\n\nautoplot(instance, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2023-02-07- recursive-feature-elimination-013-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(viridisLite)\nlibrary(mlr3misc)\n\ndata = as.data.table(instance$archive)\ndata[, n:= map_int(importance, length)]\n\nggplot(data, aes(x = n, y = classif.auc)) +\n  geom_line(\n    color = viridis(1, begin = 0.5),\n    linewidth = 1) +\n  geom_point(\n    fill = viridis(1, begin = 0.5),\n    shape = 21,\n    size = 3,\n    stroke = 0.5,\n    alpha = 0.8) +\n  xlab(\"Number of Features\") +\n  scale_x_reverse() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Optimization path of the feature selection.](index_files/figure-html/fig-svm-1.png){#fig-svm fig-align='center' width=672}\n:::\n:::\n\n\nFor datasets with a lot of features, it is more efficient to remove several features per iteration. We show an example where 25% of the features are removed in each iteration.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptimizer = fs(\"rfe\", n_features = 1, feature_fraction = 0.75)\n\ninstance = fsi(\n  task = task,\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 6),\n  measures = msr(\"classif.auc\"),\n  terminator = trm(\"none\"),\n  callback = clbk(\"mlr3fselect.svm_rfe\"),\n  store_models = TRUE)\n\noptimizer$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     V1   V10   V11  V12   V13   V14   V15   V16   V17   V18   V19    V2   V20   V21   V22  V23   V24   V25   V26   V27\n1: TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE\n     V28   V29   V3  V30  V31   V32   V33   V34   V35   V36  V37   V38   V39   V4   V40   V41   V42   V43   V44  V45\n1: FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE TRUE\n     V46   V47   V48  V49    V5  V50   V51  V52   V53   V54   V55   V56   V57   V58   V59    V6   V60    V7   V8   V9\n1: FALSE FALSE FALSE TRUE FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE\n                    features classif.auc\n1: V1,V12,V23,V3,V30,V31,...   0.9162626\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(viridisLite)\nlibrary(mlr3misc)\n\ndata = as.data.table(instance$archive)\ndata[, n:= map_int(importance, length)]\n\nggplot(data, aes(x = n, y = classif.auc)) +\n  geom_line(\n    color = viridis(1, begin = 0.5),\n    linewidth = 1) +\n  geom_point(\n    fill = viridis(1, begin = 0.5),\n    shape = 21,\n    size = 3,\n    stroke = 0.5,\n    alpha = 0.8) +\n  xlab(\"Number of Features\") +\n  scale_x_reverse() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Optimization path of the feature selection.](index_files/figure-html/fig-svm-2-1.png){#fig-svm-2 fig-align='center' width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}