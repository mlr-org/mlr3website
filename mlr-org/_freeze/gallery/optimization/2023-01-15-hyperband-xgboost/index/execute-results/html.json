{
  "hash": "f16821cb826bf3351a72ad01b3a2c54c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hyperband Series - Iterative Training\"\ndescription: |\n  Optimize the hyperparameters of an XGBoost model with Hyperband.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\n  - name: Sebastian Fischer\n    url: https://github.com/sebffischer\ndate: 2023-01-15\nbibliography: bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n      datatable.print.trunc.cols: TRUE\naliases:\n  - ../../../gallery/series/2022-12-01-hyperband-xgboost/index.html\n  - ../../../gallery/series/2023-01-15-hyperband-xgboost/index.html\nimage: cover.png\n---\n\n\n\n\n\n\n\n# Scope\n\nIncreasingly large data sets and search spaces make hyperparameter optimization a time-consuming task.\n*Hyperband* [@li_2018] solves this by approximating the performance of a configuration on a simplified version of the problem such as a small subset of the training data, with just a few training epochs in a neural network, or with only a small number of iterations in a gradient-boosting model.\nAfter starting randomly sampled configurations, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones.\nThis type of optimization is called *multi-fidelity* optimization.\nThe fidelity parameter is part of the search space and controls the tradeoff between the runtime and accuracy of the performance approximation.\nIn this post, we will optimize XGBoost and use the number of boosting iterations as the fidelity parameter.\nThis means Hyperband will allocate more boosting iterations to well-performing configurations.\nThe number of boosting iterations increases the time to train a model and improves the performance until the model is overfitting to the training data.\nIt is therefore a suitable fidelity parameter.\nWe assume that you are already familiar with tuning in the mlr3 ecosystem.\nIf not, you should start with the [book chapter on optimization](https://mlr3book.mlr-org.com/optimization.html) or the [Hyperparameter Optimization on the Palmer Penguins Data Set](/gallery/optimization/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins) post.\nThis is the first part of the Hyperband series.\nThe second part can be found here [Hyperband Series - Data Set Subsampling](/gallery/optimization/2023-01-16-hyperband-subsampling).\n\n# Hyperband\n\nHyperband is an advancement of the Successive Halving algorithm by @jamieson_2016.\nSuccessive Halving is initialized with the number of starting configurations $n$, the proportion of configurations discarded in each stage $\\eta$, and the minimum $r{_{min}}$ and maximum $r{_{max}}$ budget of a single evaluation.\nThe algorithm starts by sampling $n$ random configurations and allocating the minimum budget $r{_{min}}$ to them.\nThe configurations are evaluated and $\\frac{1}{\\eta}$ of the worst-performing configurations are discarded.\nThe remaining configurations are promoted to the next stage and evaluated on a larger budget.\nThis continues until one or more configurations are evaluated on the maximum budget $r{_{max}}$ and the best performing configuration is selected.\nThe number of stages is calculated so that each stage consumes approximately the same budget.\nThis sometimes results in the minimum budget having to be slightly adjusted by the algorithm.\nSuccessive Halving has the disadvantage that is not clear whether we should choose a large $n$ and try many configurations on a small budget or choose a small $n$ and train more configurations on the full budget.\n\nHyperband solves this problem by running Successive Halving with different numbers of stating configurations.\nThe algorithm is initialized with the same parameters as Successive Halving but without $n$.\nEach run of Successive Halving is called a bracket and starts with a different budget $r{_{0}}$.\nA smaller starting budget means that more configurations can be tried out.\nThe most explorative bracket allocated the minimum budget $r{_{min}}$.\nThe next bracket increases the starting budget by a factor of $\\eta$.\nIn each bracket, the starting budget increases further until the last bracket $s = 0$ essentially performs a random search with the full budget $r{_{max}}$.\nThe number of brackets $s{_{max}} + 1$ is calculated with $s{_{max}} = {\\log_\\eta \\frac{r{_{max}} }{r{_{min}}}}$.\nUnder the condition that $r{_{0}}$ increases by $\\eta$ with each bracket, $r{_{min}}$ sometimes has to be adjusted slightly in order not to use more than $r{_{max}}$ resources in the last bracket.\nThe number of configurations in the base stages is calculated so that each bracket uses approximately the same amount of budget.\nThe following table shows a full run of the Hyperband algorithm.\nThe bracket $s = 3$ is the most explorative bracket and $s = 0$ performance a random search on the full budget.\n\n\n\n::: {.cell .column-body-outset .fig-cap-location-top layout-align=\"center\"}\n::: {#fig-schedule .cell-output-display}\n\n```{=html}\n<div class=\"reactable html-widget html-fill-item\" id=\"htmlwidget-f843a680a1fcd62a0dd1\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-f843a680a1fcd62a0dd1\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"i\":[0,1,2,3],\"ni_3\":[8,4,2,1],\"ri_3\":[1,2,4,8],\"ni_2\":[6,3,1,\"NA\"],\"ri_2\":[2,4,8,\"NA\"],\"ni_1\":[4,2,\"NA\",\"NA\"],\"ri_1\":[4,8,\"NA\",\"NA\"],\"ni_0\":[4,\"NA\",\"NA\",\"NA\"],\"ri_0\":[8,\"NA\",\"NA\",\"NA\"]},\"columns\":[{\"id\":\"i\",\"name\":\"i\",\"type\":\"numeric\"},{\"id\":\"ni_3\",\"name\":\"ni_3\",\"type\":\"numeric\",\"header\":{\"name\":\"b\",\"attribs\":{},\"children\":[\"n\",{\"name\":\"sub\",\"attribs\":{},\"children\":[\"i\"]}]},\"style\":[{\"background\":\"#D2EEEA\"},{\"background\":\"#88C3C8\"},{\"background\":\"#498EA4\"},{\"background\":\"#2A5676\"}]},{\"id\":\"ri_3\",\"name\":\"ri_3\",\"type\":\"numeric\",\"header\":{\"name\":\"b\",\"attribs\":{},\"children\":[\"r\",{\"name\":\"sub\",\"attribs\":{},\"children\":[\"i\"]}]},\"style\":[{\"background\":\"#D2EEEA\"},{\"background\":\"#88C3C8\"},{\"background\":\"#498EA4\"},{\"background\":\"#2A5676\"}]},{\"id\":\"ni_2\",\"name\":\"ni_2\",\"type\":\"numeric\",\"header\":{\"name\":\"b\",\"attribs\":{},\"children\":[\"n\",{\"name\":\"sub\",\"attribs\":{},\"children\":[\"i\"]}]},\"style\":[{\"background\":\"#88C3C8\"},{\"background\":\"#498EA4\"},{\"background\":\"#2A5676\"},{\"background\":\"#fff\"}]},{\"id\":\"ri_2\",\"name\":\"ri_2\",\"type\":\"numeric\",\"header\":{\"name\":\"b\",\"attribs\":{},\"children\":[\"r\",{\"name\":\"sub\",\"attribs\":{},\"children\":[\"i\"]}]},\"style\":[{\"background\":\"#88C3C8\"},{\"background\":\"#498EA4\"},{\"background\":\"#2A5676\"},{\"background\":\"#fff\"}]},{\"id\":\"ni_1\",\"name\":\"ni_1\",\"type\":\"numeric\",\"header\":{\"name\":\"b\",\"attribs\":{},\"children\":[\"n\",{\"name\":\"sub\",\"attribs\":{},\"children\":[\"i\"]}]},\"style\":[{\"background\":\"#498EA4\"},{\"background\":\"#2A5676\"},{\"background\":\"#fff\"},{\"background\":\"#fff\"}]},{\"id\":\"ri_1\",\"name\":\"ri_1\",\"type\":\"numeric\",\"header\":{\"name\":\"b\",\"attribs\":{},\"children\":[\"r\",{\"name\":\"sub\",\"attribs\":{},\"children\":[\"i\"]}]},\"style\":[{\"background\":\"#498EA4\"},{\"background\":\"#2A5676\"},{\"background\":\"#fff\"},{\"background\":\"#fff\"}]},{\"id\":\"ni_0\",\"name\":\"ni_0\",\"type\":\"numeric\",\"header\":{\"name\":\"b\",\"attribs\":{},\"children\":[\"n\",{\"name\":\"sub\",\"attribs\":{},\"children\":[\"i\"]}]},\"style\":[{\"background\":\"#2A5676\"},{\"background\":\"#fff\"},{\"background\":\"#fff\"},{\"background\":\"#fff\"}]},{\"id\":\"ri_0\",\"name\":\"ri_0\",\"type\":\"numeric\",\"header\":{\"name\":\"b\",\"attribs\":{},\"children\":[\"r\",{\"name\":\"sub\",\"attribs\":{},\"children\":[\"i\"]}]},\"style\":[{\"background\":\"#2A5676\"},{\"background\":\"#fff\"},{\"background\":\"#fff\"},{\"background\":\"#fff\"}]}],\"columnGroups\":[{\"name\":\"s = 3\",\"columns\":[\"ni_3\",\"ri_3\"]},{\"name\":\"s = 2\",\"columns\":[\"ni_2\",\"ri_2\"]},{\"name\":\"s = 1\",\"columns\":[\"ni_1\",\"ri_1\"]},{\"name\":\"s = 0\",\"columns\":[\"ni_0\",\"ri_0\"]}],\"borderless\":true,\"dataKey\":\"1bca57fc321f54ae1c95649b5688d133\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n\nHyperband schedule with $\\eta = 2$ , $r{_{min}} = 1$ and $r{_{max}} = 8$\n:::\n:::\n\n\n\nThe Hyperband implementation in [mlr3hyperband](https://mlr3hyperband.mlr-org.com) evaluates configurations with the same budget in parallel.\nThis results in all brackets finishing at approximately the same time.\nThe colors in @fig-schedule indicate batches that are evaluated in parallel.\n\n# Hyperparameter Optimization\n\nIn this practical example, we will optimize the hyperparameters of XGBoost on the [`Spam`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html) data set.\nWe begin by loading the [`XGBoost learner.`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.xgboost\")\n```\n:::\n\n\n\nThe next thing we do is define the search space.\nThe `nrounds` parameter controls the number of boosting iterations.\nWe set a range from 16 to 128 boosting iterations.\nThis is used as $r{_{min}}$ and $r{_{max}}$ by the Hyperband algorithm.\nWe need to tag the parameter with `\"budget\"` to identify it as a fidelity parameter.\nFor the other hyperparameters, we take the search space for XGBoost from the @bischl_hyperparameter_2021 article.\nThis search space works for a wide range of data sets.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$param_set$set_values(\n  nrounds           = to_tune(p_int(16, 128, tags = \"budget\")),\n  eta               = to_tune(1e-4, 1, logscale = TRUE),\n  max_depth         = to_tune(1, 20),\n  colsample_bytree  = to_tune(1e-1, 1),\n  colsample_bylevel = to_tune(1e-1, 1),\n  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),\n  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),\n  subsample         = to_tune(1e-1, 1)\n)\n```\n:::\n\n\n\nWe construct the tuning instance.\nWe use the `\"none\"` terminator because Hyperband terminates itself when all brackets are evaluated.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = ti(\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TuningInstanceBatchSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuningBatch:classif.xgboost_on_spam>\n* Search Space:\n                  id    class     lower      upper nlevels\n              <char>   <char>     <num>      <num>   <num>\n1:             alpha ParamDbl -6.907755   6.907755     Inf\n2: colsample_bylevel ParamDbl  0.100000   1.000000     Inf\n3:  colsample_bytree ParamDbl  0.100000   1.000000     Inf\n4:               eta ParamDbl -9.210340   0.000000     Inf\n5:            lambda ParamDbl -6.907755   6.907755     Inf\n6:         max_depth ParamInt  1.000000  20.000000      20\n7:           nrounds ParamInt 16.000000 128.000000     113\n8:         subsample ParamDbl  0.100000   1.000000     Inf\n* Terminator: <TerminatorNone>\n```\n\n\n:::\n:::\n\n\n\nWe load the [`Hyperband tuner`](https://mlr3hyperband.mlr-org.com/reference/mlr_tuners_hyperband.html) and set `eta = 2`.\nHyperband can start from the beginning when the last bracket is evaluated.\nWe control the number of Hyperband runs with the `repetition` argument.\nThe setting `repetition = Inf` is useful when a terminator should stop the optimization.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3hyperband\")\n\ntuner = tnr(\"hyperband\", eta = 2, repetitions = 1)\n```\n:::\n\n\n\nThe Hyperband implementation in [mlr3hyperband](https://mlr3hyperband.mlr-org.com) evaluates configurations with the same budget in parallel.\nThis results in all brackets finishing at approximately the same time.\nYou can think of it as going diagonally through @fig-schedule.\nUsing `eta = 2` and a range from 16 to 128 boosting iterations results in the following schedule.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"reactable html-widget html-fill-item\" id=\"htmlwidget-ba32e0541469b5ed4906\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-ba32e0541469b5ed4906\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"budget\":[16,32,32,64,64,64,128,128,128,128],\"bracket\":[3,3,2,3,2,1,3,2,1,0],\"stage\":[0,1,0,2,1,0,3,2,1,0],\"n\":[8,4,6,2,3,4,1,1,2,4]},\"columns\":[{\"id\":\"budget\",\"name\":\"Boosting Iterations\",\"type\":\"numeric\",\"cell\":[{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 16\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"12.5%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 32\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 32\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]}],\"width\":300},{\"id\":\"bracket\",\"name\":\"Bracket\",\"type\":\"numeric\"},{\"id\":\"stage\",\"name\":\"Stage\",\"type\":\"numeric\"},{\"id\":\"n\",\"name\":\"# Configruations\",\"type\":\"numeric\"}],\"pagination\":false,\"dataKey\":\"5ae618e478b3bdc12e4bf1b0375577e1\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\nNow we are ready to start the tuning.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner$optimize(instance)\n```\n:::\n\n\n\nThe result of a run is the configuration with the best performance.\nThis does not necessarily have to be a configuration evaluated with the highest budget since we can overfit the data with too many boosting iterations.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result[, .(nrounds, eta, max_depth, colsample_bytree, colsample_bylevel, lambda, alpha, subsample)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   nrounds       eta max_depth colsample_bytree colsample_bylevel   lambda     alpha subsample\n     <num>     <num>     <int>            <num>             <num>    <num>     <num>     <num>\n1:     128 -1.985313        14        0.2378133         0.3688266 -3.76799 -6.126724 0.5369803\n```\n\n\n:::\n:::\n\n\n\nThe archive of a Hyperband run has the additional columns `\"bracket\"` and `\"stage\"`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, .(bracket, stage, classif.ce, eta, max_depth, colsample_bytree)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    bracket stage classif.ce        eta max_depth colsample_bytree\n      <int> <num>      <num>      <num>     <int>            <num>\n 1:       3     0 0.08083442 -4.6441571        19        0.4649268\n 2:       3     0 0.07627119 -3.6868263         8        0.6186939\n 3:       3     0 0.07757497 -4.2078328        20        0.9856336\n 4:       3     0 0.05215124 -1.6405650        15        0.6790190\n 5:       3     0 0.06453716 -1.2375660         4        0.8636816\n---                                                               \n31:       0     0 0.08018253 -3.6652572        20        0.8791154\n32:       3     3 0.04823990 -1.6405650        15        0.6790190\n33:       2     2 0.03911343 -1.9853130        14        0.2378133\n34:       1     1 0.06779661 -5.0578038        10        0.3014021\n35:       1     1 0.06714472 -0.2398598        11        0.7152166\n```\n\n\n:::\n:::\n\n\n\n# Conclusion\n\nThe handling of Hyperband in mlr3tuning is very similar to that of other tuners.\nWe only have to select an additional fidelity parameter and tag it with `\"budget\"`.\nWe have tried to keep the runtime of the example low.\nFor your optimization, you should use cross-validation and increase the maximum number of boosting rounds.\nThe @bischl_hyperparameter_2021 search space suggests 5000 boosting rounds.\nCheck out our [next post](/gallery/optimization/2023-01-16-hyperband-subsampling) on Hyperband which uses the size of the training data set as the fidelity parameter.\n\n\n# Session Information\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessioninfo::session_info(info = \"packages\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace        * 2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   crosstalk           1.2.1      2023-11-23 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future              1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2             3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools         * 0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect         1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband     * 0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3measures        1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc            0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning        * 1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox           * 1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   reactable         * 0.4.4      2023-03-12 [1] CRAN (R 4.4.1)\n   reactR              0.6.1      2024-09-14 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n   scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"../../../site_libs/react-18.2.0/react.min.js\"></script>\n<script src=\"../../../site_libs/react-18.2.0/react-dom.min.js\"></script>\n<script src=\"../../../site_libs/reactwidget-2.0.0/react-tools.js\"></script>\n<link href=\"../../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../../../site_libs/reactable-0.4.4/reactable.css\" rel=\"stylesheet\" />\n<script src=\"../../../site_libs/reactable-binding-0.4.4/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}