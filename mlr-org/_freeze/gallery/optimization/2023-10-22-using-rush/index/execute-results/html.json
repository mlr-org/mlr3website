{
  "hash": "810e53ee99d42a63bf2319ff5fa43c67",
  "result": {
    "markdown": "---\ntitle: \"Parallel Computing with Rush\"\ndescription: |\n  Run a centralized parallel computing network with the `rush` package.\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-10-23\nimage: cover.jpg\nbibliography: ../../bibliography.bib\n---\n\n\n\n\n\n\n\n# Scope\n\nParallel computing plays an important role in machine learning, especially as the size of datasets and the computational intensity of algorithms increase.\nThis trend necessitates the distribution of computing tasks across multiple workers.\nIn this context, we present `rush`, our novel framework for parallel and distributed computing in R.\n`rush` enables the parallelization of arbitrary R expressions in a network of workers.\nThe framework implements a queue system and an efficient data store based on [Redis](https://redis.io).\nIt integrates the [future](https://cran.r-project.org/package=future) package to start workers on the local machine or remote machines.\nRush is engineered to impose minimal overhead, with the aim of a few milliseconds per task, and is optimized for both rapid and long-running tasks.\nThe package is designed to be lightweight and easy to use, with a simple interface and minimal dependencies.\nWe integrate `rush` into our optimization packages [bbotk](https://bbotk.mlr-org.com) and [mlr3tuning](https://mlr3tuning.mlr-org.com) but still keep it as a general-purpose package.\n\nWe start the article with an overview of parallel computing in R (@sec-related-work).\nNext we show how to install Redis and the rush package.\nThen we present the network architecture of the package (@sec-rush-network) and how to use the package to parallelize a simple task (@sec-example).\nIn this example, we present the basic functionality of the package.\nNext we present advanced features of the package (@sec-advanced-functionality).\nFinally, we look at the runtime performance of the package (@sec-benchmark).\n\n# Related Work {#sec-related-work}\n\nAs multi-core processors became commonplace in the 2000s, there was a growing need to utilize these resources effectively for computational tasks in R.\nThe first packages to address this need were [snow](https://cran.r-project.org/package=snow) and [multicore](https://cran.r-project.org/package=multicore).\nWith R version 2.14.0 (released in 2011), parallel computing capabilities were integrated into the base R system through the `parallel` package.\nThe functions `parallel::mclapply()` and `parallel::parLapply()` are parallel versions of the `lapply()` function for multicore and cluster computing, respectively.\nBoth functions are widely used in R packages but have some limitations.\nThe R session is blocked until all tasks are finished and it is not possible to retrieve partial results.\nMoreover, load balancing can be an issue when the tasks have different runtimes.\n\nThe landscape further evolved with the release of the `future` package in 2016, which provided a unified and flexible parallel computing interface in R, supporting various backends such as `multisession`, `multicore`, and `callr`.\nThe [future.apply](https://cran.r-project.org/package=future.apply) package implements parallel versions of the `*apply()` family functions, compatible with the `future` backends.\n\nWith the rise of high-performance computing (HPC) clusters, the [batchtools](https://cran.r-project.org/package=batchtools) package was developed to facilitate the execution of long-running tasks on these systems.\nThe communication between the main process and the workers runs completely over the file system.\nA notable feature of the package is the assistance in conducting large-scale computer experiments.\nA more recent development in distributed computing is the [crew](https://cran.r-project.org/package=crew) package.\nThe package is designed for long-running tasks in distributed systems, ranging from traditional high-performance clusters to cloud computing platforms.\nA drawback of both systems is the high overhead per task.\n\nThe [rrq](https://github.com/mrc-ide/rrq) package is a task queue system for R using Redis.\nIt addresses the limitations of the packages by providing a non-blocking interface to parallel computing and keeping the overhead per task low.\nThe package allows non-interacting queues with priority levels within a queue and dependencies among tasks.\nThe package has an advanced error-handling mechanism, heavily influencing the heartbeat mechanism of `rush`.\n\n`rush` aligns closely with `rrq` but differentiates itself with its integration into our optimization packages packages `botk` and `mlr3tuning`.\nThis includes a data structure in Redis that can be efficiently converted to a [`data.table::data.table()`](https://www.rdocumentation.org/packages/data.table/topics/data.table) and a cache mechanism that minimizes the number of read and write operations in the R session.\nMoreover, the start of workers with minimal user configuration is integrated with the `future` package.\nLooking ahead, rush allows a decentralized network architecture devoid of a central controller.\nThis allows the implementation of recently developed optimization algorithms such as Asynchronous Decentralized Bayesian Optimization [@Egele2023].\nFinally, the availability of the package on CRAN is a significant consideration for us.\n\n::: {.callout-note}\n\n## Question\n\n* Maybe we can elaborate more on the differences between `rrq` and `rush`?\n* What could be an advantage for the normal user?\n* The seamless integration with `bbotk` and `mlr3tuning` is only a big advantage for us.\n\n:::\n\n\n# Install {#sec-install}\n\nThere are several options to install Redis depending on your operating system.\nYou can find instructions on how to install Redis on [redis.io](https://redis.io/docs/install/install-redis/).\nThe `rush` package is not yet on CRAN.\nYou can install the development version from GitHub with [pak](https://cran.r-project.org/package=pak).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npak::pkg_install(\"mlr-org/rush\")\n```\n:::\n\n\n`rush` is designed to be light on dependencies.\nIt utilizes a select few packages to establish its functionality:\n\n* `redux` - This package is integral to rush, facilitating robust communication with the Redis server for task queuing and data storage operations.\n* `callr` - Enables rush to maintain a heartbeat process, essential for monitoring the status of remote workers.\n* `future` - Provides the core mechanism for initiating worker processes, whether they are on local or remote machines.\n\n# Rush Network {#sec-rush-network}\n\nThe rush network is orchestrated through a combination of a controller and multiple workers (as illustrated in @fig-rush).\nThe controller initializes the system and starts the workers (@sec-controller).\nThis stage prepares the environment for task processing which includes loading the function to be evaluated and any required packages.\nThe controller pushes tasks to the queue and retrieves their outcomes (@sec-start-workers).\nThe workers pop tasks from the queue, evaluate them, and push the results to the database.\nA task life cycle consists of four states: `\"queued\"`, `\"running\"`, `\"finished\"`, and `\"failed\"`.\nTasks initially enter a `\"queued\"` state, awaiting processing (@sec-push-task).\nThey remain in this state until a worker is available to handle them.\nWhen a worker picks up a task, its status transitions to `\"running\"`.\nThis stage marks the active processing of the task.\nUpon completion, a task's state is updated to `\"finished\"`, and its result is stored in the database (@sec-retrieve-results).\nIn cases where a task encounters an error or issue, its state is marked as `\"failed\"`.\n\n![Centralized rush network](rush.png){#fig-rush width=100%}\n\nThe architecture of a centralized rush network.\n\n## Example {#sec-example}\n\nTo demonstrate the core capabilities of `rush`, we present a simple, practical example utilizing the `mlr3` package for machine learning.\nThe example involves the assessment of a Support Vector Machine (SVM) model performance on the widely-used `spam` dataset.\nWe first define a function, `evaluate_svm()`, that will be dispatched to the workers for execution.\nThis function is designed to accept two parameters: `cost` and `gamma`.\nThese parameters represent the cost and the gamma hyperparameters of the SVM model, respectively.\nInside `evaluate_svm()`, the SVM model is trained on the `spam` dataset using the provided `cost` and `gamma` values.\nAfter training the model, `evaluate_svm()` computes the classification error, which serves as the performance metric.\nThe function concludes by returning the classification error.\nThis returned value is then captured by `rush`, which manages the collection and storage of results from all the workers.\nBy employing `rush`, we can parallelize the evaluation of the SVM model over a grid of `cost` and `gamma` values, significantly accelerating the hyperparameter tuning process.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3)\nlibrary(mlr3learners)\n\ntask = tsk(\"spam\")\nsplits = partition(task)\nlearner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\")\n\neval_svm = function(cost, gamma, ...) {\n  learner$param_set$set_values(cost = cost, gamma = gamma)\n  learner$train(task, row_ids = splits$train)\n  pred = learner$predict(task, row_ids = splits$test)\n  list(ce = pred$score())\n}\n```\n:::\n\n\n## Controller {#sec-controller}\n\nThe `Rush` instance is the controller of the centralized network.\nThe controller starts and stops the workers, pushes tasks to the queue and fetches their results.\nThe controller is initialized with the function `rsh()`.\nThe `network_id` argument is used to identify the controller and workers belonging to the same network.\nThe `config` argument is a list of Redis configuration options used by the `redux` package to connect to the Redis server.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rush)\n\nconfig = redux::redis_config()\nrush = rsh(network_id = \"svm\", config = config)\nrush\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Rush>\n* Running Workers: 0\n* Queued Tasks: 0\n* Queued Priority Tasks: 0\n* Running Tasks: 0\n* Finished Tasks: 0\n* Failed Tasks: 0\n```\n:::\n:::\n\n\n::: {.callout-note}\n\n## Question\n\n* We could create a `rush_plan(config)` function that rush controllers could use to connect to the Redis server.\n\n:::\n\n\n## Start Workers {#sec-start-workers}\n\nNow we are ready to start the workers.\nThe `RushWorker` class represents a worker in the network.\nThe class inherits from the `Rush` controller class but adds methods to pop tasks from the queue and push results to the database.\nOn the worker runs a loop that processes the tasks from the queue.\nThe default worker loop is `fun_loop`.\nThis function fetches a task from the queue, evaluates the user-defined function `fun`, pushes the results back to the database and waits for the next task.\nUsually, we do not need to define a custom worker loop and pass the function `fun`.\n\nWorkers can be started on the local machine or a remote machine.\nA local worker runs on the same machine as the controller.\nA remote worker runs on a different machine.\nWe distinguish between local and remote workers because the mechanism to kill and monitor a remote worker is different.\n\nThe `$start_workers()` method starts the workers with the `future` package.\nWe pass the `host = \"local\"` argument to mark the workers as local.\nOptionally, we can pass a `n_workers` argument to specify the number of workers.\nIf we do not pass a `n_workers` argument, the number of workers is set to available future workers.\nIf `fun` depends on global variables, we can pass them to the `globals` argument.\nOur `eval_svm()` function depends on the `learner`, `task` and `splits` objects.\nPackages that are needed by `fun` can be passed to the `packages` argument.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multisession\")\n\nrush$start_workers(\n  worker_loop = fun_loop,\n  n_workers = 2,\n  globals = c(\"learner\", \"task\", \"splits\"),\n  packages = c(\"mlr3\", \"mlr3learners\", \"e1071\"),\n  host = \"local\",\n  fun = eval_svm)\n\nrush\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Rush>\n* Running Workers: 2\n* Queued Tasks: 0\n* Queued Priority Tasks: 0\n* Running Tasks: 0\n* Finished Tasks: 0\n* Failed Tasks: 0\n```\n:::\n:::\n\n\n::: {.callout-note}\n\n## Question\n * We could simplify `$start_workers()` by hiding the `worker_loop` argument.\nBut `bbotk` and `mlr3tuning` use a custom worker loop.\n * We could replace `future` and just use `parallely`.\n * We could replace the `host` argument with something like `Sys.info()[[\"nodename\"]]`.\n\n:::\n\nWe get more information about the workers with `$worker_info`.\nThe `worker_id` identifies the worker.\nThe `pid` is the process id of the worker process.\nThe `heartbeat` is the process id of the heartbeat process (see @sec-heartbeat).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$worker_info\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                              worker_id    pid   host heartbeat\n                                 <char>  <int> <char>    <char>\n1: c2e6cb55-8a89-4052-a9c0-20018d0ececb 267458  local        NA\n2: d85e7b74-e041-4ad5-a114-6ff6268f4c56 267455  local        NA\n```\n:::\n:::\n\n\nOn a remote machine, we need to start the workers manually or use the future `cluster` backend.\nSee @sec-start-script for more information on how to start workers manually.\n\nA worker can have four states: `\"running\"`, `\"terminated\"`, `\"killed\"` (@sec-stop-workers) and `\"lost\"` (@sec-error-handling).\n\n## Push Task {#sec-push-task}\n\nThe `$push_tasks()` method pushes tasks to the queue.\nThe method takes a list of tasks as input.\nEach task is a list of parameters.\nWe push 2 tasks to the queue.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkeys = rush$push_tasks(list(\n  list(cost = 1e-4, gamma = 1e-4),\n  list(cost = 1e-3, gamma = 1e-3)))\n```\n:::\n\n\nThe keys of the pushed tasks are returned.\nPushing a task takes around 0.5 milliseconds (see @sec-benchmark).\nPushing a task is non-blocking i.e. the method returns immediately.\nWe can wait for a task to finish with the `$await_tasks()` method.\nThe method blocks until all tasks are processed by the workers.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$await_tasks(keys)\n```\n:::\n\n\n## Retrieve Results {#sec-retrieve-results}\n\nThe `$fetch_finished_tasks()` method retrieves the results of finished tasks.\nThe method returns a `data.table` with additional meta information.\nThe `worker_id` of the worker that evaluated the task and the `pid` of the worker process are stored in the table.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$fetch_finished_tasks()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    cost gamma    pid                            worker_id        ce    state                                 keys\n   <num> <num>  <int>                               <char>     <num>   <char>                               <char>\n1: 1e-03 1e-03 267455 d85e7b74-e041-4ad5-a114-6ff6268f4c56 0.3939394 finished 334b654f-3faa-4120-8025-f51673177d8f\n2: 1e-04 1e-04 267458 c2e6cb55-8a89-4052-a9c0-20018d0ececb 0.3939394 finished e2c55360-5ddc-4549-a57f-d872a91f6b22\n```\n:::\n:::\n\n\nThere are multiple `$fetch_*()` methods available for retrieving data from the Redis database.\nA matching method is defined for each task state e.g. `$fetch_running_tasks()` and `$fetch_finished_tasks()`.\nIf only the result of the function evaluation is needed, `$fetch_results()` and `$fetch_latest_results()` are faster than `$fetch_finished_tasks()`.\nThe methods `$fetch_results()` and `$fetch_finished_tasks()` cache the already queried data.\n\nThe `$block_*()` variants wait until a new result is available.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$block_latest_results()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          ce\n       <num>\n1: 0.3939394\n2: 0.3939394\n```\n:::\n:::\n\n\nWe push another task to the queue and block until the result is available.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$push_tasks(list(list(cost = 1e-2, gamma = 1e-2)))\nrush$block_latest_results()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          ce\n       <num>\n1: 0.3320158\n```\n:::\n:::\n\n\n## Caching {#sec-caching}\n\nThe results of the `$fetch_results()` and `$fetch_finished_tasks()` are cached.\nThe cache is a `data.table` that is stored in the controller.\nNew results are rbined to the `data.table`.\nThis takes constant time independent of the number of results in the cache.\nFor our example around 4 milliseconds (see @sec-benchmark).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$fetch_results()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          ce                                 keys\n       <num>                               <char>\n1: 0.3939394 334b654f-3faa-4120-8025-f51673177d8f\n2: 0.3939394 e2c55360-5ddc-4549-a57f-d872a91f6b22\n3: 0.3320158 4fa1d894-f7df-4d71-ab03-d96afe6d8abb\n```\n:::\n:::\n\n\n## Stop Workers {#sec-stop-workers}\n\nLocal and remote workers can be terminated with the `$stop_workers(type = \"terminate\")` method.\nThe workers evaluate the currently running task and then terminate.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$stop_workers(type = \"terminate\")\n```\n:::\n\n\n\n\nWhen we wait for a few seconds, we see that the workers are terminated.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$worker_states\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        state                            worker_id\n       <char>                               <char>\n1: terminated c2e6cb55-8a89-4052-a9c0-20018d0ececb\n2: terminated d85e7b74-e041-4ad5-a114-6ff6268f4c56\n```\n:::\n:::\n\n\nThe option `type = \"kill\"` stops the workers immediately.\nKilling a local worker is done with the `tools::pskill()` function.\nRemote workers are killed by pushing a kill signal to the heartbeat process.\nWithout a heartbeat process a remote worker cannot be killed (see @sec-heartbeat).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$stop_workers(type = \"kill\")\n```\n:::\n\n\n\n# Advanced Functionality {#sec-advanced-functionality}\n\n## Error Handling {#sec-error-handling}\n\nWhen evaluating tasks in a distributed system, many things can go wrong.\nSimple R errors in the worker loop are caught and written to the archive.\nThe task is marked as `\"failed\"`.\nIf the connection to a worker is lost, it looks like a task is `\"running\"` forever.\nThe methods `$detect_lost_workers()` and `$detect_lost_tasks()` detect lost workers.\nRunning these methods periodically adds a small overhead.\nRemote workers are monitored with the heartbeat mechanism (see @sec-heartbeat).\nOn local workers, we check the running processes.\nThis can be very slow on Windows.\n\n\n::: {.callout-note}\n\n# Question\n\n* Use `mlr3misc::encapsulate()`?\nAllows using `callr` for encapsulation.\nWorkers would not get lost.\nBut we would save the log messages twice since `lgr` messages are directly written to the database by `rush`.\nWe have encapsulation already in mlr3.\n* Restarting a lost worker is not possible with `future`.\n\n:::\n\n## Logging {#sec-logging}\n\nThe worker logs all messages written with the `lgr` package to the database.\nThe `lgr_thresholds` argument pf `$start_workers()` defines the logging level for each logger e.g. `c(rush = \"debug\")`.\nSaving log messages adds a small overhead but is useful for debugging.\nBy default, no log messages are stored.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$read_log()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNull data.table (0 rows and 0 cols)\n```\n:::\n:::\n\n\n## Start Script {#sec-start-script}\n\nWe are not limited to start workers with the `future` package.\nA work can be started manually with a script on a remote machine.\nThe only requirement is that the machine has access to the Redis server and can run R scripts.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$create_worker_script()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nINFO  [12:07:25.194] [rush] Start worker with:\nINFO  [12:07:25.223] [rush] Rscript -e 'rush::start_worker(svm, url = \"redis://127.0.0.1:6379\")'\nINFO  [12:07:25.234] [rush] See ?rush::start_worker for more details.\n```\n:::\n:::\n\n\nThe `$create_worker_script()` takes also a `globals` and `packages` argument.\nThe globals are serialized and passed to the worker via Redis.\nThe packages must be installed on the remote machine but they are loaded automatically.\n\n## Reproducibility {#sec-reproducibility}\n\n::: {.callout-note}\n\n# Question\n\n* How to pass a seed to the workers?\nJust bind `.Random.seed` to the tasks?\n* The results of hyperband are not reproducible. See [Optuna](https://optuna.readthedocs.io/en/stable/faq.html#how-can-i-obtain-reproducible-optimization-results).\n:::\n\n## Queues\n\nRush uses a shared queue and a queue for each worker.\nThe shared queue is used to push tasks to the workers.\nThe first worker that pops a task from the shared queue evaluates the task.\nUntil now we only used the shared queue.\nThe worker queues are used to push tasks to specific workers.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nworker_id = rush$worker_ids[1]\nrush$push_priority_tasks(list(list(cost = 1e-1, gamma = 1e-1)), priority = worker_id)\n```\n:::\n\n\n## Heartbeats {#sec-heartbeat}\n\nThe heartbeat is a mechanism to monitor the status of remote workers in distributed computing systems.\nThe mechanism consists of a heartbeat key with a set [expiration timeout](https://redis.io/commands/expire/) and a dedicated heartbeat process that refreshes the timeout periodically.\nThe heartbeat process is started with `callr` and is linked to main process of the worker.\nIn the event of a worker's failure, the associated heartbeat process also ceases to function, thus halting the renewal of the timeout.\nThe absence of the heartbeat key acts as an indicator to the controller that the worker is no longer operational.\nConsequently, the controller updates the worker's status to `\"lost\"`.\n\nHeartbeats are initiated upon worker startup by specifying the `heartbeat_period` and `heartbeat_expire` parameters.\nThe `heartbeat_period` defines the frequency at which the heartbeat process will update the timeout.\nThe `heartbeat_expire` sets the duration, in seconds, before the heartbeat key expires.\nThe expiration time should be set to a value greater than the heartbeat period to ensure that the heartbeat process has sufficient time to refresh the timeout.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$start_workers(\n  worker_loop = fun_loop,\n  n_workers = 2,\n  globals = c(\"learner\", \"task\", \"splits\"),\n  packages = c(\"mlr3\", \"mlr3learners\", \"e1071\"),\n  host = \"remote\",\n  heartbeat_period = 1,\n  heartbeat_expire = 3,\n  fun = eval_svm)\n```\n:::\n\n\nThe heartbeat process is also the only way to kill a remote worker.\nThe `$stop_workers(type = \"kill\")` method pushes a kill signal to the heartbeat process.\nThe heartbeat process terminates the main process of the worker.\n\n\n## Rush Data Store {#sec-data-store}\n\nRush writes a task and its result and additional meta information into a Redis [hash](https://redis.io/docs/data-types/hashes/).\n\n```\nkey : xs | ys | extra | state\n```\n\nThe key of the hash identifies the task in `rush`.\nThe fields are written by different methods, e.g. `$push_result()` writes `ys` when the result is available.\nThe value of a field is a serialized list e.g. unserializing `xs` gives `list(x1 = 1, x2 = 2)`.\nThis data structure allows quickly converting a hash into a row and joining multiple hashes into a table.\nFor example, three hashes from the above example are converted to the following table.\n\n\n| key | x1 | x2 | y | timestamp | state    |\n|-----|----|----|---|-----------|----------|\n| 1.. |  3 |  4 | 7 |  12:04:11 | finished |\n| 2.. |  1 |  4 | 5 |  12:04:12 | finished |\n| 3.. |  1 |  1 | 2 |  12:04:13 | finished |\n\n\nNotice that a value of a field can store multiple columns of the table.\nThe methods `$push_tasks()` and `$push_results()` write into multiple hashes.\nOne for each task or result.\nFor example, `$push_tasks(xss = list(list(x1 = 1, x2 = 2), list(x1 = 2, x2 = 2))` writes `xs` in two hashes.\n\n# Benchmark {#sec-benchmark}\n\nWe benchmark the performance of `rush` with the [microbenchmark](https://cran.r-project.org/package=microbenchmark) package with 100 repetitions.\nThe benchmark was run on cluster that is not optimized for single-core performance.\nThe runtime might be faster on a local machine.\n\nWe pushed tasks to the queue and measured the time (@fig-benchmark-push).\nThe number of tasks was increased from 1 to 10,000.\nOne task consisted of two numbers.\nThe runtime increased sublinearly with the number of tasks.\nWhen pushing one task, overhead per task was around 0.5 millisecond.\nWhen pushing 10,000 tasks, the overhead per task was around 0.05 millisecond.\nWe use the pipeline mechanism of Redis to push multiple tasks at once what explains the sublinear increase.\nIt is beneficial to push multiple tasks at once.\nBut the overhead per task is already low for one task.\nPoping a task from the queue takes around 0.5 millisecond independent of the number of tasks in the queue.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Runtime Performance, measured in milliseconds, of pushing a task. Both axes are on a logarithmic scale.](index_files/figure-html/fig-benchmark-push-1.png){#fig-benchmark-push fig-align='center' width=672}\n:::\n:::\n\n\nWe fetched the results from the database and measured the time (@fig-benchmark-fetch).\nThis includes fetching the results from the database and converting them to a `data.table`.\nThe number of results was increased from 1 to 10,000.\nThe runtime increased sublinearly with the number of results.\nWhen fetching one result, overhead per result was around 1 millisecond.\nWhen fetching 10,000 results, the overhead per result was around 0.034 millisecond.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Runtime Performance, measured in milliseconds, of fetching the results. Both axes are on a logarithmic scale.](index_files/figure-html/fig-benchmark-fetch-1.png){#fig-benchmark-fetch fig-align='center' width=672}\n:::\n:::\n\n\n`rush` caches already fetched results.\nFetching 100 new tasks and adding them to the cached `data.table` takes around 4 milliseconds independent of the number of results in the cache.\n\n# Optimization {#sec-bbotk-mlr3tuning}\n\n## Install\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npak::pkg_install(c(\"mlr-org/bbotk@rush\", \"mlr-org/mlr3tuning@rush\"))\n```\n:::\n\n\n## Benchmark\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Runtime of a random search in seconds on 100 hyperparameter configuration with 3-fold cross-validation on 3 workers depending on batch size and chunk size for different training times. The first three runs use a constant training time of 10 ms, 100 ms and 1 second. The last three runs use a homogenous training time ranging from 10 ms to 10 seconds. Rush does not have a batch size and chunk size parameter.](index_files/figure-html/fig-sleep-benchmark-1-1.png){#fig-sleep-benchmark-1 fig-align='center' width=672}\n:::\n:::\n\n\n* A batch size of 100 evaluates all 100 configurations with `benchmark()` at once.\n* A chunk size of 100 sends 100 resampling iterations to a worker at once.\n* A batch size of 10 results in 10 tuning iterations and 10 `benchmark()` calls.\n* The tuned learner takes the data but only sleeps for the specified time.\n\n* The runtimes include the start of the workers.\n* Starting a worker with rush is slower than with `future` because the globals are a little bit larger and we initialize the `RushWorker` on the worker first. The globals are smaller with `future` and only `workhorse()` is called.\n\n* This results in smaller runtimes with `future` for the HPCs with constant training time (row 1-3).\n* Interestingly, a chunk size of 10 is as fast as a chunk size of 100.\n* When the training time is heterogeneous, `rush` is a little bit faster than `future`.\n\n* When the tuner has multiple iterations, `rush` is faster than `future` because we do not start the workers again with `benchmark()` in each iteration (row 5). This becomes very costly with many workers (see next plot).\n* The default chunk size of 1 is always slower than `rush` (row 4 and 6)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Runtime of a random search in seconds on 1000 hyperparameter configuration with 3-fold cross-validation on 30 workers depending on batch size and chunk size for different training times. The first three runs use a constant training time of 10 ms, 100 ms and 1 second. The last three runs use a homogenous training time ranging from 10 ms to 10 seconds. Rush does not have a batch size and chunk size parameter.](index_files/figure-html/fig-sleep-benchmark-2-1.png){#fig-sleep-benchmark-2 fig-align='center' width=672}\n:::\n:::\n\n\n* `rush` is always faster than `future` when we start 30 workers. `future` starts the workers sequentially, while `rush` initializes the `RushWorker` in parallel. This could be further improved in `rush` when we serialize over Redis instead of `future`.\n* Starting 30 workers in each tuning iteration is very costly with `future` (row 5).\n\n## Implementation\n\nThe current implementation in `mlr3tunig`.\nThe `ti()` function returns a `TuningInstanceRushSingleCrit` instead of a `TuningInstanceSingleCrit` when a `rush` controller is passed.\nThe new instance has a new method `$start_workers()` to start the workers.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rush)\nlibrary(mlr3tuning)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: paradox\n```\n:::\n\n```{.r .cell-code}\nrush = rsh(network_id = \"mlr3tuning\")\n\ninstance = ti(\n  task = tsk(\"pima\"),\n  learner = lrn(\"classif.rpart\", cp = to_tune(0.01, 0.1), minsplit = to_tune(1, 100)),\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 100),\n  rush = rush\n)\n\nfuture::plan(\"multisession\", workers = 2L)\n\ninstance$start_workers(await_workers = TRUE)\ninstance$rush\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Rush>\n* Running Workers: 2\n* Queued Tasks: 0\n* Queued Priority Tasks: 0\n* Running Tasks: 0\n* Finished Tasks: 0\n* Failed Tasks: 0\n```\n:::\n:::\n\n\nThe tuner classes of `mlr3tuning` are extended to support rush.\nJust use the tuners as normal.\nInternally a slightly different algorithm is used when parallelizing with `rush`.\nThe optimization is started as always with the `$optimize()` method.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner = tnr(\"random_search\")\n\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           cp minsplit learner_param_vals  x_domain classif.ce\n        <num>    <int>             <list>    <list>      <num>\n1: 0.01574264       49          <list[3]> <list[2]>  0.2369792\n```\n:::\n:::\n\n\nMore info in the tuning rush post.\n\n**New draft**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rush)\nlibrary(mlr3tuning)\n\n# provides rush controller with configuration\nrush_plan(config = redux::redis_config())\n\n# creates rush controller internally\n# accessible with instance$rush\ninstance = ti(\n  task = tsk(\"pima\"),\n  learner = lrn(\"classif.rpart\", cp = to_tune(0.01, 0.1), minsplit = to_tune(1, 100)),\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 100)\n)\n\ntuner = tnr(\"random_search\")\n\n# starts workers before optimization\n# when workers were started manually, use them instead of starting new ones\n# allows in next releases to send a specific objective to worker e.g. async mbo\ntuner$optimize(instance)\n```\n:::\n\n\n* `rush_plan` can be used by other functions\n* `tune()` and `auto_tuner()` work better with this draft.\n\n## Discussion\n\n**Advantages**\n\n* non-blocking allows new algorithms like async hyperband\n* no batch means no waiting and idling cores when heterogeneous runtimes are in a batch\n* less serialization overhead when we have multiple tuning iterations because we do not start the worker again with `benchmark()` in each iteration\n* no configuration of batch size and chunk size which is error prone or is ignored by most users\n* main process gets not overloaded with many fast-fitting models (future just got stuck)\n* Terminators works more precise without batches\n* Hotstarting could work better because we do not need to serialize the models\n\n**Disadvantages**\n\n* installing Redis is more complicated than using future\n* the resampling of a hyperparameter configuration is not split into individual resampling iterations\n  - a worker evaluates the whole resampling with `resample()`\n  - when we split, the 1:1 mapping between the Redis data store and the archive does not work\n  - the tuner operates directly on the data store/ archive which produces almost no overhead\n  - when we split, a process would have to summarise the resamplings which adds overhead\n* external packages like gensa are slower with Redis\n  - external packages propose only a single hyperparameter configuration and the resampling cannot be split\n  - we could implement a rush powered `benchmark()` function that does not stop the workers between the tuning iterations\n* not always reproducible (optuna suggests running in sequential mode)\n\n# Conclusion {#sec-conclusion}\n\n\n#### Glossary\n\n* Parallel computing: The use of multiple processing elements simultaneously for solving a computational problem.\n* Distributed computing: Utilizing multiple computers in a network to solve a computational problem.\n* Worker: A process that performs tasks as part of a larger computation.\n* Computing task: A discrete portion of a larger computational problem, designed to be executed by a worker.\n* Redis: An open-source, in-memory data store, used as a database, cache, and message broker.\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}