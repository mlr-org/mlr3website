{
  "hash": "6f492065917dca574fb19b25d2da8dc8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Parallel Computing with Rush\"\ndescription: |\n  Run a centralized parallel computing network with the `rush` package.\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-10-23\nimage: cover.jpg\nbibliography: ../../bibliography.bib\n---\n\n\n\n\n\n\n# Scope\n\nParallel computing plays an important role in machine learning, especially as the size of datasets and the computational intensity of algorithms increase.\nThis trend necessitates the distribution of computing tasks across multiple workers.\nIn this context, we present `rush`, our novel framework for parallel and distributed computing in R.\n`rush` enables the parallelization of arbitrary R expressions in a network of workers.\nThe framework implements a queue system and an efficient data store based on [Redis](https://redis.io).\nIt integrates the [future](https://cran.r-project.org/package=future) package to start workers on the local machine or remote machines.\nRush is engineered to impose minimal overhead, with the aim of a few milliseconds per task, and is optimized for both rapid and long-running tasks.\nThe package is designed to be lightweight and easy to use, with a simple interface and minimal dependencies.\nWe integrate `rush` into our optimization packages [bbotk](https://bbotk.mlr-org.com) and [mlr3tuning](https://mlr3tuning.mlr-org.com) but still keep it as a general-purpose package.\n\nWe start the article with an overview of parallel computing in R (@sec-related-work).\nNext, we show how to install Redis and the rush package.\nThen we present the network architecture of the package (@sec-rush-network) and how to use the package to parallelize a simple task (@sec-example).\nIn this example, we present the basic functionality of the package.\nNext, we present advanced features of the package (@sec-advanced-functionality).\nFinally, we look at the runtime performance of the package (@sec-benchmark).\n\n# Related Work {#sec-related-work}\n\nAs multi-core processors became commonplace in the 2000s, there was a growing need to utilize these resources effectively for computational tasks in R.\nThe first packages to address this need were [snow](https://cran.r-project.org/package=snow) and [multicore](https://cran.r-project.org/package=multicore).\nWith R version 2.14.0 (released in 2011), parallel computing capabilities were integrated into the base R system through the `parallel` package.\nThe functions `parallel::mclapply()` and `parallel::parLapply()` are parallel versions of the `lapply()` function for multicore and cluster computing, respectively.\nBoth functions are widely used in R packages but have some limitations.\nThe R session is blocked until all tasks are finished and it is not possible to retrieve partial results.\nMoreover, load balancing can be an issue when the tasks have different runtimes.\n\nThe landscape further evolved with the release of the `future` package in 2016, which provided a unified and flexible parallel computing interface in R, supporting various backends such as `multisession`, `multicore`, and `callr`.\nThe [future.apply](https://cran.r-project.org/package=future.apply) package implements parallel versions of the `*apply()` family functions, compatible with the `future` backends.\n\nWith the rise of high-performance computing (HPC) clusters, the [batchtools](https://cran.r-project.org/package=batchtools) package was developed to facilitate the execution of long-running tasks on these systems.\nThe communication between the main process and the workers runs completely over the file system.\nA notable feature of the package is the assistance in conducting large-scale computer experiments.\nA more recent development in distributed computing is the [crew](https://cran.r-project.org/package=crew) package.\nThe package is designed for long-running tasks in distributed systems, ranging from traditional high-performance clusters to cloud computing platforms.\nA drawback of both systems is the high overhead per task.\n\nThe [rrq](https://github.com/mrc-ide/rrq) package is a task queue system for R using Redis.\nIt addresses the limitations of the packages by providing a non-blocking interface to parallel computing and keeping the overhead per task low.\nThe package allows non-interacting queues with priority levels within a queue and dependencies among tasks.\nThe package has an advanced error-handling mechanism, heavily influencing the heartbeat mechanism of `rush`.\n\n`rush` aligns closely with `rrq` but differentiates itself with its integration into our optimization packages packages `botk` and `mlr3tuning`.\nThis includes a data structure in Redis that can be efficiently converted to a [`data.table::data.table()`](https://www.rdocumentation.org/packages/data.table/topics/data.table) and a cache mechanism that minimizes the number of read and write operations in the R session.\nMoreover, the start of workers with minimal user configuration is integrated with the [processx](https://cran.r-project.org/package=processx) package.\nLooking ahead, rush allows a decentralized network architecture devoid of a central controller.\nThis allows the implementation of recently developed optimization algorithms such as Asynchronous Decentralized Bayesian Optimization [@Egele2023].\n\n# Install {#sec-install}\n\nThere are several options to install Redis depending on your operating system.\nYou can find instructions on how to install Redis on [redis.io](https://redis.io/docs/install/install-redis/).\nThe `rush` package is not yet on CRAN.\nYou can install the development version from GitHub with [pak](https://cran.r-project.org/package=pak).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npak::pkg_install(\"mlr-org/rush\")\n```\n:::\n\n\n`rush` is designed to be light on dependencies.\nIt utilizes a select few packages to establish its functionality:\n\n* `redux` - This package is integral to rush, facilitating robust communication with the Redis server for task queuing and data storage operations.\n* `processx` - Provides the core mechanism for initiating worker processes.\n* `callr` - Enables rush to maintain a heartbeat process, essential for monitoring the status of remote workers.\n* `parallel` - Adds L'Ecuyer-CMRG seeds to tasks, ensuring reproducibility of results.\n* `checkmate` and `mlr3misc` - Used for argument validation and miscellaneous utility functions.\n* `data.table` - Efficient data manipulation.\n* `lgr` and `jsonlite` - Logging to the Redis database.\n\n# Rush Network {#sec-rush-network}\n\nThe rush network is orchestrated through a combination of a controller and multiple workers (as illustrated in @fig-rush).\nThe controller initializes the system and starts the workers (@sec-controller).\nThis stage prepares the environment for task processing which includes loading the function to be evaluated and any required packages.\nThe controller pushes tasks to the queue and retrieves their outcomes (@sec-start-workers).\nThe workers pop tasks from the queue, evaluate them, and push the results to the database.\nA task life cycle consists of four states: `\"queued\"`, `\"running\"`, `\"finished\"`, and `\"failed\"`.\nTasks initially enter a `\"queued\"` state, awaiting processing (@sec-push-task).\nThey remain in this state until a worker is available to handle them.\nWhen a worker picks up a task, its status transitions to `\"running\"`.\nThis stage marks the active processing of the task.\nUpon completion, a task's state is updated to `\"finished\"`, and its result is stored in the database (@sec-retrieve-results).\nIn cases where a task encounters an error or issue, its state is marked as `\"failed\"`.\n\n![The architecture of a centralized rush network.](rush_centralized.png){#fig-rush width=100%}\n\n## Example {#sec-example}\n\nTo demonstrate the core capabilities of `rush`, we present a simple, practical example utilizing the `mlr3` package for machine learning.\nThe example involves the assessment of a Support Vector Machine (SVM) model performance on the widely-used `spam` dataset.\nWe first define a function, `evaluate_svm()`, that will be dispatched to the workers for execution.\nThis function is designed to accept two parameters: `cost` and `gamma`.\nThese parameters represent the cost and the gamma hyperparameters of the SVM model, respectively.\nInside `evaluate_svm()`, the SVM model is trained on the `spam` dataset using the provided `cost` and `gamma` values.\nAfter training the model, `evaluate_svm()` computes the classification error, which serves as the performance metric.\nThe function concludes by returning the classification error.\nThis returned value is then captured by `rush`, which manages the collection and storage of results from all the workers.\nBy employing `rush`, we can parallelize the evaluation of the SVM model over a grid of `cost` and `gamma` values, significantly accelerating the hyperparameter tuning process.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3)\nlibrary(mlr3learners)\n\ntask = tsk(\"spam\")\nsplits = partition(task)\nlearner = lrn(\"classif.svm\", type = \"C-classification\", kernel = \"radial\")\n\neval_svm = function(cost, gamma, ...) {\n  learner$param_set$set_values(cost = cost, gamma = gamma)\n  learner$train(task, row_ids = splits$train)\n  pred = learner$predict(task, row_ids = splits$test)\n  list(ce = pred$score())\n}\n```\n:::\n\n\n## Controller {#sec-controller}\n\nThe `Rush` instance is the controller of the centralized network.\nThe controller starts and stops the workers, pushes tasks to the queue and fetches their results.\nThe controller is initialized with the function `rsh()`.\nThe `network_id` argument is used to identify the controller and workers belonging to the same network.\nThe `config` argument is a list of Redis configuration options used by the `redux` package to connect to the Redis server.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rush)\n\nconfig = redux::redis_config()\nrush = rsh(network_id = \"svm\", config = config)\nrush\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Rush>\n* Running Workers: 0\n* Queued Tasks: 0\n* Queued Priority Tasks: 0\n* Running Tasks: 0\n* Finished Tasks: 0\n* Failed Tasks: 0\n```\n\n\n:::\n:::\n\n\nInstead of passing the config to `rsh()`, we can use the `rush_plan()` function to set the config globally.\nThe function stores the Redis configuration options and the maximum number of workers.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfig = redux::redis_config()\nrush_plan(n_workers = 2, config = config)\n\nrush = rsh(network_id = \"svm\")\n```\n:::\n\n\nPackages that use `rush` internally can access the config provided by the user to create `Rush` instances.\n\n## Start Workers {#sec-start-workers}\n\nNow we are ready to start the workers.\nThe `RushWorker` class represents a worker in the network.\nThe class inherits from the `Rush` controller class but adds methods to pop tasks from the queue and push results to the database.\nThe worker runs a loop that processes the tasks from the queue.\nThe default worker loop is `worker_loop_default`.\nThis function fetches a task from the queue, evaluates the user-defined function `fun`, pushes the results back to the database and waits for the next task.\nUsually, we do not need to define a custom worker loop and pass the function `fun`.\n\nWorkers can be started on the local machine or a remote machine.\nA local worker runs on the same machine as the controller.\nA remote worker runs on a different machine.\nWe distinguish between local and remote workers because the mechanism to kill and monitor a remote worker is different.\n\nThe `$start_workers()` method starts the workers with the `processx` package.\nWe pass the `n_workers` argument to specify the number of workers.\nIf `fun` depends on global variables, we can pass them to the `globals` argument.\nOur `eval_svm()` function depends on the `learner`, `task` and `splits` objects.\nPackages that are needed by `fun` can be passed to the `packages` argument.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$start_workers(\n  fun = eval_svm,\n  n_workers = 2,\n  globals = c(\"learner\", \"task\", \"splits\"),\n  packages = c(\"mlr3\", \"mlr3learners\", \"e1071\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [10:02:57.346] [rush] Starting 2 worker(s)\n```\n\n\n:::\n\n```{.r .cell-code}\nrush\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Rush>\n* Running Workers: 2\n* Queued Tasks: 0\n* Queued Priority Tasks: 0\n* Running Tasks: 0\n* Finished Tasks: 0\n* Failed Tasks: 0\n```\n\n\n:::\n:::\n\n\nWe get more information about the workers with `$worker_info`.\nThe `worker_id` identifies the worker.\nThe `pid` is the process id of the worker process.\nThe `heartbeat` is the process id of the heartbeat process (see @sec-heartbeat).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$worker_info\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   worker_id    pid   host hostname heartbeat\n      <char>  <int> <char>   <char>    <char>\n1:  2e1d9... 308149  local Think...        NA\n2:  399ab... 308138  local Think...        NA\n```\n\n\n:::\n:::\n\n\nOn a remote machine, we need to start the workers manually.\nSee @sec-start-script for more information on how to start workers manually.\n\nA worker can have four states: `\"running\"`, `\"terminated\"`, `\"killed\"` (@sec-stop-workers) and `\"lost\"` (@sec-error-handling).\n\n## Push Task {#sec-push-task}\n\nThe `$push_tasks()` method pushes tasks to the queue.\nThe method takes a list of tasks as input.\nEach task is a list of parameters.\nWe push 2 tasks to the queue.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkeys = rush$push_tasks(list(\n  list(cost = 1e-4, gamma = 1e-4),\n  list(cost = 1e-3, gamma = 1e-3)))\n```\n:::\n\n\nThe keys of the pushed tasks are returned.\nPushing a task takes around 0.5 milliseconds, and pushing 10,000 takes around 200 milliseconds (see @sec-benchmark).\nSee @sec-data-store for more information on how the tasks are stored in the Redis database.\n\nPushing a task is non-blocking i.e. the method returns immediately.\nWe can wait for a task to finish with the `$wait_for_tasks()` method.\nThe method blocks until all tasks are processed by the workers.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$wait_for_tasks(keys)\n```\n:::\n\n\n\n## Retrieve Results {#sec-retrieve-results}\n\nThe `$fetch_finished_tasks()` method retrieves the results of finished tasks.\nThe method returns a `data.table` with additional meta information.\nThe `worker_id` of the worker that evaluated the task and the `pid` of the worker process are stored in the table.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$fetch_finished_tasks()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    cost gamma        ce    pid worker_id     keys\n   <num> <num>     <num>  <int>    <char>   <char>\n1: 1e-03 1e-03 0.3939394 308138  399ab... 680c8...\n2: 1e-04 1e-04 0.3939394 308149  2e1d9... 1c3be...\n```\n\n\n:::\n:::\n\n\nThere are multiple `$fetch_*()` methods available for retrieving data from the Redis database.\nA matching method is defined for each task state e.g. `$fetch_queued_tasks()` and `$fetch_running_tasks()`.\nFetching the results takes around 1 millisecond for one task and 130 milliseconds for 10,000 tasks.\nThe method `$fetch_finished_tasks()` caches the already queried data (see @sec-caching).\n\nWe can change the included columns with the `fields` argument.\nThe default of `$fetch_finished_tasks()` is `c(\"xs\", \"xs_extra\", \"worker_extra\", \"ys\", \"ys_extra\")`.\nIf we don't want all that extra information, we can just query `\"xs\"` and `\"ys\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$fetch_finished_tasks(fields = c(\"xs\", \"ys\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    cost gamma        ce    pid worker_id     keys\n   <num> <num>     <num>  <int>    <char>   <char>\n1: 1e-03 1e-03 0.3939394 308138  399ab... 680c8...\n2: 1e-04 1e-04 0.3939394 308149  2e1d9... 1c3be...\n```\n\n\n:::\n:::\n\n\nThe option`data_format = \"list\"` returns a `list` instead of a `data.table`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$fetch_finished_tasks(data_format = \"list\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`680c83fb-1cb7-41bc-80ea-655831d4e51d`\n$`680c83fb-1cb7-41bc-80ea-655831d4e51d`$cost\n[1] 0.001\n\n$`680c83fb-1cb7-41bc-80ea-655831d4e51d`$gamma\n[1] 0.001\n\n$`680c83fb-1cb7-41bc-80ea-655831d4e51d`$ce\nclassif.ce \n 0.3939394 \n\n$`680c83fb-1cb7-41bc-80ea-655831d4e51d`$pid\n[1] 308138\n\n$`680c83fb-1cb7-41bc-80ea-655831d4e51d`$worker_id\n[1] \"399abfc3-b98d-4225-b863-1f869c15a54e\"\n\n\n$`1c3bec3e-ce45-4f99-b979-032bfafbeae2`\n$`1c3bec3e-ce45-4f99-b979-032bfafbeae2`$cost\n[1] 1e-04\n\n$`1c3bec3e-ce45-4f99-b979-032bfafbeae2`$gamma\n[1] 1e-04\n\n$`1c3bec3e-ce45-4f99-b979-032bfafbeae2`$ce\nclassif.ce \n 0.3939394 \n\n$`1c3bec3e-ce45-4f99-b979-032bfafbeae2`$pid\n[1] 308149\n\n$`1c3bec3e-ce45-4f99-b979-032bfafbeae2`$worker_id\n[1] \"2e1d978f-0f77-4860-b7ed-42a8444ae915\"\n```\n\n\n:::\n:::\n\n\nAn advantage of the `data.table` format is that we can easily sort the results and give us access to the best result.\nHowever, when we only need the result, a `list` has less overhead.\n\n## Wait for Results {#sec-wait-for-results}\n\nThe `$wait_for_finished_tasks()` methods wait until a new result is available.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$wait_for_finished_tasks(timeout = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNULL\n```\n\n\n:::\n:::\n\n\nThe `timeout` argument sets a timeout in seconds.\n\n## Caching {#sec-caching}\n\nThe tasks queried with `$fetch_finished_tasks()` and `$fetch_new_tasks()` are cached.\nThis gives a significant speedup when we query the same data multiple times.\nThe cache is a `list` that is stored in the controller.\nThis takes around 4 milliseconds for 100 tasks, independent of the number of results in the cache (see @sec-benchmark).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$fetch_finished_tasks()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    cost gamma        ce    pid worker_id     keys\n   <num> <num>     <num>  <int>    <char>   <char>\n1: 1e-03 1e-03 0.3939394 308138  399ab... 680c8...\n2: 1e-04 1e-04 0.3939394 308149  2e1d9... 1c3be...\n```\n\n\n:::\n:::\n\n\nIf we change the queried fields, the change is not reflected in the cache.\nIn this case, we can clear the cache with the `reset_cache = TRUE` option.\n\n## Stop Workers {#sec-stop-workers}\n\nLocal and remote workers can be terminated with the `$stop_workers(type = \"terminate\")` method.\nThe workers evaluate the currently running task and then terminate.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$stop_workers(type = \"terminate\")\n```\n:::\n\n\n\n\nWhen we wait for a few seconds, we see that the workers are terminated.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$worker_states\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      state worker_id\n     <char>    <char>\n1: termi...  399ab...\n2: termi...  2e1d9...\n```\n\n\n:::\n:::\n\n\nThe option `type = \"kill\"` stops the workers immediately.\nKilling a local worker is done via the `processx` package.\nRemote workers are killed by pushing a kill signal to the heartbeat process.\nWithout a heartbeat process a remote worker cannot be killed (see @sec-heartbeat).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$stop_workers(type = \"kill\")\n```\n:::\n\n\n# Advanced Functionality {#sec-advanced-functionality}\n\n## Reproducibility {#sec-reproducibility}\n\nEnsuring reproducibility is important in computational research.\nA critical aspect of achieving this is the consistent generation of pseudo-random numbers, which underpins the replicability of tasks that involve random processes.\nThe Rush package addresses this need by integrating the `\"L'Ecuyer-CMRG\"` random number generator, as introduced by @Ecuyer2002.\nThis generator is designed to prevent the synchronization of random number sequences, a critical aspect in parallel computing environments.\n\nFor each task, a distinct stream of pseudo-random numbers is produced, guaranteeing consistent results independent of the number of workers.\nRush allows the specification of an initial seed during the creation of a Rush instance.\nUsers have the flexibility to initialize the rush controller with either a specific `\"L'Ecuyer-CMRG\"` seed or a standard seed.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush = rsh(network_id = \"svm\", seed = 123)\n```\n:::\n\n\nThis ensures that every task is assigned a unique stream based on the initial seed.\nFurthermore, for finer control, it is possible to assign unique `\"L'Ecuyer-CMRG\"` seeds directly to individual tasks.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$push_tasks(list(list(x1 = 1, x2 = 2)), seeds = list(c(10407L, 1801422725L, -2057975723L, 1156894209L, 1595475487L, 210384600L, -1655729657L)))\n```\n:::\n\n\n## Error Handling {#sec-error-handling}\n\nThe `rush` package is equipped with an advanced error-handling mechanism designed to manage and mitigate errors encountered during the execution of tasks.\nIt adeptly handles a range of error scenarios, from standard R errors to more complex issues such as segmentation faults and network errors.\n\n### Simple R Errors {#sec-error-handling-simple}\n\nFor errors occurring within the R environment, rush employs a strategy where such errors are caught during the task evaluation.\nTo illustrate, consider a scenario where we define a function intended to trigger a simple R error.\nThe occurring error leads to the task being labeled as `\"failed\"`, with the corresponding error message captured and stored in the \"message\" column of the data store.\nThis process is demonstrated in the following example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush = rsh(network_id = \"simple_error\")\n\nfun = function(x) {\n  stop(\"Simple R Error\")\n}\n\nrush$start_workers(fun = fun, n_workers = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [10:03:09.032] [rush] Starting 2 worker(s)\n```\n\n\n:::\n\n```{.r .cell-code}\nrush$push_tasks(list(list(x = 1)))\n\nSys.sleep(5)\n\nrush$fetch_failed_tasks()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x    pid worker_id  message     keys\n   <num>  <int>    <char>   <char>   <char>\n1:     1 308264  e77c3... Simpl... b39bc...\n```\n\n\n:::\n:::\n\n\nThis approach ensures that errors do not halt the overall execution process, allowing for error inspection and task reevaluation as necessary.\n\n### Handling Failing Workers {#sec-error-handling-workers}\n\nThe rush package also addresses scenarios where workers may fail due to crashes or lost connections, potentially causing tasks to appear as if they are in the `\"running\"` state indefinitely.\nAn example of this can be seen when simulating a segmentation fault, leading to the termination of a worker process.\n\nIf a worker crashes or the connection gets lost, it looks like a task is `\"running\"` forever.\nAs an example, we define a function that simulates a segmentation fault by killing the worker process.\nThe package includes the method `$detect_lost_workers()` designed to identify and manage such instances effectively.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush = rsh(network_id = \"segmenation_fault\")\n\nfun = function(x) {\n  tools::pskill(Sys.getpid())\n}\n\nrush$start_workers(fun = fun, n_workers = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [10:03:14.638] [rush] Starting 2 worker(s)\n```\n\n\n:::\n\n```{.r .cell-code}\nrush$push_tasks(list(list(x = 1)))\n\nSys.sleep(5)\n\nrush$detect_lost_workers()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nERROR [10:03:20.205] [rush] Lost 1 worker(s): e0599eee-bb80-4906-81c8-f66375401d8e\nERROR [10:03:20.210] [rush] Lost 1 task(s): 8126a04b-1e0b-4002-b964-24e1f98a89e9\n```\n\n\n:::\n:::\n\n\nRunning this method periodically adds a small overhead.\nFor remote workers, the package utilizes the heartbeat mechanism to check the status of the workers (see @sec-heartbeat).\nLocal workers are monitored using the `processx` package.\nUpon identifying a lost worker, its status is updated to `\"lost\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$worker_states\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      state worker_id\n     <char>    <char>\n1: runni...  882fb...\n2:     lost  e0599...\n```\n\n\n:::\n:::\n\n\nThe `$detect_lost_workers()` method allows to restart lost workers when the option `restart_workers = TRUE` is specified.\nWorkers that have been lost may also be restarted manually using `$restart_workers()`.\nAutomatically restarting workers only works for local workers.\n\nThe status of the task that caused the worker to fail is changed to `\"failed\"`.\nSee @sec-retry-tasks for more information on restarting tasks.\n\n### Restarting Tasks {#sec-retry-tasks}\n\nOne of the resilient features of the rush package is its ability to retry tasks that have failed, thereby enhancing the robustness of task execution processes.\nThe `$retry_tasks()` method is central to this functionality, offering a straightforward way to reattempt the execution of tasks that did not complete successfully on their first run.\nAn intriguing aspect of this method is its ability to generate a new random number stream for the task being retried, if the `next_seed = TRUE` option is specified.\nThis ensures that the retry attempts are not merely repetitions but are executed under potentially different conditions, enhancing the chances of success.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$retry_task(keys)\n```\n:::\n\n\nTasks can be pushed with a specific maximum number of retries.\nThis setting allows tasks to be attempted up to a pre-defined limit before being definitively marked as `\"failed\"`.\nFor example, setting `max_retries = 3` will ensure that a task is retried up to three additional times if initial attempts fail.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$push_tasks(list(list(x = 1), list(x = 2)), max_retries = 3)\n```\n:::\n\n\n### Encapsulation {#sec-error-handling-encapsulation}\n\nTo bolster the resilience of the tasks, the `rush` package offers a sophisticated mechanism for function evaluation encapsulation using the `callr` package.\nThis approach significantly enhances the stability of workers, particularly in scenarios where executed functions may lead to severe errors such as segmentation faults.\nBy encapsulating function evaluations, workers are safeguarded against crashes that would otherwise tear down the entire worker.\n\nThe encapsulation is achieved by passing a specialized worker loop function to the `$start_workers()` method, which leverages the `callr` package's capabilities to execute tasks in isolated R sessions.\nTo illustrate, consider the scenario where we intentionally trigger a segmentation fault by terminating the process of the callr R session.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush = rsh(network_id = \"callr\")\n\nfun = function(x) {\n  tools::pskill(Sys.getpid())\n}\n\nrush$start_workers(fun = fun, n_workers = 2, worker_loop = worker_loop_callr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [10:03:20.303] [rush] Starting 2 worker(s)\n```\n\n\n:::\n\n```{.r .cell-code}\nrush$push_tasks(list(list(x = 1)))\n\nSys.sleep(5)\n\nrush$fetch_failed_tasks()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x    pid worker_id  message     keys\n   <num>  <int>    <char>   <char>   <char>\n1:     1 308442  a2ee9... Exter... e9e23...\n```\n\n\n:::\n:::\n\n\nDespite this aggressive action, which would typically result in a worker crash, the encapsulation provided by `callr` ensures that the worker remains operational, marking the task as `\"failed\"` without affecting the stability of the worker process itself.\nFollowing this procedure, the worker's status is queried, confirming its continued operation despite the encountered error.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$worker_states\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      state worker_id\n     <char>    <char>\n1: runni...  a2ee9...\n2: runni...  1ebbf...\n```\n\n\n:::\n:::\n\n\n## Timeouts {#sec-timeouts}\n\nIn the context of executing tasks within parallel computing environments, the ability to enforce execution time limits is paramount.\nThis ensures that no single task consumes disproportionate system resources, thereby maintaining overall system efficiency and responsiveness.\nThe timeout parameter of the `$push_tasks()` method allows users to set a maximum execution time for each task.\n\nAs an illustrative example, consider a scenario where a timeout of 100 milliseconds is applied to a task.\nThis short duration is strictly enforced, guaranteeing that if the task execution surpasses this window, it is promptly halted and marked as `\"failed\"`.\nIt is important to note that this timeout functionality is exclusively available when employing `callr` for task execution, alongside the specialized `worker_loop_callr`.\nSetting a timeout comes with an overhead of around 500 milliseconds (see @sec-benchmark).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush = rsh(network_id = \"svm\")\n\nrush$start_workers(\n  fun = eval_svm,\n  n_workers = 2,\n  globals = c(\"learner\", \"task\", \"splits\"),\n  packages = c(\"mlr3\", \"mlr3learners\", \"e1071\"),\n  worker_loop = worker_loop_callr\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [10:03:25.861] [rush] Starting 2 worker(s)\n```\n\n\n:::\n\n```{.r .cell-code}\nrush$push_tasks(list(list(cost = 1e-3, gamma = 1e-2)), timeouts = 0.1)\n\nrush$fetch_tasks()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    cost gamma        ce    pid worker_id     keys\n   <num> <num>     <num>  <int>    <char>   <char>\n1: 1e-04 1e-04 0.3939394 308149  2e1d9... 1c3be...\n2: 1e-03 1e-03 0.3939394 308138  399ab... 680c8...\n3: 1e-03 1e-02        NA     NA      <NA> 24c71...\n```\n\n\n:::\n:::\n\n\n\n## Logging {#sec-logging}\n\nThe worker logs all messages written with the `lgr` package to the database.\nThe `lgr_thresholds` argument of `$start_workers()` defines the logging level for each logger e.g. `c(rush = \"debug\")`.\nSaving log messages adds a small overhead but is useful for debugging.\nBy default, no log messages are stored.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$read_log()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNull data.table (0 rows and 0 cols)\n```\n\n\n:::\n:::\n\n\nThe log level can also be changed with the `lgr_thresholds` argument of `rush_plan()`.\n\n## Start Script {#sec-start-script}\n\nWe are not limited to start workers with the `processx` package.\nA work can be started manually with a script on a remote machine.\nThe only requirement is that the machine has access to the Redis server and can run R scripts.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$create_worker_script()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [10:03:25.963] [rush] Start worker with:\nINFO  [10:03:25.964] [rush] Rscript -e 'rush::start_worker(network_id = 'svm', hostname = 'ThinkPad-T14-Gen-3', url = 'redis://127.0.0.1:6379')'\nINFO  [10:03:25.965] [rush] See ?rush::start_worker for more details.\n```\n\n\n:::\n:::\n\n\nThe `$create_worker_script()` takes also a `globals` and `packages` argument.\nThe globals are serialized and passed to the worker via Redis.\nThe packages must be installed on the remote machine but they are loaded automatically.\n\n## Large Objects {#sec-large-objects}\n\nThe maximum size of a Redis string is 512 MiB.\nIf the constants of the worker loop function are larger than 512 MiB, `rush` throws an error.\nIf the controller and workers can access the same file system, `rush` writes the large objects to disk.\nThe `large_objects_path` argument of `rush_plan()` defines the directory where the large objects are stored.\n\n::: {.callout-note}\n\nIn a future version, tasks larger than 512 MiB can be stored on disk with the same mechanism.\n\n:::\n\n## Queues\n\nRush uses a shared queue and a queue for each worker.\nThe shared queue is used to push tasks to the workers.\nThe first worker that pops a task from the shared queue evaluates the task.\nUntil now we only used the shared queue.\nThe worker queues are used to push tasks to specific workers.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nworker_id = rush$worker_ids[1]\nrush$push_priority_tasks(list(list(cost = 1e-1, gamma = 1e-1)), priority = worker_id)\n```\n:::\n\n\n## Heartbeats {#sec-heartbeat}\n\nThe heartbeat is a mechanism to monitor the status of remote workers in distributed computing systems.\nThe mechanism consists of a heartbeat key with a set [expiration timeout](https://redis.io/commands/expire/) and a dedicated heartbeat process that refreshes the timeout periodically.\nThe heartbeat process is started with `callr` and is linked to main process of the worker.\nIn the event of a worker's failure, the associated heartbeat process also ceases to function, thus halting the renewal of the timeout.\nThe absence of the heartbeat key acts as an indicator to the controller that the worker is no longer operational.\nConsequently, the controller updates the worker's status to `\"lost\"`.\n\nHeartbeats are initiated upon worker startup by specifying the `heartbeat_period` and `heartbeat_expire` parameters.\nThe `heartbeat_period` defines the frequency at which the heartbeat process will update the timeout.\nThe `heartbeat_expire` sets the duration, in seconds, before the heartbeat key expires.\nThe expiration time should be set to a value greater than the heartbeat period to ensure that the heartbeat process has sufficient time to refresh the timeout.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$start_workers(\n  worker_loop = fun_loop,\n  n_workers = 2,\n  globals = c(\"learner\", \"task\", \"splits\"),\n  packages = c(\"mlr3\", \"mlr3learners\", \"e1071\"),\n  host = \"remote\",\n  heartbeat_period = 1,\n  heartbeat_expire = 3,\n  fun = eval_svm)\n```\n:::\n\n\nThe heartbeat process is also the only way to kill a remote worker.\nThe `$stop_workers(type = \"kill\")` method pushes a kill signal to the heartbeat process.\nThe heartbeat process terminates the main process of the worker.\n\n## Rush Data Store {#sec-data-store}\n\nTasks are stored in Redis [hashes](https://redis.io/docs/data-types/hashes/).\nHashes are collections of field-value pairs.\nThe key of the hash identifies the task in Redis and `rush`.\n\n```\nkey : xs | ys | xs_extra\n```\n\nThe field-value pairs are written by different methods, e.g. `$push_tasks()` writes `xs` and `$push_results()` writes `ys`.\nThe values of the fields are serialized lists or atomic values e.g. unserializing `xs` gives `list(x1 = 1, x2 = 2)`\nThis data structure allows quick converting of a hash into a row and joining multiple hashes into a table.\n\n```\n| key | x1 | x2 | y | timestamp |\n| 1.. |  3 |  4 | 7 |  12:04:11 |\n| 2.. |  1 |  4 | 5 |  12:04:12 |\n| 3.. |  1 |  1 | 2 |  12:04:13 |\n```\nWhen the value of a field is a named list, the field can store the cells of multiple columns of the table.\nWhen the value of a field is an atomic value, the field stores a single cell of a column named after the field.\nThe methods `$push_tasks()` and `$push_results()` write into multiple hashes.\nFor example, `$push_tasks(xss = list(list(x1 = 1, x2 = 2), list(x1 = 2, x2 = 2))` writes `xs` in two hashes.\n\n::: {.callout-note}\n\nThe alternative of writing each cell in a separate field turned out to be too slow because each element has to be serialized individually.\nIn addition, grouping has the advantage that the worker can simply access the arguments of the function without having to store information about the search space.\nThe other alternative of writing each row in a string is too slow because serialization and deserialization must be performed each time the row is accessed.\n\n:::\n\n## Fetch Tasks {#sec-fetch-tasks}\n\nRunning the `$fetch_*` methods directly one after another could lead to tasks appearing twice because the worker could change the state of a task between the calls.\nThe `$fetch_tasks_with_state()` method fetches tasks with different states in one call.\nThe `states` parameter defines the states of the tasks that should be fetched.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrush$fetch_tasks_with_state(states = c(\"queued\", \"running\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      state  cost gamma     keys\n     <char> <num> <num>   <char>\n1: queue... 0.100  0.10 bff5c...\n2: queue... 0.001  0.01 24c71...\n```\n\n\n:::\n:::\n\n\n# Benchmark {#sec-benchmark}\n\nWe benchmark the performance of `rush` with the [microbenchmark](https://cran.r-project.org/package=microbenchmark) package with 100 repetitions.\n\nWe pushed tasks to the queue and measured the time (@fig-benchmark-push).\nThe number of tasks was increased from 1 to 10,000.\nOne task consisted of two numbers.\nThe runtime increased sublinearly with the number of tasks.\nWhen pushing one task, the overhead per task was around 0.5 milliseconds.\nWhen pushing 10,000 tasks, the overhead per task was around 0.05 milliseconds.\nWe use the pipeline mechanism of Redis to push multiple tasks at once which explains the sublinear increase.\nIt is beneficial to push multiple tasks at once.\nBut the overhead per task is already low for one task.\nPoping a task from the queue takes around 0.5 milliseconds independent of the number of tasks in the queue.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Runtime Performance, measured in milliseconds, of pushing a task. Both axes are on a logarithmic scale.](index_files/figure-html/fig-benchmark-push-1.png){#fig-benchmark-push fig-align='center' width=672}\n:::\n:::\n\n\nWe fetched the results from the database and measured the time (@fig-benchmark-fetch).\nThis includes fetching the results from the database and converting them to a `data.table`.\nThe number of results was increased from 1 to 10,000.\nThe runtime increased sublinearly with the number of results.\nWhen fetching one result,  the overhead per result was around 0.5 milliseconds.\nWhen fetching 10,000 results, the overhead per result was around 0.02 milliseconds.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Runtime Performance, measured in milliseconds, of fetching the results. Both axes are on a logarithmic scale.](index_files/figure-html/fig-benchmark-fetch-1.png){#fig-benchmark-fetch fig-align='center' width=672}\n:::\n:::\n\n\n`rush` caches already fetched results.\nFetching 100 new tasks and adding them to the cache takes around 2 milliseconds independent of the number of results in the cache.\n\nRunning the complete `woker_loop_default` function once only comes with a small overhead of around 0.8 milliseconds.\nWhen the task evaluation is encapsulated with `callr`, the overhead is around 500 milliseconds.\n\n# Optimization {#sec-bbotk-mlr3tuning}\n\n## Install\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npak::pkg_install(c(\"mlr-org/bbotk@rush\", \"mlr-org/mlr3tuning@rush\"))\n```\n:::\n\n\n## Centralized Design\n\n![The architecture of a centralized rush network.](rush_centralized.png){#fig-rush width=100%}\n\nThe `rush_plan()` makes the Redis configuration and the maximum number of workers available to any rush controller.\nThe `TuningInstanceRushSingleCrit` class creates the `Rush` controller internally.\nThe archive of the instance (`ArchiveRedis`) is connected to the Redis database via the rush controller.\nWhen the instance is passed to the tuner, the workers are started automatically and the `ObjectiveTuning` is copied to the workers.\nThe grid search tuner has a new method `$optimize_async()` for tuning with rush.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3tuning)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: paradox\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(rush)\n\nrush_plan(n_workers = 2, config = redux::redis_config())\n\ninstance = TuningInstanceRushSingleCrit$new(\n  task = tsk(\"pima\"),\n  learner = lrn(\"classif.rpart\", cp = to_tune(0.01, 0.1), minsplit = to_tune(1, 100)),\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 10)\n)\n\ntuner = tnr(\"grid_search\")\n\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [10:03:27.776] [rush] Starting 2 worker(s)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      cp minsplit learner_param_vals  x_domain classif.ce\n   <num>    <int>             <list>    <list>      <num>\n1:  0.03        1          <list[3]> <list[2]>  0.2460938\n```\n\n\n:::\n\n```{.r .cell-code}\ninstance$archive$data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       cp minsplit classif.ce warnings errors       log runtime_learners  resample_result        timestamp_xs    pid\n    <num>    <int>      <num>    <int>  <int>    <list>            <num>           <list>              <POSc>  <int>\n 1:  0.05       67  0.2552083        0      0 <list[3]>            0.073 <ResampleResult> 2024-04-04 10:03:28 308570\n 2:  0.05       78  0.2617188        0      0 <list[3]>            0.082 <ResampleResult> 2024-04-04 10:03:28 308574\n 3:  0.03       56  0.2552083        0      0 <list[3]>            0.053 <ResampleResult> 2024-04-04 10:03:28 308570\n 4:  0.06       78  0.2617188        0      0 <list[3]>            0.071 <ResampleResult> 2024-04-04 10:03:28 308574\n 5:  0.08       34  0.2552083        0      0 <list[3]>            0.058 <ResampleResult> 2024-04-04 10:03:28 308570\n 6:  0.07       45  0.2552083        0      0 <list[3]>            0.051 <ResampleResult> 2024-04-04 10:03:28 308570\n 7:  0.10       23  0.2617188        0      0 <list[3]>            0.076 <ResampleResult> 2024-04-04 10:03:28 308574\n 8:  0.03       23  0.2552083        0      0 <list[3]>            0.055 <ResampleResult> 2024-04-04 10:03:28 308570\n 9:  0.03        1  0.2460938        0      0 <list[3]>            0.076 <ResampleResult> 2024-04-04 10:03:28 308574\n10:  0.10       78  0.2669271        0      0 <list[3]>            0.065 <ResampleResult> 2024-04-04 10:03:28 308570\n    worker_id  x_domain        timestamp_ys     keys\n       <char>    <list>              <POSc>   <char>\n 1:  ba02c... <list[2]> 2024-04-04 10:03:29 376a2...\n 2:  4a18a... <list[2]> 2024-04-04 10:03:29 b97f7...\n 3:  ba02c... <list[2]> 2024-04-04 10:03:29 47cae...\n 4:  4a18a... <list[2]> 2024-04-04 10:03:29 cfc86...\n 5:  ba02c... <list[2]> 2024-04-04 10:03:29 422f0...\n 6:  ba02c... <list[2]> 2024-04-04 10:03:29 60d9f...\n 7:  4a18a... <list[2]> 2024-04-04 10:03:29 ffaa3...\n 8:  ba02c... <list[2]> 2024-04-04 10:03:30 c0de5...\n 9:  4a18a... <list[2]> 2024-04-04 10:03:30 21179...\n10:  ba02c... <list[2]> 2024-04-04 10:03:30 2c094...\n```\n\n\n:::\n:::\n\n\nThe `$.optimize_async()` method runs in the main process.\nThe method generates the design grid and sends the hyperparameter configurations to the workers via `inst$eval_async()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n.optimize_async = function(inst) {\n  pv = self$param_set$values\n\n  data = generate_design_grid(\n    param_set = inst$search_space,\n    resolution = pv$resolution,\n    param_resolutions = pv$param_resolutions)$data\n\n  inst$eval_async(data)\n\n  repeat({\n    if (inst$is_terminated) stop(bbotk::terminated_error(inst))\n    Sys.sleep(0.01)\n  })\n}\n```\n:::\n\n\nThe worker loop evaluates the hyperparameter configurations with `ObjectiveTuning`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbbotk_worker_loop_centralized = function(rush, objective, search_space) {\n  while(!rush$terminated) {\n    task = rush$pop_task(fields = c(\"xs\", \"seed\"))\n    xs_trafoed = trafo_xs(task$xs, search_space)\n\n    if (!is.null(task)) {\n      tryCatch({\n        ys = with_rng_state(objective$eval, args = list(xs = xs_trafoed), seed = task$seed)\n        rush$push_results(task$key, yss = list(ys), extra = list(list(x_domain = list(xs_trafoed), timestamp_ys = Sys.time())))\n      }, error = function(e) {\n        condition = list(message = e$message)\n        rush$push_failed(task$key, conditions = list(condition))\n      })\n    }\n  }\n  return(NULL)\n}\n```\n:::\n\n\n## Decentralized Design\n\n![The architecture of a decentralized rush network.](rush_decentralized.png){#fig-rush width=100%}\n\nThe same user interface as the centralized design.\nWhen the instance is passed to the tuner, the workers are started automatically and both are copied to the workers.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3tuning)\nlibrary(rush)\n\nrush_plan(n_workers = 2, config = redux::redis_config())\n\ninstance = TuningInstanceRushSingleCrit$new(\n  task = tsk(\"pima\"),\n  learner = lrn(\"classif.rpart\", cp = to_tune(0.01, 0.1), minsplit = to_tune(1, 100)),\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 10)\n)\n\ntuner = tnr(\"random_search_v2\")\n\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [10:03:31.050] [rush] Starting 2 worker(s)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           cp minsplit learner_param_vals  x_domain classif.ce\n        <num>    <int>             <list>    <list>      <num>\n1: 0.02117172       27          <list[3]> <list[2]>  0.2460938\n```\n\n\n:::\n\n```{.r .cell-code}\ninstance$archive$data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            cp minsplit classif.ce warnings errors       log runtime_learners  resample_result        timestamp_xs\n         <num>    <int>      <num>    <int>  <int>    <list>            <num>           <list>              <POSc>\n 1: 0.04714893       47  0.2747396        0      0 <list[3]>            0.074 <ResampleResult> 2024-04-04 10:03:32\n 2: 0.03122045       42  0.2500000        0      0 <list[3]>            0.047 <ResampleResult> 2024-04-04 10:03:33\n 3: 0.02183890       53  0.2591146        0      0 <list[3]>            0.090 <ResampleResult> 2024-04-04 10:03:32\n 4: 0.07835128       87  0.2747396        0      0 <list[3]>            0.048 <ResampleResult> 2024-04-04 10:03:33\n 5: 0.09199043       57  0.2526042        0      0 <list[3]>            0.064 <ResampleResult> 2024-04-04 10:03:33\n 6: 0.03426045       74  0.2552083        0      0 <list[3]>            0.048 <ResampleResult> 2024-04-04 10:03:33\n 7: 0.05192350       80  0.2747396        0      0 <list[3]>            0.053 <ResampleResult> 2024-04-04 10:03:33\n 8: 0.03949048       33  0.2565104        0      0 <list[3]>            0.076 <ResampleResult> 2024-04-04 10:03:33\n 9: 0.04628583       26  0.2747396        0      0 <list[3]>            0.085 <ResampleResult> 2024-04-04 10:03:33\n10: 0.08626224       43  0.2526042        0      0 <list[3]>            0.077 <ResampleResult> 2024-04-04 10:03:33\n11: 0.02117172       27  0.2460938        0      0 <list[3]>            0.082 <ResampleResult> 2024-04-04 10:03:33\n       pid worker_id  x_domain        timestamp_ys     keys\n     <int>    <char>    <list>              <POSc>   <char>\n 1: 308631  a8c08... <list[2]> 2024-04-04 10:03:32 4f5b8...\n 2: 308631  a8c08... <list[2]> 2024-04-04 10:03:33 1d768...\n 3: 308637  9cc31... <list[2]> 2024-04-04 10:03:33 98768...\n 4: 308631  a8c08... <list[2]> 2024-04-04 10:03:33 ba62a...\n 5: 308637  9cc31... <list[2]> 2024-04-04 10:03:33 16747...\n 6: 308631  a8c08... <list[2]> 2024-04-04 10:03:33 2f783...\n 7: 308631  a8c08... <list[2]> 2024-04-04 10:03:33 9419f...\n 8: 308637  9cc31... <list[2]> 2024-04-04 10:03:33 872a5...\n 9: 308631  a8c08... <list[2]> 2024-04-04 10:03:33 1a0ff...\n10: 308637  9cc31... <list[2]> 2024-04-04 10:03:33 2b26b...\n11: 308631  a8c08... <list[2]> 2024-04-04 10:03:34 5067c...\n```\n\n\n:::\n:::\n\n\nThe `$.optimize()` method runs on each worker.\nThe configuration is sampled and on the worker.\nThe worker evaluates the configuration with `ObjectiveTuning` and pushes the results to the database.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n.optimize = function(inst) {\n  search_space = inst$search_space\n  rush = inst$rush\n\n  while(!inst$is_terminated) {\n    # sample and transform\n    sampler = SamplerUnif$new(search_space)\n    xdt = sampler$sample(1)$data\n    xss = transpose_list(xdt)\n    xs = xss[[1]][inst$archive$cols_x]\n    xs_trafoed = trafo_xs(xs, search_space)\n\n    # mark as running\n    keys = inst$rush$push_running_task(list(xs), extra = list(list(timestamp_xs = Sys.time())))\n\n    # evaluate\n    ys = inst$objective$eval(xs_trafoed)\n\n    # push results\n    rush$push_results(keys, yss = list(ys), extra = list(list(\n      x_domain = list(xs_trafoed),\n      timestamp_ys = Sys.time())))\n  }\n}\n```\n:::\n\n\n\n# Conclusion {#sec-conclusion}\n\n\n#### Glossary\n\n* Parallel computing: The use of multiple processing elements simultaneously for solving a computational problem.\n* Distributed computing: Utilizing multiple computers in a network to solve a computational problem.\n* Worker: A process that performs tasks as part of a larger computation.\n* Computing task: A discrete portion of a larger computational problem, designed to be executed by a worker.\n* Redis: An open-source, in-memory data store, used as a database, cache, and message broker.\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}