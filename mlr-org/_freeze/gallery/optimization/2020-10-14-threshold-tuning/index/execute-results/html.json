{
  "hash": "7981d3e45b7ee247fe55c16096543022",
  "result": {
    "markdown": "---\ntitle: Threshold Tuning for Classification Tasks\ncategories:\n  - tuning\n  - resampling\n  - mlr3pipelines\n  - classification\nauthor:\n  - name: Florian Pfisterer\ndate: 10-14-2020\ndescription: |\n  Adjust the probability thresholds of classes.\naliases:\n  - ../../../gallery/2020-10-14-threshold-tuning/index.html\n---\n\n\n\n\n## Intro\n\nPredicting probabilities in classification tasks allows us to adjust the probability thresholds required for assigning an observation to a certain class.\nThis can lead to improved classification performance, especially for cases where we e.g. aim to balance off metrics such as false positive and false negative rates.\n\nThis is for example often done in ROC Analysis. The mlr3book also has a chapter on [ROC Analysis](https://mlr3book.mlr-org.com/binary-classification.html#binary-roc)) for the interested reader.\nThis post does not focus on ROC analysis, but instead focusses on the general problem of adjusting classification thresholds for arbitrary metrics.\n\nThis post assumes some familiarity with the [mlr3](https://mlr3.mlr-org.com), and also the [mlr3pipelines](https://mlr3pipelines.mlr-org.com) and [mlr3tuning](https://mlr3tuning.mlr-org.com) packages, as both are used during the post.\nThe [mlr3book](https://mlr3book.mlr-org.com/) contains more details on those two packages.\nThis post is a more in-depth version of the [article on threshold tuning in the mlr3book](https://mlr3book.mlr-org.com/cost-sens.html#threshold-tuning-1).\n\n\n## Prerequisites\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com) package which pulls in the most important packages for this example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\n## Thresholds: A short intro\n\nIn order to understand thresholds, we will quickly showcase the effect of setting different thresholds:\n\nFirst we create a learner that predicts probabilities and use it to predict on holdout data, storing the prediction.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nrr = resample(tsk(\"pima\"), learner, rsmp(\"holdout\"))\nprd = rr$prediction()\nprd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 256 observations:\n    row_ids truth response  prob.pos  prob.neg\n          4   neg      neg 0.1057692 0.8942308\n          6   neg      neg 0.0200000 0.9800000\n         10   pos      neg 0.1428571 0.8571429\n---                                           \n        764   neg      neg 0.2777778 0.7222222\n        766   neg      neg 0.0200000 0.9800000\n        767   pos      pos 0.8000000 0.2000000\n```\n:::\n:::\n\n\nIf we now look at the confusion matrix, the off-diagonal elements are errors made by our model (*false positives* and *false negatives*) while on-diagol ements are where our model predicted correctly.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Print confusion matrix\nprd$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        truth\nresponse pos neg\n     pos  53  27\n     neg  37 139\n```\n:::\n\n```{.r .cell-code}\n# Print False Positives and False Negatives\nprd$score(list(msr(\"classif.fp\"), msr(\"classif.fn\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.fp classif.fn \n        27         37 \n```\n:::\n:::\n\n\nBy adjusting the **classification threshold**, in this case the probability required to predict the positive class, we can now trade off predicting more positive cases (first row)\nagainst predicting fewer negative cases (second row) or vice versa.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Lower threshold: More positives\nprd$set_threshold(0.25)$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        truth\nresponse pos neg\n     pos  78  71\n     neg  12  95\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Higher threshold: Fewer positives\nprd$set_threshold(0.75)$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        truth\nresponse pos neg\n     pos  52  20\n     neg  38 146\n```\n:::\n:::\n\n\nThis threshold value can now be adjusted optimally for a given measure, such as accuracy. How this can be done is discussed in the following section.\n\n## Adjusting thresholds: Two strategies\n\nCurrently [mlr3pipelines](https://mlr3pipelines.mlr-org.com)  offers two main strategies towards adjusting `classification thresholds`.\nWe can either expose the thresholds as a `hyperparameter` of the Learner by using [`PipeOpThreshold`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_threshold.html).\nThis allows us to tune the `thresholds` via an outside optimizer from [mlr3tuning](https://mlr3tuning.mlr-org.com).\n\nAlternatively, we can also use [`PipeOpTuneThreshold`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_tunethreshold.html) which automatically tunes the threshold after each learner fit.\n\nIn this blog-post, we'll go through both strategies.\n\n## PipeOpThreshold\n\n[`PipeOpThreshold`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_threshold.html) can be put directly after a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html).\n\nA simple example would be:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngr = lrn(\"classif.rpart\", predict_type = \"prob\") %>>% po(\"threshold\")\nl = GraphLearner$new(gr)\n```\n:::\n\n\nNote, that `predict_type` = \"prob\" is required for `po(\"threshold\")` to have any effect.\n\nThe `thresholds` are now exposed as a `hyperparameter` of the [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) we created:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(l$param_set)[, .(id, class, lower, upper, nlevels)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                              id    class lower upper nlevels\n 1:             classif.rpart.cp ParamDbl     0     1     Inf\n 2:     classif.rpart.keep_model ParamLgl    NA    NA       2\n 3:     classif.rpart.maxcompete ParamInt     0   Inf     Inf\n 4:       classif.rpart.maxdepth ParamInt     1    30      30\n 5:   classif.rpart.maxsurrogate ParamInt     0   Inf     Inf\n 6:      classif.rpart.minbucket ParamInt     1   Inf     Inf\n 7:       classif.rpart.minsplit ParamInt     1   Inf     Inf\n 8: classif.rpart.surrogatestyle ParamInt     0     1       2\n 9:   classif.rpart.usesurrogate ParamInt     0     2       3\n10:           classif.rpart.xval ParamInt     0   Inf     Inf\n11:         threshold.thresholds ParamUty    NA    NA     Inf\n```\n:::\n:::\n\n\nWe can now tune those thresholds from the outside as follows:\n\nBefore `tuning`, we have to define which hyperparameters we want to tune over.\nIn this example, we only tune over the `thresholds` parameter of the `threshold` [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html).\nyou can easily imagine, that we can also jointly tune over additional hyperparameters, i.e. rpart's `cp` parameter.\n\nAs the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) we aim to optimize for is a binary task, we can simply specify the threshold parameter:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(\n  threshold.thresholds = p_dbl(lower = 0, upper = 1)\n)\n```\n:::\n\n\nWe now create a [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html), which automatically tunes the supplied learner over the [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) we supplied above.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat = auto_tuner(\n  method = \"random_search\",\n  learner = l,\n  resampling = rsmp(\"cv\", folds = 3L),\n  measure = msr(\"classif.ce\"),\n  search_space = search_space,\n  term_evals = 5L,\n)\n\nat$train(tsk(\"german_credit\"))\n```\n:::\n\n\nFor multi-class [`Tasks`](https://mlr3.mlr-org.com/reference/Task.html), this is a little more complicated.\nWe have to use a `trafo` to transform a set of `ParamDbl` into the desired format for `threshold.thresholds`:\nA named numeric vector containing the thresholds.\nThis can be easily achieved via a `trafo` function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(\n  versicolor = p_dbl(lower = 0, upper = 1),\n  setosa = p_dbl(lower = 0, upper = 1),\n  virginica = p_dbl(lower = 0, upper = 1),\n  .extra_trafo = function(x, param_set) {\n    list(threshold.thresholds = mlr3misc::map_dbl(x, identity))\n  }\n)\n```\n:::\n\n\nInside the `.exta_trafo`, we simply collect all set params into a named vector via `map_dbl` and store it\nin the `threshold.thresholds` slot expected by the learner.\n\nAgain, we create a [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html), which automatically tunes the supplied learner over the [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) we supplied above.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat_2 = auto_tuner(\n  method = \"random_search\",\n  learner = l,\n  resampling = rsmp(\"cv\", folds = 3L),\n  measure = msr(\"classif.ce\"),\n  search_space = search_space,\n  term_evals = 5L,\n)\n\nat_2$train(tsk(\"iris\"))\n```\n:::\n\n\nOne drawback of this strategy is, that this requires us to fit a new model for each new threshold setting.\nWhile setting a threshold and computing performance is relatively cheap, fitting the learner is often\nmore computationally demanding.\nA better strategy is therefore often to optimize the thresholds separately after each model fit.\n\n## PipeOpTuneThreshold\n\n[`PipeOpTuneThreshold`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_tunethreshold.html) on the other hand works together with [`PipeOpLearnerCV`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_learner_cv.html).\nIt directly optimizes the `cross-validated` predictions made by this [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html).\n\nA simple example would be:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngr = po(\"learner_cv\", lrn(\"classif.rpart\", predict_type = \"prob\")) %>>%\n  po(\"tunethreshold\")\nl2 = GraphLearner$new(gr)\n```\n:::\n\n\nNote, that `predict_type` = \"prob\" is required for `po(\"tunethreshold\")` to have any effect.\nAdditionally, note that this time no `threshold` parameter is exposed, it is automatically tuned internally.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(l2$param_set)[, .(id, class, lower, upper, nlevels)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                        id    class lower upper nlevels\n 1:        classif.rpart.resampling.method ParamFct    NA    NA       2\n 2:         classif.rpart.resampling.folds ParamInt     2   Inf     Inf\n 3: classif.rpart.resampling.keep_response ParamLgl    NA    NA       2\n 4:                       classif.rpart.cp ParamDbl     0     1     Inf\n 5:               classif.rpart.keep_model ParamLgl    NA    NA       2\n 6:               classif.rpart.maxcompete ParamInt     0   Inf     Inf\n 7:                 classif.rpart.maxdepth ParamInt     1    30      30\n 8:             classif.rpart.maxsurrogate ParamInt     0   Inf     Inf\n 9:                classif.rpart.minbucket ParamInt     1   Inf     Inf\n10:                 classif.rpart.minsplit ParamInt     1   Inf     Inf\n11:           classif.rpart.surrogatestyle ParamInt     0     1       2\n12:             classif.rpart.usesurrogate ParamInt     0     2       3\n13:                     classif.rpart.xval ParamInt     0   Inf     Inf\n14:           classif.rpart.affect_columns ParamUty    NA    NA     Inf\n15:                  tunethreshold.measure ParamUty    NA    NA     Inf\n16:                tunethreshold.optimizer ParamUty    NA    NA     Inf\n17:                tunethreshold.log_level ParamUty    NA    NA     Inf\n```\n:::\n:::\n\n\nIf we now use the [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html), it automatically adjusts the `thresholds` during prediction.\n\nNote that we can set [`ResamplingInsample`](https://mlr3.mlr-org.com/reference/mlr_resamplings_insample.html) as a resampling strategy for [`PipeOpLearnerCV`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_learner_cv.html) in order to evaluate\npredictions on the \"training\" data. This is generally not advised, as it might lead to over-fitting\non the thresholds but can significantly reduce runtime.\n\nFinally, we can compare no threshold tuning to the `tunethreshold` approach:\n\n### Comparison of the approaches\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr = benchmark(benchmark_grid(\n  learners = list(no_tuning = lrn(\"classif.rpart\"), internal = l2),\n  tasks = tsk(\"german_credit\"),\n  rsmp(\"cv\", folds = 3L)\n))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr$aggregate(list(msr(\"classif.ce\"), msr(\"classif.fnr\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nr      resample_result       task_id                  learner_id resampling_id iters classif.ce classif.fnr\n1:  1 <ResampleResult[21]> german_credit               classif.rpart            cv     3  0.2760095  0.12723983\n2:  2 <ResampleResult[21]> german_credit classif.rpart.tunethreshold            cv     3  0.2879916  0.04485325\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}