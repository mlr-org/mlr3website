{
  "hash": "2207da082b3eae620cf9a13088b31daf",
  "result": {
    "markdown": "---\ntitle: Practical Tuning Series - Tuning and Parallel Processing\ndescription: |\n  Run various jobs in mlr3 in parallel.\ncategories:\n  - tuning\n  - resampling\n  - parallelization\n  - classification\n  - practical tuning series\nauthor:\n  - name: Marc Becker\n  - name: Theresa Ullmann\n  - name: Michel Lang\n  - name: Bernd Bischl\n  - name: Jakob Richter\n  - name: Martin Binder\ndate: 03-12-2021\naliases:\n  - ../../../gallery/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/index.html\n  - ../../../gallery/series/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/index.html\n---\n\n\n\n\n# Scope\n\nThis is the fourth part of the practical tuning series.\nThe other parts can be found here:\n\n* [Part I - Tune a Support Vector Machine](https://mlr-org.com/gallery/optimization/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/)\n* [Part II - Tune a Preprocessing Pipeline](https://mlr-org.com/gallery/optimization/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline/)\n* [Part III - Build an Automated Machine Learning System](https://mlr-org.com/gallery/optimization/2021-03-11-practical-tuning-series-build-an-automated-machine-learning-system/)\n\nIn this post, we teach how to run various jobs in [mlr3](https://mlr3.mlr-org.com) in parallel.\nThe goal is to map *computational jobs* (e.g. evaluation of one configuration) to a pool of *workers* (usually physical CPU cores, sometimes remote computational nodes) to reduce the run time needed for tuning.\n\n# Prerequisites\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com)  package which pulls in the most important packages for this example.\nAdditionally, make sure you have installed the packages [future](https://cran.r-project.org/package=future) and [future.apply](https://cran.r-project.org/package=future.apply).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\nWe decrease the verbosity of the logger to keep the output clearly represented.\nThe [`lgr`](https://mlr3book.mlr-org.com/logging.html) package is used for logging in all [mlr3](https://mlr3.mlr-org.com) packages.\nThe [mlr3](https://mlr3.mlr-org.com) logger prints the logging messages from the base package, whereas the [bbotk](https://bbotk.mlr-org.com)  logger is responsible for logging messages from the optimization packages (e.g. [mlr3tuning](https://mlr3tuning.mlr-org.com) ).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\n# Parallel Backend\n\nThe workers are specified by the parallel backend which orchestrates starting up, shutting down, and communication with the workers.\nOn a single machine, `multisession` and `multicore` are common backends.\nThe `multisession` backend spawns new background R processes. It is available on all platforms.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multisession\")\n```\n:::\n\n\nThe `multicore` backend uses forked R processes which allows the workers to access R objects in a shared memory.\nThis reduces the overhead since R objects are only copied in memory if they are modified.\nUnfortunately, forking processes is not supported on Windows and when running R from within RStudio.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multicore\")\n```\n:::\n\n\nBoth backends support the `workers` argument that specifies the number of used cores.\n\nUse this code if your code should run with the `multicore` backend when possible.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nif (future::supportsMulticore()) {\n  future::plan(future::multicore)\n} else {\n  future::plan(future::multisession)\n}\n```\n:::\n\n\n# Resampling\n\nThe [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) and [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) functions in [mlr3](https://mlr3.mlr-org.com) can be executed in parallel.\nThe parallelization is triggered by simply declaring a plan via [`future::plan()`](https://www.rdocumentation.org/packages/future/topics/plan).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multisession\")\n\ntask = tsk(\"pima\")\nlearner = lrn(\"classif.rpart\") # classification tree\nresampling = rsmp(\"cv\", folds = 3)\n\nresample(task, learner, resampling)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResampleResult> of 3 iterations\n* Task: pima\n* Learner: classif.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n```\n:::\n:::\n\n\nThe 3-fold cross-validation gives us 3 jobs since each resampling iteration is executed in parallel.\n\nThe [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) function  accepts  a design of experiments as input where each experiment is defined as a combination of a task, a learner, and a resampling strategy.\nFor each experiment, resampling is performed.\nThe nested loop over experiments and resampling iterations is flattened so that all resampling iterations of all experiments can be executed in parallel.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multisession\")\n\ntasks = list(tsk(\"pima\"), tsk(\"iris\"))\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\n\ngrid = benchmark_grid(tasks, learner, resampling)\n\nbenchmark(grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<BenchmarkResult> of 6 rows with 2 resampling runs\n nr task_id    learner_id resampling_id iters warnings errors\n  1    pima classif.rpart            cv     3        0      0\n  2    iris classif.rpart            cv     3        0      0\n```\n:::\n:::\n\n\nThe 2 experiments and the 3-fold cross-validation result in 6 jobs which are executed in parallel.\n\n# Tuning\n\nThe [mlr3tuning](https://mlr3tuning.mlr-org.com) package internally calls [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) during tuning.\nIf the tuner is capable of suggesting multiple configurations per iteration (such as random search,  grid search,  or hyperband), these configurations  represent  individual  experiments,  and  the  loop  flattening of [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) is triggered.\nE.g., all resampling iterations of all hyperparameter configurations on a grid can be executed in parallel.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multisession\")\n\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$values$cp = to_tune(0.001, 0.1)\nlearner$param_set$values$minsplit = to_tune(1, 10)\n\ninstance = tune(\n  method = \"random_search\",\n  task = tsk(\"pima\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10,\n  batch_size = 5 # random search suggests 5 configurations per batch\n)\n```\n:::\n\n\nThe batch size of 5 and the 3-fold cross-validation gives us 15 jobs.\nThis is done twice because of the limit of 10 evaluations in total.\n\n# Nested Resampling\n\nNested resampling results in two nested resampling loops.\nFor this, an [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) is passed to [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) or [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html).\nWe can choose different parallelization backends for the inner and outer resampling loop, respectively.\nWe just have to pass a list of backends.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Runs the outer loop in parallel and the inner loop sequentially\nfuture::plan(list(\"multisession\", \"sequential\"))\n\n# Runs the outer loop sequentially and the inner loop in parallel\nfuture::plan(list(\"sequential\", \"multisession\"))\n\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$values$cp = to_tune(0.001, 0.1)\nlearner$param_set$values$minsplit = to_tune(1, 10)\n\nrr = tune_nested(\n  method = \"random_search\",\n  task = tsk(\"pima\"),\n  learner = learner,\n  inner_resampling = rsmp (\"cv\", folds = 3),\n  outer_resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10,\n  batch_size = 5\n)\n```\n:::\n\n\nWhile nesting real parallelization backends is often unintended and causes unnecessary overhead, it is useful in some distributed computing setups. It can be achieved with future by forcing a fixed number of workers for each loop.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Runs both loops in parallel\nfuture::plan(list(future::tweak(\"multisession\", workers = 2),\n                  future::tweak(\"multisession\", workers = 4)))\n```\n:::\n\n\nThis example would run on 8 cores (`= 2 * 4`) on the local machine.\n\n# Resources\n\nThe [mlr3book](https://mlr3book.mlr-org.com/) includes a chapters on [parallelization](https://mlr3book.mlr-org.com/parallelization.html).\nThe [mlr3cheatsheets](https://cheatsheets.mlr-org.com/) contain frequently used commands and workflows of mlr3.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}