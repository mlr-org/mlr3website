{
  "hash": "c483d0ce477dd1b115fdccb3c728bd3a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Practical Tuning Series - Tuning and Parallel Processing\ndescription: |\n  Run various jobs in mlr3 in parallel.\ncategories:\n  - tuning\n  - resampling\n  - parallelization\n  - classification\n  - practical tuning series\nauthor:\n  - name: Marc Becker\n  - name: Theresa Ullmann\n  - name: Michel Lang\n  - name: Bernd Bischl\n  - name: Jakob Richter\n  - name: Martin Binder\ndate: 03-12-2021\naliases:\n  - ../../../gallery/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/index.html\n  - ../../../gallery/series/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/index.html\n---\n\n\n\n\n\n\n# Scope\n\nThis is the fourth part of the practical tuning series.\nThe other parts can be found here:\n\n* [Part I - Tune a Support Vector Machine](/gallery/optimization/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/)\n* [Part II - Tune a Preprocessing Pipeline](/gallery/optimization/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline/)\n* [Part III - Build an Automated Machine Learning System](/gallery/optimization/2021-03-11-practical-tuning-series-build-an-automated-machine-learning-system/)\n\nIn this post, we teach how to run various jobs in [mlr3](https://mlr3.mlr-org.com) in parallel.\nThe goal is to map *computational jobs* (e.g. evaluation of one configuration) to a pool of *workers* (usually physical CPU cores, sometimes remote computational nodes) to reduce the run time needed for tuning.\n\n# Prerequisites\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com)  package which pulls in the most important packages for this example.\nAdditionally, make sure you have installed the packages [future](https://cran.r-project.org/package=future) and [future.apply](https://cran.r-project.org/package=future.apply).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\n\nWe decrease the verbosity of the logger to keep the output clearly represented.\nThe [`lgr`](https://mlr3book.mlr-org.com/logging.html) package is used for logging in all [mlr3](https://mlr3.mlr-org.com) packages.\nThe [mlr3](https://mlr3.mlr-org.com) logger prints the logging messages from the base package, whereas the [bbotk](https://bbotk.mlr-org.com)  logger is responsible for logging messages from the optimization packages (e.g. [mlr3tuning](https://mlr3tuning.mlr-org.com) ).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\n\n# Parallel Backend\n\nThe workers are specified by the parallel backend which orchestrates starting up, shutting down, and communication with the workers.\nOn a single machine, `multisession` and `multicore` are common backends.\nThe `multisession` backend spawns new background R processes. It is available on all platforms.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multisession\")\n```\n:::\n\n\n\nThe `multicore` backend uses forked R processes which allows the workers to access R objects in a shared memory.\nThis reduces the overhead since R objects are only copied in memory if they are modified.\nUnfortunately, forking processes is not supported on Windows and when running R from within RStudio.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multicore\")\n```\n:::\n\n\n\nBoth backends support the `workers` argument that specifies the number of used cores.\n\nUse this code if your code should run with the `multicore` backend when possible.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nif (future::supportsMulticore()) {\n  future::plan(future::multicore)\n} else {\n  future::plan(future::multisession)\n}\n```\n:::\n\n\n\n# Resampling\n\nThe [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) and [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) functions in [mlr3](https://mlr3.mlr-org.com) can be executed in parallel.\nThe parallelization is triggered by simply declaring a plan via [`future::plan()`](https://www.rdocumentation.org/packages/future/topics/plan).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multisession\")\n\ntask = tsk(\"pima\")\nlearner = lrn(\"classif.rpart\") # classification tree\nresampling = rsmp(\"cv\", folds = 3)\n\nresample(task, learner, resampling)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ResampleResult> with 3 resampling iterations\n task_id    learner_id resampling_id iteration     prediction_test warnings errors\n    pima classif.rpart            cv         1 <PredictionClassif>        0      0\n    pima classif.rpart            cv         2 <PredictionClassif>        0      0\n    pima classif.rpart            cv         3 <PredictionClassif>        0      0\n```\n\n\n:::\n:::\n\n\n\nThe 3-fold cross-validation gives us 3 jobs since each resampling iteration is executed in parallel.\n\nThe [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) function  accepts  a design of experiments as input where each experiment is defined as a combination of a task, a learner, and a resampling strategy.\nFor each experiment, resampling is performed.\nThe nested loop over experiments and resampling iterations is flattened so that all resampling iterations of all experiments can be executed in parallel.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multisession\")\n\ntasks = list(tsk(\"pima\"), tsk(\"iris\"))\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 3)\n\ngrid = benchmark_grid(tasks, learner, resampling)\n\nbenchmark(grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<BenchmarkResult> of 6 rows with 2 resampling runs\n nr task_id    learner_id resampling_id iters warnings errors\n  1    pima classif.rpart            cv     3        0      0\n  2    iris classif.rpart            cv     3        0      0\n```\n\n\n:::\n:::\n\n\n\nThe 2 experiments and the 3-fold cross-validation result in 6 jobs which are executed in parallel.\n\n# Tuning\n\nThe [mlr3tuning](https://mlr3tuning.mlr-org.com) package internally calls [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) during tuning.\nIf the tuner is capable of suggesting multiple configurations per iteration (such as random search,  grid search,  or hyperband), these configurations  represent  individual  experiments,  and  the  loop  flattening of [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) is triggered.\nE.g., all resampling iterations of all hyperparameter configurations on a grid can be executed in parallel.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfuture::plan(\"multisession\")\n\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$values$cp = to_tune(0.001, 0.1)\nlearner$param_set$values$minsplit = to_tune(1, 10)\n\ninstance = tune(\n  tuner = tnr(\"random_search\", batch_size = 5), # random search suggests 5 configurations per batch\n  task = tsk(\"pima\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10\n)\n```\n:::\n\n\n\nThe batch size of 5 and the 3-fold cross-validation gives us 15 jobs.\nThis is done twice because of the limit of 10 evaluations in total.\n\n# Nested Resampling\n\nNested resampling results in two nested resampling loops.\nFor this, an [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) is passed to [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) or [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html).\nWe can choose different parallelization backends for the inner and outer resampling loop, respectively.\nWe just have to pass a list of backends.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Runs the outer loop in parallel and the inner loop sequentially\nfuture::plan(list(\"multisession\", \"sequential\"))\n\n# Runs the outer loop sequentially and the inner loop in parallel\nfuture::plan(list(\"sequential\", \"multisession\"))\n\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$values$cp = to_tune(0.001, 0.1)\nlearner$param_set$values$minsplit = to_tune(1, 10)\n\nrr = tune_nested(\n  tuner = tnr(\"random_search\", batch_size = 5), # random search suggests 5 configurations per batch\n  task = tsk(\"pima\"),\n  learner = learner,\n  inner_resampling = rsmp (\"cv\", folds = 3),\n  outer_resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10\n)\n```\n:::\n\n\n\nWhile nesting real parallelization backends is often unintended and causes unnecessary overhead, it is useful in some distributed computing setups. It can be achieved with future by forcing a fixed number of workers for each loop.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Runs both loops in parallel\nfuture::plan(list(future::tweak(\"multisession\", workers = 2),\n                  future::tweak(\"multisession\", workers = 4)))\n```\n:::\n\n\n\nThis example would run on 8 cores (`= 2 * 4`) on the local machine.\n\n# Resources\n\nThe [mlr3book](https://mlr3book.mlr-org.com/) includes a chapters on [parallelization](https://mlr3book.mlr-org.com/parallelization.html).\nThe [mlr3cheatsheets](https://cheatsheets.mlr-org.com/) contain frequently used commands and workflows of mlr3.\n\n\n# Session Information\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessioninfo::session_info(info = \"packages\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   backports           1.5.0      2024-05-23 [1] CRAN (R 4.4.1)\n   bbotk               1.1.1      2024-10-15 [1] CRAN (R 4.4.1)\n   checkmate           2.3.2      2024-07-29 [1] CRAN (R 4.4.1)\n P class               7.3-22     2023-05-03 [?] CRAN (R 4.4.0)\n   cli                 3.6.3      2024-06-21 [1] CRAN (R 4.4.1)\n   clue                0.3-65     2023-09-23 [1] CRAN (R 4.4.1)\n P cluster             2.1.6      2023-12-01 [?] CRAN (R 4.4.0)\n P codetools           0.2-20     2024-03-31 [?] CRAN (R 4.4.0)\n   colorspace          2.1-1      2024-07-26 [1] CRAN (R 4.4.1)\n   crayon              1.5.3      2024-06-20 [1] CRAN (R 4.4.1)\n   data.table        * 1.16.2     2024-10-10 [1] CRAN (R 4.4.1)\n   DEoptimR            1.1-3      2023-10-07 [1] CRAN (R 4.4.1)\n   digest              0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n   diptest             0.77-1     2024-04-10 [1] CRAN (R 4.4.1)\n   dplyr               1.1.4      2023-11-17 [1] CRAN (R 4.4.1)\n   evaluate            1.0.1      2024-10-10 [1] CRAN (R 4.4.1)\n   fansi               1.0.6      2023-12-08 [1] CRAN (R 4.4.1)\n   fastmap             1.2.0      2024-05-15 [1] CRAN (R 4.4.1)\n   flexmix             2.3-19     2023-03-16 [1] CRAN (R 4.4.1)\n   fpc                 2.2-13     2024-09-24 [1] CRAN (R 4.4.1)\n   future            * 1.34.0     2024-07-29 [1] CRAN (R 4.4.1)\n   future.apply        1.11.2     2024-03-28 [1] CRAN (R 4.4.1)\n   generics            0.1.3      2022-07-05 [1] CRAN (R 4.4.1)\n   ggplot2             3.5.1      2024-04-23 [1] CRAN (R 4.4.1)\n   globals             0.16.3     2024-03-08 [1] CRAN (R 4.4.1)\n   glue                1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n   gtable              0.3.5      2024-04-22 [1] CRAN (R 4.4.1)\n   htmltools           0.5.8.1    2024-04-04 [1] CRAN (R 4.4.1)\n   htmlwidgets         1.6.4      2023-12-06 [1] CRAN (R 4.4.1)\n   jsonlite            1.8.9      2024-09-20 [1] CRAN (R 4.4.1)\n   kernlab             0.9-33     2024-08-13 [1] CRAN (R 4.4.1)\n   knitr               1.48       2024-07-07 [1] CRAN (R 4.4.1)\n P lattice             0.22-5     2023-10-24 [?] CRAN (R 4.3.3)\n   lgr                 0.4.4      2022-09-05 [1] CRAN (R 4.4.1)\n   lifecycle           1.0.4      2023-11-07 [1] CRAN (R 4.4.1)\n   listenv             0.9.1      2024-01-29 [1] CRAN (R 4.4.1)\n   magrittr            2.0.3      2022-03-30 [1] CRAN (R 4.4.1)\n P MASS                7.3-61     2024-06-13 [?] CRAN (R 4.4.1)\n   mclust              6.1.1      2024-04-29 [1] CRAN (R 4.4.1)\n   mlr3              * 0.21.1     2024-10-18 [1] CRAN (R 4.4.1)\n   mlr3cluster         0.1.10     2024-10-03 [1] CRAN (R 4.4.1)\n   mlr3data            0.7.0      2023-06-29 [1] CRAN (R 4.4.1)\n   mlr3extralearners   0.9.0-9000 2024-10-18 [1] Github (mlr-org/mlr3extralearners@a622524)\n   mlr3filters         0.8.0      2024-04-10 [1] CRAN (R 4.4.1)\n   mlr3fselect       * 1.1.1.9000 2024-10-18 [1] Github (mlr-org/mlr3fselect@e917a02)\n   mlr3hyperband       0.6.0      2024-06-29 [1] CRAN (R 4.4.1)\n   mlr3learners        0.7.0      2024-06-28 [1] CRAN (R 4.4.1)\n   mlr3mbo             0.2.6      2024-10-16 [1] CRAN (R 4.4.1)\n   mlr3measures        1.0.0      2024-09-11 [1] CRAN (R 4.4.1)\n   mlr3misc            0.15.1     2024-06-24 [1] CRAN (R 4.4.1)\n   mlr3pipelines       0.7.0      2024-09-24 [1] CRAN (R 4.4.1)\n   mlr3tuning          1.0.2      2024-10-14 [1] CRAN (R 4.4.1)\n   mlr3tuningspaces    0.5.1      2024-06-21 [1] CRAN (R 4.4.1)\n   mlr3verse         * 0.3.0      2024-06-30 [1] CRAN (R 4.4.1)\n   mlr3viz             0.9.0      2024-07-01 [1] CRAN (R 4.4.1)\n   mlr3website       * 0.0.0.9000 2024-10-18 [1] Github (mlr-org/mlr3website@20d1ddf)\n   modeltools          0.2-23     2020-03-05 [1] CRAN (R 4.4.1)\n   munsell             0.5.1      2024-04-01 [1] CRAN (R 4.4.1)\n P nnet                7.3-19     2023-05-03 [?] CRAN (R 4.3.3)\n   palmerpenguins      0.1.1      2022-08-15 [1] CRAN (R 4.4.1)\n   paradox             1.0.1      2024-07-09 [1] CRAN (R 4.4.1)\n   parallelly          1.38.0     2024-07-27 [1] CRAN (R 4.4.1)\n   pillar              1.9.0      2023-03-22 [1] CRAN (R 4.4.1)\n   pkgconfig           2.0.3      2019-09-22 [1] CRAN (R 4.4.1)\n   prabclus            2.3-4      2024-09-24 [1] CRAN (R 4.4.1)\n   R6                  2.5.1      2021-08-19 [1] CRAN (R 4.4.1)\n   Rcpp                1.0.13     2024-07-17 [1] CRAN (R 4.4.1)\n   renv                1.0.11     2024-10-12 [1] CRAN (R 4.4.1)\n   rlang               1.1.4      2024-06-04 [1] CRAN (R 4.4.1)\n   rmarkdown           2.28       2024-08-17 [1] CRAN (R 4.4.1)\n   robustbase          0.99-4-1   2024-09-27 [1] CRAN (R 4.4.1)\n P rpart               4.1.23     2023-12-05 [?] CRAN (R 4.4.0)\n   scales              1.3.0      2023-11-28 [1] CRAN (R 4.4.1)\n   sessioninfo         1.2.2      2021-12-06 [1] CRAN (R 4.4.1)\n   spacefillr          0.3.3      2024-05-22 [1] CRAN (R 4.4.1)\n   stringi             1.8.4      2024-05-06 [1] CRAN (R 4.4.1)\n   tibble              3.2.1      2023-03-20 [1] CRAN (R 4.4.1)\n   tidyselect          1.2.1      2024-03-11 [1] CRAN (R 4.4.1)\n   utf8                1.2.4      2023-10-22 [1] CRAN (R 4.4.1)\n   uuid                1.2-1      2024-07-29 [1] CRAN (R 4.4.1)\n   vctrs               0.6.5      2023-12-01 [1] CRAN (R 4.4.1)\n   withr               3.0.1      2024-07-31 [1] CRAN (R 4.4.1)\n   xfun                0.48       2024-10-03 [1] CRAN (R 4.4.1)\n   yaml                2.3.10     2024-07-26 [1] CRAN (R 4.4.1)\n\n [1] /home/marc/repositories/mlr3website/mlr-org/renv/library/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu\n [2] /home/marc/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.4/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}