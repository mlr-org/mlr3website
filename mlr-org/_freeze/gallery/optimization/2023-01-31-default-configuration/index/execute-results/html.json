{
  "hash": "c58a61042efe94f9a739cf338b6e0369",
  "result": {
    "markdown": "---\ntitle: \"Default Hyperparameter Configuration\"\ndescription: |\n  Run the default hyperparameter configuration of learners as a baseline.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-01-31\nbibliography: bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 10\n      datatable.print.class: FALSE\n      datatable.print.keys: FALSE\n      datatable.print.trunc.cols: TRUE\n---\n\n\n\n\n\n\n# Scope\n\nThe predictive performance of modern machine learning algorithms is highly dependent on the choice of their hyperparameter configuration.\nOptions for setting hyperparameters are tuning, manual selection by the user, and using the default configuration of the algorithm.\nThe default configurations are chosen to work with a wide range of data sets but they usually do not achieve the best predictive performance.\nWhen tuning a learner in mlr3, we can run the default configuration as a baseline.\nSeeing how well it performs will tell us whether tuning pays off.\nIf the optimized configurations perform worse, we could expand the search space or try a different optimization algorithm.\nOf course, it could also be that tuning on the given data set is simply not worth it.\n\n@probst_tunability_2019 studied the tunability of machine learning algorithms.\nThey found that the tunability of algorithms varies widely.\nAlgorithms like glmnet and XGBoost are highly tunable, while algorithms like random forests work well with their default configuration.\nThe highly tunable algorithms should thus beat their baselines more easily with optimized hyperparameters.\nIn this article, we will tune the hyperparameters of a random forest and compare the performance of the default configuration with the optimized configurations.\n\n# Example\n\nWe tune the hyperparameters of the [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) on the [`spam`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html) data set.\nThe search space is taken from @bischl_hyperparameter_2021.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n\nlearner = lrn(\"classif.ranger\",\n  mtry.ratio      = to_tune(0, 1),\n  replace         = to_tune(),\n  sample.fraction = to_tune(1e-1, 1),\n  num.trees       = to_tune(1, 2000)\n)\n```\n:::\n\n\nWhen creating the tuning instance, we set `evaluate_default = TRUE` to test the default hyperparameter configuration.\nThe default configuration is evaluated in the first batch of the tuning run.\nThe other batches use the specified tuning method.\nIn this example, they are randomly drawn configurations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  method = tnr(\"random_search\", batch_size = 5),\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp (\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  term_evals = 51,\n  evaluate_default = TRUE\n)\n```\n:::\n\n\nThe default configuration is recorded in the first row of the archive.\nThe other rows contain the results of the random search.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, .(batch_nr, mtry.ratio, replace, sample.fraction, num.trees, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    batch_nr mtry.ratio replace sample.fraction num.trees classif.ce\n 1:        1 0.12280702    TRUE       1.0000000       500 0.04889179\n 2:        2 0.81757154   FALSE       0.8117389      1528 0.06518905\n 3:        2 0.90097848   FALSE       0.9188504       571 0.06975228\n 4:        2 0.65584252    TRUE       0.3145144       681 0.06323338\n 5:        2 0.40363652   FALSE       0.7508936      1807 0.05801825\n---                                                                 \n47:       11 0.71528316    TRUE       0.4398745      1394 0.06127771\n48:       11 0.19136788   FALSE       0.8293552       249 0.04889179\n49:       11 0.09430346   FALSE       0.6233559      1307 0.04889179\n50:       11 0.52643368   FALSE       0.5993606      1403 0.05997392\n51:       11 0.17115160    TRUE       0.3309041       114 0.05867014\n```\n:::\n:::\n\n\nWe plot the performances of the evaluated hyperparameter configurations.\nThe blue line connects the best configuration of each batch.\nWe see that the default configuration already performs well and the optimized configurations can not beat it.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3viz)\n\nautoplot(instance, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2023-01-31-default-configuration-005-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n# Conlcusion\n\nThe time required to test the default configuration is negligible compared to the time required to run the hyperparameter optimization.\nIt gives us a valuable indication of whether our tuning is properly configured.\nRunning the default configuration as a baseline is a good practice that should be used in every tuning run.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}