{
  "hash": "80eeab5107f8a8dc2d686c981d8ff72a",
  "result": {
    "markdown": "---\ntitle: \"Default Hyperparameter Configuration\"\ndescription: |\n  Run the default hyperparameter configuration of learners as a baseline.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-01-31\nbibliography: bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 10\n      datatable.print.class: FALSE\n      datatable.print.keys: FALSE\n      datatable.print.trunc.cols: TRUE\n---\n\n\n\n\n\n\n# Scope\n\nThe predictive performance of modern machine learning algorithms is highly dependent on the choice of their hyperparameter configuration.\nOptions for setting hyperparameters are tuning, manual selection by the user, and using the default configuration of the algorithm.\nThe default configurations are chosen to work with a wide range of data sets but they usually do not achieve the best predictive performance.\nWhen tuning a learner in mlr3, we can run the default configuration as a baseline.\nSeeing how well it performs will tell us whether tuning pays off.\nIf the optimized configurations perform worse, we could expand the search space or try a different optimization algorithm.\nOf course, it could also be that tuning on the given data set is simply not worth it.\n\n@probst_tunability_2019 studied the tunability of machine learning algorithms.\nThey found that the tunability of algorithms varies widely.\nAlgorithms like glmnet and XGBoost are highly tunable, while algorithms like random forests work well with their default configuration.\nThe highly tunable algorithms should thus beat their baselines more easily with optimized hyperparameters.\nIn this article, we will tune the hyperparameters of a random forest and compare the performance of the default configuration with the optimized configurations.\n\n# Example\n\nWe tune the hyperparameters of the [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) on the [`spam`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html) data set.\nThe search space is taken from @bischl_hyperparameter_2021.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n\nlearner = lrn(\"classif.ranger\",\n  mtry.ratio      = to_tune(0, 1),\n  replace         = to_tune(),\n  sample.fraction = to_tune(1e-1, 1),\n  num.trees       = to_tune(1, 2000)\n)\n```\n:::\n\n\nWhen creating the tuning instance, we set `evaluate_default = TRUE` to test the default hyperparameter configuration.\nThe default configuration is evaluated in the first batch of the tuning run.\nThe other batches use the specified tuning method.\nIn this example, they are randomly drawn configurations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  tuner = tnr(\"random_search\", batch_size = 5),\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp (\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  term_evals = 51,\n  evaluate_default = TRUE\n)\n```\n:::\n\n\nThe default configuration is recorded in the first row of the archive.\nThe other rows contain the results of the random search.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, .(batch_nr, mtry.ratio, replace, sample.fraction, num.trees, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    batch_nr  mtry.ratio replace sample.fraction num.trees classif.ce\n 1:        1 0.122807018    TRUE       1.0000000       500 0.04954368\n 2:        2 0.285388074    TRUE       0.1794772       204 0.06584094\n 3:        2 0.097424099   FALSE       0.9475526      1441 0.04237288\n 4:        2 0.008888587   FALSE       0.3216562      1868 0.08409387\n 5:        2 0.335543330    TRUE       0.8122653       106 0.05345502\n---                                                                  \n47:       11 0.788995735   FALSE       0.3692454       344 0.06258149\n48:       11 0.459305038    TRUE       0.3153485      1354 0.06258149\n49:       11 0.220334408    TRUE       0.9357554       817 0.05345502\n50:       11 0.868385877    TRUE       0.6743246      1040 0.06127771\n51:       11 0.015417312   FALSE       0.5627943      1836 0.08213820\n```\n:::\n:::\n\n\nWe plot the performances of the evaluated hyperparameter configurations.\nThe blue line connects the best configuration of each batch.\nWe see that the default configuration already performs well and the optimized configurations can not beat it.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3viz)\n\nautoplot(instance, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2023-01-31-default-configuration-005-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n# Conlcusion\n\nThe time required to test the default configuration is negligible compared to the time required to run the hyperparameter optimization.\nIt gives us a valuable indication of whether our tuning is properly configured.\nRunning the default configuration as a baseline is a good practice that should be used in every tuning run.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}