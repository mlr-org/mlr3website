{
  "hash": "20631e8c102c654862f707cbeb4d227c",
  "result": {
    "markdown": "---\ntitle: \"Hyperband Series I - XGBoost\"\ndescription: |\n  Optimize the hyperparameters of an XGBoost model with Hyperband.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2022-12-01\nbibliography: bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n---\n\n\n\n\n\n\n# Scope\n\nIncreasingly large data sets and search spaces make hyperparameter optimization a very time-consuming task.\nHyperband [@li_2018] solves this problem by approximating the performance of a configuration on a small subset of the training data, with just a few training epochs in a neural network, or with only a small number of trees in a gradient-boosting model.\nAfter sampling random configurations, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones.\nThis type of optimization is called *multi-fidelity* optimization.\nThe fidelity parameter is part of the search space and influences the computational cost of fitting a model.\nIn the context of hyperband, the fidelity parameter is often called the budget parameter.\nIn this post, we will optimize XGBoost and use the number of boosting iterations as the fidelity parameter.\nThe time to train XGBoost increases with the number of boosting iterations.\n\nWe assume that you are already familiar with tuning in the mlr3 ecosystem.\nIf not, you should start with the [Hyperparameter Optimization on the Palmer Penguins Data Set](gallery/optimization/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins) post.\n\n# Hyperband\n\nHyperband is an advancement of the Successive Halving algorithm by @jamieson_2016.\nSuccessive Halving is initialized with the number of starting configurations $n$, the proportion of configurations discarded in each stage $\\eta$, and the minimum $r{_{min}}$ and maximum $r{_{max}}$ budget of a single evaluation.\nThe algorithm starts by sampling $n$ random configurations and allocating the minimum budget $r{_{min}}$ to them.\nThe configurations are evaluated and $\\frac{1}{\\eta}$ of the worst-performing configurations are discarded.\nThe remaining configurations are promoted to the next stage and evaluated on a larger budget.\nThis continues until one or more configurations are evaluated on the maximum budget $r{_{max}}$.\nThe number of stages is calculated so that each stage consumes approximately the same budget.\nThis sometimes results in the minimum budget having to be slightly adjusted by the algorithm.\nSuccessive Halving has the disadvantage that is not clear whether we should choose a large $n$ and try many configurations on a small budget or choose a small $n$ and train more configurations on the full budget.\n\nHyperband solves this problem by running Successive Halving with different numbers of stating configurations.\nThe algorithm is initialized with the same parameters as Successive Halving but without $n$.\nEach run of Successive Halving is called a bracket and starts with a different budget $r{_{i}}$.\nA smaller starting budget means that more configurations can be tried out.\nThe most explorative bracket $s = 3$ allocated the minimum budget $r{_{min}}$.\nThe next bracket increases the starting budget by a factor of $\\eta$.\nIn each bracket, the starting budget increases further until the last bracket $s = 0$ essentially performs a random search with the full budget $r{_{max}}$.\nThe number of configurations in the base stages is calculated so that each bracket uses approximately the same amount of budget.\n\n\n|     |     |           |   $s = 3$ |     |           |   $s = 2$ |     |           |   $s = 1$ |     |           |   $s = 0$ |\n| --: | --- | --------: | --------: | --- | --------: | --------: | --- | --------: | --------: | --- | --------: | --------: |\n| $i$ |     | $n{_{i}}$ | $r{_{i}}$ |     | $n{_{i}}$ | $r{_{i}}$ |     | $n{_{i}}$ | $r{_{i}}$ |     | $n{_{i}}$ | $r{_{i}}$ |\n|   0 |     |         8 |         1 |     |         6 |         2 |     |         4 |         4 |     |         8 |         4 |\n|   1 |     |         4 |         2 |     |         3 |         4 |     |         2 |         8 |     |           |           |\n|   2 |     |         2 |         4 |     |         1 |         8 |     |           |           |     |           |           |\n|   3 |     |         1 |         8 |     |           |           |     |           |           |     |           |           |\n\n: Hyperband schedule with $\\eta = 2$ , $r{_{min}} = 1$ and $r{_{max}} = 8$\n\n\n# Hyperparameter Optimization\n\nIn this practical example, we will optimize the hyperparameters of XGBoost on the [`Spam`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html) data set.\nWe begin by loading the [`XGBoost learner.`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.xgboost\")\n```\n:::\n\n\nThe next thing we do is define the search space.\nThe `nrounds` parameter controls the number of boosting iterations.\nWe set a range from 16 to 128 boosting iterations.\nThis is used as $r{_{min}}$ and $r{_{max}}$ by the Hyperband algorithm.\nWe need to tag the parameter with `\"budget\"` to identify it as a fidelity parameter.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$param_set$set_values(\n  nrounds           = to_tune(p_int(16, 128, tags = \"budget\")),\n  eta               = to_tune(1e-4, 1, logscale = TRUE),\n  max_depth         = to_tune(1, 20),\n  colsample_bytree  = to_tune(1e-1, 1),\n  colsample_bylevel = to_tune(1e-1, 1),\n  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),\n  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),\n  subsample         = to_tune(1e-1, 1)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = ti(\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:classif.xgboost_on_spam>\n* Search Space:\n                  id    class     lower      upper nlevels\n1:           nrounds ParamInt 16.000000 128.000000     113\n2:               eta ParamDbl -9.210340   0.000000     Inf\n3:         max_depth ParamInt  1.000000  20.000000      20\n4:  colsample_bytree ParamDbl  0.100000   1.000000     Inf\n5: colsample_bylevel ParamDbl  0.100000   1.000000     Inf\n6:            lambda ParamDbl -6.907755   6.907755     Inf\n7:             alpha ParamDbl -6.907755   6.907755     Inf\n8:         subsample ParamDbl  0.100000   1.000000     Inf\n* Terminator: <TerminatorNone>\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3hyperband\")\n\ntuner = tnr(\"hyperband\", eta = 2)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-7dc74ca6e554b9a2f508\" class=\"reactable html-widget\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-7dc74ca6e554b9a2f508\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"budget\":[16,32,32,64,64,64,128,128,128,128],\"bracket\":[3,3,2,3,2,1,3,2,1,0],\"stage\":[0,1,0,2,1,0,3,2,1,0],\"n\":[8,4,6,2,3,4,1,1,2,4]},\"columns\":[{\"accessor\":\"budget\",\"name\":\"Data Set Size\",\"type\":\"numeric\",\"cell\":[{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"       16\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"12.5%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"       32\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"       32\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"       64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"       64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"       64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]}]},{\"accessor\":\"bracket\",\"name\":\"Bracket\",\"type\":\"numeric\"},{\"accessor\":\"stage\",\"name\":\"Stage\",\"type\":\"numeric\"},{\"accessor\":\"n\",\"name\":\"# Configruations\",\"type\":\"numeric\"}],\"pagination\":false,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"20f55057ce45d899eecab55262bb2283\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nrounds       eta max_depth colsample_bytree colsample_bylevel    lambda     alpha subsample learner_param_vals\n1:     128 -1.680228        17         0.356807         0.1188155 0.4784036 -5.973041  0.923161         <list[11]>\n    x_domain classif.ce\n1: <list[8]> 0.04889179\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"../../../site_libs/react-17.0.0/react.min.js\"></script>\n<script src=\"../../../site_libs/react-17.0.0/react-dom.min.js\"></script>\n<script src=\"../../../site_libs/reactwidget-1.0.0/react-tools.js\"></script>\n<script src=\"../../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<script src=\"../../../site_libs/reactable-binding-0.3.0/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}