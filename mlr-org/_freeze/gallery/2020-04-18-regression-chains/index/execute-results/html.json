{
  "hash": "0b113573d41ef465b1405f36d04c1c76",
  "result": {
    "markdown": "---\ntitle: Regression Chains\ncategories:\n  - regression\n  - mlr3pipelines\nauthor:\n  - name: Lennart Schneider\ndate: 2020-04-18\ndescription: |\n  We show how to use mlr3pipelines to do regression chains.\nbibliography: biblio.bib\nimage: thumbnail.png\n---\n\n\n\n\nIn this tutorial we demonstrate how to use [mlr3pipelines](https://mlr3pipelines.mlr-org.com) to handle multi-target regression by arranging regression models as a chain, i.e., creating a linear sequence of regression models.\n\n# Regression Chains\n\nIn a simple regression chain, regression models are arranged in a linear sequence.\nHere, the first model will use the input to predict a single output and the second model will use the input and the prediction output of the first model to make its own prediction and so on.\nFor more details, see e.g. @spyromitros2016.\n\n# Before you start\n\nThe following sections describe an approach towards working with [`tasks`](https://mlr3.mlr-org.com/reference/Task.html) that have multiple targets.\nE.g., in the example below, we have three target variables $y_{1}$ to $y_{3}$.\nThis type of [`Task`](https://mlr3.mlr-org.com/reference/Task.html) can be created via the [mlr3multioutput](https://github.com/mlr-org/mlr3multioutput/) package (currently under development) in the future.\n`mlr3multioutput` will also offer simple chaining approaches as pre-built pipelines (so called [`ppl`](https://mlr3pipelines.mlr-org.com/reference/ppl.html)s).\nThe current goal of this post is to show how such modeling steps can be written as a relatively small amount of pipeline steps and how such steps can be put together.\nWriting pipelines with such steps allows for great flexibility in modeling more complicated scenarios such as the ones described below.\n\n# Prerequisites\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com) package which pulls in the most important packages for this example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: mlr3\n```\n:::\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n```\n:::\n\n\n# Data\n\nIn the following, we rely on some toy data.\nWe simulate 100 responses to three target variables, $y_{1}$, $y_{2}$, and $y_{3}$ following a multivariate normal distribution with a mean and covariance matrix of:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(mvtnorm)\nset.seed(2409)\nn = 100\n(mean <- c(y1 = 1, y2 = 2, y3 = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ny1 y2 y3 \n 1  2  3 \n```\n:::\n\n```{.r .cell-code}\n(sigma <- matrix(c(1, -0.5, 0.25, -0.5, 1, -0.25, 0.25, -0.25, 1),\n  nrow = 3, ncol = 3, byrow = TRUE\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      [,1]  [,2]  [,3]\n[1,]  1.00 -0.50  0.25\n[2,] -0.50  1.00 -0.25\n[3,]  0.25 -0.25  1.00\n```\n:::\n\n```{.r .cell-code}\nY = rmvnorm(n, mean = mean, sigma = sigma)\n```\n:::\n\n\nThe feature variables $x_{1}$, and $x_{2}$ are simulated as follows: $x_{1}$ is simply given by $y_{1}$ and an independent normally distributed error term and $x_{2}$ is given by $y_{2}$ and an independent normally distributed error term.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx1 = Y[, 1] + rnorm(n, sd = 0.1)\nx2 = Y[, 2] + rnorm(n, sd = 0.1)\n```\n:::\n\n\nThe final data is given as:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata = as.data.table(cbind(Y, x1, x2))\nstr(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClasses 'data.table' and 'data.frame':\t100 obs. of  5 variables:\n $ y1: num  0.681 1.836 0.355 1.783 0.974 ...\n $ y2: num  2.33 1.735 3.126 0.691 1.573 ...\n $ y3: num  3.19 3.14 2.74 4.31 2.77 ...\n $ x1: num  0.788 1.754 0.174 1.844 1.05 ...\n $ x2: num  2.336 1.665 2.967 0.651 1.634 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n```\n:::\n:::\n\n\nThis simulates a situation where we have multiple target variables that are correlated with each other, such that predicting them along with each other can improve the resulting prediction model.\nAs a real-world example for such a situation, consider e.g. hospital data, where time spent in the ICU (not known a priori) heavily influences the cost incurred by a patient's treatment.\n\n# 3D Visualization of the Data\n\nIf you feel confident to already have a good feeling of the data, feel free to skip this section.\nIf not, you can use the [rgl](https://cran.r-project.org/package=rgl) package to play around with the following four 3D plots with either the feature variables or $y_{1}$ and $y_{2}$ on the x- and y-axis and the target variables on the respective z-axes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rgl)\ncolfun = colorRampPalette(c(\"#161B1D\", \"#ADD8E6\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsetorder(data, y1)\nplot3d(data$x1, data$x2, data$y1,\n  xlab = \"x1\", ylab = \"x2\", zlab = \"y1\",\n  type = \"s\", radius = 0.1, col = colfun(n)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsetorder(data, y2)\nplot3d(data$x1, data$x2, data$y2,\n  xlab = \"x1\", ylab = \"x2\", zlab = \"y2\",\n  type = \"s\", radius = 0.1, col = colfun(n)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsetorder(data, y3)\nplot3d(data$x1, data$x2, data$y3,\n  xlab = \"x1\", ylab = \"x2\", zlab = \"y3\",\n  type = \"s\", radius = 0.1, col = colfun(n)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsetorder(data, y3)\nplot3d(data$y1, data$y2, data$y3,\n  xlab = \"y1\", ylab = \"y2\", zlab = \"y3\",\n  type = \"s\", radius = 0.1, col = colfun(n)\n)\n```\n:::\n\n\n# Building the Pipeline\n\nIn our regression chain, the first model will predict $y_{1}$.\nTherefore, we initialize our [`Task`](https://mlr3.mlr-org.com/reference/Task.html) with respect to this target:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(data, id = \"multiregression\", target = \"y1\")\n```\n:::\n\n\nAs [`Learners`](https://mlr3.mlr-org.com/reference/Learner.html) we will use simple [`linear regression models`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.lm.html).\nOur pipeline building the regression chain then has to do the following:\n\n* Use the input to predict $y_{1}$ within the first learner (i.e., $y_{1} \\sim x_{1} + x_{2}$).\n* Combine the input with the prediction of $y_{1}$, $\\hat{y_{1}}$ and use this to predict $y_{2}$ within the second learner (i.e., $y_{2} \\sim x_{1} + x_{2} + \\hat{y_{1}}$).\n* Combine the input with the prediction of $y_{2}$ and use this to predict $y_{3}$ within the final third learner (i.e., $y_{3} \\sim x_{1} + x_{2} + \\hat{y_{1}} + \\hat{y_{2}}$).\n\nTo combine predictions of a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) with the previous input, we rely on [`PipeOpLearnerCV`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_learner_cv.html) and [`PipeOpNOP`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_nop.html) arranged in parallel via [`gunion()`](https://mlr3pipelines.mlr-org.com/reference/gunion.html) combined via [`PipeOpFeatureUnion`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_featureunion.html).\nTo drop the respective remaining target variables as features, we rely on [`PipeOpColRoles`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_colroles.html).\nThe first step of predicting $y_{1}$ looks like the following:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstep1 = po(\"copy\", outnum = 2, id = \"copy1\") %>>%\n  gunion(list(\n    po(\"colroles\",\n      id = \"drop_y2_y3\",\n      new_role = list(y2 = character(), y3 = character())\n    ) %>>%\n      po(\"learner_cv\", learner = lrn(\"regr.lm\"), id = \"y1_learner\"),\n    po(\"nop\", id = \"nop1\")\n  )) %>>%\n  po(\"featureunion\", id = \"union1\")\nstep1$plot(html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/regression-chains-013-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nTraining using the input [`Task`](https://mlr3.mlr-org.com/reference/Task.html), shows us how the output and the `$state` look like:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstep1_out = step1$train(task)[[1]]\nstep1_out\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TaskRegr:multiregression> (100 x 6)\n* Target: y1\n* Properties: -\n* Features (5):\n  - dbl (5): x1, x2, y1_learner.response, y2, y3\n```\n:::\n\n```{.r .cell-code}\nstep1$state\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$copy1\nlist()\n\n$drop_y2_y3\n$drop_y2_y3$dt_columns\n[1] \"x1\" \"x2\" \"y2\" \"y3\"\n\n$drop_y2_y3$affected_cols\n[1] \"x1\" \"x2\" \"y2\" \"y3\"\n\n$drop_y2_y3$intasklayout\n   id    type\n1: x1 numeric\n2: x2 numeric\n3: y2 numeric\n4: y3 numeric\n\n$drop_y2_y3$outtasklayout\n   id    type\n1: x1 numeric\n2: x2 numeric\n\n$drop_y2_y3$outtaskshell\nEmpty data.table (0 rows and 3 cols): y1,x1,x2\n\n\n$y1_learner\n$y1_learner$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n(Intercept)           x1           x2  \n   -0.03762      0.99851      0.01364  \n\n\n$y1_learner$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$y1_learner$train_time\n[1] 0.005\n\n$y1_learner$param_vals\nnamed list()\n\n$y1_learner$task_hash\n[1] \"933a5b384a9aaebf\"\n\n$y1_learner$data_prototype\nEmpty data.table (0 rows and 3 cols): y1,x1,x2\n\n$y1_learner$task_prototype\nEmpty data.table (0 rows and 3 cols): y1,x1,x2\n\n$y1_learner$mlr3_version\n[1] '0.14.1'\n\n$y1_learner$train_task\n<TaskRegr:multiregression> (100 x 3)\n* Target: y1\n* Properties: -\n* Features (2):\n  - dbl (2): x1, x2\n\n$y1_learner$affected_cols\n[1] \"x1\" \"x2\"\n\n$y1_learner$intasklayout\n   id    type\n1: x1 numeric\n2: x2 numeric\n\n$y1_learner$outtasklayout\n                    id    type\n1: y1_learner.response numeric\n\n$y1_learner$outtaskshell\nEmpty data.table (0 rows and 2 cols): y1,y1_learner.response\n\n\n$nop1\nlist()\n\n$union1\nlist()\n```\n:::\n:::\n\n\nWithin the second step we then have to define $y_{2}$ as the new target.\nThis can be done using [`PipeOpUpdateTarget`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_updatetarget.html) (note that `PipeOpUpdateTarget` currently is not exported but will be in a future version).\nBy default, `PipeOpUpdateTarget` drops the original target from the feature set, here $y_{1}$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlr_pipeops$add(\"update_target\", mlr3pipelines:::PipeOpUpdateTarget)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstep2 = po(\"update_target\",\n  id = \"y2_target\",\n  new_target_name = \"y2\"\n) %>>%\n  po(\"copy\", outnum = 2, id = \"copy2\") %>>%\n  gunion(list(\n    po(\"colroles\",\n      id = \"drop_y3\",\n      new_role = list(y3 = character())\n    ) %>>%\n      po(\"learner_cv\", learner = lrn(\"regr.lm\"), id = \"y2_learner\"),\n    po(\"nop\", id = \"nop2\")\n  )) %>>%\n  po(\"featureunion\", id = \"union2\")\n```\n:::\n\n\nAgain, we can train to see how the output and `$state` look like, but now using the output of `step1` as the input:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstep2_out = step2$train(step1_out)[[1]]\nstep2_out\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TaskRegr:multiregression> (100 x 6)\n* Target: y2\n* Properties: -\n* Features (5):\n  - dbl (5): x1, x2, y1_learner.response, y2_learner.response, y3\n```\n:::\n\n```{.r .cell-code}\nstep2$state\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$y2_target\nlist()\n\n$copy2\nlist()\n\n$drop_y3\n$drop_y3$dt_columns\n[1] \"x1\"                  \"x2\"                  \"y1_learner.response\" \"y3\"                 \n\n$drop_y3$affected_cols\n[1] \"y1_learner.response\" \"x1\"                  \"x2\"                  \"y3\"                 \n\n$drop_y3$intasklayout\n                    id    type\n1:                  x1 numeric\n2:                  x2 numeric\n3: y1_learner.response numeric\n4:                  y3 numeric\n\n$drop_y3$outtasklayout\n                    id    type\n1:                  x1 numeric\n2:                  x2 numeric\n3: y1_learner.response numeric\n\n$drop_y3$outtaskshell\nEmpty data.table (0 rows and 4 cols): y2,y1_learner.response,x1,x2\n\n\n$y2_learner\n$y2_learner$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n        (Intercept)  y1_learner.response                   x1                   x2  \n            0.07135              0.22773             -0.25186              0.97877  \n\n\n$y2_learner$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$y2_learner$train_time\n[1] 0.009\n\n$y2_learner$param_vals\nnamed list()\n\n$y2_learner$task_hash\n[1] \"1bc5196bab655ff5\"\n\n$y2_learner$data_prototype\nEmpty data.table (0 rows and 4 cols): y2,y1_learner.response,x1,x2\n\n$y2_learner$task_prototype\nEmpty data.table (0 rows and 4 cols): y2,y1_learner.response,x1,x2\n\n$y2_learner$mlr3_version\n[1] '0.14.1'\n\n$y2_learner$train_task\n<TaskRegr:multiregression> (100 x 4)\n* Target: y2\n* Properties: -\n* Features (3):\n  - dbl (3): x1, x2, y1_learner.response\n\n$y2_learner$affected_cols\n[1] \"y1_learner.response\" \"x1\"                  \"x2\"                 \n\n$y2_learner$intasklayout\n                    id    type\n1:                  x1 numeric\n2:                  x2 numeric\n3: y1_learner.response numeric\n\n$y2_learner$outtasklayout\n                    id    type\n1: y2_learner.response numeric\n\n$y2_learner$outtaskshell\nEmpty data.table (0 rows and 2 cols): y2,y2_learner.response\n\n\n$nop2\nlist()\n\n$union2\nlist()\n```\n:::\n:::\n\n\nIn the final third step we define $y_{3}$ as the new target (again, `PipeOpUpdateTarget` drops the previous original target from the feature set, here $y_{2}$):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstep3 = po(\"update_target\",\n  id = \"y3_target\",\n  new_target_name = \"y3\"\n) %>>%\n  po(\"learner\", learner = lrn(\"regr.lm\"), id = \"y3_learner\")\n```\n:::\n\n\nUsing the output of `step2` as input:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstep3_out = step3$train(step2_out)[[1]]\nstep3_out\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n\n```{.r .cell-code}\nstep3$state\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$y3_target\nlist()\n\n$y3_learner\n$y3_learner$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n        (Intercept)  y2_learner.response  y1_learner.response                   x1                   x2  \n             2.6445               0.8155               3.8776              -3.5217              -0.7304  \n\n\n$y3_learner$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$y3_learner$train_time\n[1] 0.009\n\n$y3_learner$param_vals\nnamed list()\n\n$y3_learner$task_hash\n[1] \"24dbd64658d33d6d\"\n\n$y3_learner$data_prototype\nEmpty data.table (0 rows and 5 cols): y3,y2_learner.response,y1_learner.response,x1,x2\n\n$y3_learner$task_prototype\nEmpty data.table (0 rows and 5 cols): y3,y2_learner.response,y1_learner.response,x1,x2\n\n$y3_learner$mlr3_version\n[1] '0.14.1'\n\n$y3_learner$train_task\n<TaskRegr:multiregression> (100 x 5)\n* Target: y3\n* Properties: -\n* Features (4):\n  - dbl (4): x1, x2, y1_learner.response, y2_learner.response\n```\n:::\n:::\n\n\nThe complete pipeline, more precisely [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html), looks like the following:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = step1 %>>% step2 %>>% step3\ngraph$plot(html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/regression-chains-020-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n# Evaluating the Pipeline\n\nBy wrapping our [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) in a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html), we can perform [`3-fold cross-validation`](https://mlr3.mlr-org.com/reference/mlr_resamplings_cv.html) and get an estimated average of the root-mean-square error (of course, in a real world setting splitting the data in a training and test set should have been done):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = as_learner(graph)\nrr = resample(task, learner, rsmp(\"cv\", folds = 3))\nrr$aggregate(msr(\"regr.mse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n regr.mse \n0.7265587 \n```\n:::\n:::\n\n\n# Predicting with the Pipeline\n\nFor completeness, we also show how a prediction step without having any target variable data available would look like:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata_predict = as.data.table(cbind(x1, x2, y1 = NA, y2 = NA, y3 = NA))\nlearner$train(task)\nlearner$predict_newdata(data_predict)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionRegr> for 100 observations:\n    row_ids truth response\n          1    NA 3.116960\n          2    NA 3.327345\n          3    NA 3.010821\n---                       \n         98    NA 3.462541\n         99    NA 3.020585\n        100    NA 3.664326\n```\n:::\n:::\n\n\nNote that we have to initialize the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) with $y_{1}$ as the target but the pipeline will automatically predict $y_{3}$ in the final step as our final target, which was our ultimate goal here.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}