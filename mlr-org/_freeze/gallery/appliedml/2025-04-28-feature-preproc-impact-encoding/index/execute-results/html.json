{
  "hash": "a6406a442d62c33a00854c3110179908",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Impact of Encoding\ncategories:\n  - hyperparameter optimization\n  - xgboost\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Construct pipelines for benchmark experiments on kc_housing set.\ndate: \"\"\nparams:\n  showsolution: true\n  base64encode: true\nlisting: false\nsearch: false\nformat:\n  html:\n    filters:\n      - ../../b64_solution.lua\n---\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n```{=html}\n<script>\nconst correctHash = \"c8d82fef8b39153ae105b7904c267d2f00be7867369214bf76f6c868cad77182\";   // value injected by knitr\n\n/* ---------- reusable helper ---------- */\nfunction b64DecodeUtf8(b64) {\n  // 1) atob  -> binary-string   (bytes 0…255)\n  // 2) map   -> Uint8Array      (array of bytes)\n  // 3) TextDecoder('utf-8')     -> real JS string\n  const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));\n  return new TextDecoder('utf-8').decode(bytes);\n}\n\nasync function sha256(txt) {\n  const buf = await crypto.subtle.digest('SHA-256',\n                 new TextEncoder().encode(txt));\n  return Array.from(new Uint8Array(buf))\n              .map(b => b.toString(16).padStart(2, '0')).join('');\n}\n\nasync function unlockOne(btn) {\n  const pass = prompt(\"Password:\");\n  if (!pass) return;\n  if (await sha256(pass) !== correctHash) {\n    alert(\"Wrong password\"); return;\n  }\n\n  /* --- decode only the solution that belongs to THIS button --- */\n  const wrapper = btn.parentElement;             // .b64-wrapper\n  wrapper.querySelectorAll('.hidden-solution').forEach(div => {\n    div.innerHTML = b64DecodeUtf8(div.dataset.encoded);\n    div.classList.remove('hidden-solution');\n    div.style.display = 'block';\n  });\n\n  /* Remove the button so the user can’t click it again */\n  btn.remove();\n}\n</script>\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n    <strong>JavaScript is required to unlock solutions.</strong><br>\n    Please enable JavaScript and reload the page,<br>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n```\n\n\n\n\n\n# Goal\n\nApply what you have learned about using pipelines for efficient pre-processing and model training on a regression problem.\n\n# House Prices in King county\n\nIn this exercise, we want to model house sale prices in King county in the state of Washington, USA. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(124)\nlibrary(mlr3verse)\nlibrary(\"mlr3tuningspaces\")\ndata(\"kc_housing\", package = \"mlr3data\")\n```\n:::\n\n\n\nWe do some simple feature pre-processing first:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Transform time to numeric variable:\nlibrary(anytime)\ndates = anytime(kc_housing$date)\nkc_housing$date = as.numeric(difftime(dates, min(dates), units = \"days\"))\n# Scale prices:\nkc_housing$price = kc_housing$price / 1000\n# For this task, delete columns containing NAs:\nkc_housing[,c(13, 15)] = NULL\n# Create factor columns:\nkc_housing[,c(8, 14)] = lapply(c(8, 14), function(x) {as.factor(kc_housing[,x])})\n# Get an overview:\nstr(kc_housing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t21613 obs. of  18 variables:\n $ date         : num  164 221 299 221 292 ...\n $ price        : num  222 538 180 604 510 ...\n $ bedrooms     : int  3 3 2 4 3 4 3 3 3 3 ...\n $ bathrooms    : num  1 2.25 1 3 2 4.5 2.25 1.5 1 2.5 ...\n $ sqft_living  : int  1180 2570 770 1960 1680 5420 1715 1060 1780 1890 ...\n $ sqft_lot     : int  5650 7242 10000 5000 8080 101930 6819 9711 7470 6560 ...\n $ floors       : num  1 2 1 1 1 1 2 1 1 2 ...\n $ waterfront   : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 1 1 1 1 1 1 1 1 1 ...\n $ view         : int  0 0 0 0 0 0 0 0 0 0 ...\n $ condition    : int  3 3 3 5 3 3 3 3 3 3 ...\n $ grade        : int  7 7 6 7 8 11 7 7 7 7 ...\n $ sqft_above   : int  1180 2170 770 1050 1680 3890 1715 1060 1050 1890 ...\n $ yr_built     : int  1955 1951 1933 1965 1987 2001 1995 1963 1960 2003 ...\n $ zipcode      : Factor w/ 70 levels \"98001\",\"98002\",..: 67 56 17 59 38 30 3 69 61 24 ...\n $ lat          : num  47.5 47.7 47.7 47.5 47.6 ...\n $ long         : num  -122 -122 -122 -122 -122 ...\n $ sqft_living15: int  1340 1690 2720 1360 1800 4760 2238 1650 1780 2390 ...\n $ sqft_lot15   : int  5650 7639 8062 5000 7503 101930 6819 9711 8113 7570 ...\n - attr(*, \"index\")= int(0) \n```\n\n\n:::\n:::\n\n\n\n# Train-test Split\n\nBefore we train a model, let's reserve some data for evaluating our model later on:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(kc_housing, target = \"price\")\nsplit = partition(task, ratio = 0.6)\n\ntasktrain = task$clone()\ntasktrain$filter(split$train)\ntasktrain\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskRegr:kc_housing> (12968 x 18)\n* Target: price\n* Properties: -\n* Features (17):\n  - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15,\n    view, yr_built\n  - dbl (5): bathrooms, date, floors, lat, long\n  - fct (2): waterfront, zipcode\n```\n\n\n:::\n\n```{.r .cell-code}\ntasktest = task$clone()\ntasktest$filter(split$test)\ntasktest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskRegr:kc_housing> (8645 x 18)\n* Target: price\n* Properties: -\n* Features (17):\n  - int (10): bedrooms, condition, grade, sqft_above, sqft_living, sqft_living15, sqft_lot, sqft_lot15,\n    view, yr_built\n  - dbl (5): bathrooms, date, floors, lat, long\n  - fct (2): waterfront, zipcode\n```\n\n\n:::\n:::\n\n\n\n# XGBoost\n\nXGBoost ([Chen and Guestrin, 2016](https://dl.acm.org/doi/10.1145/2939672.2939785) is a highly performant library for gradient-boosted trees. As some other ML learners, it cannot handle categorical data, so categorical features must be encoded as numerical variables. In the King county data, there are two categorical features encoded as `factor`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nft = task$feature_types\nft[ft[[2]] == \"factor\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKey: <id>\n           id   type\n       <char> <char>\n1: waterfront factor\n2:    zipcode factor\n```\n\n\n:::\n:::\n\n\n\nCategorical features can be grouped by their cardinality, which refers to the number of levels they contain: binary features (two levels), low-cardinality features, and high-cardinality features; there is no universal threshold for when a feature should be considered high-cardinality and this threshold can even be tuned. Low-cardinality features can be handled by one-hot encoding. One-hot encoding is a process of converting categorical features into a binary representation, where each possible category is represented as a separate binary feature. Theoretically, it is sufficient to create one less binary feature than levels. This is typically called dummy or treatment encoding and is required if the learner is a generalized linear model (GLM) or additive model (GAM). For now, let's check the cardinality of `waterfront` and `zipcode`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlengths(task$levels())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nwaterfront    zipcode \n         2         70 \n```\n\n\n:::\n:::\n\n\n\nObviously, `waterfront` is a low-cardinality feature suitable for dummy (also called treatment) encoding and `zipcode` is a very high-cardinality feature. Some learners support handling categorical features but may still crash for high-cardinality features if they internally apply encodings that are only suitable for low-cardinality features, such as one-hot encoding. \n\n# Impact encoding\n\nImpact encoding ([Micci-Barreca 2001](https://dl.acm.org/doi/10.1145/507533.507538)) is a good approach for handling high-cardinality features. Impact encoding converts categorical features into numeric values. The idea behind impact encoding is to use the target feature to create a mapping between the categorical feature and a numerical value that reflects its importance in predicting the target feature. Impact encoding involves the following steps:\n\n1. Group the target variable by the categorical feature.\n2. Compute the mean of the target variable for each group.\n3. Compute the global mean of the target variable.\n4. Compute the impact score for each group as the difference between the mean of the target variable for the group and the global mean of the target variable.\n5. Replace the categorical feature with the impact scores.\n\nImpact encoding preserves the information of the categorical feature while also creating a numerical representation that reflects its importance in predicting the target. Compared to one-hot encoding, the main advantage is that only a single numeric feature is created regardless of the number of levels of the categorical features, hence it is especially useful for high-cardinality features. As information from the target is used to compute the impact scores, the encoding process must be embedded in cross-validation to avoid leakage between training and testing data.\n\n# Exercises\n\n## Exercise 1: Create a pipeline\n\nCreate a pipeline that pre-processes each factor variable with impact encoding. The pipeline should run an `autotuner` that automatically conducts hyperparameter optimization (HPO) with an XGBoost learner that learns on the pre-processed features using random search and MSE as performance measure. You can use CV with a suitable number of folds for the resampling stragegy. For the search space, you can use `lts(\"regr.xgboost.default\")` from the `mlr3tuningspaces` package. This constructs a search space customized for Xgboost based on theoretically and empirically validated considerations on which variables to tune or not. However, you should set the parameter `nrounds = 100` for speed reasons. Further, set `nthread = parallel::detectCores()` to prepare multi-core computing later on.\n\n\n<details>\n  <summary>**Hint 1:**</summary>\n  \nThe pipeline must be embedded in the `autotuner`: the learner supplied to the `autotuner` must include the feature preprocessing and the XGboost learner.\n\n</details>\n\n<details>\n  <summary>**Hint 2:**</summary>\n  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create xgboost learner:\nxgb = lrn(...)\n\n# Set search space from mlr3tuningspaces:\nxgb_ts = ...\n\n# Set nrounds and nthread:\nxgb_ts$... = ....\nxgb_ts$... = ....\n\n# Combine xgb_ts with impact encoding:\nxgb_ts_impact = as_learner(...)\n\n# Use random search:\ntuner = tnr(...)\n\n#Autotuner pipeline component:\nat = auto_tuner(\n  tuner = ...,\n  learner = ...,\n  search_space = ...,\n  resampling = ...,\n  measure = ...,\n  term_time = ...) # Maximum allowed time in seconds.\n\n# Combine pipeline:\nglrn_xgb_impact = as_learner(...)\nglrn_xgb_impact$id = \"XGB_enc_impact\"\n```\n:::\n\n\n\n</details>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create xgboost learner \nxgb = lrn(\"regr.xgboost\")\nas.data.table(xgb$param_set$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   nrounds nthread verbose\n     <int>   <int>   <int>\n1:    1000       1       0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Set search space from mlr3tuningspaces\nxgb_ts = lts(xgb)\nas.data.table(xgb_ts$param_set$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                   alpha colsample_bylevel colsample_bytree                                eta\n                        <RangeTuneToken>  <RangeTuneToken> <RangeTuneToken>                   <RangeTuneToken>\n1:                             <list[3]>         <list[3]>        <list[3]>                          <list[3]>\n2: to_tune(0.001, 1000, logscale = TRUE)   to_tune(0.1, 1)  to_tune(0.1, 1) to_tune(1e-04, 1, logscale = TRUE)\n                                  lambda        max_depth          nrounds nthread        subsample verbose\n                        <RangeTuneToken> <RangeTuneToken> <RangeTuneToken>   <int> <RangeTuneToken>   <int>\n1:                             <list[3]>        <list[3]>        <list[3]>       1        <list[3]>       0\n2: to_tune(0.001, 1000, logscale = TRUE)   to_tune(1, 20) to_tune(1, 5000)       1  to_tune(0.1, 1)       0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Set nrounds and nthread:\nxgb_ts$param_set$values$nrounds = 100\nxgb_ts$param_set$values$nthread = parallel::detectCores()\n\n# Combine xgb_ts with impact encoding\nxgb_ts_impact = as_learner(po(\"encodeimpact\") %>>% xgb_ts)\nas.data.table(xgb_ts_impact$param_set$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   encodeimpact.smoothing encodeimpact.impute_zero                    regr.xgboost.alpha regr.xgboost.colsample_bylevel\n                    <num>                   <lgcl>                      <RangeTuneToken>               <RangeTuneToken>\n1:                  1e-04                    FALSE                             <list[3]>                      <list[3]>\n2:                  1e-04                    FALSE to_tune(0.001, 1000, logscale = TRUE)                to_tune(0.1, 1)\n   regr.xgboost.colsample_bytree                   regr.xgboost.eta                   regr.xgboost.lambda\n                <RangeTuneToken>                   <RangeTuneToken>                      <RangeTuneToken>\n1:                     <list[3]>                          <list[3]>                             <list[3]>\n2:               to_tune(0.1, 1) to_tune(1e-04, 1, logscale = TRUE) to_tune(0.001, 1000, logscale = TRUE)\n   regr.xgboost.max_depth regr.xgboost.nrounds regr.xgboost.nthread regr.xgboost.subsample regr.xgboost.verbose\n         <RangeTuneToken>                <int>                <int>       <RangeTuneToken>                <int>\n1:              <list[3]>                  100                   10              <list[3]>                    0\n2:         to_tune(1, 20)                  100                   10        to_tune(0.1, 1)                    0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Use random search:\ntuner = tnr(\"random_search\")\n\n# Create auto-tuned xgboost\nauto_xgb_impact = auto_tuner(\n  tuner = tuner,\n  learner = xgb_ts_impact,\n  resampling = rsmp(\"cv\", folds = 2),\n  measure = msr(\"regr.mse\"),\n  term_time = 20\n)\n```\n:::\n\n\n\n\n:::\n\n:::\n\n## Exercise 2: Benchmark a pipeline\n\nBenchmark your impact encoding pipeline from the previous task against a simple one-hot encoding pipeline that uses one-hot encoding for all factor variables. Use the same `autotuner` setup as element for both. Use two-fold CV as resampling strategy for the benchmark. Afterwards, evaluate the benchmark with MSE. Finally, assess the performance via the \"untouched test set principle\" by training both autotuners on `tasktrain` and evaluate their performance on `tasktest`.\n\n\n<details>\n  <summary>**Hint 1:**</summary>\n  \nTo conduct the benchmark, use `benchmark(benchmark_grid(...))`.\n\n</details>\n\n<details>\n  <summary>**Hint 2:**</summary>\n  \nTo conduct performance evaluation, use `$aggregate()` on the benchmark object.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Pipeline for One-Hot Encoding only:\nauto_xgb_oh = auto_tuner(\n  tuner = tuner,\n  learner = as_learner(po(\"encode\") %>>% xgb_ts),\n  resampling = rsmp(\"cv\", folds = 2),\n  measure = msr(\"regr.mse\"),\n  term_time = 20\n)\n\n# Resampling design for benchmark:\nrsmp_cv2 = rsmp(\"cv\", folds = 2)\nrsmp_cv2$instantiate(tasktrain)\n\n# Conduct benchmark:\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nbmr = benchmark(benchmark_grid(tasktrain, c(auto_xgb_oh, auto_xgb_impact), rsmp_cv2))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Aggregate results:\nbmr$aggregate(measure = msr(\"regr.mse\"))[, .(learner_id, regr.mse)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                        learner_id regr.mse\n                            <char>    <num>\n1:       encode.regr.xgboost.tuned 16864.71\n2: encodeimpact.regr.xgboost.tuned 17532.47\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nauto_xgb_oh$train(tasktrain)\nauto_xgb_impact$train(tasktrain)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nauto_xgb_oh$predict(tasktest)$score()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n14693.66 \n```\n\n\n:::\n\n```{.r .cell-code}\nauto_xgb_impact$predict(tasktest)$score()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n13556.18 \n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n# Summary\n\nWe learned how to apply pre-processing steps together with tuning to construct refined pipelines for benchmark experiments.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}