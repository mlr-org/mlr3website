{
  "hash": "1987739b85a7670053ec633e73d88e4e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Imabalanced ROC-Analysis threshold Tuning\ngroup: Imbalanced\ncategories:\n  - imputation\n  - mlr3benchmarking\nauthor:\n  - name: Fiona Ewald\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Train a classifier on German Credit set and tune the output of a probabilistic model with ROC threshold analysis.\ndate: \"\"\nparams:\n  showsolution: true\n  base64encode: true\nlisting: false\nsearch: false\nformat:\n  html:\n    filters:\n      - ../../b64_solution.lua\n---\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n```{=html}\n<script>\nconst correctHash = \"13295499017961d4575b5a28c2aa7575df37b95c26301a222beb644da7cb9564\";   // value injected by knitr\n\n/* ---------- reusable helper ---------- */\nfunction b64DecodeUtf8(b64) {\n  // 1) atob  -> binary-string   (bytes 0…255)\n  // 2) map   -> Uint8Array      (array of bytes)\n  // 3) TextDecoder('utf-8')     -> real JS string\n  const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));\n  return new TextDecoder('utf-8').decode(bytes);\n}\n\nasync function sha256(txt) {\n  const buf = await crypto.subtle.digest('SHA-256',\n                 new TextEncoder().encode(txt));\n  return Array.from(new Uint8Array(buf))\n              .map(b => b.toString(16).padStart(2, '0')).join('');\n}\n\nasync function unlockOne(btn) {\n  const pass = prompt(\"Password:\");\n  if (!pass) return;\n  if (await sha256(pass) !== correctHash) {\n    alert(\"Wrong password\"); return;\n  }\n\n  /* --- decode only the solution that belongs to THIS button --- */\n  const wrapper = btn.parentElement;             // .b64-wrapper\n  wrapper.querySelectorAll('.hidden-solution').forEach(div => {\n    div.innerHTML = b64DecodeUtf8(div.dataset.encoded);\n    div.classList.remove('hidden-solution');\n    div.style.display = 'block';\n  });\n\n  /* Remove the button so the user can’t click it again */\n  btn.remove();\n}\n</script>\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n    <strong>JavaScript is required to unlock solutions.</strong><br>\n    Please enable JavaScript and reload the page,<br>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n```\n\n\n\n\n\n\n# Goal\n\nIn this exercise, we will create a machine learning model that predicts the\ncredit risk of an individual (e.g., the probability of being a `good` or `bad` credit applicant for the bank). Our goal is not to\nobtain an optimal classifier for this task, but to learn how to get a better\nunderstanding of the  predictions made by this model. This means looking at its\nsensitivity (ability to correctly identify positives) and specificity (ability\nto correctly identify negatives). The sensitivity is also known as the true positive rate (TPR) and the\nspecificity is equal to (1 - FPR) where FPR is the false positive rate.\n\nWe will also cover how to obtain different response predictions from a probabilistic\nmodel by modifying the threshold. We will inspect this relationship via the ROC curve and tune\nthe threshold for a given classifier to optimize our response predictions.\n\n\n# 1 Training a classification tree on the german credit task\n\nFirst load the pre-defined German credit task and set the positive class to `\"good\"`.\nTrain a random forest on 2/3 of the data (training data) and make probabilistic predictions on the remaining 1/3 (test data).\n\n<details>\n<summary>**Hint 1:**</summary>\n- Create the German credit task using `tsk()` and set the positive class by modifying e.g. `task$positive`.\n- Create a learner using `lrn()` and make sure to specify the `predict_type` so that the learner will predict probabilities instead of classes.\n- When calling the methods `$train()` and `$predict()` of the learner, you can pass an argument `row_ids` to specify which observations should be used for the train and test data. \n- You can generate random train-test splits using, e.g., the `partition()` function.\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n\ntask = tsk(...)\ntask$positive = ...\nlearner = lrn(..., predict_type = ...)\nids = partition(...)\nlearner$train(..., row_ids = ...)\npred = learner$predict(..., row_ids = ...)\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\ntask = tsk(\"german_credit\")\ntask$positive = \"good\"\nlearner = lrn(\"classif.ranger\", predict_type = \"prob\")\nids = partition(task)\nstr(ids)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 3\n $ train     : int [1:670] 2 5 6 8 10 11 13 14 16 19 ...\n $ test      : int [1:330] 1 3 4 7 9 12 15 17 18 21 ...\n $ validation: int(0) \n```\n\n\n:::\n\n```{.r .cell-code}\nlearner$train(task, row_ids = ids$train)\npred = learner$predict(task, row_ids = ids$test)\npred\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionClassif> for 330 observations:\n row_ids truth response prob.good   prob.bad\n       1  good     good 0.8613921 0.13860794\n       3  good     good 0.9230675 0.07693254\n       4  good      bad 0.4290706 0.57092937\n     ---   ---      ---       ---        ---\n     995  good     good 0.9698952 0.03010476\n     997  good      bad 0.4063794 0.59362063\n     998  good     good 0.9197286 0.08027143\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n# 2 Confusion matrices and classification thresholds\n\nInspect and save the confusion matrix of the predictions made in the previous exercise.\nManually calculate the FPR and TPR using the values from the confusion matrix.\nCan you think of another way to compute the TPR and FPR using `mlr3` instead of manually computing them using the confusion matrix?\n\n<details>\n  <summary>**Recap**</summary>\nA confusion matrix is a special kind of contingency table with two\ndimensions \"actual\" and \"predicted\" to summarize the ground truth classes (truth) vs. the predicted classes of a classifier (response).\n\nBinary classifiers can be understood as first predicting a score (possibly a probability) and then classifying all instances with a score greater than a certain threshold $t$ as positive and all others as negative. This means that one can obtain different class predictions using different threshold values $t$.\n</details>\n\n<details>\n  <summary>**Hint 1:**</summary>\nA prediction object has a field `$confusion`.\nSince `good` was used as the positive class here, the TPR is $P(\\hat{Y} = good | Y = good)$ and the FPR is $P(\\hat{Y} = good | Y = bad)$ (where $\\hat{Y}$ refers to the predicted response of the classifier and $Y$ to the ground truth class labels). Instead of manually computing the TPR and FPR, there are appropriate performance measures implemented in `mlr3` that you could use.\n</details>\n\n<details>\n  <summary>**Hint 2:**</summary>\nYou need to replace `...` in the code below to access the appropriate columns and rows, e.g., `confusion1[1, 1]` is the element in the first row and first column of the confusion matrix and tells you how many observations with ground truth $Y = good$ were classified into the class $\\hat{Y} = good$ by the learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfusion1 = pred$confusion\nTPR1 =  confusion1[...] / sum(confusion1[...])\nTPR1\nFPR1 = confusion1[...] / sum(confusion1[...])\nFPR1\n```\n:::\n\n\n\nThe names of the TPR and FPR performance measures implemented in `mlr3` can be found by looking at `as.data.table(mlr_measures)`. You can use the code below and pass the names of the `mlr3` measures in a vector to compute both the TPR and FPR: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred$score(msrs(...))\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfusion1 = pred$confusion\nTPR1 = confusion1[1, 1] / sum(confusion1[, 1])\nTPR1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.938326\n```\n\n\n:::\n\n```{.r .cell-code}\nFPR1 = confusion1[1, 2] / sum(confusion1[, 2])\nFPR1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5728155\n```\n\n\n:::\n:::\n\n\n\nInstead of manually computing the TPR and FPR, you could also just use \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred$score(msrs(c(\"classif.tpr\", \"classif.fpr\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.tpr classif.fpr \n  0.9383260   0.5728155 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n# 3 Asymmetric costs\n\nThink about which type of error is worse for the given task and obtain new predictions\n(without retraining the model) that takes this into account.\n\nThen calculate the FPR and TPR and compare it with the results from the previous exercise.\n\n<details>\n<summary>**Hint 1:**</summary>\nA prediction object has the method `$set_threshold()` that can be used to set a custom threshold and which will update the predicted classes according to the selected threshold value.\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred$set_threshold(...)\nconfusion2 = pred$confusion\nTPR2 =  confusion2[...] / sum(...)\nFPR2 = confusion2[...] / sum(...)\n\nTPR2 - TPR1\nFPR2 - FPR1\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\nThe error of making a false positive - in this case classifying someone who is not creditworthy ($Y = bad$)\nas creditworthy ($\\hat Y = good$) - is likely considerably higher than classifying someone who is creditworthy as not creditworthy. \nIn the first scenario, the company may lose all the money that was not paid back duly. In the letter case, it only misses out on the profit.\n\nWe can take this fact into account by using a higher threshold to predict the positive class (`good`), i.e., being more conservative in classifying a good credit risk.\nFor illustration purposes, we will use the threshold $0.7$ which is higher than the default threshold $0.5$.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred$set_threshold(0.7)\npred\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionClassif> for 330 observations:\n row_ids truth response prob.good   prob.bad\n       1  good     good 0.8613921 0.13860794\n       3  good     good 0.9230675 0.07693254\n       4  good      bad 0.4290706 0.57092937\n     ---   ---      ---       ---        ---\n     995  good     good 0.9698952 0.03010476\n     997  good      bad 0.4063794 0.59362063\n     998  good     good 0.9197286 0.08027143\n```\n\n\n:::\n:::\n\n\nWe can then access the updated confusion matrix and calculate the new FPR and TPR as before.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfusion2 = pred$confusion\nTPR2 =  confusion2[1, 1] / sum(confusion2[, 1])\nFPR2 = confusion2[1, 2] / sum(confusion2[, 2])\n```\n:::\n\n\n\nWhen comparing it with the previous values, we observe a lower TPR and FPR.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nTPR2 - TPR1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.2378855\n```\n\n\n:::\n\n```{.r .cell-code}\nFPR2 - FPR1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.3203883\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n# 4 ROC curve\n\nIn the previous two exercises, we have calculated the FPR and TPR for two thresholds.\nNow visualize the FPR and TPR for all possible thresholds, i.e. the ROC curve.\n\n<details>\n<summary>**Recap**</summary>\nThe receiver operating characteristic (ROC) displays the sensitivity and specificity for all\npossible thresholds.\n</details>\n\n\n<details>\n<summary>**Hint 1:**</summary>\nYou can use `autoplot()` on the prediction object and set the `type` argument to produce a ROC curve. \nYou can open the help page of `autoplot` for a prediction object using `?mlr3viz::autoplot.PredictionClassif`.\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(pred, type = ...)\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(pred, type = \"roc\", show_cb = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n:::\n\n:::\n\n# 5 Threshold tuning\n\nIn this exercise, we assume that predicting a false positive is 4 times worse than a false negative.\nUse a measure that considers classification costs (e.g., misclassification costs `msr(\"classif.costs\")$help()`) and tune the threshold of our classifier to systematically optimize the asymmetric cost function. \n\n## 5.1 Cost Matrix\n\nFirst, define the cost matrix. Here, this is a 2x2 matrix with rows corresponding to the predicted class and columns corresponding to the true class. The first row/column implies `\"good\"`, the second `\"bad\"` credit rating.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nThe order of the classes in the rows and columns of the matrix must correspond to the order of classes in `task$class_names`.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncosts = matrix(c(0, 1, 4, 0), nrow = 2, dimnames =\n  list(\"Predicted Credit\" = c(\"good\", \"bad\"),\n    Truth = c(\"good\", \"bad\")))\ncosts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                Truth\nPredicted Credit good bad\n            good    0   4\n            bad     1   0\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## 5.2 Cost-Sensitive Measure\n\nNext, define a cost-sensitive measure. This measure takes one argument, which is a matrix with row and column names corresponding to the class labels in the task of interest. \n\n<details>\n<summary>**Hint 1:**</summary>\nYou can use `as.data.table(mlr_measures)` to find the relevant measure.\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmsr_costs = msr(\"classif.costs\", costs = costs)\nmsr_costs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<MeasureClassifCosts:classif.costs>: Cost-sensitive Classification\n* Packages: mlr3\n* Range: [0, Inf]\n* Minimize: TRUE\n* Average: macro\n* Parameters: normalize=TRUE\n* Properties: -\n* Predict type: response\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## 5.3 Thresholding\n\nIn default settings, a model will classify a customer as good credit if the predicted probability is greater than 0.5. Here, this might not be a sensible approach as we would likely act more conservatively and reject more credit applications with a higher threshold due to the non-uniform costs. Use the `autplot()` function to plot the costs associated with predicting at various thresholds between 0 and 1 for the random forest predictions stored in the `pred` object from before.\n\n<details>\n<summary>**Hint 1:**</summary>\nYou need to specify `type = \"threshold` within autoplot as well as the previously defined measure.\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(pred, type = \"threshold\", measure = msr_costs)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nAs expected, the optimal threshold is greater than 0.5 which means the optimal model should predict ‘bad’ credit more often than not.\n\n# Alternative without autoplot\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nthresholds = seq(0, 1, by = 0.01)\neval_threshold = function(pred, threshold, measure, task) {\n  pred$set_threshold(threshold)\n  score = pred$score(measure, task = task)\n  return(score)\n}\ncosts = matrix(c(0, 1, 4, 0), nrow = 2)\ndimnames(costs) = list(truth = task$class_names, response = task$class_names)\nmeasure = msr(\"classif.costs\", costs = costs)\nscores = sapply(thresholds, function(threshold) eval_threshold(pred, threshold, measure, task))\nthresholds[which.min(scores)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.74\n```\n\n\n:::\n\n```{.r .cell-code}\nmin(scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4878788\n```\n\n\n:::\n\n```{.r .cell-code}\nqplot(x = thresholds, y = scores) + geom_vline(xintercept = thresholds[which.min(scores)])\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n:::\n\n:::\n\n## 5.4 Tuning the Threshold\n\nThe optimal threshold can be automated via `po(\"tunethreshold\")`. Create a graph that consists of a logistic regression learner and this threshold tuning pipeline object. Then, turn the graph into a learner as in previous tutorials. Finally, benchmark the pipeline against a standard logistic regression learner using 3-fold CV.\n\n<details>\n<summary>**Hint 1:**</summary>\nYou can use this code skeleton for the pipeline:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogreg = po(\"learner_cv\", lrn(...)) # base learner\ngraph =  logreg %>>% po(...) # graph with threshold tuning\n```\n:::\n\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\nYou can use this code skeleton for the benchmark:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearners = list(..., lrn(\"classif.log_reg\"))\nbmr = benchmark(benchmark_grid(task, learners,\n  rsmp(\"cv\", folds = 3)))\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Pipeline:\npo_cv = po(\"learner_cv\", lrn(\"classif.log_reg\", predict_type = \"prob\"))\ngraph =  po_cv %>>% po(\"tunethreshold\", measure = msr_costs)\n\n# Benchmark:\nlearners = list(as_learner(graph), lrn(\"classif.log_reg\"))\nbmr = benchmark(benchmark_grid(task, learners,\n  rsmp(\"cv\", folds = 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [18:07:56.459] [mlr3] Running benchmark with 6 resampling iterations\nINFO  [18:07:56.884] [mlr3] Applying learner 'classif.log_reg.tunethreshold' on task 'german_credit' (iter 1/3)\nINFO  [18:07:56.966] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 1/3)\nINFO  [18:07:56.988] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 2/3)\nINFO  [18:07:57.008] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 3/3)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [18:07:57.315] [mlr3] Applying learner 'classif.log_reg.tunethreshold' on task 'german_credit' (iter 2/3)\nINFO  [18:07:57.398] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 1/3)\nINFO  [18:07:57.416] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 2/3)\nINFO  [18:07:57.440] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 3/3)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [18:07:57.821] [mlr3] Applying learner 'classif.log_reg.tunethreshold' on task 'german_credit' (iter 3/3)\nINFO  [18:07:57.915] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 1/3)\nINFO  [18:07:57.940] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 2/3)\nINFO  [18:07:57.964] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 3/3)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [18:07:57.968] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 1/3)\nINFO  [18:07:58.084] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 2/3)\nINFO  [18:07:58.197] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 3/3)\nINFO  [18:07:58.491] [mlr3] Finished benchmark\n```\n\n\n:::\n\n```{.r .cell-code}\n# Evaluate:\nbmr$aggregate(msr_costs)[, c(4, 7)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                      learner_id classif.costs\n                          <char>         <num>\n1: classif.log_reg.tunethreshold     0.5289841\n2:               classif.log_reg     0.7000593\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n# 6 ROC Comparison\n\nIn this exercise, we will explore how to compare to learners by looking at their ROC curve.\n\nThe basis for this exercise will be a benchmark experiment that\ncompares a classification tree with a random forest on the german credit task.\n\nBecause we are now not only focused on the analysis of a given prediction, but on the\ncomparison of two learners, we selected a 10-fold cross-validation to reduce the\nuncertainty of this comparison.\n\nConduct the benchmark experiment and show both ROC curves in one plot.\nWhich learner learner performs better in this case?\n\n<details>\n<summary>**Hint 1:**</summary>\nUse `benchmark_grid()` to create the experiment design and execute it using `benchmark()`.\nYou can also apply the function `autoplot()` to benchmark results.\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling = rsmp(..)\nlearners = lrns(...)\ndesign = benchmark_grid(...)\nbmr = benchmark(...)\nautoplot(...)\n```\n:::\n\n\n\n</details>\n\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\nWe create and execute the benchmark experiment in the usual fashion.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling = rsmp(\"cv\", folds = 10)\nlearners = list(\n  lrn(\"classif.rpart\", predict_type = \"prob\"),\n  lrn(\"classif.ranger\", predict_type = \"prob\")\n)\ndesign = benchmark_grid(task, learners, resampling)\nbmr = benchmark(design)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [18:07:58.529] [mlr3] Running benchmark with 20 resampling iterations\nINFO  [18:07:58.560] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 1/10)\nINFO  [18:07:58.589] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 2/10)\nINFO  [18:07:58.617] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 3/10)\nINFO  [18:07:58.678] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 4/10)\nINFO  [18:07:58.708] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 5/10)\nINFO  [18:07:58.739] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 6/10)\nINFO  [18:07:58.853] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 7/10)\nINFO  [18:07:58.948] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 8/10)\nINFO  [18:07:59.046] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 9/10)\nINFO  [18:07:59.144] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 10/10)\nINFO  [18:07:59.143] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/10)\nINFO  [18:07:59.190] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/10)\nINFO  [18:07:59.235] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/10)\nINFO  [18:07:59.282] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 4/10)\nINFO  [18:07:59.335] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 5/10)\nINFO  [18:07:59.392] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 6/10)\nINFO  [18:07:59.516] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 7/10)\nINFO  [18:07:59.580] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 8/10)\nINFO  [18:07:59.656] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 9/10)\nINFO  [18:07:59.723] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 10/10)\nINFO  [18:08:00.185] [mlr3] Finished benchmark\n```\n\n\n:::\n:::\n\n\n\nNow we proceed with showing the ROC curve. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bmr, type = \"roc\", show_cb = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nThe random forest is clearly better, as for virtually every specificity it has a higher\nsensitivity.\n\n:::\n\n:::\n\n# 7 Area under the curve\n\nIn the previous exercise we have learned how to compare to learners using the ROC curve.\nAlthough the random forest was dominating the classification tree in this specific case,\nthe more common case is that the ROC curves of two models are crossing, making a comparison\nin the sense of $>$ / $<$ impossible.\n\nThe area under the curve tries to solve this problem by summarizing the ROC curve by its area\nunder the curve (normalized to 1), which allows for a scalar comparison.\n\nCompare the AUC for the benchmark result.\n\n<details>\n<summary>**Hint 1:**</summary>\nYou can use the `autoplot()` function and use the AUC as performance measure in the `measure` argument.\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bmr, measure = msr(...))\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bmr, measure = msr(\"classif.auc\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nAs expected, the AUC for the random forest is higher than the AUC for the classification tree.\n\n:::\n\n:::\n\n# Bonus exercise: Unbiased performance estimation\n\nRevisit the exercise where we tuned the threshold.\nIs the performance estimate for the best threshold unbiased?\nIf no, does this mean that our tuning strategy was invalid?\n\n<details>\n<summary>**Hint 1:**</summary>\nDid we use an independent test set?\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\nThink of the uncertainty when estimating the ROC curve.\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\nNo, although our method is valid with respect to determining our \"best guess\"\nfor the optimal threshold, the obtained performance estimate suffers from\noptimization bias. We would need an independent test set to evaluate our\nmethod, i.e. nested holdout or cross-validation.\n\n:::\n\n:::\n\n\n# Summary\n\nIn this exercise we improved our understanding of the performance of binary classifiers\nby the means of the confusion matrix and a focus on different error types.\nWe have seen how we can analyze and compare classifiers using the ROC and\nhow to improve our response predictions using threshold tuning.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}