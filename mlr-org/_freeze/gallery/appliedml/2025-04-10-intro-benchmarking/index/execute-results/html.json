{
  "hash": "4c26bccd5be319c3e4ff17e60789d886",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Benchmarking\ncategories:\n  - benchmarking\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Hyperparameter tuning and benchmarking on german credit task.\ndate: 04-14-2025\nparams:\n  showsolution: false\n---\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n# Goal\n\nWe will go beyond resampling single learners.\nWe will learn how to compare a large number of different models using benchmarking.\nIn this exercise, we will not show you how to tune a learner. \nInstead, we will compare identical learners with different hyperparameters that are set manually.\nIn particular, we will learn how to set up benchmarking instances in `mlr3`.\n\n# German Credit Data\n\nWe create the task as for the resampling exercise:\nAgain, we make us of our work horse:\nThe German Credit Data set.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLade n√∂tiges Paket: mlr3\n```\n\n\n:::\n\n```{.r .cell-code}\ntask = tsk(\"german_credit\")\nset.seed(20240801)\n```\n:::\n\n\n\n# Exercise: Benchmark multiple learners\n\nWe are going to compare a range of different KNN models with even $k$ values from 4 to 30.\nFurthermore, we want to assess the performance of a logistic regression.\n\n## Create the learners\n\nCreate a logistic regression learner and many KNN learners.\nYou should evaluate all KNN models with even $k$ values from 4 to 30 (i.e., every second $k$ value between 4 and 30).\nSave all learners in a list.\nGive the KNN learners an appropriate `id` that reflects their $k$.\n\n<details>\n  <summary>**Show Hint 1:**</summary>\n  Create a sequence from 4 to 30, e.g., using the `seq()` function.\n  Use the `lapply` function or a for-loop to create the list of learners with even $k$ values from 4 to 30.\n  Don't forget to also include the logistic regression learner in your list (the `append` function might be helpful here to extend a created list).\n  The `lrn` function has an argument `id` that can be used to change the name of the learner (here, you should give the KNN learners an appropriate `id` that reflects their value of $k$ to be able to distinguish the learners).\n  </details>\n  \n<details>\n  <summary>**Show Hint 2:**</summary>\n  To create a list of KNN learners, you can use this template:\n  `lapply(..., function(i) lrn(\"classif.kknn\", k = i, id = paste0(\"classif.knn\", i))`\n  </details>\n\n:::{.content-hidden unless-meta=\"params.showsolution\"}\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlog_reg = lrn(\"classif.log_reg\")\nkval = seq(from = 4, to = 30, by = 2)\nkval\nknn = lapply(kval, function(i) lrn(\"classif.kknn\", k = i, id = paste0(\"classif.knn\", i)))\nlrns = append(log_reg, knn)\n```\n:::\n\n\n:::\n\n:::\n\n## Create the resampling\n\nCreate a 4-fold cross-validation resampling.\nCreate a list that only contains this resampling (this is needed later for the `benchmark_grid` function).\n\n<details>\n  <summary>**Show Hint:**</summary>\n  See the previous resampling use case.\n  </details>\n\n:::{.content-hidden unless-meta=\"params.showsolution\"}\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv4 = rsmp(\"cv\", folds = 4)\n```\n:::\n\n\n:::\n\n:::\n\n## Create a benchmarking design\n\nTo design your benchmark experiment consisting of tasks, learners and resampling technique, you can use the `benchmark_grid` function from `mlr3`.\nHere, we will use only one task and one resampling technique but multiple learners.\nUse the previously created task (german credit), learners (the list of many KNN learners and a single logistic regression learner) and resampling (4 fold CV) as input.\n\n<details>\n  <summary>**Show Hint 1:**</summary>\n  Also make sure that the task is included in a list as the arguments of the `benchmark_grid` function requires lists as input.\n  </details>\n<details>\n  <summary>**Show Hint 2:**</summary>\n  `benchmark_grid(...)`\n  </details>\n\n:::{.content-hidden unless-meta=\"params.showsolution\"}\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndesign = benchmark_grid(task, lrns, cv4)\n```\n:::\n\n\n:::\n\n:::\n\n## Run the benchmark\n\nNow you still need to run all experiments specified in the design.\nDo so by using the `benchmark` function.\nThis may take some time.\n(Still less than a minute.)\nMake sure to store the benchmark in a new object called `bmr` as you will reuse and inspect the benchmark result in the subsequent exercises.\n\n<details>\n  <summary>**Show Hint 1:**</summary>\n  </details>\n\n:::{.content-hidden unless-meta=\"params.showsolution\"}\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr = benchmark(design)\n```\n:::\n\n\n:::\n\n:::\n\n## Evaluate the benchmark\n\nChoose two appropriate metrics to evaluate the different learners performance on the task.\nCompute these metrics and also visualize at least one of them using the `autoplot` function.\n\n<details>\n  <summary>**Show Hint 1:**</summary>\n  The previously stored benchmark object has a method `$aggregate(...)` just like the objects created with the `resample` function from the previous use case.\n  </details>\n<details>\n  <summary>**Show Hint 2:**</summary>\n  `autoplot(..., measure = msr(...))`\n  </details>\n\n:::{.content-hidden unless-meta=\"params.showsolution\"}\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres = bmr$aggregate(measures = c(msr(\"classif.fp\"), msr(\"classif.acc\")))\nhead(res)\nautoplot(bmr, measure = msr(\"classif.acc\"))\n```\n:::\n\n\n:::\n\n:::\n\n## Interpret the results\n\nInterpret the plot.\nWhich $k$ seems to work well given the task?\nWould you prefer a logistic regression over a KNN learner?\n\n:::{.content-hidden unless-meta=\"params.showsolution\"}\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n$k$ around 22 seems to perform best (in terms of accuracy). \nA too small $k$ underfits, a large one overfits.\nNot knowing the true $k$, a logistic regression seems preferable.\nIf $k$ is too small, the average performance of the logistic regression is much better.\nIf $k$ is too large, the variance of the performance is much higher compared to the logistic regression.\nHowever, a KNN an optimal $k$ has a consistently high accuracy.\n\n:::\n\n:::\n\n# Extra: Parallelize your efforts\n\nBenchmarking is **embarassingly** parallel.\nThat means it is very easy to run the experiments of the benchmarking on different machines or cores.\nIn many cases (not all!), this can significantly speed up computation time.\nWe recommend to do this using the `future::plan` function when paralellizing `mlr3` benchmarks.\n\n<details>\n  <summary>**Show Hint 1:**</summary>\n  You need to use the `plan` function twice.\n  Once to set up a `multisession`, then go back to `parallel`.\n  </details>\n<details>\n  <summary>**Show Hint 2:**</summary>\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(future)\nplan(multisession)\n# your code                     \nplan(sequential)\n```\n:::\n\n  </details>\n\n:::{.content-hidden unless-meta=\"params.showsolution\"}\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load the packages\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(kknn)\nlibrary(future)\nlibrary(future.apply)\n\n# parallel plan\nplan(multisession)\nset.seed(100) # it is good practice to set a seed before \nbmr_par = benchmark(design)                     \nplan(sequential)\n```\n:::\n\n\n:::\n\n:::\n\n# Summary\n\nWe learnt how to set benchmark in `mlr3`.\nWhile we only looked at a single task and a single resampling, the procedure easily applies to more complex benchmarks with many tasks.\nAdditionally, we learnt how to understand benchmark results.\nLast but not least, you may have parallelized your benchmark if you still had some time left.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}