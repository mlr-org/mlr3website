{
  "hash": "11411ad6bfd01c63f0ee717d013ee33c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Calibration with probably \ncategories:\n  - calibration\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Learn the basics of `tidymodels` for supervised learning, assess if a model is well-calibrated, and calibrate it with `probably`.\ndate: \"\"\nparams:\n  showsolution: true\n  base64encode: true\nlisting: false\nsearch: false\nformat:\n  html:\n    filters:\n      - ../../b64_solution.lua\n---\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n```{=html}\n<script>\nconst correctHash = \"288280f289520aa6fb221ca3a16e024958bb131a0dcabbca6a5439023d6a2e4e\";   // value injected by knitr\n\n/* ---------- reusable helper ---------- */\nfunction b64DecodeUtf8(b64) {\n  // 1) atob  -> binary-string   (bytes 0…255)\n  // 2) map   -> Uint8Array      (array of bytes)\n  // 3) TextDecoder('utf-8')     -> real JS string\n  const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));\n  return new TextDecoder('utf-8').decode(bytes);\n}\n\nasync function sha256(txt) {\n  const buf = await crypto.subtle.digest('SHA-256',\n                 new TextEncoder().encode(txt));\n  return Array.from(new Uint8Array(buf))\n              .map(b => b.toString(16).padStart(2, '0')).join('');\n}\n\nasync function unlockOne(btn) {\n  const pass = prompt(\"Password:\");\n  if (!pass) return;\n  if (await sha256(pass) !== correctHash) {\n    alert(\"Wrong password\"); return;\n  }\n\n  /* --- decode only the solution that belongs to THIS button --- */\n  const wrapper = btn.parentElement;             // .b64-wrapper\n  wrapper.querySelectorAll('.hidden-solution').forEach(div => {\n    div.innerHTML = b64DecodeUtf8(div.dataset.encoded);\n    div.classList.remove('hidden-solution');\n    div.style.display = 'block';\n  });\n\n  /* Remove the button so the user can’t click it again */\n  btn.remove();\n}\n</script>\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n    <strong>JavaScript is required to unlock solutions.</strong><br>\n    Please enable JavaScript and reload the page,<br>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n```\n\n\n\n\n\n# Goal\n\nOur goal for this exercise sheet is to learn the basics of `tidymodels` for supervised learning, assess if a model is \nwell-calibrated, and calibrate it with `probably`.\nWe will do so by using a train-validate-test split: the model is first fitted on training data and then calibrated on validation data. Test data will be used to compare the performance of the un-calibrated and calibrated models. Later, we will see how `probably` can be used to construct more refined resampling workflows for calibration.\n\n# Required packages\n\nWe will use `tidymodels` for machine learning and `probably` for calibration:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(probably)\nlibrary(discrim)\nlibrary(gridExtra)\n\ntidymodels_prefer()\n\nset.seed(12345)\n```\n:::\n\n\n\n# Data: predicting cell segmentation quality\n\nThe `modeldata` package contains a data set called `cells`. Initially distributed by [Hill and Haney (2007)](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340), they showed how to create models that predict the _quality_ of the image analysis of cells. The outcome has two levels `\"PS\"` (for poorly segmented images) or `\"WS\"` (well-segmented). There are 56 image features that can be used to build a classifier. \n\nLet's load the data and remove an unwanted column: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(cells)\ncells$case <- NULL\n```\n:::\n\n\n\n# 1 Checking cardinality properties\n\n## 1.1 Creating a split with rsample\n\nLet's use `initial_split()` from `rsample`, a package integrated in tidymodels, to create a train-test-validate split. To do so, split the data twice: First, create a simple train-test split, and then split the training data again into training and validation data.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can find an overview of functions within `rsample` on the [package website](https://rsample.tidymodels.org/reference/index.html).\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nIn principle, you will need three functions to create the necessary split objects: `initial_split()`, `training()`, and `testing()`.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrainval_test_split <- initial_split(cells, prop = 0.6, strata = class)\ncells_train_val <- training(trainval_test_split)\ntrain_val_split <- initial_split(cells_train_val, prop = 0.7, strata = class)\n\ncells_train <- training(train_val_split)\ncells_val <- testing(train_val_split)\ncells_test <- testing(trainval_test_split)\n```\n:::\n\n\n\n:::\n\n:::\n\n## 1.2 Training a Naive Bayes model\n\nWe'll show the utility of calibration tools by using a type of model that, in this instance, is likely to produce a poorly calibrated model. The Naive Bayes classifier is a well-established model that assumes that the predictors are statistically _independent_ of one another (to simplify the calculations).  While that is certainly not the case for this data, the model can be effective at discriminating between the classes. Unfortunately, when there are many predictors in the model, it has a tendency to produce class probability distributions that are pathological. The predictions tend to gravitate to values near zero or one, producing distributions that are \"U\"-shaped ([Kuhn and Johnson, 2013](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Applied+Predictive+Modeling%22&btnG=)). \n\nTo demonstrate, let's set up a Naive Bayes learner and train it using the training data.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nFirst, initialize a learner object. You can find a list of available learners [here](https://www.tidymodels.org/find/parsnip/).\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nOnce the learner object has been created, train the model using the `%>%` pipe operator and the function `fit()`.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(klaR)\nlearner <- naive_Bayes()\nnb_fit <- learner %>% fit(class ~ ., data = cells_train)\n```\n:::\n\n\n\n:::\n\n:::\n\n## 1.3 Predicting on unseen test data\n\nNext, use the trained model to predict on unseen test data. In `tidymodels`, this will return two columns of predicted probabilities, one for `\"PS\"` and one for `\"WS\"`. Select for the predicted probabilities of class `\"PS\"` since for binary classification, we only need one column representing label probabilities to compute the model performance in the next step. You can do this by repeatedly using the `%>%` pipe operator.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can use this code skeleton to produce the desired output:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredictions_test <- # the model # %>%\n  # predict probabilities on test data # %>%\n  select(.pred_PS) %>%\n  bind_cols(cells_test)\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredictions_test <- nb_fit %>%\n  predict(cells_test, type = \"prob\") %>%\n  select(.pred_PS) %>%\n  bind_cols(cells_test)\n\nhead(predictions_test)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 5\n  .pred_PS class angle_ch_1 area_ch_1 avg_inten_ch_1\n     <dbl> <fct>      <dbl>     <int>          <dbl>\n1 1.00e+ 0 PS         134.        819           31.9\n2 1.00e+ 0 PS          69.2       298           19.5\n3 5.84e- 9 WS         174.        177          260. \n4 7.88e-10 WS          13.7       424          174. \n5 1.60e-10 WS          23.1       313          215. \n6 1.00e+ 0 PS          93.6       762           35.6\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## 1.4 Assessing model performance\n\nWe want to use two metrics from the `yardstick` package that comes with `tidymodels` to judge how well the model performs. First, the area under the ROC curve (ROC AUC) is used to measure the ability of the model to separate the classes (its mere ability to discriminate correctly). Second, the Brier score is used to measure how close the probability estimates are to the actual outcome values (its ability to express confidence in the label predictions). Compute the ROC AUC and the Brier Score on the test data predictions.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nLook at the [functions available](https://yardstick.tidymodels.org/reference/index.html) in `yardstick` to find the desired metrics.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nroc_auc(predictions_test, truth = class, .pred_PS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.859\n```\n\n\n:::\n\n```{.r .cell-code}\nbrier_class(predictions_test, class, .pred_PS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 brier_class binary         0.209\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## 1.5 Assessing model calibration\n\nLet's assess if our model is well-calibrated. Spoiler: it is not. The first clue is the extremely U-shaped distribution of the probability scores:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(predictions_test$.pred_PS, breaks = (0:10)/10)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nThere are almost no cells with moderate probability estimates.\n\nThe `probably` package has tools for visualizing and correcting models with poor calibration properties. \n\nThe most common plot is to break the predictions into about ten equally sized buckets and compute the actual event rate within each. For example, if a bin captures the samples predicted to be poorly segmented with probabilities between 20% and 30%, we should expect about a 25% event rate (i.e., the bin midpoint) within that partition.\nLet's do this with 10 and 5 bins using `cal_plot_breaks()` from the `probably` package.\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nuncalibrated10 <- cal_plot_breaks(predictions_test, truth = class, num_breaks = 10) + ggtitle(\"Uncalibrated - 10 bins\")\nuncalibrated5 <- cal_plot_breaks(predictions_test, truth = class, num_breaks = 5) + ggtitle(\"Uncalibrated - 5 bins\")\ngrid.arrange(uncalibrated10, uncalibrated5, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n:::\n\n:::\n\n# 2 Platt scaling\n\nThe good news is that we can do something about this. There are tools to \"fix\" the probability estimates so that they have better properties, such as falling along the diagonal lines in the diagnostic plots shown above. Different methods improve the predictions in different ways. \n\nThe most common approach is Platt scaling, also called logistic calibration. This works in the following way: The original model is used to predict on the validation data set, returning predicted probabilities. We already know these probabilities are off, i.e., not well-calibrated. Then, we fit a simple logistic regression model, using the predicted probabilities from the original model as \"x/feature\" and the true labels as \"y/target\". In this way, we stack a second model on top of the first one: Original Model -> Probabilities -> Logistic Regression Model -> Calibrated Probabilities.\n\nIf effective, the logistic regression model estimates the probability regions where the original model is off (as shown in the diagnostic plot). For example, suppose that when the model predicts a 2% event rate, the logistic regression model estimates that it under-predicts the probability by 5% (relative to the observed data). Given this gap, new predictions are adjusted up so that the probability estimates are more in-line with the data.  \n\n## 2.1 Platt scaling with {base R}\n\n## 2.1.1 Task\n\nLet's do this with base R. First, we need predictions of the model on the validation data:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create predictions on the validation set\npredictions_val <- nb_fit %>%\n  predict(cells_val, type = \"prob\") %>%\n  select(.pred_PS) %>%\n  bind_cols(cells_val)\n```\n:::\n\n\n\nAlso, we transform the class variable (a factor) to binary (0-1) format:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredictions_val$class <- abs(as.numeric(predictions_val$class) - 2)\n```\n:::\n\n\n\nNow, fit a logistic regression with y as the true label of the validation data and x as the predictions of the uncalibrated model on the validation data.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nAs we want to fit a logistic regression model, you can use `glm()`.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncalibrator_logreg <- glm(class ~ .pred_PS, family = binomial, data = predictions_val)\n```\n:::\n\n\n\n:::\n\n:::\n\n## 2.1.2 Discussion\n\nHow do we know if this works? As we used training data to fit the Naive Bayes model and validation data to fit the logistic regression model, we need to use testing data to assess the performance of the combined (and hopefully well-calibrated) model:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Use logistic calibration model to adjust predictions on test data\npredictions_test_calibrated <- predictions_test\npredictions_test_calibrated$.pred_PS <- predict(calibrator_logreg, newdata = predictions_test_calibrated, type = \"response\")\n```\n:::\n\n\n\nHow well-calibrated is this model? Assess the model performance by checking the ROC AUC and the Brier score of the calibrated model and the model calibration by looking at the histogram of predicted probabilities and by comparing calibration plots of the calibrated model to the uncalibrated model (as in exercises 4 and 5).\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nroc_auc(predictions_test_calibrated, truth = class, .pred_PS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.859\n```\n\n\n:::\n\n```{.r .cell-code}\nbrier_class(predictions_test_calibrated, class, .pred_PS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 brier_class binary         0.159\n```\n\n\n:::\n:::\n\n\n\nWe can see a slight improvement: The brier score as evaluated on the test data improved. Interestingly, the ROC AUC remains unaffected. This should be no surprise, however, as the ROC AUC assesses a model's ability to discriminate between classes. If we simply squash the probabilities using the logistic function, this will likely not change the predicted labels too much.\n\nLet's check the histogram of predicted probabilities and compare calibration plots of the calibrated model to the uncalibrated model.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(predictions_test_calibrated$.pred_PS, breaks = (0:10)/10)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nlog_calibrated10 <- cal_plot_breaks(predictions_test_calibrated, truth = class, num_breaks = 10) + ggtitle(\"Log-calibrated - 10 bins\")\nlog_calibrated5 <- cal_plot_breaks(predictions_test_calibrated, truth = class, num_breaks = 5) + ggtitle(\"Log-calibrated - 5 bins\")\ngrid.arrange(uncalibrated10, log_calibrated10, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-2.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\ngrid.arrange(uncalibrated5, log_calibrated5, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-3.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nAn improvement can also be observed here, as the calibration diagrams of the calibrated model are closer to the diagonal than those of the uncalibrated model.\n\n:::\n\n:::\n\n## 2.2 Platt scaling with {probably}\n\n## 2.2.1 Task\n\nNow, apply Platt scaling using `cal_*` functions from `probably`.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can use the code skeleton below:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make predictions on the validation set (again because probably wants a factor as target)\npredictions_val <- nb_fit %>%\n  predict(cells_val, type = \"prob\") %>%\n  bind_cols(cells_val)\n\n# Calibrate the model using validation data\n## -> your code here <- ##\n\n# Predict with original model on test data\npredictions_test <- nb_fit %>%\n  predict(cells_test, type = \"prob\") %>%\n  bind_cols(cells_test)\n\n# Apply calibration model on original model predictions\n## -> your code here <- ##\n```\n:::\n\n\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nTake a look at the [function reference](https://probably.tidymodels.org/reference/index.html#calibration). You will need one function to compute the calibration model, and another function to recompute the model probabilities. As a sanity check, we should score the same metrics as when using base R.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Make predictions on the validation set (again because probably wants a factor as target)\npredictions_val <- nb_fit %>%\n  predict(cells_val, type = \"prob\") %>%\n  bind_cols(cells_val)\n\n# Calibrate the model using validation data\ncalibrator_logreg_alt = cal_estimate_logistic(predictions_val, truth = class)\n\n# Predict with original model on test data\npredictions_test <- nb_fit %>%\n  predict(cells_test, type = \"prob\") %>%\n  bind_cols(cells_test)\n\n# Apply calibration model on original model predictions\npredictions_test_calibrated_alt <- cal_apply(predictions_test, calibrator_logreg_alt)\n```\n:::\n\n\n\n:::\n\n:::\n\n## 2.2.2 Discussion\n\nLet's check if the metrics for the approach using `probably` match with the results we got using base R.\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Sanity check:\nroc_auc(predictions_test_calibrated_alt, truth = class, .pred_PS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.859\n```\n\n\n:::\n\n```{.r .cell-code}\nbrier_class(predictions_test_calibrated_alt, class, .pred_PS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 brier_class binary         0.159\n```\n\n\n:::\n:::\n\n\n\nIndeed, both approaches yield the same results.\n\n:::\n\n:::\n\n# 3 Isotonic regression\n\nA different approach to calibration is to use isotonic regression. In a manner similar to logistic calibration, use `probably` to calibrate the Naive Bayes model using isotonic regression. Do we improve the calibration in comparison to Platt scaling?\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\nNo, it seems like isotonic regression can achieve similar calibration performance here, but not improve further than what Platt scaling has achieved:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calibrate the model using validation data\ncalibrator_iso = cal_estimate_isotonic(predictions_val, truth = class)\n\n# Apply calibration model on original model predictions\npredictions_test_calibrated_iso <- cal_apply(predictions_test, calibrator_iso)\n\n# Do we improve?\nroc_auc(predictions_test_calibrated_iso, truth = class, .pred_PS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.857\n```\n\n\n:::\n\n```{.r .cell-code}\nbrier_class(predictions_test_calibrated_iso, class, .pred_PS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 brier_class binary         0.164\n```\n\n\n:::\n\n```{.r .cell-code}\niso_calibrated10 <- cal_plot_breaks(predictions_test_calibrated_iso, truth = class, num_breaks = 10) + ggtitle(\"Iso-calibrated - 10 bins\")\niso_calibrated5 <- cal_plot_breaks(predictions_test_calibrated_iso, truth = class, num_breaks = 5) + ggtitle(\"Iso-calibrated - 5 bins\")\ngrid.arrange(log_calibrated10, iso_calibrated10, log_calibrated5, iso_calibrated5, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n:::\n\n:::\n\n# 4 Resamping for Calibration with probably\n\nThe advantage of using `probably` for calibration is that one can easily access different calibration methods within a unified framework.\nIn the examples above, we have used a simple train-validate-test split to perform calibration and evaluate the resulting models. In practice, we would prefer to use resampling, e.g. as in cross-validation (CV), to assess calibration methods. This can also be done with `probably`. In fact, we can use the `cal_validate_*()` function family to calibrate models contained in `resample_results` objects created with `tidymodels`. Fit 10 Naive Bayes models using 10-fold CV and calibrate each of these models using beta calibration, another calibration method.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can use the following code skeleton, detailing the required steps:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create CV specification\ncells_cv <- vfold_cv(...)\n\n# Specify metrics\nmetr <- metric_set(...)\n\n# Save predictions\nctrl <- control_resamples(save_pred = TRUE)\n\n# Model workflow\nbayes_wflow <- workflow() %>%\n  add_formula(class ~ .) %>%\n  add_model(naive_Bayes())\n\n# Fit resamples\nbayes_res <- fit_resamples(...)\n\n# Calibrate each of the models\nbeta_val <- cal_validate_beta(...)\n\n# Evaluate calibrated models\ncollect_metrics(beta_val)\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create CV specification\ncells_cv <- vfold_cv(cells, strata = class) # 10 folds per default\n\n# Specify metrics\nmetr <- metric_set(roc_auc, brier_class)\n\n# Save predictions\nctrl <- control_resamples(save_pred = TRUE)\n\n# Model workflow\nbayes_wflow <- workflow() %>%\n  add_formula(class ~ .) %>%\n  add_model(naive_Bayes())\n\n# Fit resamples\nbayes_res <- fit_resamples(\n  object = bayes_wflow,\n  metrics = metr,\n  resamples = cells_cv,\n  control = ctrl\n)\n\n# Calibrate each of the models\nbeta_val <- cal_validate_beta(bayes_res, metrics = metr, save_pred = TRUE)\n\n# Evaluate calibrated models\ncollect_metrics(beta_val)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 7\n  .metric     .type        .estimator  mean     n std_err .config\n  <chr>       <chr>        <chr>      <dbl> <int>   <dbl> <chr>  \n1 brier_class uncalibrated binary     0.210    10 0.0111  config \n2 roc_auc     uncalibrated binary     0.849    10 0.0133  config \n3 brier_class calibrated   binary     0.149    10 0.00602 config \n4 roc_auc     calibrated   binary     0.849    10 0.0132  config \n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\nLet's inspect the calibration plot: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_predictions(beta_val) %>%\n  filter(.type == \"calibrated\") %>%\n  cal_plot_breaks(truth = class, estimate = .pred_PS, num_breaks = 10) +\n  ggtitle(\"Beta calibration\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n# Summary\n\nIn this exercise sheet we learned how to fit a classification model on\na training task and how to assess its performance on unseen test data with\nthe help of `tidymodels`.\nWe showed how to split data manually into training and test data, and use `probably` to assess if a model is calibrated.\nWe discussed how Platt scaling/logistic calibration and isotonic regression can be used to improve calibration (although not perfectly calibrate the model here). Also note that in a real-world scenario, we would have wanted to refit the Naive Bayes classifier on the combined train-validate data once we have calibrated it, to mirror nested resampling and reduce bias.\nThis tutorial is a modified version of the [tidymodels introduction to calibration](https://www.tidymodels.org/learn/models/calibration/), used under [CC BY-SA license](https://github.com/tidymodels/tidymodels.org/).",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}