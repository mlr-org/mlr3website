{
  "hash": "32e7a3ebaae2683300b33ef8923e7404",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Feature Selection\ngroup: Advanced Feature Preprocessing\ncategories:\n  - feature preprocessing\n  - performance evaluation\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Select features from the german credit set and evaluate model performance.\ndate: 05-29-2025\nparams:\n  showsolution: true\n  base64encode: true\nlisting: false\nsearch: false\nformat:\n  html:\n    filters:\n      - ../../b64_solution.lua\n---\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n```{=html}\n<script>\nconst correctHash = \"a9c0353702122dc639b89b6bf83a2d0631ca712ee32a79f9ecbe9e853f18914b\";   // value injected by knitr\n\n/* ---------- reusable helper ---------- */\nfunction b64DecodeUtf8(b64) {\n  // 1) atob  -> binary-string   (bytes 0…255)\n  // 2) map   -> Uint8Array      (array of bytes)\n  // 3) TextDecoder('utf-8')     -> real JS string\n  const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));\n  return new TextDecoder('utf-8').decode(bytes);\n}\n\nasync function sha256(txt) {\n  const buf = await crypto.subtle.digest('SHA-256',\n                 new TextEncoder().encode(txt));\n  return Array.from(new Uint8Array(buf))\n              .map(b => b.toString(16).padStart(2, '0')).join('');\n}\n\nasync function unlockOne(btn) {\n  const pass = prompt(\"Password:\");\n  if (!pass) return;\n  if (await sha256(pass) !== correctHash) {\n    alert(\"Wrong password\"); return;\n  }\n\n  /* --- decode only the solution that belongs to THIS button --- */\n  const wrapper = btn.parentElement;             // .b64-wrapper\n  wrapper.querySelectorAll('.hidden-solution').forEach(div => {\n    div.innerHTML = b64DecodeUtf8(div.dataset.encoded);\n    div.classList.remove('hidden-solution');\n    div.style.display = 'block';\n  });\n\n  /* Remove the button so the user can’t click it again */\n  btn.remove();\n}\n</script>\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n    <strong>JavaScript is required to unlock solutions.</strong><br>\n    Please enable JavaScript and reload the page,<br>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n```\n\n\n\n\n\n# Goal\n\nAfter this exercise, you should understand and be able to perform feature selection using wrapper functions with `mlr3fselect`. You should also be able to integrate various performance measures and calculate the generalization error.\n\n# Wrapper Methods\n\nIn addition to filtering, wrapper methods are another variant of selecting features. While in filtering conditions for the feature values are set, in wrapper methods the learner is applied to different subsets of the feature set. As models need to be refitted, this method is computationally expensive.\n\nFor wrapper methods, we need the package `mlr3fselect`, at whose heart the following `R6` classes are:\n\n- `FSelectInstanceSingleCrit`, `FSelectInstanceMultiCrit`: These two classes describe the feature selection problem and store the results.\n- `FSelector`: This class is the base class for implementations of feature selection algorithms.\n\n# Prerequisites\n\nWe load the most important packages and use a fixed seed for reproducibility. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(data.table)\nlibrary(mlr3fselect)\nset.seed(7891)\n```\n:::\n\n\n\nIn this exercise, we will use the `german_credit` data and the learner `classif.ranger`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_gc = tsk(\"german_credit\")\nlrn_ranger = lrn(\"classif.ranger\")\n```\n:::\n\n\n\n# 1 Basic Application\n\n## 1.1 Create the Framework\n\nCreate an `FSelectInstanceSingleCrit` object using `fsi()`. The instance should use a 3-fold cross validation, classification accuracy as the `measure` and terminate after 20 evaluations. For simplification only consider the features `age`, `amount`, `credit_history` and `duration`.\n\n<details>\n<summary>**Hint 1:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_gc$select(...)\n\ninstance = fsi(\n  task = ...,\n  learner = ...,\n  resampling = ...,\n  measure = ...,\n  terminator = ...\n)\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_gc$select(c(\"age\", \"amount\", \"credit_history\", \"duration\"))\n\ninstance = fsi(\n  task = task_gc,\n  learner = lrn_ranger,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals = 20)\n)\n```\n:::\n\n\n\n:::\n\n:::\n\n\n## 1.2 Start the Feature Selection\n\nStart the feature selection step by selecting `sequential` using the `FSelector` class via `fs()` and pass the `FSelectInstanceSingleCrit` object to the `$optimize()` method of the initialized `FSelector` object.\n\n<details>\n<summary>**Hint 1:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfselector = fs(...)\n```\n:::\n\n\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfselector = fs(...)\nfselector$optimize(...)\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfselector = fs(\"sequential\")\nfselector$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [16:25:55.661] [bbotk] Starting to optimize 4 parameter(s) with '<FSelectorBatchSequential>' and '<TerminatorEvals> [n_evals=20, k=0]'\nINFO  [16:25:55.670] [bbotk] Evaluating 4 configuration(s)\nINFO  [16:25:55.678] [mlr3] Running benchmark with 12 resampling iterations\nINFO  [16:25:55.896] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/3)\nINFO  [16:25:56.110] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/3)\nINFO  [16:25:56.341] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/3)\nINFO  [16:25:56.572] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/3)\nINFO  [16:25:56.800] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/3)\nINFO  [16:25:57.030] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/3)\nINFO  [16:25:57.261] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/3)\nINFO  [16:25:57.489] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/3)\nINFO  [16:25:57.714] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/3)\nINFO  [16:25:57.941] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/3)\nINFO  [16:25:58.126] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/3)\nINFO  [16:25:58.186] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/3)\nINFO  [16:25:58.296] [mlr3] Finished benchmark\nINFO  [16:25:58.330] [bbotk] Result of batch 1:\nINFO  [16:25:58.331] [bbotk]    age amount credit_history duration classif.acc warnings errors runtime_learners                                uhash\nINFO  [16:25:58.331] [bbotk]   TRUE  FALSE          FALSE    FALSE   0.6620063        0      0            0.207 f423e41e-2cc8-4e20-9dc4-9744cf10f108\nINFO  [16:25:58.331] [bbotk]  FALSE   TRUE          FALSE    FALSE   0.6020272        0      0            0.351 a5eac816-9a3c-4e55-9a6e-b8358f66d771\nINFO  [16:25:58.331] [bbotk]  FALSE  FALSE           TRUE    FALSE   0.7170314        0      0            0.090 e1ceaad7-836f-40fb-9513-24f0bd52732f\nINFO  [16:25:58.331] [bbotk]  FALSE  FALSE          FALSE     TRUE   0.7020224        0      0            0.139 35df5fa5-2835-4d88-92fe-2efea391dc15\nINFO  [16:25:58.334] [bbotk] Evaluating 3 configuration(s)\nINFO  [16:25:58.338] [mlr3] Running benchmark with 9 resampling iterations\nINFO  [16:25:58.423] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/3)\nINFO  [16:25:58.514] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/3)\nINFO  [16:25:58.658] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/3)\nINFO  [16:25:58.749] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/3)\nINFO  [16:25:58.841] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/3)\nINFO  [16:25:58.931] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/3)\nINFO  [16:25:59.062] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/3)\nINFO  [16:25:59.159] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/3)\nINFO  [16:25:59.251] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/3)\nINFO  [16:25:59.295] [mlr3] Finished benchmark\nINFO  [16:25:59.311] [bbotk] Result of batch 2:\nINFO  [16:25:59.311] [bbotk]    age amount credit_history duration classif.acc warnings errors runtime_learners                                uhash\nINFO  [16:25:59.311] [bbotk]   TRUE  FALSE           TRUE    FALSE   0.7110314        0      0            0.148 30dbdae2-b21d-4d86-ae96-0a905ee911e2\nINFO  [16:25:59.311] [bbotk]  FALSE   TRUE           TRUE    FALSE   0.7170314        0      0            0.166 5aeb18af-c79d-47f6-b22d-32e1ecae53fb\nINFO  [16:25:59.311] [bbotk]  FALSE  FALSE           TRUE     TRUE   0.7180324        0      0            0.131 2a0fb85b-9bb0-47a7-bfce-775a159f5f3d\nINFO  [16:25:59.314] [bbotk] Evaluating 2 configuration(s)\nINFO  [16:25:59.317] [mlr3] Running benchmark with 6 resampling iterations\nINFO  [16:25:59.402] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/3)\nINFO  [16:25:59.493] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/3)\nINFO  [16:25:59.634] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/3)\nINFO  [16:25:59.716] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/3)\nINFO  [16:25:59.844] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/3)\nINFO  [16:25:59.937] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/3)\nINFO  [16:26:00.001] [mlr3] Finished benchmark\nINFO  [16:26:00.014] [bbotk] Result of batch 3:\nINFO  [16:26:00.017] [bbotk]    age amount credit_history duration classif.acc warnings errors runtime_learners                                uhash\nINFO  [16:26:00.017] [bbotk]   TRUE  FALSE           TRUE     TRUE   0.7180264        0      0            0.193 0540feff-5a26-44da-b2d2-28652628a48d\nINFO  [16:26:00.017] [bbotk]  FALSE   TRUE           TRUE     TRUE   0.7180324        0      0            0.200 f2bdb4d9-1e8b-467b-8e05-8a7c98a8b028\nINFO  [16:26:00.020] [bbotk] Evaluating 1 configuration(s)\nINFO  [16:26:00.022] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [16:26:00.110] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 1/3)\nINFO  [16:26:00.206] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 2/3)\nINFO  [16:26:00.294] [mlr3] Applying learner 'classif.ranger' on task 'german_credit' (iter 3/3)\nINFO  [16:26:00.406] [mlr3] Finished benchmark\nINFO  [16:26:00.415] [bbotk] Result of batch 4:\nINFO  [16:26:00.416] [bbotk]   age amount credit_history duration classif.acc warnings errors runtime_learners                                uhash\nINFO  [16:26:00.416] [bbotk]  TRUE   TRUE           TRUE     TRUE   0.6990254        0      0            0.322 8e050827-2673-4821-8dfb-777372ca4470\nINFO  [16:26:00.421] [bbotk] Finished optimizing after 10 evaluation(s)\nINFO  [16:26:00.421] [bbotk] Result:\nINFO  [16:26:00.421] [bbotk]     age amount credit_history duration                features n_features classif.acc\nINFO  [16:26:00.421] [bbotk]  <lgcl> <lgcl>         <lgcl>   <lgcl>                  <list>      <int>       <num>\nINFO  [16:26:00.421] [bbotk]   FALSE  FALSE           TRUE     TRUE credit_history,duration          2   0.7180324\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      age amount credit_history duration                features n_features classif.acc\n   <lgcl> <lgcl>         <lgcl>   <lgcl>                  <list>      <int>       <num>\n1:  FALSE  FALSE           TRUE     TRUE credit_history,duration          2   0.7180324\n```\n\n\n:::\n:::\n\n\n\nThe two calls (`fsi()`and `fs()`) can also be executed by one sugar function (`fselect()`): \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = fselect(\n  fselector = fs(\"sequential\"),\n  task =  task_gc,\n  learner = lrn_ranger,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\")\n)\n```\n:::\n\n\n\n:::\n\n:::\n\n## 1.3 Evaluate \n\nView the four characteristics and the accuracy from the instance archive for each of the first two batches. \n\n<details>\n<summary>**Hint 1:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$archive$data[...]\n```\n:::\n\n\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$archive$data[batch_nr == ..., ...]\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$archive$data[batch_nr == 1, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      age amount credit_history duration classif.acc\n   <lgcl> <lgcl>         <lgcl>   <lgcl>       <num>\n1:   TRUE  FALSE          FALSE    FALSE   0.6620063\n2:  FALSE   TRUE          FALSE    FALSE   0.6020272\n3:  FALSE  FALSE           TRUE    FALSE   0.7170314\n4:  FALSE  FALSE          FALSE     TRUE   0.7020224\n```\n\n\n:::\n:::\n\n\n\nThe highest accuracy results when the model relies on `credit_history`. So for the next batch, only combinations with `credit_history` are considered:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$archive$data[batch_nr == 2, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      age amount credit_history duration classif.acc\n   <lgcl> <lgcl>         <lgcl>   <lgcl>       <num>\n1:   TRUE  FALSE           TRUE    FALSE   0.7110314\n2:  FALSE   TRUE           TRUE    FALSE   0.7170314\n3:  FALSE  FALSE           TRUE     TRUE   0.7180324\n```\n\n\n:::\n:::\n\n\n\nWe see that the accuracy increases when using the two features `amount` and `credit_history` compared to using only `credit_history`.\n\nAlternatively with `data.table`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[batch_nr == 1, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      age amount credit_history duration classif.acc\n   <lgcl> <lgcl>         <lgcl>   <lgcl>       <num>\n1:   TRUE  FALSE          FALSE    FALSE   0.6620063\n2:  FALSE   TRUE          FALSE    FALSE   0.6020272\n3:  FALSE  FALSE           TRUE    FALSE   0.7170314\n4:  FALSE  FALSE          FALSE     TRUE   0.7020224\n```\n\n\n:::\n\n```{.r .cell-code}\nas.data.table(instance$archive)[batch_nr == 2, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      age amount credit_history duration classif.acc\n   <lgcl> <lgcl>         <lgcl>   <lgcl>       <num>\n1:   TRUE  FALSE           TRUE    FALSE   0.7110314\n2:  FALSE   TRUE           TRUE    FALSE   0.7170314\n3:  FALSE  FALSE           TRUE     TRUE   0.7180324\n```\n\n\n:::\n:::\n\n\n\nA visualization of all batches can be created via `autoplot`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(instance, type = \"performance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n:::\n\n:::\n\n## 1.4 Model Training\n\nWhich feature(s) should be selected? Train the model.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nCompare the accuracy values for the different feature combinations and select the feature(s) accordingly.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_gc = ...\ntask_gc$select(...)\nlrn_ranger$train(...)\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\nThe feature(s) resulting in the highest accuracy should be selected, so, here `amount` and `credit_history`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_gc = tsk(\"german_credit\")\ntask_gc$select(instance$result_feature_set)\ntask_gc$feature_names\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"credit_history\" \"duration\"      \n```\n\n\n:::\n\n```{.r .cell-code}\nlrn_ranger$train(task_gc)\n```\n:::\n\n\n\n:::\n\n:::\n\n# 2 Multiple Performance Measures\n\nTo optimize multiple performance metrics, the same steps must be followed as above except that multiple metrics are passed. Create an ´instance´ object as above considering the measures `classif.tpr` and `classif.tnr`. For the second step use `random search` and take a look at the results in a third step.\n\nWe again use the `german_credit` data:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_gc = tsk(\"german_credit\")\n```\n:::\n\n\n\n<details>\n<summary>**Hint 1:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = fsi(...)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfselector = fs(...)\nfselector$...(...)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeatures = unlist(lapply(...))\ncbind(features,...)\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = fsi(\n  task = task_gc,\n  learner = lrn_ranger,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msrs(c(\"classif.tpr\", \"classif.tnr\")),\n  terminator = trm(\"evals\", n_evals = 20)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfselector = fs(\"random_search\")\nfselector$optimize(instance)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeatures = unlist(lapply(instance$result$features, paste, collapse = \", \"))\ncbind(features,instance$result[,c(\"classif.tpr\", \"classif.tnr\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                                                                                                                                                                                                       features\n                                                                                                                                                                                                                         <char>\n1:                                                                                                                                                                                                          age, number_credits\n2:               age, amount, duration, employment_duration, foreign_worker, housing, installment_rate, job, number_credits, other_debtors, people_liable, personal_status_sex, present_residence, property, savings, telephone\n3:                                                                                                                                                                              amount, credit_history, other_installment_plans\n4:                 age, amount, credit_history, duration, foreign_worker, housing, other_debtors, other_installment_plans, people_liable, personal_status_sex, present_residence, property, purpose, savings, status, telephone\n5:                                                                                                                                                                                                          employment_duration\n6:                                                                                                                                                                                                  employment_duration, status\n7: age, amount, credit_history, duration, employment_duration, foreign_worker, housing, installment_rate, job, number_credits, other_debtors, people_liable, personal_status_sex, property, purpose, savings, status, telephone\n   classif.tpr classif.tnr\n         <num>       <num>\n1:   0.9913654 0.003333333\n2:   0.9187075 0.276176362\n3:   0.9431759 0.162080855\n4:   0.8956744 0.450217428\n5:   1.0000000 0.000000000\n6:   0.9613529 0.075780489\n7:   0.9152751 0.447213765\n```\n\n\n:::\n:::\n\n\nOr with `data.table`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$result)[, .(features, classif.tpr, classif.tnr)]\n```\n:::\n\n\n\nNote that the measures can not be optimal at the same time so one has to choose according to their preferences. Here, we see different tradeoffs of sensitivity and specificity but no feature subset is dominated by another, i.e. has worse sensitivity and specificity than any other subset.\n\n:::\n\n:::\n\n# 3 Nested Resampling\n\nNested resampling enables finding unbiased performance estimators for the selection of features. In `mlr3` this is possible with the class `AutoFSelector`, whose instance can be created by the function `auto_fselector()`.\n\n## 3.1 Create an `AutoFSelector` Instance\n\nImplement an `AutoFSelector` object that uses random search to find a feature selection that gives the highest accuracy for a logistic regression with holdout resampling. It should terminate after 10 evaluations.\n\n<details>\n<summary>**Hint 1:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nafs = auto_fselector(\n  fselector = ...,\n  learner = ...,\n  resampling = ...,\n  measure = ...,\n  terminator = ...\n)\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nafs = auto_fselector(\n  fselector = fs(\"random_search\"),\n  learner = lrn(\"classif.log_reg\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"evals\", n_evals = 10)\n)\nafs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<AutoFSelector:classif.log_reg.fselector>\n* Model: list\n* Packages: mlr3, mlr3fselect, mlr3learners, stats\n* Predict Type: response\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: offset, twoclass, weights\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## 3.2 Benchmark\n\nCompare the `AutoFSelector` with a normal logistic regression using 3 fold cross-validation.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nThe `AutoFSelector` inherits from the `Learner` base class, which is why it can be used like any other learner.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nImplement a benchmark grid and aggregate the result.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrid = benchmark_grid(tsk(\"sonar\"), list(afs, lrn(\"classif.log_reg\")),\n  rsmp(\"cv\", folds = 3))\n\nbmr = benchmark(grid)$aggregate(msr(\"classif.acc\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(bmr)[, .(learner_id, classif.acc)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  learner_id classif.acc\n                      <char>       <num>\n1: classif.log_reg.fselector   0.7209800\n2:           classif.log_reg   0.6970324\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n# Summary\n\n- Wrapper methods calculate performance measures for various combinations of features in order to perform feature selection.\n- They are computationally expensive since several models need to be fitted.\n- The `AutoFSelector` inherits from the `Learner` base class, which is why it can be used like any other learner.\n  \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}