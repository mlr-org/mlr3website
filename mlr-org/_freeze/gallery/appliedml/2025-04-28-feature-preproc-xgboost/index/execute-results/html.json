{
  "hash": "9ab7006a6bbdd2e097434efe06cca6b6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Xgboost\ncategories:\n  - hyperparameter optimization\n  - xgboost\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Optimize hyperparameters of xgboost for german credit task.\ndate: \"\"\nparams:\n  showsolution: true\n  base64encode: true\nlisting: false\nsearch: false\nformat:\n  html:\n    filters:\n      - ../../b64_solution.lua\n---\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n```{=html}\n<script>\nconst correctHash = \"d85ff4c089269173da11bc1ce6701753d1f2ea8b49e5b907efeedfbb37568708\";   // value injected by knitr\n\n/* ---------- reusable helper ---------- */\nfunction b64DecodeUtf8(b64) {\n  // 1) atob  -> binary-string   (bytes 0…255)\n  // 2) map   -> Uint8Array      (array of bytes)\n  // 3) TextDecoder('utf-8')     -> real JS string\n  const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));\n  return new TextDecoder('utf-8').decode(bytes);\n}\n\nasync function sha256(txt) {\n  const buf = await crypto.subtle.digest('SHA-256',\n                 new TextEncoder().encode(txt));\n  return Array.from(new Uint8Array(buf))\n              .map(b => b.toString(16).padStart(2, '0')).join('');\n}\n\nasync function unlockOne(btn) {\n  const pass = prompt(\"Password:\");\n  if (!pass) return;\n  if (await sha256(pass) !== correctHash) {\n    alert(\"Wrong password\"); return;\n  }\n\n  /* --- decode only the solution that belongs to THIS button --- */\n  const wrapper = btn.parentElement;             // .b64-wrapper\n  wrapper.querySelectorAll('.hidden-solution').forEach(div => {\n    div.innerHTML = b64DecodeUtf8(div.dataset.encoded);\n    div.classList.remove('hidden-solution');\n    div.style.display = 'block';\n  });\n\n  /* Remove the button so the user can’t click it again */\n  btn.remove();\n}\n</script>\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n    <strong>JavaScript is required to unlock solutions.</strong><br>\n    Please enable JavaScript and reload the page,<br>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n```\n\n\n\n\n# Goal\nOur goal for this exercise sheet is to understand how to apply and work with XGBoost. \nThe XGBoost algorithm has a large range of hyperparameters. \nWe learn specifically how to tune these hyperparameters to optimize our \nXGBoost model for the task at hand.\n\n# German Credit Dataset\nAs in previous exercises, we use the German credit dataset of Prof. Dr. Hans Hoffman of the \nUniversity of Hamburg in 1994.\nBy using XGBoost, we want to classify people as a good or bad credit risk based \non 20 personal, demographic and financial features. \nThe dataset is available at the UCI repository as\n[Statlog (German Credit Data) Data Set](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).\n\n## Preprocessing\n\nTo apply the XGBoost algorithm to the `credit` dataset, categorical features \nneed to be converted into numeric features e.g. using one-hot-encoding. \nWe use a factor encoding `PipeOp` from `mlr3pipelines` to do so.\n\nFirst, we setup a classification task:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\ntask = tsk(\"german_credit\")\ntask$positive = \"good\"\n```\n:::\n\n\nNext, we can initialize a factor encoding and apply it to the task at hand.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoe = po(\"encode\")\ntask = poe$train(list(task))[[1]]\n```\n:::\n\n\n# 1 XGBoost Learner\n\n## 1.1 Initialize an XGBoost Learner\n\nInitialize a XGBoost `mlr3` learner with 100 iterations.\nMake sure that that you have installed the `xgboost` R package.\n\n<details>\n<summary>**Details on iterations:**</summary>\n\nThe number of iterations must always be chosen by the user, since \nthe hyperparameter has no proper default value in `mlr3`.\n\n\"No proper default value\" means that `mlr3` has an adjusted default \nof 1 iteration to avoid errors when constructing the learner. \nOne single iteration is, in general, not a good default, since we only conduct a \nsingle boosting step. \n\nThere is a trade-off between underfitting (not enough iterations) and \noverfitting (too many iterations). \nTherefore, it is always better to tune such a hyperparameter.\nIn this exercise, we chose 100 iterations because we believe it is an upper bound for the \nnumber of iterations. We will later conduct early stopping to avoid overfitting.\n\n</details>\n\n<details>\n<summary>**Hint 1:**</summary>\n\nThe number of iterations can be specified via the `nrounds` hyperparameter \nof the `classif.xgboost` learner, set this hyperparameter to `100`.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxgboost_lrn = lrn(..., nrounds = ...)\n```\n:::\n\n\n</details>\n\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxgboost_lrn = lrn(\"classif.xgboost\")\nxgboost_lrn$param_set$set_values(nrounds = 100L)\n```\n:::\n\n\n:::\n\n:::\n\n## 1.2 Performance Assessment using Cross-validation\n\nUse 5-fold cross-validation to estimate the generalization error of the\nXGBoost learner with 100 boosting iterations on the one-hot-encoded credit task. \nMeasure the performance of the learner using the classification error.\nSet up a seed to make your results reproducible (e.g., `set.seed(8002L)`). \n\n<details>\n<summary>**Hint 1:**</summary>\n\nSpecifically, you need to conduct three steps: \n\n1. Specify a `Resampling` object using `rsmp()`.\n2. Use this object together with the task and learner specified above \nas an input to the `resample()` method. \n3. Measure the performance with the `$aggregate()` method of the resulting `ResampleResult` object.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(8002L)\nresampling = rsmp(\"cv\", ...)\nrr = resample(task = ..., learner = ..., resampling = ...)\nrr$aggregate()\n```\n:::\n\n\n</details>\n\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(8002L)\nresampling = rsmp(\"cv\", folds = 5L)\nrr = resample(task = task, learner = xgboost_lrn, resampling = resampling)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [14:46:09.794] [mlr3] Applying learner 'classif.xgboost' on task 'german_credit' (iter 1/5)\nINFO  [14:46:10.428] [mlr3] Applying learner 'classif.xgboost' on task 'german_credit' (iter 2/5)\nINFO  [14:46:11.158] [mlr3] Applying learner 'classif.xgboost' on task 'german_credit' (iter 3/5)\nINFO  [14:46:12.009] [mlr3] Applying learner 'classif.xgboost' on task 'german_credit' (iter 4/5)\nINFO  [14:46:13.500] [mlr3] Applying learner 'classif.xgboost' on task 'german_credit' (iter 5/5)\n```\n\n\n:::\n\n```{.r .cell-code}\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.ce \n     0.253 \n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n# 2 Hyperparameters\n\n## 2.1 Overview of Hyperparameters\n\nApart from the number of iterations (`nrounds`), the XGBoost learner has a several other hyperparameters \nwhich were kept to their default values in the previous exercise. \nExtract an overview of all hyperparameters from the initalized XGBoost learner (previous exercise) as well as its default values. \n\nGiven the extracted hyperparameter list above and the help page of `xgboost` (`?xgboost`), \nanswer the following questions:\n\n- Does the learner rely on a tree or a linear booster by default?\n- Do more hyperparameters exist for the tree or the linear booster?\n- What do `max_depth` and `eta` mean and what are their default values?\n- Does a larger value for `eta` imply a larger value for `nrounds`? \n\n<details>\n<summary>**Hint 1:**</summary>\n\nThe hyperparameters and their default values could be extracted by\nthe `$param_set` field of the XGBoost learner.\nAlternatively, you could call the help page of `LearnerClassifXgboost`. \n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nYou can answer all questions concerning defaults with the output of the \n`$param_set`. A description of the hyperparameters could be found on \nthe `xgboost` help page (`?xgboost`). \nThe help page also offers an answer to the last question concerning the \nconnection between `eta` and `nrounds`. \n\n</details>\n\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\nThe `param_set` field gives an overview of the hyperparameters and their default values: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxgboost_lrn$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ParamSet(61)>\nKey: <id>\n                             id    class lower upper nlevels         default                  parents  value\n                         <char>   <char> <num> <num>   <num>          <list>                   <list> <list>\n 1:                       alpha ParamDbl     0   Inf     Inf               0                   [NULL] [NULL]\n 2:               approxcontrib ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]\n 3:                  base_score ParamDbl  -Inf   Inf     Inf             0.5                   [NULL] [NULL]\n 4:                     booster ParamFct    NA    NA       3          gbtree                   [NULL] [NULL]\n 5:                   callbacks ParamUty    NA    NA     Inf       <list[0]>                   [NULL] [NULL]\n 6:           colsample_bylevel ParamDbl     0     1     Inf               1                   [NULL] [NULL]\n 7:            colsample_bynode ParamDbl     0     1     Inf               1                   [NULL] [NULL]\n 8:            colsample_bytree ParamDbl     0     1     Inf               1                   [NULL] [NULL]\n 9:                      device ParamUty    NA    NA     Inf             cpu                   [NULL] [NULL]\n10: disable_default_eval_metric ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]\n11:       early_stopping_rounds ParamInt     1   Inf     Inf          [NULL]                   [NULL] [NULL]\n12:                         eta ParamDbl     0     1     Inf             0.3                   [NULL] [NULL]\n13:                 eval_metric ParamUty    NA    NA     Inf  <NoDefault[0]>                   [NULL] [NULL]\n14:            feature_selector ParamFct    NA    NA       5          cyclic                  booster [NULL]\n15:                       gamma ParamDbl     0   Inf     Inf               0                   [NULL] [NULL]\n16:                 grow_policy ParamFct    NA    NA       2       depthwise              tree_method [NULL]\n17:     interaction_constraints ParamUty    NA    NA     Inf  <NoDefault[0]>                   [NULL] [NULL]\n18:              iterationrange ParamUty    NA    NA     Inf  <NoDefault[0]>                   [NULL] [NULL]\n19:                      lambda ParamDbl     0   Inf     Inf               1                   [NULL] [NULL]\n20:                 lambda_bias ParamDbl     0   Inf     Inf               0                  booster [NULL]\n21:                     max_bin ParamInt     2   Inf     Inf             256              tree_method [NULL]\n22:              max_delta_step ParamDbl     0   Inf     Inf               0                   [NULL] [NULL]\n23:                   max_depth ParamInt     0   Inf     Inf               6                   [NULL] [NULL]\n24:                  max_leaves ParamInt     0   Inf     Inf               0              grow_policy [NULL]\n25:                    maximize ParamLgl    NA    NA       2          [NULL]                   [NULL] [NULL]\n26:            min_child_weight ParamDbl     0   Inf     Inf               1                   [NULL] [NULL]\n27:                     missing ParamDbl  -Inf   Inf     Inf              NA                   [NULL] [NULL]\n28:        monotone_constraints ParamUty    NA    NA     Inf               0                   [NULL] [NULL]\n29:              normalize_type ParamFct    NA    NA       2            tree                  booster [NULL]\n30:                     nrounds ParamInt     1   Inf     Inf  <NoDefault[0]>                   [NULL]    100\n31:                     nthread ParamInt     1   Inf     Inf               1                   [NULL]      1\n32:                  ntreelimit ParamInt     1   Inf     Inf          [NULL]                   [NULL] [NULL]\n33:           num_parallel_tree ParamInt     1   Inf     Inf               1                   [NULL] [NULL]\n34:                   objective ParamUty    NA    NA     Inf binary:logistic                   [NULL] [NULL]\n35:                    one_drop ParamLgl    NA    NA       2           FALSE                  booster [NULL]\n36:                outputmargin ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]\n37:                 predcontrib ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]\n38:             predinteraction ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]\n39:                    predleaf ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]\n40:               print_every_n ParamInt     1   Inf     Inf               1                  verbose [NULL]\n41:                process_type ParamFct    NA    NA       2         default                   [NULL] [NULL]\n42:                   rate_drop ParamDbl     0     1     Inf               0                  booster [NULL]\n43:                refresh_leaf ParamLgl    NA    NA       2            TRUE                   [NULL] [NULL]\n44:                     reshape ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]\n45:                 sample_type ParamFct    NA    NA       2         uniform                  booster [NULL]\n46:             sampling_method ParamFct    NA    NA       2         uniform                  booster [NULL]\n47:                   save_name ParamUty    NA    NA     Inf          [NULL]                   [NULL] [NULL]\n48:                 save_period ParamInt     0   Inf     Inf          [NULL]                   [NULL] [NULL]\n49:            scale_pos_weight ParamDbl  -Inf   Inf     Inf               1                   [NULL] [NULL]\n50:          seed_per_iteration ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]\n51:                   skip_drop ParamDbl     0     1     Inf               0                  booster [NULL]\n52:                strict_shape ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]\n53:                   subsample ParamDbl     0     1     Inf               1                   [NULL] [NULL]\n54:                       top_k ParamInt     0   Inf     Inf               0 feature_selector,booster [NULL]\n55:                    training ParamLgl    NA    NA       2           FALSE                   [NULL] [NULL]\n56:                 tree_method ParamFct    NA    NA       5            auto                  booster [NULL]\n57:      tweedie_variance_power ParamDbl     1     2     Inf             1.5                objective [NULL]\n58:                     updater ParamUty    NA    NA     Inf  <NoDefault[0]>                   [NULL] [NULL]\n59:                     verbose ParamInt     0     2       3               1                   [NULL]      0\n60:                   watchlist ParamUty    NA    NA     Inf          [NULL]                   [NULL] [NULL]\n61:                   xgb_model ParamUty    NA    NA     Inf          [NULL]                   [NULL] [NULL]\n                             id    class lower upper nlevels         default                  parents  value\n```\n\n\n:::\n:::\n\n\nTogether with the help page of `xgboost` the answers to the above questions are: \n\n- Does the learner rely on a tree or a linear booster per default?\n\nThe `booster` hyperparameter reveals that it relies on a `gbtree` and, therefore, \na tree booster per default.\n\n- Do more hyperparameters exist for the tree or the linear booster?\n\nAccording to the help page of `xgboost`, tree boosters have more hyperparameters than \nthe linear booster (only three are mentioned for the latter: `lambda`, `lambda_bias` and `alpha` for \nregularization of the linear booster).\n\n- What do `max_depth`, `eta` and `nrounds` mean and what are their default values?\n\n`max_depth` and `eta` affect the tree booster: `max_depth` gives the \ndepth of the tree with a default of 6 and `eta` specifies the learning rate, i.e., \nhow each tree contributes to the overall model, the default is `0.3`.\n\n- Does a larger value for `eta` imply a larger value for `nrounds`? \n\nA larger value of `eta` implies a lower value of `nrounds` according to the\nhelp page. Since each tree contributes more to the overall model due to a larger \n`eta`, the boosting \nmodel also starts to overfit faster which necessitates a lower value for `nrounds`.\n\n:::\n\n:::\n\n## 2.2 Tune Hyperparameters\n\nTune the the depth of tree of the `xgboost` learner on the German credit data using random search\n\n  - with the search space for `max_depth` between 1 and 8 and for `eta` between 0.2 and 0.4\n  - with 20 evaluations as termination criterion\n  - the classification error `msr(\"classif.ce\")` as performance measure\n  - 3-fold CV as resampling strategy.\n  \nSet a seed for reproducibility (e.g., `set.seed(8002L)`).\n\n<details>\n<summary>**Hint 1:**</summary>\n\nSpecifically, you should conduct the following steps: \n\n1. Setup a search space with `ps()` consisting of a `p_int()` for `max_depth` and `p_dbl()` for `eta`.\n2. Setup the classification error as a tuning measure with `msr()`. \n3. Initialize cross-validation as the resampling strategy using `rsmp()`. \n4. Setup 10 evaluations as the termination criterion using `trm()`. \n5. Initialize a `TuningInstanceSingleCrit` object using `ti()` and the objects\nproduced in steps 1.-4. as well as the task and learner as an input. \n6. Define random search as the tuner object using `tnr()`. \n7. Call the `$optimize()` method of the tuner object with the initialized \n`TuningInstanceSingleCrit` as an input. \n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(8002L)\n\nsearch_space = ps(\n  max_depth = ...(1L, 8L),\n  eta = ...(0.2, 0.4)\n)\nmeasure = msr(\"classif....\")\nresampling = rsmp(\"cv\", folds = ...)\nterminator = trm(\"evals\", n_evals = ...)\n\ninstance_random = ti(\n  task = ..., \n  learner = ..., \n  measure = ..., \n  resampling = ..., \n  terminator = ..., \n  search_space = ...\n)\n \ntuner_random = tnr(...)\ntuner_random$optimize(...)\n```\n:::\n\n\n</details>\n\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\nDefine all tuning related objects\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(8002L)\n\nsearch_space = ps(\n  max_depth = p_int(1L, 8L),\n  eta = p_dbl(0.2, 0.4)\n)\nmeasure = msr(\"classif.ce\")\nresampling = rsmp(\"cv\", folds = 3L)\nterminator = trm(\"evals\", n_evals = 20L)\n```\n:::\n\n\nInstantiate a tuning instance\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance_random = ti(\n  task = task, \n  learner = xgboost_lrn, \n  measure = measure, \n  resampling = resampling, \n  terminator = terminator, \n  search_space = search_space\n)\n```\n:::\n\n\nUse random search as the tuning approach\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner_random = tnr(\"random_search\")\ntuner_random$optimize(instance_random)\n```\n:::\n\n\n:::\n\n:::\n\n## 2.3 Inspect the the Best Performing Setup\n\nWhich tree depth was the best performing one?\n\n<details>\n<summary>**Hint 1:**</summary>\n\nInspect the tuned instance (of class `TuningInstanceSingleCrit`, it was the input to `$optimize()`). \nLook, for example, at the `$result` field. \n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance_random$result\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance_random$result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   max_depth       eta learner_param_vals  x_domain classif.ce\n       <int>     <num>             <list>    <list>      <num>\n1:         1 0.2517648          <list[5]> <list[2]>  0.2509845\n```\n\n\n:::\n:::\n\n\nThe best performing instance had `eta` set to 0.2517648  \nand `max_depth` set to 1. \n\n:::\n\n:::\n\n# 3 Early Stopping\n\n## 3.1 Set up an XGBoost Learner with Early Stopping\n\nNow that we derived the best hyperparameter for the maximum depth and eta, \nwe could train our final model. \nTo avoid overfitting we conduct early stopping - \nmeaning that the algorithm stops as soon as a the performance \ndoes not improve for a given number of rounds to avoid overfitting.\nThe performance for stopping should be assessed via a validation data set.\n\nSet up an XGBoost learner with the following hyperparameters:\n\n- `max_depth` and `eta` set to the best configurations according to the previous \ntuning task.\n- `nrounds` set to 100L.\n- The number of early stopping rounds set to 5 (this could be tuned, as well, but we simplify things) in order to stop earlier if there was no improvement in the previous 5 iterations.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"xgboost\")\nset.seed(2001L)\n```\n:::\n\n\n\n<details>\n<summary>**Hint 1:**</summary>\n\nSpecify the hyperparameters within `lrn()`.\nThe number of rounds could be specified with `early_stopping_rounds`. \n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn(\"...\", nrounds = ..., \n  max_depth = instance_random$result$..., \n  eta = instance_random$result$..., \n  early_stopping_rounds = ....\n) \n```\n:::\n\n\n</details>\n\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxgboost_lrn2 = lrn(\"classif.xgboost\")\nxgboost_lrn2$param_set$set_values(nrounds = 100L)\nxgboost_lrn2$param_set$set_values(max_depth = instance_random$result$max_depth)\nxgboost_lrn2$param_set$set_values(eta = instance_random$result$eta)\nxgboost_lrn2$param_set$set_values(early_stopping_rounds = 5L)\nxgboost_lrn2$validate = 0.9\n```\n:::\n\n\n:::\n\n:::\n\n\n## 3.2 Training on Credit Data\n\nTrain the XGBoost learner from the previous exercise on the credit data set \nHow many iterations were conducted before the boosting algorithm stopped?\n\n<details>\n<summary>**Hint 1:**</summary>\n\nBy calling `$train()` a model is trained which can be accessed via \n`$model`. This model has a field `$niter` - the number of conducted iterations.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxgboost$train(...)\nxgboost$...$niter\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxgboost_lrn2$train(task)\nxgboost_lrn2$model$niter\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 17\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n# 4 Extra: Nested Resampling\n\nTo receive an unbiased performance estimate when tuning hyperparameters, conduct nested resampling with\n\n- 3-fold cross-validation for the outer and inner resampling loop. \n- a search space for `max_depth` between 1 and 8 and `eta` between 0.2 and 0.4.\n- random search with 20 evaluations\n- the classification error `msr(\"classif.ce\")` as performance measure.\n\nExtract the performance estimate on the outer resampling folds. \n\n<details>\n<summary>**Hint 1:**</summary>\n\nSpecifically, you need to conduct the following steps:\n\n1. Set up an XGBoost learner.\n2. Initialize a search space for `max_depth` and `eta` using `ps()`. \n3. Initialize an `AutoTuner` with the xgboost model from the previous exercise an an input. \nThe `AutoTuner` reflects the inner resampling loop. It should be initialized for\n3-fold CV, random search with 20 evaluations and the classification error as performance \nmeasure.\n4. Specify a `Resampling` object using `rsmp()`.\n5. Use this object with the credit task and `AutoTuner` \nas an input to `resample()`. \n6. Extract the results via `$aggregate()`.\n\nImportant: Early stopping requires a validation set. But `AutoTuner` uses internal resampling instead of splitting the data manually, and does not provide a \"validate\" set to the learner by default. That is why we should not use early stopping here.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxgboost_lrn3 = lrn(...)\n\ntune_ps = ps(\n  max_depth = p_int(..., ...),\n  eta = p_dbl(..., ...)\n)\n\nat = auto_tuner(xgboost_lrn2, \n  resampling = rsmp(\"cv\", folds = ...),\n  search_space = ...,\n  measure = msr(\"...\"),\n  terminator = trm(\"none\"),\n  tuner = tnr(\"...\", resolution = 5L))\n\nresampling = rsmp(\"...\", folds = ...)\n\nset.seed(8002L)\nnestrr = resample(task = ..., learner = ..., resampling = resampling)\n\nnestrr$aggregate()\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\nAt first, we setup the learner\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxgboost_lrn3 = lrn(\"classif.xgboost\")\nxgboost_lrn3$param_set$set_values(nrounds = 100L)\nxgboost_lrn3$param_set$set_values(max_depth = instance_random$result$max_depth)\nxgboost_lrn3$param_set$set_values(eta = instance_random$result$eta)\n```\n:::\n\n\nThen, we setup a search space for tuning `max_depth`\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntune_ps = ps(\n  max_depth = p_int(1L, 8L),\n  eta = p_dbl(0.2, 0.4)\n)\n```\n:::\n\n\nNext, we setup the inner resampling loop and the tuning approach using \nan `AutoTuner`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat = auto_tuner(xgboost_lrn3, \n  resampling = rsmp(\"cv\", folds = 3L),\n  search_space = tune_ps,\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 20L),\n  tuner = tnr(\"random_search\"))\n```\n:::\n\n\nThen, we can setup an outer resampling loop and call `resample()`. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling = rsmp(\"cv\", folds = 3L)\n\nset.seed(8002L)\nnestrr = resample(task = task, learner = at, resampling = resampling)\n```\n:::\n\n\nWe can extract the results via `$aggregate()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnestrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.ce \n 0.2489945 \n```\n\n\n:::\n:::\n\n\nWe obtain a similar classification error as we received without \nnested resampling. \n\n:::\n\n:::\n\n\n# Summary\nIn this exercise sheet, we learned how to apply a XGBoost learner to the credit \ndata set By using resampling, we estimated the performance. \nXGBoost has a lot of hyperparameters and we only had a closer look on two of them. \nWe also saw how early stopping could be facilitated which should help to \navoid overfitting of the XGBoost model.\n\nInterestingly, we obtained best results, when we used 100 iterations, \nwithout tuning or early stopping. However, performance differences were quite \nsmall - if we set a different seed, we might see a different ranking. \nFurthermore, we could extend our tuning search space such that more hyperparameters are \nconsidered to increase overall performance of the learner for the task at hand. \nOf course, this also requires more budget for the tuning (e.g., more evaluations \nof random search).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}