{
  "hash": "b22ab7801bb2164d2729719e0b14d6aa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Calibration with mlr3 V2\ngroup: Advanced Performance Evaluation\ncategories:\n  - calibration\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Learn the basics of `tidymodels` for supervised learning, assess if a model is well-calibrated, and calibrate it with `mlr3`.\ndate: \"\"\nparams:\n  showsolution: true\n  base64encode: true\nlisting: false\nsearch: false\nformat:\n  html:\n    filters:\n      - ../../b64_solution.lua\n---\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n```{=html}\n<script>\nconst correctHash = \"2a44d234f16255743fd547b3942f265383c022b2176692051a945033ad795998\";   // value injected by knitr\n\n/* ---------- reusable helper ---------- */\nfunction b64DecodeUtf8(b64) {\n  // 1) atob  -> binary-string   (bytes 0…255)\n  // 2) map   -> Uint8Array      (array of bytes)\n  // 3) TextDecoder('utf-8')     -> real JS string\n  const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));\n  return new TextDecoder('utf-8').decode(bytes);\n}\n\nasync function sha256(txt) {\n  const buf = await crypto.subtle.digest('SHA-256',\n                 new TextEncoder().encode(txt));\n  return Array.from(new Uint8Array(buf))\n              .map(b => b.toString(16).padStart(2, '0')).join('');\n}\n\nasync function unlockOne(btn) {\n  const pass = prompt(\"Password:\");\n  if (!pass) return;\n  if (await sha256(pass) !== correctHash) {\n    alert(\"Wrong password\"); return;\n  }\n\n  /* --- decode only the solution that belongs to THIS button --- */\n  const wrapper = btn.parentElement;             // .b64-wrapper\n  wrapper.querySelectorAll('.hidden-solution').forEach(div => {\n    div.innerHTML = b64DecodeUtf8(div.dataset.encoded);\n    div.classList.remove('hidden-solution');\n    div.style.display = 'block';\n  });\n\n  /* Remove the button so the user can’t click it again */\n  btn.remove();\n}\n</script>\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n    <strong>JavaScript is required to unlock solutions.</strong><br>\n    Please enable JavaScript and reload the page,<br>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n```\n\n\n\n\n\n# Goal\n\nOur goal for this exercise sheet is to learn the basics of model calibration for supervised classification with `mlr3calibration`.\nIn a calibrated model, the predicted probability for an input feature vector can be interpreted as the true likelihood of the outcome belonging to the positive class, meaning that among all instances assigned a probability of $p$, approximately $p\\%$ will belong to the positive class.\n\n# Required packages\n\nWe will use `mlr3` for machine learning, and `mlr3calibration` specifically for calibration:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nif (!require(\"mlr3calibration\")) {\n  remotes::install_github(\"AdriGl117/mlr3calibration\")\n}\nlibrary(mlr3calibration)\nlibrary(mlr3verse)\nlibrary(mlr3learners)\n\nset.seed(12345)\n```\n:::\n\n\n\n# Data: predicting cell segmentation quality\n\nThe `modeldata` package contains a data set called `cells`. Initially distributed by [Hill and Haney (2007)](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340), they showed how to create models that predict the _quality_ of the image analysis of cells. The outcome has two levels: `\"PS\"` (for poorly segmented images) or `\"WS\"` (well-segmented). There are 56 image features that can be used to build a classifier. \n\nLet's load the data and remove an unwanted column: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(modeldata)\ndata(cells, package = \"modeldata\")\ncells$case <- NULL\n```\n:::\n\n\n\n# 1 Checking cardinality properties\n\n## 1.1 Creating a split\n\nFirst, define a `task` object for the `cells` data set. Then, create a simple train-test split on the task to reserve test data for performance evaluation later on. As result, there should be a `cells_train` and a `cells_test`..\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can use `partition()` on a given task object to create simple train-test split.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_classif(cells, target = \"class\")\nsplits = partition(task)\ncells_train = task$clone()$filter(splits$train)\ncells_test = task$clone()$filter(splits$test)\n```\n:::\n\n\n\n:::\n\n:::\n\n## 1.2 Training a Naive Bayes model\n\nWe'll show the utility of calibration tools by using a type of model that, in this instance, is likely to produce a poorly calibrated model. The Naive Bayes classifier is a well-established model that assumes that the predictors are statistically _independent_ of one another (to simplify the calculations).  While that is certainly not the case for this data, the model can be effective at discriminating between the classes. Unfortunately, when there are many predictors in the model, it has a tendency to produce class probability distributions that are pathological. The predictions tend to gravitate to values near zero or one, producing distributions that are \"U\"-shaped ([Kuhn and Johnson, 2013](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C7&q=%22Applied+Predictive+Modeling%22&btnG=)). \n\nTo demonstrate, let's set up a Naive Bayes learner and train it using the training data.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nFirst, create and train the learner.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.naive_bayes\", predict_type = \"prob\")\nlearner$id <- \"Uncalibrated Learner\"\nlearner$train(cells_train)\n```\n:::\n\n\n\n:::\n\n:::\n\n## 1.3 Predicting on unseen test data\n\nNext, use the trained model to predict on unseen test data. In `mlr3`, this will return two columns of predicted probabilities, one for `\"PS\"` and one for `\"WS\"`.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can use this code skeleton to produce the desired output:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$predict(...)\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction = learner$predict(cells_test)\nprediction\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<PredictionClassif> for 666 observations:\n row_ids truth response      prob.PS      prob.WS\n       2    PS       PS 9.998201e-01 1.798710e-04\n       3    WS       WS 8.687551e-04 9.991312e-01\n       7    WS       WS 4.666062e-06 9.999953e-01\n     ---   ---      ---          ---          ---\n    2012    WS       WS 1.555306e-07 9.999998e-01\n    2013    PS       PS 1.000000e+00 3.243699e-23\n    2014    PS       PS 5.959642e-01 4.040358e-01\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## 1.4 Assessing model performance\n\nWe want to use two metrics to judge how well the model performs. First, the area under the ROC curve (ROC AUC) is used to measure the ability of the model to separate the classes (its mere ability to discriminate correctly). Second, the Brier score is used to measure how close the probability estimates are to the actual outcome values (its ability to express confidence in the label predictions). Compute the ROC AUC and the Brier Score on the test data predictions.\n\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"classif.auc\", \"classif.bbrier\"))\nprediction$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   classif.auc classif.bbrier \n     0.8317646      0.2226456 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## 1.5 Assessing model calibration\n\nLet's assess if our model is well-calibrated. Spoiler: it is not. The first clue is the extremely U-shaped distribution of the probability scores:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(prediction$data$prob, breaks = (0:10)/10)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nThere are almost no cells with moderate probability estimates.\n\nThen, assess if the model is calibrated with `calibrationplot()`. The calibration plot shows the relationship between the predicted probabilities and the true outcomes. The plot is divided into bins, and within each bin, the mean predicted probability and the mean observed outcome are calculated. The calibration plot can be smoothed by setting `smooth = TRUE`.\n\n<details>\n<summary>**Hint 1:**</summary>\n\n`calibrationplot()` requires a `list` of learners even if the list contains only one argument.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncalibrationplot(list(learner), cells_test, smooth = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe model is not well-calibrated. For predicted probabilities from 0 to 0.4, it seems to underestimate the true probability, for predicted probabilities higher than 0.4, it tends to overestimate the true probability.\n\n:::\n\n:::\n\n## 1.6 Platt Scaling\n\nThe good news is that we can do something about this. There are tools to \"fix\" the probability estimates so that they have better properties, such as falling along the diagonal lines in the diagnostic plots shown above. Different methods improve the predictions in different ways. \n\nThe most common approach is Platt scaling, also called logistic calibration. This works in the following way: The original model is used to predict on the validation data set, returning predicted probabilities. We already know these probabilities are off, i.e., not well-calibrated. Then, we fit a simple logistic regression model, using the predicted probabilities from the original model as \"x/feature\" and the true labels as \"y/target\". In this way, we stack a second model on top of the first one: Original Model -> Probabilities -> Logistic Regression Model -> Calibrated Probabilities.\n\nIf effective, the logistic regression model estimates the probability regions where the original model is off (as shown in the diagnostic plot). For example, suppose that when the model predicts a 2% event rate, the logistic regression model estimates that it under-predicts the probability by 5% (relative to the observed data). Given this gap, new predictions are adjusted up so that the probability estimates are more in-line with the data.\n\nIn `mlr3calibration`, to calibrate a learner you need a base learner (which will fit a model that is calibrated afterwards), a resampling strategy, and a calibration method (Platt, Beta or Isotonic). Initialize 1) another Naive Bayes base learner, 2) a 5-fold CV resampling object, and 3) a calibration strategy. The calibration strategy in `mlr3calibration` is implemented as `PipeOpCalibration` object. It requires the base learner (`learner`), the calibration method (`method`), and the resampling method (`rsmp`) as arguments to be initialized. Practically, we want to use the calibration strategy as learner, so we have to express the pipeline operator within `as_learner()`. After that, set `learner_cal$id <- \"Platt Calibrated Learner\"` for later reference.\n\n\n<details>\n<summary>**Hint 1:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_uncal = ...\nrsmp = ...\nlearner_cal = as_learner(PipeOpCalibration$new(...))\nlearner_cal$id <- \"Platt Calibrated Learner\"\n```\n:::\n\n\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nCheck the documentation of `PipeOpCalibration` with `??PipeOpCalibration`.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_uncal = lrn(\"classif.naive_bayes\", predict_type = \"prob\")\nrsmp = rsmp(\"cv\", folds = 5)\nlearner_cal = as_learner(PipeOpCalibration$new(learner = learner_uncal, method = \"platt\", rsmp = rsmp))\nlearner_cal$id <- \"Platt Calibrated Learner\"\n```\n:::\n\n\n\n:::\n\n:::\n\nThe calibrated learner can be trained on a `task` as any other learner. Train the learner on `cells_train`. Afterwards, plot the calibration plot again, comparing the uncalibrated Naive Bayes model with the Platt-scaled Naive Bayes model.\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_cal$train(cells_train)\ncalibrationplot(list(learner, learner_cal), cells_test)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nWhile the calibrated learner does not exhibit perfect calibration, it is better calibrated than the plain learner. #TODO\n\n:::\n\n:::\n\n \n# 2 Isotonic regression\n\nA different approach to calibration is to use isotonic regression. In a manner similar to logistic calibration, use `mlr3` to calibrate the Naive Bayes model using isotonic regression. Do we improve the calibration in comparison to Platt scaling?\n\n<details>\n<summary>**Hint 1:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_uncal = ...\nrsmp = ...\nlearner_cal = as_learner(PipeOpCalibration$new(...))\nlearner_cal$id <- \"Isotonic Calibrated Learner\"\nlearner_cal$train(cells_train)\n\n# Did we improve?\nprediction_iso = ...\nmeasures = ...\nprediction_iso$score(measures)\n\ncalibrationplot(...)\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\nYes, we can see in the plot that it is very close to the true probabilities: #TODO why other way round?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Train base model on training set\nlearner_uncal <- lrn(\"classif.naive_bayes\", predict_type = \"prob\")\nrsmp = rsmp(\"cv\", folds = 5)\n\n# Train isotonic calibrator using validation predictions\nlearner_cal_iso = as_learner(PipeOpCalibration$new(learner = learner_uncal, method = \"isotonic\", rsmp = rsmp))\nlearner_cal_iso$id <- \"Isotonic Calibrated Learner\"\nlearner_cal_iso$train(cells_train)\n\n# Did we improve?\nprediction_iso = learner_cal_iso$predict(cells_test)\nmeasures = msrs(c(\"classif.auc\", \"classif.bbrier\"))\nprediction_iso$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   classif.auc classif.bbrier \n     0.8326989      0.1536569 \n```\n\n\n:::\n\n```{.r .cell-code}\ncalibrationplot(list(learner, learner_cal, learner_cal_iso), cells_test, smooth = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n:::\n\n:::\n\n# 3 Resamping for Calibration with mlr3\n\n`PipeOpCalibration` can be treated as any other `PipeOp` object. Therefore, we can use them within more complex tuning and pipeline constructs, i.e. in cross-validation (CV), to assess calibration methods.\n\nFit 10 Naive Bayes models using 10-fold CV and calibrate each of these models using beta calibration, another calibration method.\n\n\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can use the following code skeleton, detailing the required steps:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npipeline_cal = as_learner(PipeOpCalibration$new(learner = ...,\n                                                rsmp = ...,\n                                                method = ...))\npipeline_cal$id <- \"Beta Calibrated Learner\"\npipeline_cal$...\ncalibrationplot(..., smooth = TRUE)\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npipeline_cal = as_learner(PipeOpCalibration$new(learner = lrn(\"classif.naive_bayes\", predict_type = \"prob\"),\n                                                rsmp = rsmp(\"cv\", folds = 10),\n                                                method = \"beta\"))\npipeline_cal$id <- \"Beta Calibrated Learner\"\npipeline_cal$train(cells_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.40344\n[1] 7.057705\n[1] -3.737607\n[1] 0.2957318\n[1] -5.619081\n[1] 1.72715\n[1] -3.445605\n[1] 0.08173093\n[1] -4.934836\n[1] -0.1786735\n[1] -4.579396\n[1] 1.165392\n[1] -3.927632\n[1] 8.596771\n[1] -0.5694413\n[1] 4.81301\n[1] -1.981375\n[1] 2.298299\n[1] -3.41622\n[1] 1.608697\n```\n\n\n:::\n\n```{.r .cell-code}\ncalibrationplot(list(pipeline_cal), cells_test, smooth = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n:::\n\n:::\n\n# Summary\n\nIn this exercise sheet we learned how to fit a classification model on\na training task and how to assess its performance on unseen test data with\nthe help of `mlr3`.\nWe showed how to split data manually into training and test data, and use `mlr3` to assess if a model is calibrated.\nWe discussed how Platt scaling/logistic calibration and isotonic regression can be used to improve calibration (although not perfectly calibrate the model here).\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}