{

  "hash": "236d6d3fd0318d7f285dfb0d9dabfc0d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Train Predict Evaluate Basics Solution\ncategories:\n  - classification\n  - visualization\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Introduction to German Credit dataset and classification. Train predict and evaluate a logistic regression learner with hold-out split.\ndate: 04-14-2025\naliases:\n  - ../../../gallery/2020-08-14-comparison-of-decision-boundaries/index.html\n\nparams:\n  showsolution: true\n\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n# Goal\nOur goal for this exercise sheet is to learn the basics of mlr3 for supervised learning\nby training a first simple model on training data and by evaluating its performance on\nhold-out/test data.\n\n# German Credit Dataset\n\nThe German credit dataset was donated by Prof. Dr. Hans Hoffman of the\nUniversity of Hamburg in 1994 and contains 1000 datapoints reflecting\nbank customers.\nThe goal is to classify people as a good or bad credit risk based\non 20 personal, demographic and financial features.\nThe dataset is available at the UCI repository as\n[Statlog (German Credit Data) Data Set](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).\n\n## Motivation of Risk Prediction\n\nCustomers who do not repay the distributed loan on time represent an enormous risk for a bank:\nFirst, because they create an unintended gap in the bank's planning,\nand second, because the collection of the repayment amount additionally\ncauses additional time and cost for the bank.\n\nOn the other hand, (interest rates for) loans are an important revenue stream for banks.\nIf a person's loan is rejected, even though they would have met the repayment deadlines,\nrevenue is lost, as well as potential upselling opportunities.\n\nBanks are therefore highly interested in a risk prediction model that accurately\npredicts the risk of future customers.\nThis is where supervised learning models come into play.\n\n## Data Overview\n\nn = 1,000 observations of bank customers\n\n- `credit_risk`: is the customer a good or bad credit risk?\n- `age`: age in years\n- `amount`: \tamount asked by applicant\n- `credit_history`: past credit history of applicant at this bank\n- `duration`: duration of the credit in months\n- `employment_duration`: present employment since\n- `foreign_worker`: is applicant foreign worker?\n- `housing`: type of apartment rented, owned, for free / no payment\n- `installment_rate`: installment rate in percentage of disposable income\n- `job`: current job information\n- `number_credits`: number of existing credits at this bank\n- `other_debtors`: other debtors/guarantors present?\n- `other_installment_plans`: other installment plans the applicant is paying\n- `people_liable`: number of people being liable to provide maintenance\n- `personal_status_sex`: combination of sex and personal status of applicant\n- `present_residence`: present residence since\n- `property`: properties that applicant has\n- `purpose`: reason customer is applying for a loan\n- `savings`: savings accounts/bonds at this bank\n- `status`: status/balance of checking account at this bank\n- `telephone`: \tis there any telephone registered for this customer?\n\n## Preprocessing\n\nWe first load the data from the `rchallenge` package (you may need to install it first) and get a brief overview.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# install.packages(\"rchallenge\")\nlibrary(\"rchallenge\")\ndata(\"german\")\nskimr::skim(german)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |       |\n|:------------------------|:------|\n|Name                     |german |\n|Number of rows           |1000   |\n|Number of columns        |21     |\n|_______________________  |       |\n|Column type frequency:   |       |\n|factor                   |18     |\n|numeric                  |3      |\n|________________________ |       |\n|Group variables          |None   |\n\n\n**Variable type: factor**\n\n|skim_variable           | n_missing| complete_rate|ordered | n_unique|top_counts                             |\n|:-----------------------|---------:|-------------:|:-------|--------:|:--------------------------------------|\n|status                  |         0|             1|FALSE   |        4|...: 394, no : 274, ...: 269, 0<=: 63  |\n|credit_history          |         0|             1|FALSE   |        5|no : 530, all: 293, exi: 88, cri: 49   |\n|purpose                 |         0|             1|FALSE   |       10|fur: 280, oth: 234, car: 181, car: 103 |\n|savings                 |         0|             1|FALSE   |        5|unk: 603, ...: 183, ...: 103, 100: 63  |\n|employment_duration     |         0|             1|FALSE   |        5|1 <: 339, >= : 253, 4 <: 174, < 1: 172 |\n|installment_rate        |         0|             1|TRUE    |        4|< 2: 476, 25 : 231, 20 : 157, >= : 136 |\n|personal_status_sex     |         0|             1|FALSE   |        4|mal: 548, fem: 310, fem: 92, mal: 50   |\n|other_debtors           |         0|             1|FALSE   |        3|non: 907, gua: 52, co-: 41             |\n|present_residence       |         0|             1|TRUE    |        4|>= : 413, 1 <: 308, 4 <: 149, < 1: 130 |\n|property                |         0|             1|FALSE   |        4|bui: 332, unk: 282, car: 232, rea: 154 |\n|other_installment_plans |         0|             1|FALSE   |        3|non: 814, ban: 139, sto: 47            |\n|housing                 |         0|             1|FALSE   |        3|ren: 714, for: 179, own: 107           |\n|number_credits          |         0|             1|TRUE    |        4|1: 633, 2-3: 333, 4-5: 28, >= : 6      |\n|job                     |         0|             1|FALSE   |        4|ski: 630, uns: 200, man: 148, une: 22  |\n|people_liable           |         0|             1|FALSE   |        2|0 t: 845, 3 o: 155                     |\n|telephone               |         0|             1|FALSE   |        2|no: 596, yes: 404                      |\n|foreign_worker          |         0|             1|FALSE   |        2|no: 963, yes: 37                       |\n|credit_risk             |         0|             1|FALSE   |        2|goo: 700, bad: 300                     |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|    mean|      sd|  p0|    p25|    p50|     p75|  p100|hist  |\n|:-------------|---------:|-------------:|-------:|-------:|---:|------:|------:|-------:|-----:|:-----|\n|duration      |         0|             1|   20.90|   12.06|   4|   12.0|   18.0|   24.00|    72|▇▇▂▁▁ |\n|amount        |         0|             1| 3271.25| 2822.75| 250| 1365.5| 2319.5| 3972.25| 18424|▇▂▁▁▁ |\n|age           |         0|             1|   35.54|   11.35|  19|   27.0|   33.0|   42.00|    75|▇▆▃▁▁ |\n\n\n:::\n:::\n\n\n\n# Exercises:\nNow, we can start building a model. To do so, we need to address the following questions:\n\n- What is the problem we are trying to solve?\n- What is an appropriate learning algorithm?\n- How do we evaluate \"good\" performance?\n\nMore systematically in `mlr3` they can be expressed via five components:\n\n- The `Task` definition.\n- The `Learner` definition.\n- The training via `$train()`.\n- The prediction via `$predict()`.\n- The evaluation via one `$score()`.\n\n## Split Data in Training and Test Data\n\nYour task is to split the `german` dataset into 70 \\% training data and 30 \\% \ntest data by randomly sampling rows.\nLater, we will use the training data to learn an ML model and use the test data \nto assess its performance.\n\n<details>\n  <summary>Recap: Why do we need train and test data?</summary>\n\nWe use part of the available data (the training data) to train our model.\nThe remaining/hold-out data (test data) is used to evaluate the trained model.\nThis is exactly how we anticipate using the model in practice:\nWe want to fit the model to existing data and then make predictions on \nnew, unseen data points for which we do not know the outcome/target values.\n\nNote: Hold-out splitting requires a dataset that is sufficiently\nlarge such that both the training and test dataset are suitable representations\nof the target population. What \"sufficiently large\" means depends on the\ndataset at hand and the complexity of the problem.\n\nThe ratio of training to test data is also context dependent.\nIn practice, a 70\\% to 30\\% (~ 2:1) ratio is a good starting point.\n\n</details>\n\n<details>\n<summary>**Hint 1:**</summary>\n\nUse `sample()` to sample 70 % of the data ids as training data ids from `row.names(german)`.\nThe remaining row ids are obtained via `setdiff()`.\nBased on the ids, set up two datasets, one for training and one for testing/evaluating.\n\nSet a seed (e.g, `set.seed(100L)`) to make your results reproducible.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Sample ids for training and test split\nset.seed(100L)\ntrain_ids = sample(row.names(german), 0.7*nrow(...))\ntest_ids = setdiff(..., train_ids)\n\n# Create two datasets based on ids\ntrain_set = german[...,]\ntest_set = german[...,]\n```\n:::\n\n\n\n</details>\n\n\n\n::: {.content-hidden unless-meta=\"params.showsolution\"}\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Solution\n\nWe first sample row ids by using `sample()` and identify the non-selected rows via `setdiff()`.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(100L)\ntrain_ids = sample(row.names(german), 0.7*nrow(german))\ntest_ids = setdiff(row.names(german), train_ids)\nstr(train_ids)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n chr [1:700] \"714\" \"503\" \"358\" \"624\" \"985\" \"718\" \"919\" \"470\" \"966\" \"516\" \"823\" \"838\" \"98\" \"903\" \"7\" \"183\" \"299\" ...\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(test_ids)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n chr [1:300] \"3\" \"5\" \"8\" \"17\" \"20\" \"22\" \"25\" \"29\" \"33\" \"35\" \"39\" \"40\" \"41\" \"43\" \"44\" \"49\" \"52\" \"57\" \"58\" \"59\" \"62\" ...\n```\n\n\n:::\n:::\n\n\n\nBased on that, we create two datasets: one for training and one for testing.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain_set = german[train_ids,]\ntest_set = german[test_ids, ]\n```\n:::\n\n\n:::\n\n:::\n\n\n\n\n\n\n## Create a Classification Task\n\nInstall and load the `mlr3verse` package which is a collection of multiple add-on packages in the `mlr3` universe (if you fail installing `mlr3verse`, try to install and load only the `mlr3` and `mlr3learners` packages). \nThen, create a classification task using the training data as an input and `credit_risk` as the target variable (with the class label `good` as the positive class). \nBy defining an `mlr3` task, we conceptualize the ML problem we want to solve (here we face a classification task).\nAs we have a classification task here, make sure you properly specify the class that should be used as the positive class (i.e., the class label for which we would like to predict probabilities - here `good` if you are interested in predicting a probability for the creditworthiness of customers).\n\n<details>\n<summary>**Hint 1:**</summary>\n\nUse e.g. `as_task_classif()` to create a classification task.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\ntask = as_task_classif(x = ..., target = ..., ... = \"good\")\n```\n:::\n\n\n\n</details>\n\n\n\n::: {.content-hidden unless-meta=\"params.showsolution\"}\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Solution\n\nTo initialize a `TaskClassif` object, two equivalent calls exist:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: mlr3\n```\n\n\n:::\n\n```{.r .cell-code}\ntask = TaskClassif$new(\"german_credit\", backend = train_set, target = \"credit_risk\", positive = \"good\")\ntask = as_task_classif(train_set, target = \"credit_risk\", positive = \"good\")\ntask\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:train_set> (700 x 21)\n* Target: credit_risk\n* Properties: twoclass\n* Features (20):\n  - fct (14): credit_history, employment_duration, foreign_worker, housing, job, other_debtors,\n    other_installment_plans, people_liable, personal_status_sex, property, purpose, savings, status,\n    telephone\n  - int (3): age, amount, duration\n  - ord (3): installment_rate, number_credits, present_residence\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n\n\n\n\n## Train a Model on the Training Dataset\nThe created `Task` contains the data we want to work with.\nNow that we conceptualized the ML task (i.e., classification) in a `Task` object,\nit is time to train our first supervised learning method.\nWe start with a simple classifier: a logistic regression model.\nDuring this course, you will, of course, also gain experience with more complex\nmodels.\n\nFit a logistic regression model to the `german_credit` training task.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nUse `lrn()` to initialize a `Learner` object.\nThe short cut and therefore input to this method is `\"classif.log_reg\"`.\n\nTo train a model, use the `$train()` method of your instantiated learner\nwith the task of the previous exercise as an input.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogreg = lrn(\"classif.log_reg\")\nlogreg$train(...)\n```\n:::\n\n\n\n</details>\n\n\n::: {.content-hidden unless-meta=\"params.showsolution\"}\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Solution\n\nBy using the syntactic sugar method `lrn()`, we first initialize a `LearnerClassif` model.\nUsing the `$train()` method, we derive optimal hyperparameters (i.e., coefficients)\nfor our logistic regression model.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogreg = lrn(\"classif.log_reg\")\nlogreg$train(task)\n```\n:::\n\n\n\n:::\n\n:::\n\n\n\n\n\n## Inspect the Model\n\nHave a look at the coefficients by using `summary()`.\nName at least two features that have a significant effect on the outcome.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nUse the `summary()` method of the `model` field of our trained model.\nBy looking on `task$positive`, we could see which of the two classes `good` or `bad`\nis used as the positive class (i.e., the class to which the model predictions will refer).\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(yourmodel$model)\n```\n:::\n\n\n\n</details>\n\n\n::: {.content-hidden unless-meta=\"params.showsolution\"}\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Solution\n\nSimilar to models fitted via `glm()` or `lm()`, we could receive a summary\nof the coefficients (including p-values) using `summary()`.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(logreg$model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nstats::glm(formula = form, family = \"binomial\", data = data, \n    model = FALSE)\n\nCoefficients:\n                                                            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                                               -2.342e+00  1.402e+00  -1.671 0.094676 .  \nage                                                        1.999e-02  1.154e-02   1.733 0.083116 .  \namount                                                    -4.414e-05  5.413e-05  -0.815 0.414900    \ncredit_historycritical account/other credits elsewhere    -2.267e-01  7.109e-01  -0.319 0.749824    \ncredit_historyno credits taken/all credits paid back duly  8.517e-01  5.311e-01   1.604 0.108781    \ncredit_historyexisting credits paid back duly till now     1.278e+00  5.799e-01   2.204 0.027546 *  \ncredit_historyall credits at this bank paid back duly      1.444e+00  5.311e-01   2.718 0.006569 ** \nduration                                                  -3.674e-02  1.124e-02  -3.268 0.001083 ** \nemployment_duration< 1 yr                                 -5.909e-01  5.535e-01  -1.068 0.285694    \nemployment_duration1 <= ... < 4 yrs                       -1.410e-01  5.197e-01  -0.271 0.786181    \nemployment_duration4 <= ... < 7 yrs                        2.575e-01  5.676e-01   0.454 0.650134    \nemployment_duration>= 7 yrs                               -2.026e-01  5.284e-01  -0.383 0.701449    \nforeign_workerno                                          -1.298e+00  7.418e-01  -1.749 0.080235 .  \nhousingrent                                                2.712e-01  2.800e-01   0.969 0.332719    \nhousingown                                                 3.154e-01  6.180e-01   0.510 0.609785    \ninstallment_rate.L                                        -5.018e-01  2.645e-01  -1.897 0.057784 .  \ninstallment_rate.Q                                        -3.089e-01  2.410e-01  -1.282 0.199878    \ninstallment_rate.C                                         8.274e-02  2.496e-01   0.332 0.740232    \njobunskilled - resident                                    6.929e-01  8.182e-01   0.847 0.397122    \njobskilled employee/official                               7.769e-01  7.896e-01   0.984 0.325125    \njobmanager/self-empl./highly qualif. employee              7.129e-01  7.923e-01   0.900 0.368240    \nnumber_credits.L                                          -1.358e-01  7.661e-01  -0.177 0.859348    \nnumber_credits.Q                                           6.595e-02  6.395e-01   0.103 0.917854    \nnumber_credits.C                                           7.483e-02  4.877e-01   0.153 0.878071    \nother_debtorsco-applicant                                  2.186e-01  5.266e-01   0.415 0.678024    \nother_debtorsguarantor                                     7.834e-01  4.968e-01   1.577 0.114862    \nother_installment_plansstores                              1.454e-01  5.422e-01   0.268 0.788528    \nother_installment_plansnone                                3.874e-01  3.077e-01   1.259 0.208020    \npeople_liable0 to 2                                        2.244e-01  3.184e-01   0.705 0.481005    \npersonal_status_sexfemale : non-single or male : single    6.476e-01  4.842e-01   1.338 0.181034    \npersonal_status_sexmale : married/widowed                  1.204e+00  4.825e-01   2.494 0.012621 *  \npersonal_status_sexfemale : single                         8.236e-01  5.724e-01   1.439 0.150153    \npresent_residence.L                                       -3.690e-01  2.592e-01  -1.424 0.154559    \npresent_residence.Q                                        4.980e-01  2.443e-01   2.038 0.041546 *  \npresent_residence.C                                       -4.572e-01  2.462e-01  -1.857 0.063326 .  \npropertycar or other                                       1.244e-01  3.036e-01   0.410 0.682060    \npropertybuilding soc. savings agr./life insurance          1.432e-01  2.891e-01   0.495 0.620375    \npropertyreal estate                                       -2.818e-01  5.615e-01  -0.502 0.615706    \npurposecar (new)                                           1.774e+00  4.793e-01   3.702 0.000214 ***\npurposecar (used)                                          5.974e-01  3.227e-01   1.851 0.064101 .  \npurposefurniture/equipment                                 8.264e-01  3.091e-01   2.674 0.007494 ** \npurposeradio/television                                   -9.668e-02  8.379e-01  -0.115 0.908140    \npurposedomestic appliances                                 6.065e-01  6.489e-01   0.935 0.349946    \npurposerepairs                                            -2.953e-01  4.570e-01  -0.646 0.518163    \npurposevacation                                            1.592e+00  1.272e+00   1.251 0.210816    \npurposeretraining                                          6.450e-01  4.038e-01   1.597 0.110245    \npurposebusiness                                            1.340e+00  8.364e-01   1.601 0.109268    \nsavings... <  100 DM                                       2.723e-01  3.449e-01   0.789 0.429824    \nsavings100 <= ... <  500 DM                                1.511e+00  6.117e-01   2.471 0.013477 *  \nsavings500 <= ... < 1000 DM                                1.715e+00  7.436e-01   2.306 0.021093 *  \nsavings... >= 1000 DM                                      1.195e+00  3.255e-01   3.671 0.000242 ***\nstatus... < 0 DM                                           3.827e-01  2.642e-01   1.449 0.147465    \nstatus0<= ... < 200 DM                                     9.451e-01  4.254e-01   2.222 0.026301 *  \nstatus... >= 200 DM / salary for at least 1 year           1.813e+00  2.866e-01   6.325 2.53e-10 ***\ntelephoneyes (under customer name)                         4.773e-03  2.427e-01   0.020 0.984311    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 868.33  on 699  degrees of freedom\nResidual deviance: 622.78  on 645  degrees of freedom\nAIC: 732.78\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n\nAccording to the summary, e.g., `credit_history` and `status` significantly influence the creditworthiness and the bank's risk assessment.\nBy looking on `task$positive`, we see that the class `good` (creditworthy client) is the positive class.\nThis means that a positive sign of the estimated coefficient of a feature means that the feature has a positive influence on being a creditworthy client (while a negative sign will have a negative influence).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$positive\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"good\"\n```\n\n\n:::\n:::\n\n\n\nFor example, the negative sign of the coefficients of `credit_history = delay in paying off in the past` and `credit_history = critical account/other credit elsewhere`, indicate a negative influence and therefore lower probability of being a creditworthy client compared to their reference class `credit_history = all credits at this bank paid back duly`.\nThe positive sign of the coefficient of `status >= 200 DM / salary for at least 1 year` and `status = 0 <= ... < 200 DM`, therefore, indicate a positive influence w.r.t to its reference class `status < 0 DM`.\n\n:::\n\n:::\n\n\n\n\n\n## Predict on the Test Dataset\nUse the trained model to predict on the hold-out/test dataset.\n\n<details>\n<summary>**Hint 1**</summary>\n\nSince we have a new tabular dataset as an input (and not a task),\nwe need to use `$predict_newdata()` (instead\nof `$predict()`) to derive a `PredictionClassif` object.\n\n</details>\n\n<details>\n<summary>**Hint 2**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred = yourmodel$predict_newdata(...)\n```\n:::\n\n\n\n</details>\n\n\n::: {.content-hidden unless-meta=\"params.showsolution\"}\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Solution\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_logreg = logreg$predict_newdata(test_set)\n```\n:::\n\n\n\n:::\n\n:::\n\n\n\n\n\n## Evaluation\n\nWhat is the classification error on the test data (200 observations)?\n\n<details>\n<summary>**Hint 1:**</summary>\n\nThe classification error gives the rate of observations that were\nmisclassified.\nUse the `$score()` method on the corresponding `PredictionClassif` object\nof the previous exercise.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_logreg$score()\n```\n:::\n\n\n\n</details>\n\n\n::: {.content-hidden unless-meta=\"params.showsolution\"}\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Solution\n\nBy using the `$score()` method, we obtain an estimate for the classification error\nof our model.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred_logreg$score()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.ce \n 0.7733333 \n```\n\n\n:::\n:::\n\n\n\nThe classification error is 0.255 - so 25.5 \\% of the test instances were\nmisclassified by our logistic regression model.\n\n:::\n\n:::\n\n\n## Predicting probabilities instead of labels\n\nSimilarly, we can assess the performance of our model using the AUC. However, this requires predicted probabilities instead of predicted labels. Evaluate the model using the AUC. To do so, retrain the model with a learner that returns probabilities.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can generate predictions with probabilities by specifying a `predict_type` argument inside the `lrn()` function call when constructing a learner.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nYou can get an overview of performance measures in mlr3 using `as.data.table(msr())`.\n\n</details>\n\n\n::: {.content-hidden unless-meta=\"params.showsolution\"}\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Solution\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Train a learner\nlogreg = lrn(\"classif.log_reg\", predict_type = \"prob\")\nlogreg$train(task)\n# Generate predictions\npred_logreg = logreg$predict_newdata(test_set)\n# Evaluate performance using AUC\nmeasure = msrs(c(\"classif.auc\"))\npred_logreg$score(measure)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.auc \n  0.2351757 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n\n\n\n\n\n# Summary\n\nIn this exercise sheet we learned how to fit a logistic regression model on\na training task and how to assess its performance on unseen test data with\nthe help of `mlr3`.\nWe showed how to split data manually into training and test data,\nbut in most scenarios it is a call to resample or benchmark.\nWe will learn more on this in the next sections.\n\n\n",

    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}