{
  "hash": "ad9fb3712012e74bf0bcb0c19a8cfb68",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Resampling Solution\ncategories:\n  - resampling\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Use 5-fold cross validation to evaluate logistic regression and knn learner on german credit set.\ndate: 04-14-2025\naliases:\n  - ../../../gallery/2020-08-14-comparison-of-decision-boundaries/index.html\nparams:\n  showsolution: true\nunlinked: true\n---\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n\n# Goal\n\nYou will learn how to estimate the model performance with `mlr3` using resampling techniques such as 5-fold cross-validation.\nAdditionally, you will compare k-NN model against a logistic regression model.\n\n# German Credit Data\n\nWe work with the German credit data.\nYou can either manually create the corresponding `mlr3` task as we did before or use a pre-defined task which is already included in the `mlr3` package (you can look at the output of `as.data.table(mlr_tasks)` to see which other pre-defined tasks that can be used to play around are included in the `mlr3` package).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLade n√∂tiges Paket: mlr3\n```\n\n\n:::\n\n```{.r .cell-code}\ntask = tsk(\"german_credit\")\ntask \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:german_credit> (1000 x 21): German Credit\n* Target: credit_risk\n* Properties: twoclass\n* Features (20):\n  - fct (14): credit_history, employment_duration, foreign_worker, housing, job, other_debtors,\n    other_installment_plans, people_liable, personal_status_sex, property, purpose, savings, status,\n    telephone\n  - int (3): age, amount, duration\n  - ord (3): installment_rate, number_credits, present_residence\n```\n\n\n:::\n\n```{.r .cell-code}\ntask$positive # (check the positive class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"good\"\n```\n\n\n:::\n:::\n\n\n# Exercise: Fairly evaluate the performance of two learners\n\nWe first create two `mlr3` learners, a logistic regression and a KNN learner. \nWe then compare their performance via resampling.\n\n## Create the learners\n\nCreate a logistic regression learner (store it as an R object called `log_reg`) and KNN learner with $k = 5$ (store it as an R object called `knn`).\n\n<details>\n  <summary>**Show Hint 1:**</summary>\n  Check `as.data.table(mlr_learners)` to find the appropriate learner.\n  </details>\n  \n<details>\n  <summary>**Show Hint 2:**</summary>\n  Make sure to have the `kknn` package installed.\n  </details>\n  \n  \n::: {.content-hidden unless-meta=\"params.showsolution\"}\n\n\n\n::: {.callout-note collapse=\"true\"}\n### Solution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlog_reg = lrn(\"classif.log_reg\")\nknn = lrn(\"classif.kknn\", k = 5)\n```\n:::\n\n\n\n:::\n\n::: \n\n\n## Set up a resampling instance\n\nUse the `mlr3` to set up a resampling instance and store it as an R object called `cv5`. \nHere, we aim for 5-fold cross-validation.\nA table of possible resampling techniques implemented in `mlr3` can be shown by looking at `as.data.table(mlr_resamplings)`.\n\n<details>\n  <summary>**Show Hint 1:**</summary>\n  Look at the table returned by `as.data.table(mlr_resamplings)` and use the `rsmp` function to set up a 5-fold cross-validation instance. Store the result of the `rsmp` function in an R object called `cv5`.\n  </details>\n<details>\n  <summary>**Show Hint 2:**</summary>\n  `rsmp(\"cv\")` by default sets up a 10-fold cross-validation instance.\n  The number of folds can be set using an additional argument (see the `params` column from `as.data.table(mlr_resamplings)`).\n  </details>\n::: {.content-hidden unless-meta=\"params.showsolution\"}\n::: {.callout-note collapse=\"true\"}\n### Solution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv5 = rsmp(\"cv\", folds = 5)\ncv5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ResamplingCV>: Cross-Validation\n* Iterations: 5\n* Instantiated: FALSE\n* Parameters: folds=5\n```\n\n\n:::\n:::\n\n\nNote: `Instantiated: FALSE` means that we only created the resampling instance and did not apply the resampling technique to a task yet.\n:::\n\n::: \n\n## Run the resampling\n\nAfter having created a resampling instance, use it to apply the chosen resampling technique to both previously created learners.\n\n<details>\n  <summary>**Show Hint 1:**</summary>\n  You need to supply the task, the learner and the previously created resampling instance as arguments to the `resample` function. See `?resample` for further details and examples.\n  </details>\n<details>\n  <summary>**Show Hint 2:**</summary>\n  The key ingredients for `resample()` are a task (created by `tsk()`), a learner (created by `lrn()`) and a resampling strategy (created by `rsmp()`), e.g.,\n  \n  `resample(task = task, learner = log_reg, resampling = cv5)`\n\n  </details>\n::: {.content-hidden unless-meta=\"params.showsolution\"}\n::: {.callout-note collapse=\"true\"}\n### Solution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres_log_reg = resample(task, log_reg, cv5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [14:51:01.363] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 1/5)\nINFO  [14:51:04.850] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 2/5)\nINFO  [14:51:07.933] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 3/5)\nINFO  [14:51:10.935] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 4/5)\nINFO  [14:51:13.888] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 5/5)\n```\n\n\n:::\n\n```{.r .cell-code}\nres_knn = resample(task, knn, cv5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [14:51:15.267] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 1/5)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in model.matrix.default(mt2, test, contrasts.arg = contrasts.arg): Variable 'credit_risk' fehlt, ihre Kontraste\nwerden ignoriert\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [14:51:16.672] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 2/5)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in model.matrix.default(mt2, test, contrasts.arg = contrasts.arg): Variable 'credit_risk' fehlt, ihre Kontraste\nwerden ignoriert\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [14:51:17.940] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 3/5)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in model.matrix.default(mt2, test, contrasts.arg = contrasts.arg): Variable 'credit_risk' fehlt, ihre Kontraste\nwerden ignoriert\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [14:51:19.053] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 4/5)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in model.matrix.default(mt2, test, contrasts.arg = contrasts.arg): Variable 'credit_risk' fehlt, ihre Kontraste\nwerden ignoriert\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [14:51:20.158] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 5/5)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in model.matrix.default(mt2, test, contrasts.arg = contrasts.arg): Variable 'credit_risk' fehlt, ihre Kontraste\nwerden ignoriert\n```\n\n\n:::\n\n```{.r .cell-code}\nres_log_reg\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ResampleResult> with 5 resampling iterations\n       task_id      learner_id resampling_id iteration     prediction_test warnings errors\n german_credit classif.log_reg            cv         1 <PredictionClassif>        0      0\n german_credit classif.log_reg            cv         2 <PredictionClassif>        0      0\n german_credit classif.log_reg            cv         3 <PredictionClassif>        0      0\n german_credit classif.log_reg            cv         4 <PredictionClassif>        0      0\n german_credit classif.log_reg            cv         5 <PredictionClassif>        0      0\n```\n\n\n:::\n\n```{.r .cell-code}\nres_knn\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<ResampleResult> with 5 resampling iterations\n       task_id   learner_id resampling_id iteration     prediction_test warnings errors\n german_credit classif.kknn            cv         1 <PredictionClassif>        0      0\n german_credit classif.kknn            cv         2 <PredictionClassif>        0      0\n german_credit classif.kknn            cv         3 <PredictionClassif>        0      0\n german_credit classif.kknn            cv         4 <PredictionClassif>        0      0\n german_credit classif.kknn            cv         5 <PredictionClassif>        0      0\n```\n\n\n:::\n:::\n\n:::\n\n::: \n\n## Evaluation\n\nCompute the cross-validated classification accuracy of both models.\nWhich learner performed better?\n\n<details>\n  <summary>**Show Hint 1:**</summary>\nUse `msr(\"classif.acc\")` and the `aggregate` method of the resampling object.\n  </details>\n<details>\n  <summary>**Show Hint 2:**</summary>\n`res_knn$aggregate(msr(...))` to obtain the classification accuracy averaged across all folds.\n  </details>\n::: {.content-hidden unless-meta=\"params.showsolution\"}\n::: {.callout-note collapse=\"true\"}\n### Solution\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres_knn$aggregate(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n       0.72 \n```\n\n\n:::\n\n```{.r .cell-code}\nres_log_reg$aggregate(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.acc \n      0.747 \n```\n\n\n:::\n:::\n\nNote: Use e.g. `res_knn$score(msr(...))` to look at the results of each individual fold.\n:::\n\n::: \n\n# Summary\n\nWe can now apply different resampling methods to estimate the performance of different learners and fairly compare them.\nWe now have learnt how to obtain a better (in terms of variance) estimate of our model performance instead of doing a simple train and test split.\nThis enables us to fairly compare different learners.\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}