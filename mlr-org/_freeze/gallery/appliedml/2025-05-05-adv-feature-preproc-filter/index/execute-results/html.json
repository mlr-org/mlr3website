{
  "hash": "d2714849ec126869170df00324bccfb7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Filter\ngroup: Advanced Feature Preprocessing\ncategories:\n  - advanced feature preprossesing\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Use filters in a mlr3 pipeline\ndate: \"\"\nparams:\n  showsolution: true\n  base64encode: true\nlisting: false\nsearch: false\nformat:\n  html:\n    filters:\n      - ../../b64_solution.lua\n---\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n```{=html}\n<script>\nconst correctHash = \"638e249f4a15ebb84957da130701d138a6e06d88dceeaa2e6dcd91db70cc1381\";   // value injected by knitr\n\n/* ---------- reusable helper ---------- */\nfunction b64DecodeUtf8(b64) {\n  // 1) atob  -> binary-string   (bytes 0…255)\n  // 2) map   -> Uint8Array      (array of bytes)\n  // 3) TextDecoder('utf-8')     -> real JS string\n  const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));\n  return new TextDecoder('utf-8').decode(bytes);\n}\n\nasync function sha256(txt) {\n  const buf = await crypto.subtle.digest('SHA-256',\n                 new TextEncoder().encode(txt));\n  return Array.from(new Uint8Array(buf))\n              .map(b => b.toString(16).padStart(2, '0')).join('');\n}\n\nasync function unlockOne(btn) {\n  const pass = prompt(\"Password:\");\n  if (!pass) return;\n  if (await sha256(pass) !== correctHash) {\n    alert(\"Wrong password\"); return;\n  }\n\n  /* --- decode only the solution that belongs to THIS button --- */\n  const wrapper = btn.parentElement;             // .b64-wrapper\n  wrapper.querySelectorAll('.hidden-solution').forEach(div => {\n    div.innerHTML = b64DecodeUtf8(div.dataset.encoded);\n    div.classList.remove('hidden-solution');\n    div.style.display = 'block';\n  });\n\n  /* Remove the button so the user can’t click it again */\n  btn.remove();\n}\n</script>\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n    <strong>JavaScript is required to unlock solutions.</strong><br>\n    Please enable JavaScript and reload the page,<br>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n```\n\n\n\n\n# Goal\n\nLearn how to rank features of a supervised task by their importance / strength of relationship with the target variable using a feature filter method.\n\n# German Credit Dataset \n\nWe create the task as for the resampling exercise: The German Credit Data set.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\nlibrary(\"data.table\")\ntask = tsk(\"german_credit\")\n```\n:::\n\n\n# Exercises\n\nWithin the `mlr3` ecosystem, feature filters are implemented in the `mlr3filters` package and are typically used in combination with `mlr3pipelines` to be able to include the whole preprocessing step in a pipeline.\nIn exercises 1 to 3, we apply feature filtering to preprocess the data of a task without using a pipeline. \nIn exercise 4, we will set up a pipeline that combines a learner with the feature filtering as preprocessing step.\n\n## Exercise 1: Find a suitable Feature Filter\n\n<!-- Feature filters are comprised of a set of methods for feature selection that aim at quantifying the ''usefulness'' of a feature in a supervised task. -->\n<!-- Often, it is desirable to reduce the number of features to both decrease the computational cost of fitting a learner and in some cases even improving the performance of the model. -->\n\n<!-- Based on the metric of a feature filter, features can be ranked and the ones with the strongest relationship with the target variable can be selected to be included in the modelling process. -->\n<!-- Typically, feature filters are used when a large number of similar features are available. -->\n<!-- Nevertheless, feature filters also are useful when only a medium number of features is available, as they allow for quantifying the importance of a feature in a supervised setting providing insight into the relationship of a feature and the target variable. -->\n<!-- Here, we will use feature filters to illuminate the strength of the relationship between features and the target variable. -->\n\n<!-- #FIXME: comment /additional info + link on logistic regression -->\n\nMake yourself familiar with the `mlr3filters` package ([link](https://mlr3filters.mlr-org.com/)).\nWhich `Filter`s are applicable to all feature types from the task we created above?\n\n<details>\n<summary>**Hint:**</summary>\n\nSome filters are only applicable to either classification or regression or either numeric or categorical features.\nTherefore, we are looking for a `Filter` that is applicable to our classification task and that can be computed for `integer` and `factor` features (as these types of features are present in task, see `task$feature_types`).\n\nThe website linked above includes a table that provides detailed information for each `Filter`.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\nOur task is a classification task and we have `integer`, and `factor` and features:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$task_type\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"classif\"\n```\n\n\n:::\n\n```{.r .cell-code}\nftypes = unique(task$feature_types$type)\nftypes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"integer\" \"factor\"  \"ordered\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# subset all filters that support integer and factor features\nlibrary(mlr3filters)\nfilters = as.data.table(mlr_filters)\ncheck_ftypes = sapply(filters$feature_types, function(x) all(ftypes %in% x))\nfilters[check_ftypes, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKey: <key>\n                  key                                                    label   task_types task_properties\n               <char>                                                   <char>       <list>          <list>\n 1:              cmim      Minimal Conditional Mutual Information Maximization classif,regr                \n 2:              disr                       Double Input Symmetrical Relevance classif,regr                \n 3:        importance                                         Importance Score      classif                \n 4:  information_gain                                         Information Gain classif,regr                \n 5:               jmi                                 Joint Mutual Information classif,regr                \n 6:              jmim            Minimal Joint Mutual Information Maximization classif,regr                \n 7:               mim                          Mutual Information Maximization classif,regr                \n 8:              mrmr                     Minimum Redundancy Maximal Relevancy classif,regr                \n 9:             njmim Minimal Normalised Joint Mutual Information Maximization classif,regr                \n10:       performance                                   Predictive Performance      classif                \n11:       permutation                                        Permutation Score      classif                \n12:            relief                                                   RELIEF classif,regr                \n13: selected_features                               Embedded Feature Selection      classif                \n                             params                                        feature_types          packages\n                             <list>                                               <list>            <list>\n 1:                         threads                       integer,numeric,factor,ordered           praznik\n 2:                         threads                       integer,numeric,factor,ordered           praznik\n 3:                          method logical,integer,numeric,character,factor,ordered,...              mlr3\n 4: type,equal,discIntegers,threads                       integer,numeric,factor,ordered     FSelectorRcpp\n 5:                         threads                       integer,numeric,factor,ordered           praznik\n 6:                         threads                       integer,numeric,factor,ordered           praznik\n 7:                         threads                       integer,numeric,factor,ordered           praznik\n 8:                         threads                       integer,numeric,factor,ordered           praznik\n 9:                         threads                       integer,numeric,factor,ordered           praznik\n10:                          method logical,integer,numeric,character,factor,ordered,... mlr3,mlr3measures\n11:                 standardize,nmc logical,integer,numeric,character,factor,ordered,... mlr3,mlr3measures\n12:      neighboursCount,sampleSize                       integer,numeric,factor,ordered     FSelectorRcpp\n13:                          method logical,integer,numeric,character,factor,ordered,...              mlr3\n```\n\n\n:::\n:::\n\n\nLooking at the table [here](https://mlr3filters.mlr-org.com/), potential filters are:\n\n`cmim`, `disr`, `importance`, `information_gain`, `jmi`, `jmim`, `mim`, `mrmr`, `njmim`, `performance`, `permutation`, `relief`, `selected_features`.\n\nYou can read their documentation by looking at `?mlr_filters_<id>`, (`<id>` should be replaced with the filter id, e.g., `cmim`).\n\nNote that `importance`, `performance`, `permutation`, and `selected_features` are special in the sense that they require `Learner`s themselves.\n\n:::\n\n:::\n\n## Exercise 2: Information Gain Filter\n\nWe now want to use the `information_gain` filter which requires to install the `FSelectorRcpp` package.\nThis filter quantifies the gain in information by considering the following difference: `H(Target) + H(Feature) - H(Target, Feature)`\nHere, `H(X)` is the Shannon entropy for variable `X` and `H(X, Y)` is the joint Shannon entropy for variable `X` conditioned on `Y`.\n\nCreate an information gain filter and compute the information gain for each feature.\n\nVisualize the score for each feature and decide how many and which features to include.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nUse `flt(\"information_gain\")` to create an `information_gain` filter and calculate the filter scores of the features.\nSee `?mlr_filters_information_gain` (or equivalently `flt(\"information_gain\")$help()`) for more details on how to use a filter.\nIf it does not work, you can use e.g. `flt(\"importance\", learner = lrn(\"classif.rpart\"))` which uses the feature importance of a `classif.rpart` decision tree to rank the features for the feature filter.\n\nFor visualization, you can, for example, create a scree plot (similar as in principle component analysis) that plots the filter score for each feature on the y-axis and the features on the x-axis.\n\nUsing a rule of thumb, e.g., the ''elbow rule'' you can determine the number of features to include.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3filters)\nlibrary(mlr3viz)\nlibrary(FSelectorRcpp)\nfilter = flt(...)\nfilter$calculate()\nautoplot(...)\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3filters)\nlibrary(mlr3viz)\nlibrary(FSelectorRcpp)\nfilter = flt(\"information_gain\")\nfilter$calculate(task)\nautoplot(filter)  # status, credit_history and savings\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n:::\n\n:::\n\n## Exercise 3: Create and Apply a PipeOpFilter to a Task\n\nSince the k-NN learner suffers from the curse of dimensionality, we want set up a preprocessing `PipeOp` to subset our set of features to the 5 most important ones according to the information gain filter (see `flt(\"information_gain\")$help()`). \nIn general, you can see a list of other possible filters by looking at the dictionary `as.data.table(mlr_filters)`.\nYou can construct a `PipeOp` object with the `po()` function from the `mlr3pipelines` package. See `mlr_pipeops$keys()` for possible choices. \nCreate a `PipeOp` that filters features of the `german_credit` task and creates a new task containing only the 5 most important ones according to the information gain filter.\n\n<!-- <details> -->\n<!-- <summary>**Details on the ANOVA F-test filter:**</summary> -->\n\n<!-- The filter conducts an analysis of variance for each feature, where the feature explains the target class variable. -->\n<!-- The score is determined by the F statistic's value. -->\n<!-- The more different the mean values of a feature between the target classes are, the higher is the F statistic. -->\n<!-- </details> -->\n\n<details>\n<summary>**Hint 1:**</summary>\n\n- The filter can be created by `flt(\"information_gain\")` (see also the help page `flt(\"information_gain\")$help()`). \n- In our case, we have to pass the `\"filter\"` key to the first argument of the `po()` function and the filter previously created with the `flt` function to the `filter` argument of the `po()` function to construct a `PipeOpFilter` object that performs feature filtering (see also code examples in the help page `?PipeOpFilter`).\n- The help page of `?PipeOpFilter` also reveals the parameters we can specify. For example, to select the 5 most important features, we can set `filter.nfeat`. This can be done using the `param_vals` argument of the `po()` function during construction or by adding the parameter value to the `param_set$values` field of an already created `PipeOpFilter` object (see also code examples in the help page).\n- The created `PipeOpFilter` object can be applied to a `Task` object to create the filtered `Task`. To do so, we can use the `$train(input)` field of the `PipeOpFilter` object and pass a **list** containing the task we want to filter.\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3pipelines)\n# Set the filter.nfeat parameter directly when constructing the PipeOp:\npofilter = po(\"...\",\n  filter = flt(...),\n   ... = list(filter.nfeat = ...))\n\n# Alternative (first create the filter PipeOp and then set the parameter):\npofilter = po(\"...\", filter = flt(...))\npofilter$...$filter.nfeat = ...\n\n# Train the PipeOpFilter on the task\nfiltered_task = pofilter$train(input = list(...))\nfiltered_task\ntask\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3pipelines)\n# Set the filter.nfeat parameter directly when constructing the PipeOp:\npofilter = po(\"filter\",\n  filter = flt(\"information_gain\"),\n   param_vals = list(filter.nfeat = 5L))\n\n# Alternative (first create the filter PipeOp and then set the parameter):\npofilter = po(\"filter\", filter = flt(\"information_gain\"))\npofilter$param_set$values$filter.nfeat = 5L\n\nfiltered_task = pofilter$train(list(task))[[1]]\nfiltered_task\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:german_credit> (1000 x 6): German Credit\n* Target: credit_risk\n* Properties: twoclass\n* Features (5):\n  - fct (4): credit_history, purpose, savings, status\n  - int (1): duration\n```\n\n\n:::\n\n```{.r .cell-code}\ntask\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:german_credit> (1000 x 21): German Credit\n* Target: credit_risk\n* Properties: twoclass\n* Features (20):\n  - fct (14): credit_history, employment_duration, foreign_worker, housing, job, other_debtors,\n    other_installment_plans, people_liable, personal_status_sex, property, purpose, savings, status,\n    telephone\n  - int (3): age, amount, duration\n  - ord (3): installment_rate, number_credits, present_residence\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n\n## Exercise 4: Combine PipeOpFilter with a Learner\n\nDo the following tasks:\n\n1. Combine the `PipeOpFilter` from the previous exercise with a k-NN learner to create a so-called `Graph` (it can contain multiple preprocessing steps) using the `%>>%` operator.\n2. Convert the `Graph` to a `GraphLearner` so that it behaves like a new learner that first does feature filtering and then trains a model on the filtered data and run the `resample()` function to estimate the performance of the `GraphLearner` with a 5-fold cross-validation.\n3. Change the value of the `nfeat.filter` parameter (which was set to 5 in the previous exercise) and run again `resample()`.\n\n<details>\n<summary>**Hint 1:**</summary>\n\n- Create a kNN learner using `lrn()`. Remember that the shortcut for a kNN classifier ist `\"classif.kknn\"`.\n- You can concatenate different preprocessing steps and a learner using the `%>>%` operator.\n- Use `as_learner` to create a `GraphLearner` (see also the code examples in the help page `?GraphLearner`).\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3learners)\ngraph = ... %>>% lrn(\"...\")\nglrn = as_learner(...)\nrr = resample(task = ..., learner = ..., resampling = ...)\nrr$aggregate()\n\n# Change `nfeat.filter` and run resampling again using same train-test splits\n...\nrr2 = resample(task = ..., learner = ..., resampling = rr$resampling)\nrr2$aggregate()\n```\n:::\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1)\nlibrary(mlr3learners)\ngraph = pofilter %>>% lrn(\"classif.kknn\")\nglrn = as_learner(graph)\nrr = resample(task = task, learner = glrn, resampling = rsmp(\"cv\", folds = 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [10:37:55.341] [mlr3] Applying learner 'information_gain.classif.kknn' on task 'german_credit' (iter 1/5)\nINFO  [10:37:55.933] [mlr3] Applying learner 'information_gain.classif.kknn' on task 'german_credit' (iter 2/5)\nINFO  [10:37:56.598] [mlr3] Applying learner 'information_gain.classif.kknn' on task 'german_credit' (iter 3/5)\nINFO  [10:37:57.421] [mlr3] Applying learner 'information_gain.classif.kknn' on task 'german_credit' (iter 4/5)\nINFO  [10:37:58.150] [mlr3] Applying learner 'information_gain.classif.kknn' on task 'german_credit' (iter 5/5)\n```\n\n\n:::\n\n```{.r .cell-code}\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.ce \n     0.271 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Change `nfeat.filter` and run resampling again using same train-test splits\nglrn$param_set$values$information_gain.filter.nfeat = 2\nrr2 = resample(task = task, learner = glrn, resampling = rr$resampling)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [10:38:00.091] [mlr3] Applying learner 'information_gain.classif.kknn' on task 'german_credit' (iter 1/5)\nINFO  [10:38:00.478] [mlr3] Applying learner 'information_gain.classif.kknn' on task 'german_credit' (iter 2/5)\nINFO  [10:38:00.700] [mlr3] Applying learner 'information_gain.classif.kknn' on task 'german_credit' (iter 3/5)\nINFO  [10:38:00.909] [mlr3] Applying learner 'information_gain.classif.kknn' on task 'german_credit' (iter 4/5)\nINFO  [10:38:01.103] [mlr3] Applying learner 'information_gain.classif.kknn' on task 'german_credit' (iter 5/5)\n```\n\n\n:::\n\n```{.r .cell-code}\nrr2$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclassif.ce \n     0.286 \n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n# Summary\n\nWe learned how to use feature filters to rank the features w.r.t. a feature filter method in a supervised setting and how to subset a task accordingly.\n\nIdeally, feature filtering is directly incorporated into the learning procedure by making use of a pipeline so that performance estimation after feature filtering is not biased.\n\n<!-- In later exercises, we will see how the performance of a whole pipeline can be properly evaluated. -->\n  \n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}