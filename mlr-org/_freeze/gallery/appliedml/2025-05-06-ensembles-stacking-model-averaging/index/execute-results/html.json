{
  "hash": "dcf7aad15450f88790f36354b7f12313",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Model Averaging \ngroup: Ensembles Stacking\ncategories:\n  - ensembles stacking\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Do ensembling and model averaging on german credit set.\ndate: \"\"\nparams:\n  showsolution: true\n  base64encode: true\nlisting: false\nsearch: false\nformat:\n  html:\n    filters:\n      - ../../b64_solution.lua\n---\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n```{=html}\n<script>\nconst correctHash = \"ed033956021b7f9da58bde3cce21f3bd3fa5c5b5e37f2d6105d1238fb4df4680\";   // value injected by knitr\n\n/* ---------- reusable helper ---------- */\nfunction b64DecodeUtf8(b64) {\n  // 1) atob  -> binary-string   (bytes 0…255)\n  // 2) map   -> Uint8Array      (array of bytes)\n  // 3) TextDecoder('utf-8')     -> real JS string\n  const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));\n  return new TextDecoder('utf-8').decode(bytes);\n}\n\nasync function sha256(txt) {\n  const buf = await crypto.subtle.digest('SHA-256',\n                 new TextEncoder().encode(txt));\n  return Array.from(new Uint8Array(buf))\n              .map(b => b.toString(16).padStart(2, '0')).join('');\n}\n\nasync function unlockOne(btn) {\n  const pass = prompt(\"Password:\");\n  if (!pass) return;\n  if (await sha256(pass) !== correctHash) {\n    alert(\"Wrong password\"); return;\n  }\n\n  /* --- decode only the solution that belongs to THIS button --- */\n  const wrapper = btn.parentElement;             // .b64-wrapper\n  wrapper.querySelectorAll('.hidden-solution').forEach(div => {\n    div.innerHTML = b64DecodeUtf8(div.dataset.encoded);\n    div.classList.remove('hidden-solution');\n    div.style.display = 'block';\n  });\n\n  /* Remove the button so the user can’t click it again */\n  btn.remove();\n}\n</script>\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n    <strong>JavaScript is required to unlock solutions.</strong><br>\n    Please enable JavaScript and reload the page,<br>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n```\n\n\n\n\n# Goal\n\nLearn how to do ensembling and model averaging with `mlr3pipelines` and optimizing weights with `bbotk`.\n\n# German Credit Data\n\n## Description\n\n- Data from 1973 to 1975 from a large regional bank in southern Germany classifying credits described by a set of attributes to good or bad credit risks.\n- Stratified sample of 1000 credits (300 bad ones and 700 good ones).\n- Customers with good credit risks perfectly complied with the conditions of the contract while customers with bad credit risks did not comply with the contract as required.\n- Available in `tsk(\"german_credit\")`.\n\n## Data Dictionary\n\nn = 1,000 observations of credits\n\n- `credit_risk`: Has the credit contract been complied with (good) or not (bad)?\n- `age`: Age of debtor in years\n- `amount`: Credit amount in DM\n- `credit_history`: History of compliance with previous or concurrent credit contracts\n- `duration`: Credit duration in months\n- `employment_duration`: Duration of debtor's employment with current employer\n- `foreign_worker`: Whether the debtor is a foreign worker\n- `housing`: Type of housing the debtor lives in\n- `installment_rate`: Credit installments as a percentage of debtor's disposable income\n- `job`: Quality of debtor's job\n- `number_credits`: Number of credits including the current one the debtor has (or had) at this bank\n- `other_debtors`: Whether there is another debtor or a guarantor for the credit\n- `other_installment_plans`: Installment plans from providers other than the credit-giving bank\n- `people_liable`: Number of persons who financially depend on the debtor\n- `personal_status_sex`: Combined information on sex and marital status\n- `present_residence`: Length of time (in years) the debtor lives in the present residence\n- `property`: The debtor's most valuable property\n- `purpose`: Purpose for which the credit is needed\n- `savings`: Debtor's saving\n- `status`: Status of the debtor's checking account with the bank\n- `telephone`: Whether there is a telephone landline registered on the debtor's name\n\n# Prerequisites\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(rpart.plot)\nlibrary(mlr3viz)\nlibrary(bbotk)\nlibrary(mlr3misc)\n\ntask = tsk(\"german_credit\")\n\nset.seed(2409)\n```\n:::\n\n\n# 1 Build a \"Random Forest\" From Scratch\n\n## 1.1 Create three Bagged Trees\n\nUse the classifavg `PipeOP` to combine predictions of decision trees trained on different subsamples of the `german_credit` task similarly as in a Random Forest.\nUse 3 decision trees with a maximum depth of 3.\nTo create subsamples of data, the subsample `PipeOp` comes in handy.\nPlot the graph learner representing your ensemble.\nPlot each of the 3 decision trees of the ensemble after training on all data.\nCompare this to a single decision tree with a maximum depth of 3 trained on all data.\nWhat is missing to actually mimic a Random Forest?\n\n<details>\n  <summary>**Hint 1:**</summary>\n  Subsample with the subsample `PipeOp` and pass the output into the decision tree.\n  This is one pipeline part.\n  You can then `greplicate` (creates disjoint graph union of copies of a graph) this part three times to create an actual graph and combine the output of the decision trees via the classifavg `PipeOp`.\n  To plot the trees, the `rpart.plot` package is helpful.\n  If you are unsure how the different parts fit together, maybe plot the intermediate graphs you construct via `graph$plot()`.\n</details>\n\n<details>\n  <summary>**Hint 2:**</summary>\n \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling = rsmp(\"cv\", folds  = 10L)$instantiate(task)\npl = po(\"...\") %>>% po(\"...\", learner = ...)\ngraph = ppl(\"greplicate\", ...)\ngraph = graph %>>% po(\"classifavg\")\ngraphLearner = as_learner(graph)\ngraphLearner$id = \"bagged_trees\"\ngraphLearner$graph$plot()\ngraphLearner$train(task)\n\ndt = lrn(\"classif.rpart\")\ndt$id = \"tree\"\ndt$train(task)\n\nrpart.plot(graphLearner$...)\n...\n```\n:::\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling = rsmp(\"cv\", folds  = 10L)$instantiate(task)\npl = po(\"subsample\") %>>% po(\"learner\", learner = lrn(\"classif.rpart\", maxdepth = 3L))\ngraph = ppl(\"greplicate\", graph = pl, n = 3L)\ngraph = graph %>>% po(\"classifavg\")\ngraphLearner = as_learner(graph)\ngraphLearner$id = \"bagged_trees\"\ngraphLearner$graph$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\ngraphLearner$train(task)\n\ndt = lrn(\"classif.rpart\", maxdepth = 3L)\ndt$id = \"tree\"\ndt$train(task)\n\nrpart.plot(graphLearner$state$model$classif.rpart_1$model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-2.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nrpart.plot(graphLearner$state$model$classif.rpart_2$model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-3.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nrpart.plot(graphLearner$state$model$classif.rpart_3$model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-4.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nrpart.plot(dt$state$model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-5.png){fig-align='center' width=672}\n:::\n:::\n\nAlthough each of the 3 trees sees different data during training, the first split is the same as if we had trained on all data.\nIn an actual Random Forest, feature variables are randomly selected as potential splitting candidates which makes trees more heterogeneous.\n\n:::\n\n:::\n\n## 1.2 Reset Maximum Depth and compare to Random Forest\n\nReset the maximum depth hyperparameter values for each tree of your ensemble and the decision tree.\nProceed to benchmark the ensemble of three trees against the decision tree and an actual ranger Random Forest with 3, 10 and 100 trees.\nUse 10-fold CV to evaluate the ROC AUC of the models.\nAs a follow up question, would it make sense to change the weights used by the classifavg `PipeOp` to combine the predictions?\n\n<details>\n  <summary>**Hint 1:**</summary>\n  Prior lectures should be helpful where you already benchmarked different learners.\n</details>\n\n<details>\n  <summary>**Hint 2:**</summary>\n \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndt$param_set$set_values(.values = list(maxdepth = NULL))\ngraphLearner$...\n...\n\nranger3 = lrn(\"classif.ranger\", ...)\nranger3$id = \"rf_3\"\nranger10 = lrn(\"classif.ranger\", ...)\nranger10$id = \"rf_10\"\nranger100 = lrn(\"classif.ranger\", ...)\nranger100$id = \"rf_100\"\n\ngraphLearner$predict_type = \"prob\"\n...\n\nbg = benchmark_grid(...)\nb = benchmark(...)\nautoplot(..., measure = ...)\n```\n:::\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndt$param_set$set_values(.values = list(maxdepth = NULL))\ngraphLearner$param_set$set_values(.values = list(classif.rpart_1.maxdepth = NULL, classif.rpart_2.maxdepth = NULL, classif.rpart_3.maxdepth = NULL))\n\nranger3 = lrn(\"classif.ranger\", num.trees = 3L)\nranger3$id = \"rf_3\"\nranger10 = lrn(\"classif.ranger\", num.trees = 10L)\nranger10$id = \"rf_10\"\nranger100 = lrn(\"classif.ranger\", num.trees = 100L)\nranger100$id = \"rf_100\"\n\ngraphLearner$predict_type = \"prob\"\ndt$predict_type = \"prob\"\nranger3$predict_type = \"prob\"\nranger10$predict_type = \"prob\"\nranger100$predict_type = \"prob\"\n\nbg = benchmark_grid(task, list(graphLearner, dt, ranger3, ranger10, ranger100), resampling)\nb = benchmark(bg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [16:32:21.693] [mlr3] Running benchmark with 50 resampling iterations\nINFO  [16:32:22.565] [mlr3] Applying learner 'bagged_trees' on task 'german_credit' (iter 1/10)\nINFO  [16:32:23.326] [mlr3] Applying learner 'bagged_trees' on task 'german_credit' (iter 2/10)\nINFO  [16:32:24.405] [mlr3] Applying learner 'bagged_trees' on task 'german_credit' (iter 3/10)\nINFO  [16:32:25.498] [mlr3] Applying learner 'bagged_trees' on task 'german_credit' (iter 4/10)\nINFO  [16:32:26.569] [mlr3] Applying learner 'bagged_trees' on task 'german_credit' (iter 5/10)\nINFO  [16:32:27.339] [mlr3] Applying learner 'bagged_trees' on task 'german_credit' (iter 6/10)\nINFO  [16:32:28.135] [mlr3] Applying learner 'bagged_trees' on task 'german_credit' (iter 7/10)\nINFO  [16:32:29.227] [mlr3] Applying learner 'bagged_trees' on task 'german_credit' (iter 8/10)\nINFO  [16:32:29.527] [mlr3] Applying learner 'bagged_trees' on task 'german_credit' (iter 9/10)\nINFO  [16:32:29.945] [mlr3] Applying learner 'bagged_trees' on task 'german_credit' (iter 10/10)\nINFO  [16:32:30.287] [mlr3] Applying learner 'tree' on task 'german_credit' (iter 1/10)\nINFO  [16:32:30.586] [mlr3] Applying learner 'tree' on task 'german_credit' (iter 2/10)\nINFO  [16:32:30.884] [mlr3] Applying learner 'tree' on task 'german_credit' (iter 3/10)\nINFO  [16:32:31.199] [mlr3] Applying learner 'tree' on task 'german_credit' (iter 4/10)\nINFO  [16:32:31.495] [mlr3] Applying learner 'tree' on task 'german_credit' (iter 5/10)\nINFO  [16:32:31.792] [mlr3] Applying learner 'tree' on task 'german_credit' (iter 6/10)\nINFO  [16:32:32.096] [mlr3] Applying learner 'tree' on task 'german_credit' (iter 7/10)\nINFO  [16:32:32.402] [mlr3] Applying learner 'tree' on task 'german_credit' (iter 8/10)\nINFO  [16:32:32.676] [mlr3] Applying learner 'tree' on task 'german_credit' (iter 9/10)\nINFO  [16:32:32.975] [mlr3] Applying learner 'tree' on task 'german_credit' (iter 10/10)\nINFO  [16:32:33.266] [mlr3] Applying learner 'rf_3' on task 'german_credit' (iter 1/10)\nINFO  [16:32:34.242] [mlr3] Applying learner 'rf_3' on task 'german_credit' (iter 2/10)\nINFO  [16:32:34.289] [mlr3] Applying learner 'rf_3' on task 'german_credit' (iter 3/10)\nINFO  [16:32:35.043] [mlr3] Applying learner 'rf_3' on task 'german_credit' (iter 4/10)\nINFO  [16:32:35.476] [mlr3] Applying learner 'rf_3' on task 'german_credit' (iter 5/10)\nINFO  [16:32:36.236] [mlr3] Applying learner 'rf_3' on task 'german_credit' (iter 6/10)\nINFO  [16:32:36.978] [mlr3] Applying learner 'rf_3' on task 'german_credit' (iter 7/10)\nINFO  [16:32:37.634] [mlr3] Applying learner 'rf_3' on task 'german_credit' (iter 8/10)\nINFO  [16:32:38.019] [mlr3] Applying learner 'rf_3' on task 'german_credit' (iter 9/10)\nINFO  [16:32:38.377] [mlr3] Applying learner 'rf_3' on task 'german_credit' (iter 10/10)\nINFO  [16:32:38.985] [mlr3] Applying learner 'rf_10' on task 'german_credit' (iter 1/10)\nINFO  [16:32:39.264] [mlr3] Applying learner 'rf_10' on task 'german_credit' (iter 2/10)\nINFO  [16:32:39.545] [mlr3] Applying learner 'rf_10' on task 'german_credit' (iter 3/10)\nINFO  [16:32:39.849] [mlr3] Applying learner 'rf_10' on task 'german_credit' (iter 4/10)\nINFO  [16:32:40.175] [mlr3] Applying learner 'rf_10' on task 'german_credit' (iter 5/10)\nINFO  [16:32:40.499] [mlr3] Applying learner 'rf_10' on task 'german_credit' (iter 6/10)\nINFO  [16:32:41.139] [mlr3] Applying learner 'rf_10' on task 'german_credit' (iter 7/10)\nINFO  [16:32:42.384] [mlr3] Applying learner 'rf_10' on task 'german_credit' (iter 8/10)\nINFO  [16:32:42.678] [mlr3] Applying learner 'rf_10' on task 'german_credit' (iter 9/10)\nINFO  [16:32:42.981] [mlr3] Applying learner 'rf_10' on task 'german_credit' (iter 10/10)\nINFO  [16:32:43.292] [mlr3] Applying learner 'rf_100' on task 'german_credit' (iter 1/10)\nINFO  [16:32:43.718] [mlr3] Applying learner 'rf_100' on task 'german_credit' (iter 2/10)\nINFO  [16:32:44.018] [mlr3] Applying learner 'rf_100' on task 'german_credit' (iter 3/10)\nINFO  [16:32:44.293] [mlr3] Applying learner 'rf_100' on task 'german_credit' (iter 4/10)\nINFO  [16:32:44.634] [mlr3] Applying learner 'rf_100' on task 'german_credit' (iter 5/10)\nINFO  [16:32:45.829] [mlr3] Applying learner 'rf_100' on task 'german_credit' (iter 6/10)\nINFO  [16:32:45.837] [mlr3] Applying learner 'rf_100' on task 'german_credit' (iter 7/10)\nINFO  [16:32:46.130] [mlr3] Applying learner 'rf_100' on task 'german_credit' (iter 8/10)\nINFO  [16:32:46.449] [mlr3] Applying learner 'rf_100' on task 'german_credit' (iter 9/10)\nINFO  [16:32:46.729] [mlr3] Applying learner 'rf_100' on task 'german_credit' (iter 10/10)\nINFO  [16:32:46.840] [mlr3] Finished benchmark\n```\n\n\n:::\n\n```{.r .cell-code}\nautoplot(b, measure = msr(\"classif.auc\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\nEnsemble of 3 trees maybe slightly better than a single tree.\\\\\nReal random forests with many trees much better.\\\\\nIf you have the time, maybe set the number of our tree ensemble higher and see how that compares.\n\n:::\n\n:::\n\n# 2 Model Averaging\n\nUse the classifavg `PipeOP` to combine the predictions of a decision tree, a k-NN (with k = 7) and a logistic regression.\nBenchmark the ensemble against each learner and a featureless learner.\nUse 10-fold CV evaluate the ROC AUC of the models.\nBy default classifavg uses equal weights to combine the predictions of the models.\nCan you manually find a weighting scheme that results in better performance than equal weights?\n\n<details>\n  <summary>**Hint 1:**</summary>\n  If you are not familiar with a k-NN learner, you may catch up here: https://slds-lmu.github.io/i2ml/chapters/05_knn/\n  You can largely reuse parts of the graph learner you previously constructed (excluding the subsampling part).\n  To manually set a weight vector as a hyperparameter of the graph learner, inspect its `$param_set` and make use of the `$set_values()` function.\n</details>\n\n<details>\n  <summary>**Hint 2:**</summary>\n \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndt = lrn(...)\ndt$predict_type = \"prob\"\nkknn = lrn(...)\nkknn$predict_type = \"prob\"\nlog_reg = lrn(...)\nlog_reg$predict_type = \"prob\"\nfeatureless = lrn(...)\nfeatureless$predict_type = \"prob\"\n\ngraph = gunion() %>>% po(...)\ngraphLearner = as_learner(graph)\n\nbg = benchmark_grid(...)\nb = benchmark(...)\nb$aggregate(...)\n\ngrl_weights_adjusted = graphLearner$clone(deep = TRUE)\ngrl_weights_adjusted$param_set$set_values(...)\ngrl_weights_adjusted$id = paste0(grl_weights_adjusted$id, \".weights_adjusted\")\n\nbg = benchmark_grid(...)\nb = benchmark(...)\nb$aggregate(...)\n```\n:::\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndt = lrn(\"classif.rpart\")\ndt$predict_type = \"prob\"\nkknn = lrn(\"classif.kknn\", k = 7L)\nkknn$predict_type = \"prob\"\nlog_reg = lrn(\"classif.log_reg\")\nlog_reg$predict_type = \"prob\"\nfeatureless = lrn(\"classif.featureless\")\nfeatureless$predict_type = \"prob\"\n\ngraph = gunion(list(dt, kknn, log_reg)) %>>% po(\"classifavg\")\ngraphLearner = as_learner(graph)\n\nbg = benchmark_grid(task, list(graphLearner, dt, kknn, log_reg, featureless), resampling)\nb = benchmark(bg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [16:32:48.118] [mlr3] Running benchmark with 50 resampling iterations\nINFO  [16:32:48.400] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 1/10)\nINFO  [16:32:48.693] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 2/10)\nINFO  [16:32:49.035] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 3/10)\nINFO  [16:32:49.932] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 4/10)\nINFO  [16:32:50.242] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 5/10)\nINFO  [16:32:50.575] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 6/10)\nINFO  [16:32:51.471] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 7/10)\nINFO  [16:32:52.058] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 8/10)\nINFO  [16:32:52.370] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 9/10)\nINFO  [16:32:52.694] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 10/10)\nINFO  [16:32:53.030] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 1/10)\nINFO  [16:32:53.314] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 2/10)\nINFO  [16:32:53.593] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 3/10)\nINFO  [16:32:54.225] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 4/10)\nINFO  [16:32:54.545] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 5/10)\nINFO  [16:32:54.860] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 6/10)\nINFO  [16:32:55.773] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 7/10)\nINFO  [16:32:56.054] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 8/10)\nINFO  [16:32:56.333] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 9/10)\nINFO  [16:32:56.626] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 10/10)\nINFO  [16:32:56.907] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 1/10)\nINFO  [16:32:57.190] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 2/10)\nINFO  [16:32:57.473] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 3/10)\nINFO  [16:32:57.753] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 4/10)\nINFO  [16:32:58.036] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 5/10)\nINFO  [16:32:58.310] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 6/10)\nINFO  [16:32:58.596] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 7/10)\nINFO  [16:32:59.203] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 8/10)\nINFO  [16:32:59.477] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 9/10)\nINFO  [16:32:59.759] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 10/10)\nINFO  [16:33:00.064] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 1/10)\nINFO  [16:33:00.346] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 2/10)\nINFO  [16:33:00.635] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 3/10)\nINFO  [16:33:00.929] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 4/10)\nINFO  [16:33:01.205] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 5/10)\nINFO  [16:33:01.480] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 6/10)\nINFO  [16:33:01.752] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 7/10)\nINFO  [16:33:02.054] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 8/10)\nINFO  [16:33:02.344] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 9/10)\nINFO  [16:33:02.623] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 10/10)\nINFO  [16:33:02.921] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 1/10)\nINFO  [16:33:03.200] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 2/10)\nINFO  [16:33:03.806] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 3/10)\nINFO  [16:33:04.082] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 4/10)\nINFO  [16:33:04.366] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 5/10)\nINFO  [16:33:04.643] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 6/10)\nINFO  [16:33:04.920] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 7/10)\nINFO  [16:33:05.195] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 8/10)\nINFO  [16:33:05.619] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 9/10)\nINFO  [16:33:05.897] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 10/10)\nINFO  [16:33:05.928] [mlr3] Finished benchmark\n```\n\n\n:::\n\n```{.r .cell-code}\nb$aggregate(msr(\"classif.auc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      nr       task_id                                            learner_id resampling_id iters classif.auc\n   <int>        <char>                                                <char>        <char> <int>       <num>\n1:     1 german_credit classif.rpart.classif.kknn.classif.log_reg.classifavg            cv    10   0.7873239\n2:     2 german_credit                                         classif.rpart            cv    10   0.7167840\n3:     3 german_credit                                          classif.kknn            cv    10   0.7251241\n4:     4 german_credit                                       classif.log_reg            cv    10   0.7808014\n5:     5 german_credit                                   classif.featureless            cv    10   0.5000000\nHidden columns: resample_result\n```\n\n\n:::\n\n```{.r .cell-code}\ngrl_weights_adjusted = graphLearner$clone(deep = TRUE)\ngrl_weights_adjusted$param_set$set_values(.values = list(classifavg.weights = c(0.3, 0.1, 0.6)))\ngrl_weights_adjusted$id = paste0(grl_weights_adjusted$id, \".weights_adjusted\")\n\nbg = benchmark_grid(task, list(graphLearner, grl_weights_adjusted, dt, kknn, log_reg, featureless), resampling)\nb = benchmark(bg)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [16:33:06.121] [mlr3] Running benchmark with 60 resampling iterations\nINFO  [16:33:06.407] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 1/10)\nINFO  [16:33:06.807] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 2/10)\nINFO  [16:33:07.100] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 3/10)\nINFO  [16:33:07.389] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 4/10)\nINFO  [16:33:07.694] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 5/10)\nINFO  [16:33:08.103] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 6/10)\nINFO  [16:33:08.398] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 7/10)\nINFO  [16:33:08.687] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 8/10)\nINFO  [16:33:09.010] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 9/10)\nINFO  [16:33:09.419] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg' on task 'german_credit' (iter 10/10)\nINFO  [16:33:09.731] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted' on task 'german_credit' (iter 1/10)\nINFO  [16:33:10.037] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted' on task 'german_credit' (iter 2/10)\nINFO  [16:33:10.358] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted' on task 'german_credit' (iter 3/10)\nINFO  [16:33:10.775] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted' on task 'german_credit' (iter 4/10)\nINFO  [16:33:11.099] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted' on task 'german_credit' (iter 5/10)\nINFO  [16:33:11.424] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted' on task 'german_credit' (iter 6/10)\nINFO  [16:33:11.754] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted' on task 'german_credit' (iter 7/10)\nINFO  [16:33:12.218] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted' on task 'german_credit' (iter 8/10)\nINFO  [16:33:12.823] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted' on task 'german_credit' (iter 9/10)\nINFO  [16:33:13.156] [mlr3] Applying learner 'classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted' on task 'german_credit' (iter 10/10)\nINFO  [16:33:13.481] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 1/10)\nINFO  [16:33:13.903] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 2/10)\nINFO  [16:33:14.199] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 3/10)\nINFO  [16:33:14.494] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 4/10)\nINFO  [16:33:14.801] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 5/10)\nINFO  [16:33:15.191] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 6/10)\nINFO  [16:33:15.504] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 7/10)\nINFO  [16:33:15.817] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 8/10)\nINFO  [16:33:16.161] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 9/10)\nINFO  [16:33:16.507] [mlr3] Applying learner 'classif.rpart' on task 'german_credit' (iter 10/10)\nINFO  [16:33:16.799] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 1/10)\nINFO  [16:33:17.099] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 2/10)\nINFO  [16:33:17.488] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 3/10)\nINFO  [16:33:17.806] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 4/10)\nINFO  [16:33:18.115] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 5/10)\nINFO  [16:33:18.429] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 6/10)\nINFO  [16:33:18.868] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 7/10)\nINFO  [16:33:19.155] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 8/10)\nINFO  [16:33:19.448] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 9/10)\nINFO  [16:33:19.760] [mlr3] Applying learner 'classif.kknn' on task 'german_credit' (iter 10/10)\nINFO  [16:33:20.141] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 1/10)\nINFO  [16:33:20.425] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 2/10)\nINFO  [16:33:20.718] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 3/10)\nINFO  [16:33:21.073] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 4/10)\nINFO  [16:33:21.434] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 5/10)\nINFO  [16:33:21.733] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 6/10)\nINFO  [16:33:22.033] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 7/10)\nINFO  [16:33:22.762] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 8/10)\nINFO  [16:33:23.056] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 9/10)\nINFO  [16:33:23.358] [mlr3] Applying learner 'classif.log_reg' on task 'german_credit' (iter 10/10)\nINFO  [16:33:23.660] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 1/10)\nINFO  [16:33:24.067] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 2/10)\nINFO  [16:33:24.369] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 3/10)\nINFO  [16:33:24.666] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 4/10)\nINFO  [16:33:25.031] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 5/10)\nINFO  [16:33:25.349] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 6/10)\nINFO  [16:33:25.638] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 7/10)\nINFO  [16:33:25.932] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 8/10)\nINFO  [16:33:26.314] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 9/10)\nINFO  [16:33:26.605] [mlr3] Applying learner 'classif.featureless' on task 'german_credit' (iter 10/10)\nINFO  [16:33:26.638] [mlr3] Finished benchmark\n```\n\n\n:::\n\n```{.r .cell-code}\nb$aggregate(msr(\"classif.auc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      nr       task_id                                                             learner_id resampling_id iters\n   <int>        <char>                                                                 <char>        <char> <int>\n1:     1 german_credit                  classif.rpart.classif.kknn.classif.log_reg.classifavg            cv    10\n2:     2 german_credit classif.rpart.classif.kknn.classif.log_reg.classifavg.weights_adjusted            cv    10\n3:     3 german_credit                                                          classif.rpart            cv    10\n4:     4 german_credit                                                           classif.kknn            cv    10\n5:     5 german_credit                                                        classif.log_reg            cv    10\n6:     6 german_credit                                                    classif.featureless            cv    10\n   classif.auc\n         <num>\n1:   0.7873239\n2:   0.7930589\n3:   0.7167840\n4:   0.7251241\n5:   0.7808014\n6:   0.5000000\nHidden columns: resample_result\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n# 3 Optimizing Weights\n\nBuilding upon the previous exercise, we now want to numerically optimize the weights of the ensemble via `bbotk`.\nTo do so, we will have to construct an `OptimInstanceSingleCrit` in which we pass a domain, a search space, a codomain and the actual objective function that is optimized.\nFirst, we will implement a naive solution, by changing the weighting scheme in the objective function and evaluating the ensemble based on a resampling.\nNote that when we are optimizing three weights (one for each model), this is in essence a constrained optimization problem with only two degrees of freedom:\nGiven the first weight and the second weight and the constraint that all three weights must sum to 1, we can always calculate the third weight.\nWe will ignore this in the following and simply optimize each weight on a scale from 0 to 1 and will normalize all weights to sum to 1 within the objective.\n\nTo construct the `OptimInstanceSingleCrit` do the following:\n\n* create the `objective_function` (a standard R function) accepting `xs` (a list) as input:\n  - `xs` will be the weights in the form of a list\n  - extract the weights and use them within the ensemble (e.g., clone the graph learner from the previous exercise and set the `classifavg.weights`)\n  - use `resample` on  this weighted ensemble\n  - extract the ROC AUC and return it in a list\n* create the `domain` (the space we optimize over):\n  - we optimize over the three numeric weight parameters with values from 0 to 1 (have a look at `?p_dbl`) \n* create the codomain (describing the output space):\n  - we maximize the numeric ROC AUC value\n  - to make sure that we maximize instead of minimize (the `bbotk` default) set `tags = \"maximize\"`of this `p_dbl`\n* collect everything in an `OptimInstanceSingleCrit`\n \nUse random search as an optimizer and terminate after 10 function evaluations.\nWhy is our approach (i.e., how we constructed the objective function) ineffective?\n\n<details>\n  <summary>**Hint 1:**</summary>\n  If you are not yet familiar with the `bbotk` package, a good starting point is: https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-black-box-optimization and \n  https://cran.r-project.org/web/packages/bbotk/vignettes/bbotk.html\n</details>\n\n<details>\n  <summary>**Hint 2:**</summary>\n \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrl_weights_optimized = graphLearner$clone(deep = TRUE)\ngrl_weights_optimized$id = paste0(grl_weights_optimized$id, \".weights_optimized_naive\")\n\nobjective_function = function(xs) {\n  weights = unlist(xs)\n  weights = ...  # sum to 1 normalization\n  grl_weights_optimized$param_set$set_values(...)\n  rr = resample(...)\n  # returning the normalized weights as the second element in the list allows us to also store them in the archive\n  list(classif.auc = ..., weights = list(weights))\n}\n\ndomain = ps(w_dt = ..., w_kknn = ..., w_log_reg = ...)\ncodomain = ps(classif.auc = ...) # make sure to specify `tags = \"maximize\"`\n\nobjective = ObjectiveRFun$new(\n  fun = ...,\n  domain = ...,\n  codomain = ...,\n  id = \"optimize_grl_weights_random\"\n)\n\ninstance = OptimInstanceSingleCrit$new(\n  objective = ...\n  terminator = trm(...)\n)\n\noptimizer = opt(...)\noptimizer$optimize(instance)\n```\n:::\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrl_weights_optimized = graphLearner$clone(deep = TRUE)\ngrl_weights_optimized$id = paste0(grl_weights_optimized$id, \".weights_optimized_naive\")\n\nobjective_function = function(xs) {\n  weights = unlist(xs)\n  weights = weights / sum(weights)  # sum to 1 normalization\n  grl_weights_optimized$param_set$set_values(.values = list(classifavg.weights = weights))\n  rr = resample(task, grl_weights_optimized, resampling)\n  # returning the normalized weights as the second element in the list allows us to also store them in the archive\n  list(classif.auc = rr$aggregate(msr(\"classif.auc\")), weights = list(weights))\n}\n\ndomain = ps(w_dt = p_dbl(0, 1), w_kknn = p_dbl(0, 1), w_log_reg = p_dbl(0, 1))\ncodomain = ps(classif.auc = p_dbl(0.5, 1, tags = \"maximize\"))\n\nobjective = ObjectiveRFun$new(\n  fun = objective_function,\n  domain = domain,\n  codomain = codomain,\n  id = \"optimize_grl_weights_random\"\n)\n\ninstance = OptimInstanceSingleCrit$new(\n  objective = objective,\n  terminator = trm(\"evals\", n_evals = 10L)\n)\n\noptimizer = opt(\"random_search\")\noptimizer$optimize(instance)\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         w_dt    w_kknn w_log_reg  x_domain classif.auc\n        <num>     <num>     <num>    <list>       <num>\n1: 0.02472225 0.1770757 0.4531909 <list[3]>   0.7898229\n```\n\n\n:::\n:::\n\n\nThis approach is inefficient because we use resampling in the objective function.\nThere is no need to always retrain and evaluate the learners because we only weight predictions differently and average them.\n\n:::\n\n:::\n\n# 4 Optimizing Weights Efficiently\nIn the previous exercise, we optimized the weights of our ensemble - but very inefficiently.\nIn this exercise we want to do better.\nRewrite the objective function to directly operate on the cross-validated predictions and combine the predicted probabilities directly as in model averaging.\nConstruct an `OptimInstanceSingleCrit` and optimize it via CMA-ES and terminate after 100 function evaluations.\n\nNote that you can reuse most logic from the previous exercise and the only interesting part is how to rewrite the objective function.\n\n<details>\n  <summary>**Hint 1:**</summary>\n  Store resampling results of each learner externally and use these results in the objective function.\n  For each fold weight the probability predictions and average them.\n  Then construct a new `PredictionClassif` which allows you to calculate the ROC AUC for each fold.\n  Finally, return the average ROC AUC over the folds.\n</details>\n\n<details>\n  <summary>**Hint 2:**</summary>\n \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk = resampling$param_set$get_values()[[\"folds\"]]\n\ndt_pred = resample(...)\nkknn_pred = resample(...)\nlog_reg_pred = resample(...)\n\nobjective_function = function(xs) {\n  weights = unlist(xs)\n  weights = ... # sum to 1 normalization\n  aucs = map_dbl(seq_len(k), function(fold) {\n    dt_p = dt_pred$...\n    kknn_p = kknn_pred$...\n    log_reg_p = log_reg_pred$...\n    row_ids = dt_p$row_ids\n    stopifnot(all(row_ids == kknn_p$row_ids) && all(row_ids == log_reg_p$row_ids))\n    truth = dt_p$truth\n    weighted_probs = Reduce(\"+\", list(...))\n    weighted_response = ...\n    weighted_p = PredictionClassif$new(row_ids = row_ids, truth = truth, response = weighted_response, prob = weighted_probs)\n    weighted_p$score(...)\n  })\n  list(classif.auc = ..., weights = list(weights))\n}\n\ndomain = ps(...)\ncodomain = ps(...)\n\nobjective = ObjectiveRFun$new(\n  fun = objective_function,\n  domain = domain,\n  codomain = codomain,\n  id = \"optimize_grl_weights_cmaes\"\n)\n\ninstance = OptimInstanceSingleCrit$new(\n  objective = ...,\n  terminator = ...\n)\n\noptimizer = opt(...)\noptimizer$optimize(instance)\n```\n:::\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(adagio)\n\nk = resampling$param_set$get_values()[[\"folds\"]]\n\ndt_pred = resample(task, dt, resampling)\nkknn_pred = resample(task, kknn, resampling)\nlog_reg_pred = resample(task, log_reg, resampling)\n\nobjective_function = function(xs) {\n  weights = unlist(xs)\n  weights = weights / sum(weights)  # sum to 1 normalization\n  aucs = map_dbl(seq_len(k), function(fold) {\n    dt_p = dt_pred$predictions()[[fold]]\n    kknn_p = kknn_pred$predictions()[[fold]]\n    log_reg_p = log_reg_pred$predictions()[[fold]]\n    row_ids = dt_p$row_ids\n    stopifnot(all(row_ids == kknn_p$row_ids) && all(row_ids == log_reg_p$row_ids))\n    truth = dt_p$truth\n    weighted_probs = Reduce(\"+\", list(dt_p$prob * weights[1L], kknn_p$prob * weights[2L], log_reg_p$prob * weights[3L]))\n    weighted_response = colnames(weighted_probs)[apply(weighted_probs, MARGIN = 1L, FUN = which.max)]\n    weighted_p = PredictionClassif$new(row_ids = row_ids, truth = truth, response = weighted_response, prob = weighted_probs)\n    weighted_p$score(msr(\"classif.auc\"))\n  })\n  list(classif.auc = mean(aucs), weights = list(weights))\n}\n\ndomain = ps(w_dt = p_dbl(0, 1), w_kknn = p_dbl(0, 1), w_log_reg = p_dbl(0, 1))\ncodomain = ps(classif.auc = p_dbl(0.5, 1, tags = \"maximize\"))\n\nobjective = ObjectiveRFun$new(\n  fun = objective_function,\n  domain = domain,\n  codomain = codomain,\n  id = \"optimize_grl_weights_cmaes\"\n)\n\ninstance = OptimInstanceSingleCrit$new(\n  objective = objective,\n  terminator = trm(\"evals\", n_evals = 100L)\n)\n\noptimizer = opt(\"cmaes\")\noptimizer$optimize(instance)\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        w_dt    w_kknn w_log_reg  x_domain classif.auc\n       <num>     <num>     <num>    <list>       <num>\n1: 0.5404667 0.2183187         1 <list[3]>   0.7935633\n```\n\n\n:::\n:::\n\n\n:::\n\n:::\n\n# Summary\n\nWe built a bagged ensemble of trees from scratch and compared its performance to a single tree and actual random forests with different numbers of trees.\nWe then performed model averaging of a decision tree, a k-NN and a logistic regression.\nChoosing weights manually is cumbersome so we optimized them both in a straightforward but inefficient and a slightly more demanding but efficient way.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}