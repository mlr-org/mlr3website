{
  "hash": "2cb9d3ec4cd56012ff383e1394a54b9c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Imputation\ncategories:\n  - imputation\n  - mlr3benchmarking\nauthor:\n  - name: Fiona Ewald\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Learn the basics of imputation (i.e. filling in missing data) with `mlr3pipelines`.\ndate: \"\"\nparams:\n  showsolution: true\n  base64encode: true\nlisting: false\nsearch: false\nformat:\n  html:\n    filters:\n      - ../../b64_solution.lua\n---\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n```{=html}\n<script>\nconst correctHash = \"45a4fb07856c888a4aa78ec25e14b39a3924b97ee4153345cc1c7fb3911eb7a2\";   // value injected by knitr\n\n/* ---------- reusable helper ---------- */\nfunction b64DecodeUtf8(b64) {\n  // 1) atob  -> binary-string   (bytes 0…255)\n  // 2) map   -> Uint8Array      (array of bytes)\n  // 3) TextDecoder('utf-8')     -> real JS string\n  const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));\n  return new TextDecoder('utf-8').decode(bytes);\n}\n\nasync function sha256(txt) {\n  const buf = await crypto.subtle.digest('SHA-256',\n                 new TextEncoder().encode(txt));\n  return Array.from(new Uint8Array(buf))\n              .map(b => b.toString(16).padStart(2, '0')).join('');\n}\n\nasync function unlockOne(btn) {\n  const pass = prompt(\"Password:\");\n  if (!pass) return;\n  if (await sha256(pass) !== correctHash) {\n    alert(\"Wrong password\"); return;\n  }\n\n  /* --- decode only the solution that belongs to THIS button --- */\n  const wrapper = btn.parentElement;             // .b64-wrapper\n  wrapper.querySelectorAll('.hidden-solution').forEach(div => {\n    div.innerHTML = b64DecodeUtf8(div.dataset.encoded);\n    div.classList.remove('hidden-solution');\n    div.style.display = 'block';\n  });\n\n  /* Remove the button so the user can’t click it again */\n  btn.remove();\n}\n</script>\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n    <strong>JavaScript is required to unlock solutions.</strong><br>\n    Please enable JavaScript and reload the page,<br>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n```\n\n\n\n\n\n# Goal\n\nOur goal for this exercise sheet is to learn the basics of imputation within the `mlr3` universe, specifically, `mlr3pipelines`.\nImputation is the process of filling in missing data in a data set using statistical methods like mean, median, mode, or predictive models.\n\n# Required packages\n\nWe will use `mlr3` for machine learning and `mlr3oml` for data access from OpenML:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3oml)\nset.seed(12345)\n```\n:::\n\n\n\n# Data: Miami house prices\n\nWe will use house price data on 13,932 single-family homes sold in Miami in 2016. The target variable is `\"SALE_PRC\"`.\n\nLet's load the data and remove an unwanted column: \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmiami = as.data.frame(odt(id = 43093)$data[,-c(3)])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [15:53:06.868] Retrieving JSON {url: `https://www.openml.org/api/v1/json/data/43093`, authenticated: `FALSE`}\nINFO  [15:53:07.310] Retrieving ARFF {url: `https://api.openml.org/data/v1/download/22047757/MiamiHousing2016.arff`, authenticated: `FALSE`}\nINFO  [15:53:07.793] Retrieving JSON {url: `https://www.openml.org/api/v1/json/data/features/43093`, authenticated: `FALSE`}\n```\n\n\n:::\n\n```{.r .cell-code}\nmiami[1:16] = lapply(miami[1:16], as.numeric)\nmiami[,c(14,16)] = lapply(miami[,c(14,16)], as.factor)\n```\n:::\n\n\n\nFurther, we artificially generate missing data entries for three features:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nindices = which(miami$age > 50)\n\nfor (i in c(\"OCEAN_DIST\", \"TOT_LVG_AREA\", \"structure_quality\")) {\n  sample_indices <- sample(indices, 2000, replace = FALSE)\n  miami[sample_indices, i] <- NA\n}\n```\n:::\n\n\n\n# 1 Create simple imputation PipeOps\n\nImputation can be executed via standard pipeline workflows using `PipeOp` objects. You can get an overview of the relevant options with `?PipeOpImpute`, which is the abstract base class for feature imputation. Create a `PipeOp` that imputes numerical features based on randomly sampling feature values from the non-missing values and another `PipeOp` that imputes factor/categorical (including ordinal) features by out of range imputation. The latter introduces a new level “.MISSING” for missings.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can set up a `PipeOp` with the `po()` function and use the `affect_columns` argument to address the columns to which the preprocessing should be applied (see also `?PipeOpImpute` for how to use the `affect_columns` argument).\nThere exists a shortcut for setting up the imputation based on randomly sampling feature values from the non-missing values which is `imputesample` (see also `?PipeOpImputeSample`) and for out of range imputation which is `imputeoor` (see also `?PipeOpImputeOOR`).\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimpute_numeric = po(\"...\", affect_columns = selector_type(\"...\"))\nimpute_factor = po(\"...\", affect_columns = ...(c(\"factor\", \"ordered\")))\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimpute_numeric = po(\"imputesample\", affect_columns = selector_type(\"numeric\"))\nimpute_factor = po(\"imputeoor\", affect_columns = selector_type(c(\"factor\", \"ordered\")))\n```\n:::\n\n\n\n:::\n\n:::\n\n# 2 Create and plot a graph\n\nCombine both imputation `PipeOps` with a random forest learning algorithm into a `Graph`. Then, plot the graph.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nCreate a random forest learner using `lrn()`.\nYou can concatenate different pre-processing steps and a learner using the `%>>%` operator.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nYou can plot a graph using the corresponding R6 method of the graph object.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = impute_numeric %>>%\n  impute_factor %>>%\n  lrn(\"regr.ranger\")\n\ngraph$plot()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n:::\n\n:::\n\n## Simple Imputation\n\nAlternative to a pipeline that includes a learner, we can even set up a simpler pipeline that only creates imputations for missing data and apply it to a data set. For this, define first a simpler pipeline with only the imputation steps from above, create a task for the `miami` data, use the `$train()` method to impute the missing rows. Then, inspect the imputed data set with `...[[1]]$head()`.\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_im = impute_numeric %>>% impute_factor\n\ntask = TaskRegr$new(id = \"miami\", backend = miami, target = \"SALE_PRC\")\n\nabs = graph_im$train(task)[[1]]$head()\n```\n:::\n\n\n\n:::\n\n:::\n\n## Assessing Performance\n\nUse 3-fold cross-validation to estimate the error of the first pipeline (the one that contains a random forest learner) stored in the graph.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nSpecifically, you need three things:\n\n1. A `Resampling` object using `rsmp()` and instantiate the train-test splits on the task.\n2. Use this object together with the task and the graph learner specified above as an input to the `resample()` method.\n3. Measure the performance with `$aggregate()`.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling = rsmp(\"cv\", ...)\nresampling$instantiate(...)\nrr = resample(task = ..., learner = ..., resampling = ...)\nrr$...()\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\" messages='false'}\n\n```{.r .cell-code}\nresampling = rsmp(\"cv\", folds = 3L)\nresampling$instantiate(task)\nrr = resample(task = task, learner = graph, resampling = resampling)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [15:53:08.275] [mlr3] Applying learner 'imputesample.imputeoor.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:53:08.422] [mlr3] Applying learner 'imputesample.imputeoor.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:53:08.572] [mlr3] Applying learner 'imputesample.imputeoor.regr.ranger' on task 'miami' (iter 3/3)\n```\n\n\n:::\n\n```{.r .cell-code}\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   regr.mse \n10655793452 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n# 3 Model-based imputation\n\nWe can use a learner to impute missing values, which works by learning a model that treats the feature to-be-imputed as target and the other features as covariates. This has to be done separately for each feature that we impute. Obviously, the performance of learner-based imputation can depend on the type of learner used. Set up two distinct pipelines, modifying the pipeline from the previous exercise. Now, for numeric features, use learner-based imputation, using a linear model for the first and a decision tree for the second pipeline.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can learn about the mechanics of using learners for imputation in `?mlr_pipeops_imputelearner`.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nAs the documentation states, if a learner used for imputation is itself supposed to train on features containing missing data, it needs to be able handle missing data natively. Otherwise, it needs its own imputation, requiring a more complicated pipeline. In this case, use histogram-based imputation within the learner-based imputation. Similarly, if categorical features are to be imputed, they need to be imputed before the numeric features in this case.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimpute_rpart = po(\"imputelearner\",\n                  learner = lrn(\"regr.rpart\"),\n                  affect_columns = selector_type(\"numeric\"))\n\nimpute_lm = po(\"imputelearner\",\n                  learner = po(\"imputehist\") %>>% lrn(\"regr.lm\"),\n                  affect_columns = selector_type(\"numeric\"))\n\ngraph_rpart = impute_factor %>>%\n  impute_rpart %>>%\n  lrn(\"regr.ranger\")\n\ngraph_lm = impute_factor %>>%\n  impute_lm %>>%\n  lrn(\"regr.ranger\")\n```\n:::\n\n\n\n:::\n\n:::\n\n## Assessing Performance\n\nAs before, use 3-fold cross-validation to compare the error of the two pipelines to identify which learner seems to work best for imputation for this data set.\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\" messages='false'}\n\n```{.r .cell-code}\nrr_rpart = resample(task = task, learner = graph_rpart, resampling = resampling)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [15:53:15.577] [mlr3] Applying learner 'imputeoor.imputelearner.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:53:15.613] [mlr3] Applying learner 'imputeoor.imputelearner.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:53:15.647] [mlr3] Applying learner 'imputeoor.imputelearner.regr.ranger' on task 'miami' (iter 3/3)\n```\n\n\n:::\n\n```{.r .cell-code}\nrr_lm = resample(task = task, learner = graph_lm, resampling = resampling)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [15:53:21.416] [mlr3] Applying learner 'imputeoor.imputelearner.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:53:21.446] [mlr3] Applying learner 'imputeoor.imputelearner.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:53:21.484] [mlr3] Applying learner 'imputeoor.imputelearner.regr.ranger' on task 'miami' (iter 3/3)\n```\n\n\n:::\n\n```{.r .cell-code}\nrr_rpart$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  regr.mse \n9710923238 \n```\n\n\n:::\n\n```{.r .cell-code}\nrr_lm$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  regr.mse \n9478850155 \n```\n\n\n:::\n:::\n\n\n\nIn this case, using a linear model for model-based imputation seems to outperform a decision tree with default hyperparameter settings.\n\n:::\n\n:::\n\n# 3 Branches in pipelines\n\nPipelines can become very complex. Within a pipeline, we could be interested which imputation method works best. An elegant way to find out is to treat the imputation method as just another hyperparameter that we tune alongside other hyperparameters when we tune the pipeline. A way to do this is by using path branching. Set up a graph that contains the following elements:\n1. A branch with two different imputation methods, a) histogram-based and b) learner-based using a decision tree\n2. A random forest fit on the (fully imputed) data.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nYou can read more about branching in `??mlr_pipeops_branch`.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\" messages='false'}\n\n```{.r .cell-code}\nimpute = list(\n  \"imputehist\" = po(\"imputehist\"),\n  \"imputerpart\" = po(\"imputelearner\", learner = lrn(\"regr.rpart\"))\n)\nforest = lrn(\"regr.ranger\")\ngraph_branch = ppl(\"branch\", impute) %>>% forest\nplot(graph_branch)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n:::\n\n:::\n\n## Define a search space\n\nWe want to tune a number of hyperparameters in the pipeline:\n1) The `mtry` parameter in the random forest between 2 and 8,\n2) The imputation method, as represented by our graph, and\n3) the `maxdepth` parameter of the decision tree-based imputation between 1 and 30.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nRemember that a graph can be treated as any other learner, and therefore, its parameter set can be accessed correspondingly. This means you can find the relevant parameter names in the correct field of the graph object.\n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nA parameter space can be defined using the `ps()` sugar function.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\" messages='false'}\n\n```{.r .cell-code}\ntune_ps = ps(\n  regr.ranger.mtry = p_int(2L, 8L),\n  branch.selection = p_fct(c(\"imputehist\", \"imputerpart\")),\n  imputelearner.maxdepth = p_int(1L, 30L)\n)\n```\n:::\n\n\n\n:::\n\n:::\n\n## Tuning the pipeline\n\nNow, tune the pipeline using an AutoTuner with 3-fold CV and random search. You can terminate after 10 evaluations to reduce run time. Then, display the optimal hyperparameter set as chosen by the tuner based on the mean squared error.\n\n<details>\n<summary>**Hint 1:**</summary>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# AutoTuner\nglrn_tuned = AutoTuner$new(...)\n# Train\n...\n# Optimal HP set\n...\n```\n:::\n\n\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n\n::: {.cell layout-align=\"center\" messages='false'}\n\n```{.r .cell-code}\n# AutoTuner\nglrn_tuned = AutoTuner$new(graph_branch,\n  resampling = rsmp(\"cv\", folds = 3L),\n  search_space = tune_ps,\n  measure = msr(\"regr.mse\"),\n  terminator = trm(\"evals\", n_evals = 10),\n  tuner = tnr(\"random_search\"))\n\n# Train\nglrn_tuned$train(task)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [15:53:27.787] [bbotk] Starting to optimize 3 parameter(s) with '<OptimizerBatchRandomSearch>' and '<TerminatorEvals> [n_evals=10, k=0]'\nINFO  [15:53:27.803] [bbotk] Evaluating 1 configuration(s)\nINFO  [15:53:27.806] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [15:53:27.841] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:53:27.875] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:53:27.908] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 3/3)\nINFO  [15:53:40.046] [mlr3] Finished benchmark\nINFO  [15:53:40.061] [bbotk] Result of batch 1:\nINFO  [15:53:40.063] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth    regr.mse warnings errors runtime_learners\nINFO  [15:53:40.063] [bbotk]                 7      imputerpart                     27 10493901545        0      0           36.289\nINFO  [15:53:40.063] [bbotk]                                 uhash\nINFO  [15:53:40.063] [bbotk]  ea2b4b72-0327-4d62-b542-fd0a9bf9e780\nINFO  [15:53:40.066] [bbotk] Evaluating 1 configuration(s)\nINFO  [15:53:40.069] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [15:53:40.102] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:53:40.134] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:53:40.168] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 3/3)\nINFO  [15:53:48.770] [mlr3] Finished benchmark\nINFO  [15:53:48.785] [bbotk] Result of batch 2:\nINFO  [15:53:48.786] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth    regr.mse warnings errors runtime_learners\nINFO  [15:53:48.786] [bbotk]                 5      imputerpart                     27 10332301777        0      0           25.815\nINFO  [15:53:48.786] [bbotk]                                 uhash\nINFO  [15:53:48.786] [bbotk]  8671e1d5-5ba4-48e9-a890-731d77b8b8e3\nINFO  [15:53:48.789] [bbotk] Evaluating 1 configuration(s)\nINFO  [15:53:48.791] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [15:53:48.825] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:53:48.858] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:53:48.893] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 3/3)\nINFO  [15:53:58.782] [mlr3] Finished benchmark\nINFO  [15:53:58.796] [bbotk] Result of batch 3:\nINFO  [15:53:58.797] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth    regr.mse warnings errors runtime_learners\nINFO  [15:53:58.797] [bbotk]                 6      imputerpart                      2 10603568863        0      0           29.616\nINFO  [15:53:58.797] [bbotk]                                 uhash\nINFO  [15:53:58.797] [bbotk]  e66c457b-5a6a-49e4-a1ad-c16a30f59929\nINFO  [15:53:58.800] [bbotk] Evaluating 1 configuration(s)\nINFO  [15:53:58.802] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [15:53:58.837] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:53:58.870] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:53:58.904] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 3/3)\nINFO  [15:54:07.547] [mlr3] Finished benchmark\nINFO  [15:54:07.561] [bbotk] Result of batch 4:\nINFO  [15:54:07.562] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth    regr.mse warnings errors runtime_learners\nINFO  [15:54:07.562] [bbotk]                 5      imputerpart                     16 10286216752        0      0            25.88\nINFO  [15:54:07.562] [bbotk]                                 uhash\nINFO  [15:54:07.562] [bbotk]  32364421-56ce-459d-b34a-4875254ca0f9\nINFO  [15:54:07.566] [bbotk] Evaluating 1 configuration(s)\nINFO  [15:54:07.568] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [15:54:07.603] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:54:07.636] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:54:07.670] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 3/3)\nINFO  [15:54:13.374] [mlr3] Finished benchmark\nINFO  [15:54:13.388] [bbotk] Result of batch 5:\nINFO  [15:54:13.389] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth    regr.mse warnings errors runtime_learners\nINFO  [15:54:13.389] [bbotk]                 3       imputehist                      4 11327934723        0      0           17.066\nINFO  [15:54:13.389] [bbotk]                                 uhash\nINFO  [15:54:13.389] [bbotk]  b48a38b4-2ce3-45d1-8507-ba2d80cec6a3\nINFO  [15:54:13.393] [bbotk] Evaluating 1 configuration(s)\nINFO  [15:54:13.395] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [15:54:13.430] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:54:13.464] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:54:13.499] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 3/3)\nINFO  [15:54:18.009] [mlr3] Finished benchmark\nINFO  [15:54:18.021] [bbotk] Result of batch 6:\nINFO  [15:54:18.021] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth    regr.mse warnings errors runtime_learners\nINFO  [15:54:18.021] [bbotk]                 2      imputerpart                      4 10752823555        0      0           13.456\nINFO  [15:54:18.021] [bbotk]                                 uhash\nINFO  [15:54:18.021] [bbotk]  91f54d65-3740-409a-b687-c003c56cd36c\nINFO  [15:54:18.025] [bbotk] Evaluating 1 configuration(s)\nINFO  [15:54:18.027] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [15:54:18.062] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:54:18.095] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:54:18.128] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 3/3)\nINFO  [15:54:29.191] [mlr3] Finished benchmark\nINFO  [15:54:29.203] [bbotk] Result of batch 7:\nINFO  [15:54:29.204] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth    regr.mse warnings errors runtime_learners\nINFO  [15:54:29.204] [bbotk]                 7       imputehist                     20 11177979437        0      0           33.165\nINFO  [15:54:29.204] [bbotk]                                 uhash\nINFO  [15:54:29.204] [bbotk]  11490810-1bae-4b22-bb8b-f9374292dc89\nINFO  [15:54:29.207] [bbotk] Evaluating 1 configuration(s)\nINFO  [15:54:29.210] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [15:54:29.245] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:54:29.279] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:54:29.316] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 3/3)\nINFO  [15:54:37.791] [mlr3] Finished benchmark\nINFO  [15:54:37.803] [bbotk] Result of batch 8:\nINFO  [15:54:37.804] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth    regr.mse warnings errors runtime_learners\nINFO  [15:54:37.804] [bbotk]                 5      imputerpart                     15 10343847353        0      0           25.366\nINFO  [15:54:37.804] [bbotk]                                 uhash\nINFO  [15:54:37.804] [bbotk]  3d387cb7-b953-490f-bcdb-3d2f5954db7e\nINFO  [15:54:37.807] [bbotk] Evaluating 1 configuration(s)\nINFO  [15:54:37.809] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [15:54:37.844] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:54:37.877] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:54:37.910] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 3/3)\nINFO  [15:54:43.518] [mlr3] Finished benchmark\nINFO  [15:54:43.530] [bbotk] Result of batch 9:\nINFO  [15:54:43.531] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth    regr.mse warnings errors runtime_learners\nINFO  [15:54:43.531] [bbotk]                 3       imputehist                     22 11232632300        0      0           16.761\nINFO  [15:54:43.531] [bbotk]                                 uhash\nINFO  [15:54:43.531] [bbotk]  68068a2a-af8b-4b5f-9b05-f16b65eeef40\nINFO  [15:54:43.535] [bbotk] Evaluating 1 configuration(s)\nINFO  [15:54:43.537] [mlr3] Running benchmark with 3 resampling iterations\nINFO  [15:54:43.575] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 1/3)\nINFO  [15:54:43.608] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 2/3)\nINFO  [15:54:43.641] [mlr3] Applying learner 'branch.imputehist.imputelearner.unbranch.regr.ranger' on task 'miami' (iter 3/3)\nINFO  [15:54:49.264] [mlr3] Finished benchmark\nINFO  [15:54:49.275] [bbotk] Result of batch 10:\nINFO  [15:54:49.276] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth    regr.mse warnings errors runtime_learners\nINFO  [15:54:49.276] [bbotk]                 3       imputehist                      6 11266323831        0      0           16.851\nINFO  [15:54:49.276] [bbotk]                                 uhash\nINFO  [15:54:49.276] [bbotk]  7700649c-758f-43ab-b4ea-e17fa259919e\nINFO  [15:54:49.283] [bbotk] Finished optimizing after 10 evaluation(s)\nINFO  [15:54:49.284] [bbotk] Result:\nINFO  [15:54:49.284] [bbotk]  regr.ranger.mtry branch.selection imputelearner.maxdepth learner_param_vals  x_domain    regr.mse\nINFO  [15:54:49.284] [bbotk]             <int>           <char>                  <int>             <list>    <list>       <num>\nINFO  [15:54:49.284] [bbotk]                 5      imputerpart                     16          <list[5]> <list[3]> 10286216752\n```\n\n\n:::\n\n```{.r .cell-code}\n# Optimal HP set\nglrn_tuned$tuning_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   regr.ranger.mtry branch.selection imputelearner.maxdepth learner_param_vals  x_domain    regr.mse\n              <int>           <char>                  <int>             <list>    <list>       <num>\n1:                5      imputerpart                     16          <list[5]> <list[3]> 10286216752\n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}