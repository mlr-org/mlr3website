{
  "hash": "5291f6ed268d92573777db6486865d22",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Filters \ncategories:\n  - advanced feature preprossesing\n  - pipelines\nauthor:\n  - name: Giuseppe Casalicchio\n  - name: Essential Data Science Training GmbH\n    url: https://www.essentialds.de\ndescription: |\n  Use pipelines for efficient pre-processing and model training on a the kc_housing task.\ndate: \"\"\nparams:\n  showsolution: true\n  base64encode: true\nlisting: false\nsearch: false\nformat:\n  html:\n    filters:\n      - ../../b64_solution.lua\n---\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n```{=html}\n<script>\nconst correctHash = \"546ebb8eb993ea561029d9febd84c363bdb09010bb2cb915a8287762b76b9a64\";   // value injected by knitr\n\n/* ---------- reusable helper ---------- */\nfunction b64DecodeUtf8(b64) {\n  // 1) atob  -> binary-string   (bytes 0…255)\n  // 2) map   -> Uint8Array      (array of bytes)\n  // 3) TextDecoder('utf-8')     -> real JS string\n  const bytes = Uint8Array.from(atob(b64), c => c.charCodeAt(0));\n  return new TextDecoder('utf-8').decode(bytes);\n}\n\nasync function sha256(txt) {\n  const buf = await crypto.subtle.digest('SHA-256',\n                 new TextEncoder().encode(txt));\n  return Array.from(new Uint8Array(buf))\n              .map(b => b.toString(16).padStart(2, '0')).join('');\n}\n\nasync function unlockOne(btn) {\n  const pass = prompt(\"Password:\");\n  if (!pass) return;\n  if (await sha256(pass) !== correctHash) {\n    alert(\"Wrong password\"); return;\n  }\n\n  /* --- decode only the solution that belongs to THIS button --- */\n  const wrapper = btn.parentElement;             // .b64-wrapper\n  wrapper.querySelectorAll('.hidden-solution').forEach(div => {\n    div.innerHTML = b64DecodeUtf8(div.dataset.encoded);\n    div.classList.remove('hidden-solution');\n    div.style.display = 'block';\n  });\n\n  /* Remove the button so the user can’t click it again */\n  btn.remove();\n}\n</script>\n\n<noscript>\n<div style=\"border: 1px solid #ccc; padding: 1em; margin-top: 1em; background: #f9f9f9;\">\n    <strong>JavaScript is required to unlock solutions.</strong><br>\n    Please enable JavaScript and reload the page,<br>\n    or download the source files from\n    <a href=\"https://github.com/mlr-org/mlr3website/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub</a>\n    and run the code locally.\n  </div>\n</noscript>\n```\n\n\n\n\n# Goal\n\nApply what you have learned about using pipelines for efficient pre-processing and model training on a regression problem.\n\n# House Prices in King county\n\nIn this exercise, we want to model house sale prices in King county in the state of Washington, USA. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(124)\nlibrary(mlr3verse)\nlibrary(mlr3tuningspaces)\ndata(\"kc_housing\", package = \"mlr3data\")\n```\n:::\n\n\nWe do some simple feature pre-processing first:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Transform time to numeric variable:\nlibrary(anytime)\ndates = anytime(kc_housing$date)\nkc_housing$date = as.numeric(difftime(dates, min(dates), units = \"days\"))\n# Scale prices:\nkc_housing$price = kc_housing$price / 1000\n# For this task, delete columns containing NAs:\nyr_renovated = kc_housing$yr_renovated\nsqft_basement = kc_housing$sqft_basement\nkc_housing[,c(13, 15)] = NULL\n# Create factor columns:\nkc_housing[,c(8, 14)] = lapply(c(8, 14), function(x) {as.factor(kc_housing[,x])})\n# Get an overview:\nstr(kc_housing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t21613 obs. of  18 variables:\n $ date         : num  164 221 299 221 292 ...\n $ price        : num  222 538 180 604 510 ...\n $ bedrooms     : int  3 3 2 4 3 4 3 3 3 3 ...\n $ bathrooms    : num  1 2.25 1 3 2 4.5 2.25 1.5 1 2.5 ...\n $ sqft_living  : int  1180 2570 770 1960 1680 5420 1715 1060 1780 1890 ...\n $ sqft_lot     : int  5650 7242 10000 5000 8080 101930 6819 9711 7470 6560 ...\n $ floors       : num  1 2 1 1 1 1 2 1 1 2 ...\n $ waterfront   : Factor w/ 2 levels \"FALSE\",\"TRUE\": 1 1 1 1 1 1 1 1 1 1 ...\n $ view         : int  0 0 0 0 0 0 0 0 0 0 ...\n $ condition    : int  3 3 3 5 3 3 3 3 3 3 ...\n $ grade        : int  7 7 6 7 8 11 7 7 7 7 ...\n $ sqft_above   : int  1180 2170 770 1050 1680 3890 1715 1060 1050 1890 ...\n $ yr_built     : int  1955 1951 1933 1965 1987 2001 1995 1963 1960 2003 ...\n $ zipcode      : Factor w/ 70 levels \"98001\",\"98002\",..: 67 56 17 59 38 30 3 69 61 24 ...\n $ lat          : num  47.5 47.7 47.7 47.5 47.6 ...\n $ long         : num  -122 -122 -122 -122 -122 ...\n $ sqft_living15: int  1340 1690 2720 1360 1800 4760 2238 1650 1780 2390 ...\n $ sqft_lot15   : int  5650 7639 8062 5000 7503 101930 6819 9711 8113 7570 ...\n - attr(*, \"index\")= int(0) \n```\n\n\n:::\n:::\n\n\n# Add uncorrelated features to data\n\nTo test different strategies for feature selection in this exercise, we create two artificial features that are (mostly) uncorrelated with the outcome `price`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Uncorrelated feature x1:\nkc_housing$x1 <- runif(n = nrow(kc_housing))\ncor(kc_housing$x1, kc_housing$price)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002249436\n```\n\n\n:::\n\n```{.r .cell-code}\n# Uncorrelated feature x2:\nkc_housing$x2 <- sin(0.01*kc_housing$price*kc_housing$grade)\ncor(kc_housing$x2, kc_housing$price)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01329962\n```\n\n\n:::\n:::\n\n\n# Train-test Split\n\nBefore we train a model, let's reserve some data for evaluating our model later on:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_regr(kc_housing, target = \"price\")\nsplit = partition(task, ratio = 0.6)\n\ntasktrain = task$clone()\ntasktrain$filter(split$train)\n\ntasktest = task$clone()\ntasktest$filter(split$test)\n```\n:::\n\n\n# Conditional Encoding\n\nIn the King county data, there are two categorial features encoded as `factor`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlengths(task$levels())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nwaterfront    zipcode \n         2         70 \n```\n\n\n:::\n:::\n\n\nObviously, `waterfront` is a low-cardinality feature suitable for one-hot encoding and `zipcode` is a very high-cardinality feature. Therefore, it would make sense to create a pipeline that first pre-processes each factor variable with either impact or one-hot encoding, depending on the feature cardinality.\n\n# Filters\n\nFilter algorithms select features by assigning numeric scores to each feature, e.g. correlation between features and target variable, use these to rank the features and select a feature subset based on the ranking. Features that are assigned lower scores are then omitted in subsequent modeling steps. All filters are implemented via the package `mlr3filters`. A very simple filter approach could look like this:\n\n1. Calculate the correlation coefficient between each feature and a numeric target variable\n2. Select the 10 features with the highest correlation for further modeling steps.\n\nA different strategy could entail selecting only features above a certain threshold of correlation with the outcome. For a full list of all implemented filter methods, take a look at https://mlr3filters.mlr-org.com.\n\n# Exercises\n\n## Exercise 1: Create a complex pipeline\n\nCreate a pipeline with the following sequence of elements:\n\n1. Each factor variable gets pre-processed with either one-hot or impact encoding, depending on the cardinality of the feature.\n2. A filter selector is applied to the features, sorting them by their Pearson correlation coefficient and selecting the 3 features with the highest correlation.\n3. A random forest (`regr.ranger`) is trained.\n\nThe pipeline should be tuned within an `autotuner` with random search, two-fold CV and MSE as performance measure, and a search space from `mlr3tuningspaces` but without tuning the hyperparameter `replace`. Train the `autotuner` on the training data, and evaluate the performance on the holdout test data.\n\n<details>\n<summary>**Hint 1:**</summary>\n\nCheck out the help page of `lts` from `mlr3tuningspaces`. \n\n</details>\n\n<details>\n<summary>**Hint 2:**</summary>\n\nSince we want to work with the search space right away, it’s recommended to insert the `Learner` directly.\nEnsure that the learner uses the default value for the `replace` hyperparameter.\n\n</details>\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create rf learner \nrf = lrn(\"regr.ranger\", replace = TRUE)\n\n# Set search space from mlr3tuningspaces\nrf_ts = lts(rf, replace = NULL)\nas.data.table(rf_ts$param_set$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         mtry.ratio num.threads        num.trees replace  sample.fraction\n   <RangeTuneToken>       <int> <RangeTuneToken>  <lgcl> <RangeTuneToken>\n1:        <list[3]>           1        <list[3]>    TRUE        <list[3]>\n2:    to_tune(0, 1)           1 to_tune(1, 2000)    TRUE  to_tune(0.1, 1)\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create \"correlation\" filter pipeline element\ncor_filter = po(\"filter\", filter = flt(\"correlation\"))\nas.data.table(cor_filter$param_set)$id\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"filter.nfeat\"    \"filter.frac\"     \"filter.cutoff\"   \"filter.permuted\" \"use\"             \"method\"         \n[7] \"affect_columns\" \n```\n\n\n:::\n\n```{.r .cell-code}\n# Use the 3 features with the highest filter value\ncor_filter$param_set$values$filter.nfeat = 3\n\n# Create conditional encoding pipeline element\nfactor_po = po(\"encode\", method = \"one-hot\",\n               affect_columns = selector_invert(selector_cardinality_greater_than(10)),\n               id = \"low_card_enc\") %>>%\n            po(\"encodeimpact\",\n               affect_columns = selector_cardinality_greater_than(10),\n               id = \"high_card_enc\") \n\n\n# Combine rf_ts with impact encoding and the filter\nrf_ts_cor = as_learner(factor_po %>>% cor_filter %>>% rf_ts)\n\n# Use random search\ntuner = tnr(\"random_search\")\n\n# Create autotuner\nauto_rf_cor = auto_tuner(\n  tuner = tuner,\n  learner = rf_ts_cor,\n  resampling = rsmp(\"cv\", folds = 2),\n  measure = msr(\"regr.mse\"),\n  term_evals = 20\n)\n\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\nfuture::plan(\"multisession\")\nauto_rf_cor$train(tasktrain)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [16:03:41.795] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:03:42.726] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:03:43.629] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:03:55.048] [mlr3] Finished benchmark\nINFO  [16:03:55.126] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:03:55.486] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:03:55.725] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:04:00.266] [mlr3] Finished benchmark\nINFO  [16:04:00.373] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:04:00.628] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:04:00.876] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:04:01.389] [mlr3] Finished benchmark\nINFO  [16:04:01.458] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:04:01.700] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:04:01.965] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:04:03.974] [mlr3] Finished benchmark\nINFO  [16:04:04.050] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:04:04.539] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:04:04.763] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:04:13.297] [mlr3] Finished benchmark\nINFO  [16:04:13.402] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:04:13.775] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:04:14.022] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:04:32.884] [mlr3] Finished benchmark\nINFO  [16:04:32.962] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:04:33.208] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:04:33.475] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:04:38.181] [mlr3] Finished benchmark\nINFO  [16:04:38.252] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:04:38.466] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:04:38.676] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:04:42.765] [mlr3] Finished benchmark\nINFO  [16:04:42.843] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:04:43.083] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:04:43.333] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:04:50.329] [mlr3] Finished benchmark\nINFO  [16:04:50.461] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:04:50.759] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:04:51.154] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:05:02.481] [mlr3] Finished benchmark\nINFO  [16:05:02.553] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:05:03.383] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:05:03.595] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:05:04.861] [mlr3] Finished benchmark\nINFO  [16:05:04.925] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:05:05.130] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:05:05.402] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:05:20.608] [mlr3] Finished benchmark\nINFO  [16:05:20.680] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:05:21.208] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:05:21.448] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:05:24.001] [mlr3] Finished benchmark\nINFO  [16:05:24.078] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:05:24.313] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:05:24.573] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:05:28.023] [mlr3] Finished benchmark\nINFO  [16:05:28.106] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:05:28.332] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:05:28.574] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:05:37.345] [mlr3] Finished benchmark\nINFO  [16:05:37.416] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:05:37.647] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:05:37.881] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:05:45.851] [mlr3] Finished benchmark\nINFO  [16:05:45.920] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:05:46.131] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:05:46.367] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:05:51.435] [mlr3] Finished benchmark\nINFO  [16:05:51.515] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:05:51.742] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:05:51.964] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:05:56.468] [mlr3] Finished benchmark\nINFO  [16:05:56.544] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:05:56.767] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:05:57.084] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:05:59.785] [mlr3] Finished benchmark\nINFO  [16:05:59.878] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:06:00.103] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:06:00.341] [mlr3] Applying learner 'low_card_enc.high_card_enc.correlation.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:06:05.181] [mlr3] Finished benchmark\n```\n\n\n:::\n\n```{.r .cell-code}\nscore_rf_cor = auto_rf_cor$predict(tasktest)$score()\nscore_rf_cor\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n24229.17 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## Exercise 2: Information gain\n\nAn alternative filter method is information gain (https://mlr3filters.mlr-org.com/reference/mlr_filters_information_gain.html). Recreate the pipeline from exercise 1, but use information gain as filter. Again, select the three features with the highest information gain. Train the `autotuner` on the training data, and evaluate the performance on the holdout test data.\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Create \"information_gain\" filter pipeline element\ninfo_filter = po(\"filter\", filter = flt(\"information_gain\"))\n# Use the 8 features with the highest filter value\ninfo_filter$param_set$values$filter.nfeat = 3\n\n# Combine the pipeline elements\nrf_ts_info = as_learner(factor_po %>>% info_filter %>>% rf_ts)\n\n# Create autotuner\nauto_rf_info = auto_tuner(\n  tuner = tuner,\n  learner = rf_ts_info,\n  resampling = rsmp(\"cv\", folds = 2),\n  measure = msr(\"regr.mse\"),\n  term_evals = 20\n)\n\nauto_rf_info$train(tasktrain)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nINFO  [16:06:12.772] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:06:12.982] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:06:13.197] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:06:15.354] [mlr3] Finished benchmark\nINFO  [16:06:15.672] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:06:15.865] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:06:16.083] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:06:23.281] [mlr3] Finished benchmark\nINFO  [16:06:23.344] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:06:23.541] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:06:23.750] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:06:30.419] [mlr3] Finished benchmark\nINFO  [16:06:30.488] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:06:30.702] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:06:30.946] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:06:40.636] [mlr3] Finished benchmark\nINFO  [16:06:40.708] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:06:40.937] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:06:41.193] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:06:49.497] [mlr3] Finished benchmark\nINFO  [16:06:49.576] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:06:50.130] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:06:50.399] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:07:13.301] [mlr3] Finished benchmark\nINFO  [16:07:13.375] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:07:13.585] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:07:13.828] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:07:19.280] [mlr3] Finished benchmark\nINFO  [16:07:19.361] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:07:19.599] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:07:19.855] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:07:22.046] [mlr3] Finished benchmark\nINFO  [16:07:22.128] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:07:22.719] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:07:23.300] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:07:29.962] [mlr3] Finished benchmark\nINFO  [16:07:30.034] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:07:30.289] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:07:30.599] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:07:39.966] [mlr3] Finished benchmark\nINFO  [16:07:40.050] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:07:40.246] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:07:40.479] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:07:46.701] [mlr3] Finished benchmark\nINFO  [16:07:46.772] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:07:46.990] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:07:47.225] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:08:11.263] [mlr3] Finished benchmark\nINFO  [16:08:11.331] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:08:11.555] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:08:11.810] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:08:30.719] [mlr3] Finished benchmark\nINFO  [16:08:30.795] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:08:31.007] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:08:31.253] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:08:36.806] [mlr3] Finished benchmark\nINFO  [16:08:36.885] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:08:37.439] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:08:37.679] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:08:44.593] [mlr3] Finished benchmark\nINFO  [16:08:44.685] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:08:44.909] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:08:45.159] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:08:51.800] [mlr3] Finished benchmark\nINFO  [16:08:51.871] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:08:52.104] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:08:52.365] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:09:02.208] [mlr3] Finished benchmark\nINFO  [16:09:02.289] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:09:02.498] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:09:02.747] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:09:22.068] [mlr3] Finished benchmark\nINFO  [16:09:22.137] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:09:22.432] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:09:22.860] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:09:36.363] [mlr3] Finished benchmark\nINFO  [16:09:36.431] [mlr3] Running benchmark with 2 resampling iterations\nINFO  [16:09:36.669] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 1/2)\nINFO  [16:09:36.924] [mlr3] Applying learner 'low_card_enc.high_card_enc.information_gain.regr.ranger' on task 'kc_housing' (iter 2/2)\nINFO  [16:09:39.331] [mlr3] Finished benchmark\n```\n\n\n:::\n\n```{.r .cell-code}\nscore_rf_info = auto_rf_info$predict(tasktest)$score()\nscore_rf_info\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n30589.75 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n## Exercise 3: Pearson correlation vs. Information gain\n\nWe receive the following performance scores for the two filter methods: \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscore_rf_cor\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n24229.17 \n```\n\n\n:::\n\n```{.r .cell-code}\nscore_rf_info\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n30589.75 \n```\n\n\n:::\n:::\n\n\nAs you can see, the Pearson correlation filter seems to select features that result in a better model. To investigate why that may have happened, inspect the trained autotuners. Which features have been selected? Given the selected features, reason to what extent which filter methods may be more helpful than others in determining features to select for the model training process.\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nauto_rf_cor$model$learner$model$correlation$features\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"zipcode\"     \"grade\"       \"sqft_living\"\n```\n\n\n:::\n\n```{.r .cell-code}\nauto_rf_info$model$learner$model$information_gain$features\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"zipcode\" \"grade\"   \"lat\"    \n```\n\n\n:::\n:::\n\n\nThe correlation filter does not select `x2`, a feature that we artificially created and is uncorrelated with the outcome `price`. However, the information gain filter selects for `x2`. This can be seen from how it was computed in the first place:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkc_housing$x2 <- sin(0.01*kc_housing$price*kc_housing$grade)\n```\n:::\n\n\nIndeed, it is a sin-transformed function of the `price` and `grade`, something that is not necessarily obvious from a simple visual inspection:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(kc_housing$x2, kc_housing$price)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n:::\n\n:::\n\n## Exercise 4: Imputation\n\nIn the three exercises before, we excluded two variables from the `kc_housing` data with missing values. Let's add them back to the data set and try to impute missing values automatically with a pipeline.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkc_housing$yr_renovated = yr_renovated\nkc_housing$sqft_basement = sqft_basement\ntask = as_task_regr(kc_housing, target = \"price\")\n# Check again which features in the task have NAs:\nnames(which(task$missings() > 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"sqft_basement\" \"yr_renovated\" \n```\n\n\n:::\n:::\n\n\nFurther, we use a simple train-test split as before:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsplit = partition(task, ratio = 0.6)\n\ntasktrain = task$clone()\ntasktrain$filter(split$train)\n\ntasktest = task$clone()\ntasktest$filter(split$test)\n```\n:::\n\n\nAs the two features with missing values are both numeric, we can compare two simple strategies for imputation: mean imputation and histogram imputation. While the former replaces `NAs` with the mean value of a feature, the latter samples from the empirical distribution of the non-`NA` values of the feature, ensuring that the marginal distribution is preserved. For each imputation method, construct a simple pipeline that trains a random forest without any HPO on the `tasktrain`, and evaluate the model on `tasktest` using MSE.\n\n:::{.callout-note collapse=\"true\"}\n\n### Solution\n\n:::{.b64-solution}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Mean imputation:\nimp_mean_ranger = as_learner(po(\"imputemean\") %>>% lrn(\"regr.ranger\"))\nimp_mean_ranger$train(tasktrain)\n\n# Histogram imputation:\nimp_hist_ranger = as_learner(po(\"imputehist\") %>>% lrn(\"regr.ranger\"))\nimp_hist_ranger$train(tasktrain)\n\n# Evaluate performance:\nimp_mean_ranger$predict(tasktest)$score()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n 18400.8 \n```\n\n\n:::\n\n```{.r .cell-code}\nimp_hist_ranger$predict(tasktest)$score()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nregr.mse \n18548.99 \n```\n\n\n:::\n:::\n\n\n\n:::\n\n:::\n\n# Summary\n\nWe learned about more complex pipelines, including pre-processing methods for imputation, variable encoding and feature filtering.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}