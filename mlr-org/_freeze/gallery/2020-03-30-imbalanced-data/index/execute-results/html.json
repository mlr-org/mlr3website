{
  "hash": "36b0f54e337b7b6996693217cea11575",
  "result": {
    "markdown": "---\ntitle: Imbalanced Data Handling with mlr3\ncategories:\n  - classification\n  - imbalanced data\n  - tuning\n  - resampling\nauthor:\n  - name: Giuseppe Casalicchio\ndate: 03-30-2020\ndescription: |\n  This use case compares different approaches to handle class imbalance for the optdigits binary classification data set.\nbibliography: bibliography.bib\nimage: thumbnail.png\n---\n\n\n\n\n\n\n# Intro\n\nThis use case compares different approaches to handle class imbalance for the\n[`Optical Recognition of Handwritten Digits`](https://mlr3data.mlr-org.com/reference/optdigits.html) (optdigits) binary classification data set using the [mlr3](https://mlr3.mlr-org.com) package.\nWe mainly focus on undersampling the majority class, oversampling the minority class, and the SMOTE imbalance correction [@smote] that enriches the minority class with synthetic data.\nThe use case requires prior knowledge in basic ML concepts (issues imbalanced data, hyperparameter tuning, nested cross-validation).\nThe R packages [mlr3](https://mlr3.mlr-org.com), [mlr3pipelines](https://mlr3pipelines.mlr-org.com) and [mlr3tuning](https://mlr3tuning.mlr-org.com) will be used.\nYou can find most of the content here also in the [mlr3book](https://mlr3book.mlr-org.com/) explained in a more detailed way.\n\nThese steps are performed:\n\n* Retrieve data sets from `OpenML`\n* Define imbalance correction pipeline [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html)s (undersampling, oversampling and SMOTE) with [mlr3pipelines](https://mlr3pipelines.mlr-org.com)\n* Autotune the [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) together with a learner using [mlr3tuning](https://mlr3tuning.mlr-org.com)\n* Benchmark the autotuned [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) and visualize the results using [mlr3viz](https://mlr3viz.mlr-org.com)\n\n# Prerequisites\n\nWe load the most important packages for this post.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: mlr3\n```\n:::\n\n```{.r .cell-code}\nlibrary(mlr3learners)\nlibrary(OpenML)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n```\n:::\n\n\n# Retrieve data sets from OpenML\n\n[OpenML.org](https://www.openml.org) is an open machine learning platform, which allows users to share data, code and machine learning experiments.\nThe OpenML R package can query available data sets using a filter-like approach by providing desired dataset characteristics like `number.of.classes` or `number.of.features`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# get list of curated binary classification data sets (see https://arxiv.org/abs/1708.03731v2)\nds = listOMLDataSets(\n  number.of.classes = 2,\n  number.of.features = c(1, 100),\n  number.of.instances = c(5000, 10000)\n)\n# select imbalanced data sets (without categorical features as SMOTE cannot handle them)\nds = subset(ds, minority.class.size / number.of.instances < 0.2 &\n  number.of.symbolic.features == 1)\nds\n\n# pick one data set from list above\nd = getOMLDataSet(980)\nd\n```\n:::\n\n\nAfter downloading the chosen data set, we create an [mlr3](https://mlr3.mlr-org.com) classification task:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# make sure target is a factor and create mlr3 tasks\ndata = as.data.frame(d)\ndata[[d$target.features]] = as.factor(data[[d$target.features]])\ntask = as_task_classif(data, id = d$desc$name, target = d$target.features)\ntask\n```\n:::\n\n\nPlease note that the optdigits dataset is also included in the [mlr3data](https://mlr3data.mlr-org.com)  package where you can get the preprocessed (integers properly encoded as such, etc.) data via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(\"optdigits\", package = \"mlr3data\")\ntask = as_task_classif(optdigits, target = \"binaryclass\", positive = \"P\")\n```\n:::\n\n\nQuick overview of the data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nskimr::skim(task$data())\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |            |\n|:------------------------|:-----------|\n|Name                     |task$data() |\n|Number of rows           |5620        |\n|Number of columns        |65          |\n|Key                      |NULL        |\n|_______________________  |            |\n|Column type frequency:   |            |\n|factor                   |1           |\n|numeric                  |64          |\n|________________________ |            |\n|Group variables          |None        |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts      |\n|:-------------|---------:|-------------:|:-------|--------:|:---------------|\n|binaryclass   |         0|             1|FALSE   |        2|N: 5048, P: 572 |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|  mean|   sd| p0| p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|-----:|----:|--:|---:|---:|---:|----:|:-----|\n|input1        |         0|             1|  0.00| 0.00|  0|   0|   0|   0|    0|▁▁▇▁▁ |\n|input10       |         0|             1|  1.97| 3.10|  0|   0|   0|   3|   16|▇▁▁▁▁ |\n|input11       |         0|             1| 10.51| 5.43|  0|   6|  12|  15|   16|▃▂▂▂▇ |\n|input12       |         0|             1| 11.80| 4.00|  0|   9|  13|  15|   16|▁▂▂▃▇ |\n|input13       |         0|             1| 10.51| 4.79|  0|   7|  11|  15|   16|▂▂▃▃▇ |\n|input14       |         0|             1|  8.26| 5.97|  0|   2|   9|  14|   16|▇▂▃▃▇ |\n|input15       |         0|             1|  2.09| 3.92|  0|   0|   0|   2|   16|▇▁▁▁▁ |\n|input16       |         0|             1|  0.14| 0.94|  0|   0|   0|   0|   15|▇▁▁▁▁ |\n|input17       |         0|             1|  0.00| 0.10|  0|   0|   0|   0|    5|▇▁▁▁▁ |\n|input18       |         0|             1|  2.60| 3.49|  0|   0|   1|   4|   16|▇▂▁▁▁ |\n|input19       |         0|             1|  9.68| 5.83|  0|   4|  12|  15|   16|▅▂▂▃▇ |\n|input2        |         0|             1|  0.30| 0.88|  0|   0|   0|   0|    8|▇▁▁▁▁ |\n|input20       |         0|             1|  6.82| 5.88|  0|   1|   6|  12|   16|▇▃▂▂▅ |\n|input21       |         0|             1|  7.16| 6.15|  0|   1|   6|  13|   16|▇▂▂▂▆ |\n|input22       |         0|             1|  7.97| 6.26|  0|   0|   9|  14|   16|▇▂▂▃▇ |\n|input23       |         0|             1|  1.96| 3.48|  0|   0|   0|   3|   16|▇▁▁▁▁ |\n|input24       |         0|             1|  0.05| 0.44|  0|   0|   0|   0|    8|▇▁▁▁▁ |\n|input25       |         0|             1|  0.00| 0.03|  0|   0|   0|   0|    1|▇▁▁▁▁ |\n|input26       |         0|             1|  2.38| 3.11|  0|   0|   1|   4|   16|▇▂▁▁▁ |\n|input27       |         0|             1|  9.19| 6.15|  0|   3|  11|  15|   16|▅▂▂▂▇ |\n|input28       |         0|             1|  9.03| 5.90|  0|   4|  10|  15|   16|▅▂▂▃▇ |\n|input29       |         0|             1|  9.75| 6.24|  0|   3|  12|  16|   16|▅▁▂▂▇ |\n|input3        |         0|             1|  5.39| 4.67|  0|   1|   5|   9|   16|▇▃▃▂▂ |\n|input30       |         0|             1|  7.77| 5.96|  0|   1|   8|  14|   16|▇▃▃▃▇ |\n|input31       |         0|             1|  2.33| 3.64|  0|   0|   0|   4|   16|▇▁▁▁▁ |\n|input32       |         0|             1|  0.00| 0.06|  0|   0|   0|   0|    2|▇▁▁▁▁ |\n|input33       |         0|             1|  0.00| 0.03|  0|   0|   0|   0|    1|▇▁▁▁▁ |\n|input34       |         0|             1|  2.14| 3.30|  0|   0|   0|   4|   15|▇▁▁▁▁ |\n|input35       |         0|             1|  7.66| 6.28|  0|   0|   8|  14|   16|▇▂▂▂▇ |\n|input36       |         0|             1|  9.18| 6.22|  0|   3|  11|  16|   16|▅▂▂▂▇ |\n|input37       |         0|             1| 10.33| 5.92|  0|   6|  13|  16|   16|▃▁▂▂▇ |\n|input38       |         0|             1|  9.05| 5.88|  0|   4|  10|  15|   16|▅▂▂▃▇ |\n|input39       |         0|             1|  2.91| 3.50|  0|   0|   1|   6|   14|▇▂▂▁▁ |\n|input4        |         0|             1| 11.82| 4.26|  0|  10|  13|  15|   16|▁▁▂▃▇ |\n|input40       |         0|             1|  0.00| 0.00|  0|   0|   0|   0|    0|▁▁▇▁▁ |\n|input41       |         0|             1|  0.02| 0.27|  0|   0|   0|   0|    7|▇▁▁▁▁ |\n|input42       |         0|             1|  1.46| 2.95|  0|   0|   0|   2|   16|▇▁▁▁▁ |\n|input43       |         0|             1|  6.59| 6.52|  0|   0|   4|  14|   16|▇▁▁▂▅ |\n|input44       |         0|             1|  7.20| 6.46|  0|   0|   7|  14|   16|▇▂▂▂▆ |\n|input45       |         0|             1|  7.84| 6.30|  0|   1|   8|  14|   16|▇▂▂▂▇ |\n|input46       |         0|             1|  8.53| 5.77|  0|   3|   9|  14|   16|▆▂▃▃▇ |\n|input47       |         0|             1|  3.49| 4.36|  0|   0|   1|   7|   16|▇▂▂▁▁ |\n|input48       |         0|             1|  0.02| 0.25|  0|   0|   0|   0|    6|▇▁▁▁▁ |\n|input49       |         0|             1|  0.01| 0.25|  0|   0|   0|   0|   10|▇▁▁▁▁ |\n|input5        |         0|             1| 11.58| 4.46|  0|   9|  13|  15|   16|▁▁▂▃▇ |\n|input50       |         0|             1|  0.78| 1.93|  0|   0|   0|   0|   16|▇▁▁▁▁ |\n|input51       |         0|             1|  7.75| 5.66|  0|   2|   8|  13|   16|▇▃▃▅▇ |\n|input52       |         0|             1|  9.77| 5.17|  0|   6|  10|  15|   16|▃▃▃▃▇ |\n|input53       |         0|             1|  9.65| 5.31|  0|   5|  10|  15|   16|▃▃▃▃▇ |\n|input54       |         0|             1|  9.12| 5.97|  0|   3|  11|  15|   16|▅▂▂▃▇ |\n|input55       |         0|             1|  3.74| 4.91|  0|   0|   1|   7|   16|▇▂▁▁▁ |\n|input56       |         0|             1|  0.17| 0.84|  0|   0|   0|   0|   13|▇▁▁▁▁ |\n|input57       |         0|             1|  0.00| 0.02|  0|   0|   0|   0|    1|▇▁▁▁▁ |\n|input58       |         0|             1|  0.28| 0.93|  0|   0|   0|   0|   10|▇▁▁▁▁ |\n|input59       |         0|             1|  5.76| 5.02|  0|   1|   5|  10|   16|▇▃▃▂▂ |\n|input6        |         0|             1|  5.59| 5.63|  0|   0|   4|  10|   16|▇▂▂▂▃ |\n|input60       |         0|             1| 11.99| 4.35|  0|  10|  13|  15|   16|▁▁▁▃▇ |\n|input61       |         0|             1| 11.57| 4.98|  0|   9|  13|  16|   16|▂▁▁▃▇ |\n|input62       |         0|             1|  6.72| 5.82|  0|   0|   6|  12|   16|▇▂▂▃▅ |\n|input63       |         0|             1|  2.09| 4.05|  0|   0|   0|   2|   16|▇▁▁▁▁ |\n|input64       |         0|             1|  0.25| 1.42|  0|   0|   0|   0|   16|▇▁▁▁▁ |\n|input7        |         0|             1|  1.38| 3.36|  0|   0|   0|   0|   16|▇▁▁▁▁ |\n|input8        |         0|             1|  0.14| 1.05|  0|   0|   0|   0|   16|▇▁▁▁▁ |\n|input9        |         0|             1|  0.00| 0.09|  0|   0|   0|   0|    5|▇▁▁▁▁ |\n:::\n:::\n\n\n# Imbalance correction\n\nIn [mlr3pipelines](https://mlr3pipelines.mlr-org.com) , there is a [`class balancing`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_classbalancing.html) and a [`smote`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_smote.html) pipe operator that can be combined with any learner.\nBelow, we define the undersampling, oversampling and SMOTE [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s/[`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html).\nAll three imbalance correction methods have hyperparameters to control the degree of class imbalance.\nWe apply the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s/[`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) to the current [`task`](https://mlr3.mlr-org.com/reference/Task.html) with specific hyperparameter values to see how the class balance changes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# check original class balance\ntable(task$truth())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   P    N \n 572 5048 \n```\n:::\n\n```{.r .cell-code}\n# undersample majority class (relative to majority class)\npo_under = po(\"classbalancing\",\n  id = \"undersample\", adjust = \"major\",\n  reference = \"major\", shuffle = FALSE, ratio = 1 / 6)\n# reduce majority class by factor '1/ratio'\ntable(po_under$train(list(task))$output$truth())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  P   N \n572 841 \n```\n:::\n\n```{.r .cell-code}\n# oversample majority class (relative to majority class)\npo_over = po(\"classbalancing\",\n  id = \"oversample\", adjust = \"minor\",\n  reference = \"minor\", shuffle = FALSE, ratio = 6)\n# enrich minority class by factor 'ratio'\ntable(po_over$train(list(task))$output$truth())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   P    N \n3432 5048 \n```\n:::\n:::\n\n\nNote that the original SMOTE algorithm only accepts numeric features (see [`smotefamily::SMOTE`](https://www.rdocumentation.org/packages/smotefamily/topics/SMOTE)).\nTo keep it simple, we therefore preprocess the data and coerce integers to numerics prior to running `SMOTE` and reverse this afterwards:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# SMOTE enriches the minority class with synthetic data\ngr_smote =\n  po(\"colapply\", id = \"int_to_num\",\n    applicator = as.numeric, affect_columns = selector_type(\"integer\")) %>>%\n  po(\"smote\", dup_size = 6) %>>%\n  po(\"colapply\", id = \"num_to_int\",\n    applicator = function(x) as.integer(round(x, 0L)), affect_columns = selector_type(\"numeric\"))\n# enrich minority class by factor (dup_size + 1)\ntable(gr_smote$train(task)[[1L]]$truth())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   P    N \n4004 5048 \n```\n:::\n:::\n\n\n# Construct `AutoTuner`\n\nWe combine the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s/[`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) with a learner (here [`ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html)) to make each pipeline graph behave like a learner:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# create random forest learner\nlearner = lrn(\"classif.ranger\", num.trees = 50)\n\n# combine learner with pipeline graph\nlearner_under = as_learner(po_under %>>% learner)\nlearner_under$id = \"undersample.ranger\"\nlearner_over = as_learner(po_over %>>% learner)\nlearner_over$id = \"oversample.ranger\"\nlearner_smote = as_learner(gr_smote %>>% learner)\nlearner_smote$id = \"smote.ranger\"\n```\n:::\n\n\nWe define the search space in order to tune the hyperparameters of the class imbalance methods.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# define parameter search space for each method\nsearch_space_under = ps(undersample.ratio = p_dbl(1 / 6, 1))\nsearch_space_over = ps(oversample.ratio = p_dbl(1, 6))\nsearch_space_smote = ps(\n  smote.dup_size = p_int(1, 6),\n  smote.K = p_int(1, 6),\n  # makes sure we use numbers to the power of two to better explore the parameter space\n  .extra_trafo = function(x, param_set) {\n    x$smote.K = round(2^(x$smote.K))\n    x\n  }\n)\n```\n:::\n\n\nWe create an [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) class from the learner to tune the graph (random forest learner + imbalance correction method) based on a 3-fold cross-validation using the [`F-beta Score`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.fbeta.html) as performance measure.\nTo keep runtime low, we define the search space only for the imbalance correction method.\nHowever, one can also jointly tune the hyperparameter of the learner along with the imbalance correction method by extending the search space with the learner's hyperparameters.\nNote that SMOTE has two hyperparameters `K` and `dup_size`.\nWhile `K` changes the behavior of the SMOTE algorithm, `dup_size` will affect oversampling rate.\nTo focus on the effect of the oversampling rate on the performance, we will consider SMOTE with K = 2 as a different imbalance correction method as SMOTE with K = 4 (and so on).\nHence, we use grid search with 5 different hyperparameter configurations for the undersampling method, the oversampling method and each SMOTE variant for tuning:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninner_cv3 = rsmp(\"cv\", folds = 3)\nmeasure = msr(\"classif.fbeta\")\n\nlearns = list(\n  auto_tuner(\n    method = \"grid_search\",\n    learner = learner_under,\n    resampling = inner_cv3,\n    measure = measure,\n    search_space = search_space_under,\n    resolution = 6\n  ),\n  auto_tuner(\n    method = \"grid_search\",\n    learner = learner_over,\n    resampling = inner_cv3,\n    measure = measure,\n    search_space = search_space_over,\n    resolution = 6\n  ),\n  auto_tuner(\n    method = \"grid_search\",\n    learner = learner_smote,\n    resampling = inner_cv3,\n    measure = measure,\n    search_space = search_space_smote,\n    resolution = 6\n  )\n)\n```\n:::\n\n\n# Benchmark `AutoTuner`\n\nThe [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) is a fully tuned graph that behaves like a usual learner.\nFor the tuning a 3-fold CV is used.\nNow, we use the [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) function to compare the tuned class imbalance pipeline graphs based on a holdout for the outer evaluation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nouter_holdout = rsmp(\"holdout\")\ndesign = benchmark_grid(\n  tasks = task,\n  learners = learns,\n  resamplings = outer_holdout\n)\nprint(design)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                task         learner              resampling\n1: <TaskClassif[50]> <AutoTuner[42]> <ResamplingHoldout[20]>\n2: <TaskClassif[50]> <AutoTuner[42]> <ResamplingHoldout[20]>\n3: <TaskClassif[50]> <AutoTuner[42]> <ResamplingHoldout[20]>\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n# NOTE: This code runs about 5 minutes\nbmr = benchmark(design, store_models = TRUE)\n```\n:::\n\n\n## Visualize benchmark results\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr$aggregate(measure)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nr      resample_result   task_id               learner_id resampling_id iters classif.fbeta\n1:  1 <ResampleResult[21]> optdigits undersample.ranger.tuned       holdout     1     0.9453552\n2:  2 <ResampleResult[21]> optdigits  oversample.ranger.tuned       holdout     1     0.9508197\n3:  3 <ResampleResult[21]> optdigits       smote.ranger.tuned       holdout     1     0.9539295\n```\n:::\n\n```{.r .cell-code}\n# one value per boxplot since we used holdout as outer resampling\nautoplot(bmr, measure = measure)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-03-30-imbalanced-data-018-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Visualize the tuning path\n\nWith `store_models = TRUE` we allow the [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) function to store each single model that was computed during tuning.\nTherefore, we can plot the tuning path of the best learner from the subsampling iterations:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nbmr_data_learners = as.data.table(bmr)$learner\nutune_path = bmr_data_learners[[1]]$model$tuning_instance$archive$data\nutune_gg = ggplot(utune_path, aes(x = undersample.ratio, y = classif.fbeta)) +\n  geom_point(size = 3) +\n  geom_line() + ylim(0.9, 1)\n\notune_path = bmr_data_learners[[2]]$model$tuning_instance$archive$data\notune_gg = ggplot(otune_path, aes(x = oversample.ratio, y = classif.fbeta)) +\n  geom_point(size = 3) +\n  geom_line() + ylim(0.9, 1)\n\nstune_path = bmr_data_learners[[3]]$model$tuning_instance$archive$data\nstune_gg = ggplot(stune_path, aes(\n  x = smote.dup_size,\n  y = classif.fbeta, col = factor(smote.K))) +\n  geom_point(size = 3) +\n  geom_line() + ylim(0.9, 1)\n\nlibrary(ggpubr)\nggarrange(utune_gg, otune_gg, stune_gg, common.legend = TRUE, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-03-30-imbalanced-data-019-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe results show that oversampling the minority class (for simple oversampling as well as for SMOTE) and undersampling the majority class yield a better performance for this specific data set.\n\n# Conclusion\n\nIn this post, we tuned and compared 5 different settings of sampling ratios for the undersampling method, the oversampling method and different SMOTE variants (using different values of `K` nearest neighbors during the sampling process).\nIf you want to know more, read the [mlr3book](https://mlr3book.mlr-org.com/) and the documentation of the mentioned packages.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}