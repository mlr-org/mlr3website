{
  "hash": "bdef520900b935336c1faaa31abf2bbf",
  "result": {
    "markdown": "---\ntitle: \"Time constraints in the mlr3 ecosystem\"\ndescription: |\n  Set time limits for learners, tuning and nested resampling.\nauthor:\n  - name: Marc Becker\n    orcid: 0000-0002-8115-0400\n    url: https://github.com/be-marc\ndate: 2023-12-21\nbibliography: ../../bibliography.bib\nimage: cover.jpg\n---\n\n\n\n\n# Scope\n\nSetting time limits is an important consideration when tuning unreliable or unstable learning algorithms and when working on shared computing resources.\nThe mlr3 ecosystem provides several mechanisms for setting time constraints for individual learners, tuning processes, and nested resampling.\n\n# Learner\n\nThis section demonstrates how to impose time constraints using a support vector machine (SVM) as an illustrative example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n\nlearner = lrn(\"classif.svm\")\n```\n:::\n\n\nApplying timeouts to the `$train()` and `$predict()` functions is essential for managing learners that may operate indefinitely.\nThese time constraints are set independently for both the training and prediction stages.\nGenerally, training a learner consumes more time than prediction.\nCertain learners, like k-nearest neighbors, lack a distinct training phase and require a timeout only during prediction.\nFor the SVM's training, we set a 10-second limit.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$timeout = c(train = 10, predict = Inf)\n```\n:::\n\n\nTo effectively terminate the process if necessary, it's important to run the training and prediction within a separate R process.\nThe [callr](https://cran.r-project.org/package=callr) package is recommended for this encapsulation, as it tends to be more reliable than the [evaluate](https://cran.r-project.org/package=evaluate) package, especially for terminating externally compiled code.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$encapsulate = c(train = \"callr\", predict = \"callr\")\n```\n:::\n\n\nNote that using `callr` increases the runtime due to the overhead of starting an R process.\nAdditionally, it's advisable to specify a fallback learner, such as `\"classif.featureless\"`, to provide baseline predictions in case the primary learner is terminated.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$fallback = lrn(\"classif.featureless\")\n```\n:::\n\n\nThese time constraints are now integrated into the training, resampling, and benchmarking processes.\nFor more information on encapsulation and fallback learners, see the [mlr3book](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-error-handling).\nThe next section will focus on setting time limits for the entire tuning process.\n\n# Tuning\n\nWhen working with high-performance computing clusters, jobs are often bound by strict time constraints.\nExceeding these limits results in the job being terminated and the loss of any results generated.\nTherefore, it's important to ensure that the tuning process is designed to adhere to these time constraints.\n\nThe `trm(\"runtime\")` controls the duration of the tuning process.\nWe must take into account that the terminator can only check if the time limit is reached between batches.\nWe must therefore set the time lower than the runtime of the job.\nHow much lower depends on the runtime or time limit of the individual learners.\nThe last batch should be able to finish before the time limit of the cluster is reached.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nterminator = trm(\"run_time\", secs = 60)\n\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = terminator\n)\n```\n:::\n\n\nWith these settings, our tuning operation is configured to run for 60 seconds, while individual learners are set to terminate after 10 seconds.\nThis approach ensures the tuning process is efficient and adheres to the constraints imposed by the high-performance computing cluster.\n\n# Nested Resampling\n\nWhen using [nested resampling](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-nested-resampling), time constraints become more complex as they are applied across various levels.\nAs before, the time limit for an individual learner during the tuning is set with `$timeout`.\nThe time limit for the tuning processes in the auto tuners is controlled with the `trm(\"runtime\")`.\nIt's important to note that once the auto tuner enters the final phase of fitting the model and making predictions on the outer test set, the time limit governed by the terminator no longer applies.\nAdditionally, the time limit previously set on the learner is temporarily deactivated, allowing the auto tuner to complete its task uninterrupted.\nHowever, a separate time limit can be assigned to each auto tuner using `$timeout`.\nThis limit encompasses not only the tuning phase but also the time required for fitting the final model and predictions on the outer test set.\n\nThe best way to show this is with an example.\nWe set the time limit for an individual learner to 10 seconds.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$timeout = c(train = 10, predict = Inf)\nlearner$encapsulate = c(train = \"callr\", predict = \"callr\")\nlearner$fallback = lrn(\"classif.featureless\")\n```\n:::\n\n\nNext, we give each auto tuner 60 seconds to finish the tuning process.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nterminator = trm(\"run_time\", secs = 60)\n```\n:::\n\n\nFurthermore, we impose a 120-second limit for resampling each auto tuner.\nThis effectively divides the time allocation, with around 60 seconds for tuning and another 60 seconds for final model fitting and predictions on the outer test set.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat = auto_tuner(\n  tuner = tnr(\"random_search\"),\n  learner = learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"run_time\", secs = 60)\n)\n\nat$timeout = c(train = 100, predict = 20)\nat$encapsulate = c(train = \"callr\", predict = \"callr\")\nat$fallback = lrn(\"classif.featureless\")\n```\n:::\n\n\nIn total, the entire nested resampling process is designed to be completed within 10 minutes (120 seconds multiplied by 5 folds).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = resample(task, at, rsmp(\"cv\", folds = 5))\n```\n:::\n\n\n# Conclusion\n\nWe delved into the setting of time constraints across different levels in the mlr3 ecosystem.\nFrom individual learners to the complexities of nested resampling, we've seen how effectively managing time limits can significantly enhance the efficiency and reliability of machine learning workflows.\nBy utilizing the `trm(\"runtime\")` for tuning processes and setting `$timeout` for individual learners and auto tuners, we can ensure that our machine learning tasks are not only effective but also adhere to the practical time constraints of shared computational resources.\nFor more information, see also the error handling section in the [mlr3book](https://mlr3book.mlr-org.com/chapters/chapter5/advanced_tuning_methods_and_black_box_optimization.html#sec-encapsulation-fallback).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}