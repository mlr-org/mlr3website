{
  "hash": "26e71abc65f43958207fa68c7735e60b",
  "result": {
    "markdown": "---\ntitle: \"Benchmarking mlr3\"\ndescription: |\n\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2023-02-11\nbibliography: ../../bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n---\n\n\n\n\n\n\n# Scope\n\nFrameworks significantly speed up and simplify the development process of machine learning models.\nHowever, this does not come for free but is paid with additional runtime.\nEstimating the performance of models with resampling is a fundamental task in machine learning.\nIn this article, we will benchmark the [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) function of [mlr3](https://mlr3.mlr-org.com) against a base R implementation.\n\n# Setup\n\nWe will use the [microbenchmark](https://cran.r-project.org/package=microbenchmark) package to measure the run time.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(microbenchmark)\nlibrary(mlr3verse)\n```\n:::\n\n\nWe run the benchmark on the [`Palmer Penguins`](https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html) data set.\nIt is a medium-sized data set with 7 features.\nFor the base R implementation, we extract the data from the task.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"penguins\")\ndata = task$data()\n```\n:::\n\n\nThe cross-validation in base R splits the data set into folds and then iterates the resampling splits.\nInstead of fitting a model on the train set and predicting on the test set, we suspend the execution for a specific time interval.\nWe can simulate different training times this way.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbase_cv = function(data, folds, sleep) {\n  ids = seq(nrow(data))\n  splits = split(ids, sample(0:(length(ids) -1) %% folds + 1L))\n\n  lapply(seq(folds), function(i) {\n    train_data = data[-i, ]\n    Sys.sleep(sleep / 2)\n    test_data = data[i, ]\n    Sys.sleep(sleep / 2)\n  })\n}\n```\n:::\n\n\nRunning a 3-fold cross-validation looks like this.\nWe simulate a total time for training and prediction of 1 second.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbase_cv(data, folds = 3, sleep = 1)\n```\n:::\n\n\nThe counterpart to the base R version looks like this in mlr3.\nWe use the [`debug learner`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.debug.html) that can be also stopped for a specific time interval.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.debug\",\n  sleep_train = function() 0.5,\n  sleep_predict = function() 0.5)\nresampling = rsmp(\"cv\", folds = 3)\n\nresample(task, learner, resampling)\n```\n:::\n\n\n# Benchmark\n\nWe run a benchmark with the base R and mlr3 implementation on different training times.\nThe process is repeated 10 times to get reliable timings.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr = microbenchmark(\n  mlr3_cv5_1000 = resample(task, lrn(\"classif.debug\", sleep_train = function() 0.5, sleep_predict = function() 0.5), rsmp(\"cv\", folds = 5)),\n  base_cv5_1000 = base_cv(data, 5, 1),\n  mlr3_cv5_100 = resample(task, lrn(\"classif.debug\", sleep_train = function() 0.05, sleep_predict = function() 0.05), rsmp(\"cv\", folds = 5)),\n  base_cv5_100 = base_cv(data, 5, 0.1),\n  mlr3_cv5_10 = resample(task, lrn(\"classif.debug\", sleep_train = function() 0.005, sleep_predict = function() 0.005), rsmp(\"cv\", folds = 5)),\n  base_cv5_10 = base_cv(data, 5, 0.01),\n  mlr3_cv5_1 = resample(task, lrn(\"classif.debug\", sleep_train = function() 0.0005, sleep_predict = function() 0.0005), rsmp(\"cv\", folds = 5)),\n  base_cv5_1 = base_cv(data, 5, 0.001),\n  unit = \"ms\",\n  times = 10\n)\n```\n:::\n\n\n\n\nThe runtime of a 3-fold cross-validation is about ten times longer with mlr3 if the time for training is 1 ms.\nIf the time for training is 10 ms, mlr3 takes about 3 times as long.\nWith a training time of 1 second, the overhead is slowly negligible.\nIt amounts to only 3 percent.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(data.table)\n\ndata = data.table(\n  train_time = rep(c(1000, 100, 10, 1), each = 2),\n  framework = rep(c(\"mlr3\", \"base\"), 4),\n  mean_run_time = summary(bmr)$median)\n\ndata[, factor := lapply(.SD, function(x) x[1] / x[2]), by = \"train_time\", .SDcols = \"mean_run_time\"]\n\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   train_time framework mean_run_time   factor\n1:       1000      mlr3   5172.731543 1.031442\n2:       1000      base   5015.049463 1.031442\n3:        100      mlr3    682.615241 1.335466\n4:        100      base    511.143907 1.335466\n5:         10      mlr3    143.138329 2.440856\n6:         10      base     58.642668 2.440856\n7:          1      mlr3     97.563773 9.805667\n8:          1      base      9.949734 9.805667\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(ggplot2)\n\nggplot(data, aes(x = train_time, y = mean_run_time, color = framework)) +\n  geom_line() +\n  geom_point() +\n  scale_color_viridis_d(alpha = 0.8, end = 0.8, name = \"Framework\") +\n  xlab(\"Train and Predict Time [ms]\") +\n  ylab(\"Run Time 3-fold Cross-Validation [ms]\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![Runtime of the base R and mlr3 cross-validation with different training times.](index_files/figure-html/fig-runtime-1.png){#fig-runtime fig-align='center' width=672}\n:::\n:::\n\n\nLet's look at the typical training times of well-known machine learning models on different task sizes.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ranger)\nlibrary(e1071)\nlibrary(rpart)\n\ntask = tsk(\"spam\")\ntask$filter(sample(seq(task$nrow), 4000))\ndata_4000 = task$data()\ntask$filter(sample(seq(task$nrow), 400))\ndata_400 = task$data()\n\nbmr = microbenchmark(\n  svm_4000 = svm(type ~ ., data_4000),\n  ranger_4000 = ranger(type ~ ., data_4000, num.trees = 1000),\n  rpart_4000 = rpart(type ~ ., data_4000),\n  lm_4000 = lm(type ~ ., data_4000),\n  svm_400 = svm(type ~ ., data_400),\n  ranger_400 = ranger(type ~ ., data_400, num.trees = 1000),\n  rpart_400 = rpart(type ~ ., data_400),\n  lm_400 = lm(type ~ ., data_400),\n  unit = \"ms\",\n  times = 10\n)\n```\n:::\n\n\n\n\nTraining an SVM with 4000 observations takes about one second.\nThat means we would hardly notice any overhead.\nResampling a random forest or decision tree would also create an acceptable overhead of 10 to 40 percent.\nIf we train on a small dataset with 400 observations, we have to expect mlr3 to take up to three times longer than a Base R implementation.\nA particularly high overhead is to be expected for linear models on small datasets.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata = data.table(\n  task_size = rep(c(4000, 400), each = 4),\n  learner = rep(c(\"smv\", \"ranger\", \"rpart\", \"lm\"), 2),\n  mean_train_time = summary(bmr)$median)\n\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   task_size learner mean_train_time\n1:      4000     smv       696.70440\n2:      4000  ranger       329.36231\n3:      4000   rpart       212.04762\n4:      4000      lm        10.80231\n5:       400     smv        14.14753\n6:       400  ranger        27.83206\n7:       400   rpart        15.05716\n8:       400      lm         3.14120\n```\n:::\n:::\n\n\nWhat happens if we increase the number of folds?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr = microbenchmark(\n  mlr3_cv5_1000 = resample(task, lrn(\"classif.debug\", sleep_train = function() 0.5, sleep_predict = function() 0.5), rsmp(\"cv\", folds = 5)),\n  base_cv5_1000 = base_cv(data, 5, 1),\n  mlr3_cv5_10 = resample(task, lrn(\"classif.debug\", sleep_train = function() 0.005, sleep_predict = function() 0.005), rsmp(\"cv\", folds = 5)),\n  base_cv5_10 = base_cv(data, 5, 0.01),\n  mlr3_cv10_1000 = resample(task, lrn(\"classif.debug\", sleep_train = function() 0.5, sleep_predict = function() 0.5), rsmp(\"cv\", folds = 10)),\n  base_cv10_1000 = base_cv(data, 10, 1),\n  mlr3_cv10_10 = resample(task, lrn(\"classif.debug\", sleep_train = function() 0.005, sleep_predict = function() 0.005), rsmp(\"cv\", folds = 10)),\n  base_cv10_10 = base_cv(data, 10, 0.01),\n  unit = \"ms\",\n  times = 10\n)\n```\n:::\n\n\n\n\nThe overhead slightly decreases when the number of folds increases.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndata = data.table(\n  train_time = rep(c(1000, 1000, 10, 10), 2),\n  cv = rep(c(5, 10), each = 4),\n  framework = rep(c(\"mlr3\", \"base\", \"mlr3\", \"base\"), 2),\n  mean_run_time = summary(bmr)$median)\n\ndata[, factor := lapply(.SD, function(x) x[1] / x[2]), by = c(\"train_time\", \"cv\"), .SDcols = \"mean_run_time\"]\n\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   train_time cv framework mean_run_time   factor\n1:       1000  5      mlr3    5165.53909 1.029921\n2:       1000  5      base    5015.47249 1.029921\n3:         10  5      mlr3     134.11703 2.200408\n4:         10  5      base      60.95099 2.200408\n5:       1000 10      mlr3   10297.55751 1.026457\n6:       1000 10      base   10032.14011 1.026457\n7:         10 10      mlr3     241.42607 2.006054\n8:         10 10      base     120.34874 2.006054\n```\n:::\n:::\n\n\n# Conclusion\n\nBuilding a machine learning workflow with a framework comes with additional runtime.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}