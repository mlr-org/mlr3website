{
  "hash": "171e99265b26d6f428d0803c88ebd9c4",
  "result": {
    "markdown": "---\ntitle: \"Runtime Comparison of tidymodels and mlr3\"\ndescription: |\n  Benchmark the runtime of tidymodels and mlr3.\nauthor:\n  - name: Marc Becker\n    orcid: 0000-0002-8115-0400\n    url: https://github.com/be-marc\ndate: 2023-10-30\nbibliography: ../../bibliography.bib\nimage: cover.png\n---\n\n\n\n\n\n\n# Scope\n\nIn the realm of data science, machine learning frameworks play an important role in streamlining and accelerating the development of analytical workflows.\nAmong these, [tidymodels](https://cran.r-project.org/package=tidymodels) and [mlr3](https://mlr3.mlr-org.com) stand out as prominent tools within the R community.\nThey provide a unified interface for data preprocessing, model training, resampling and tuning.\nThe streamlined and accelerated development process, while efficient, typically results in a trade-off concerning runtime performance.\nThis article undertakes a detailed comparison of the runtime efficiency of `tidymodels` and `mlr3`, focusing on their performance in training, resampling, and tuning machine learning models.\nSpecifically, we assess the time efficiency of these frameworks in running the [`rpart::rpart()`](https://www.rdocumentation.org/packages/rpart/topics/rpart) and [`ranger::ranger()`](https://www.rdocumentation.org/packages/ranger/topics/ranger) models, using the [`Sonar`](https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html) dataset as a test case.\nAdditionally, the study delves into analyzing the runtime overhead of these frameworks by comparing their performance against base R function calls.\nThrough this comparative analysis, the article aims to provide valuable insights into the operational trade-offs of using these advanced machine learning frameworks in practical data science applications.\n\n# Setup\n\nWe employ the [microbenchmark](https://cran.r-project.org/package=microbenchmark) package to measure the time required for training, resampling, and tuning models.\nThis benchmarking process is applied to the `Sonar` dataset using the `rpart` and `ranger` algorithms.\n\nTo ensure the robustness of our results, each function call within the benchmark is executed 100 times in a randomized sequence.\nThe microbenchmark package then provides us with detailed insights, including the median, lower quartile, and upper quartile of the runtimes.\nTo further enhance the reliability of our findings, we execute the benchmark on a cluster.\nEach run of `microbenchmark` is repeated 100 times, with different seeds applied for each iteration.\nResulting in a total of 10,000 function calls for each command.\nThe computing environment for each worker in the cluster consists of 3 cores and 12 GB of RAM.\nFor transparency and reproducibility, the examples of the code used for this experiment are provided as snippets in the article.\nThe complete code, along with all details of the experiment, is available in our public repository, [mlr-org/mlr-benchmark](https://github.com/mlr-org/mlr-benchmark/tree/main/tidymodels).\n\nIt's important to note that our cluster setup is not specifically optimized for single-core performance.\nConsequently, executing the same benchmark on a local machine with might yield faster results.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\nlibrary(\"tidymodels\")\nlibrary(\"microbenchmark\")\n```\n:::\n\n\n# Benchmark\n\n## Train the Models\n\nOur benchmark starts with the fundamental task of model training.\nTo facilitate a direct comparison, we have structured our presentation into two distinct segments.\nOn the left, we demonstrate the initialization of the `rpart` model, employing both `mlr3` and `tidymodels` frameworks.\nThe `rpart` model is a decision tree classifier, which is a simple yet powerful algorithm for classification tasks.\nSimultaneously, on the right, we turn our attention to the initialization of the `ranger` model, known for its efficient implementation of the random forest algorithm.\nOur aim is to mirror the configuration as closely as possible across both frameworks, maintaining consistency in parameters and settings.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = decision_tree() %>%\n  set_engine(\"rpart\",\n    xval = 0L) %>%\n  set_mode(\"classification\")\n\n# mlr3\nlearner = lrn(\"classif.rpart\",\n  xval = 0L)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = rand_forest(trees = 1000L) %>%\n  set_engine(\"ranger\",\n    num.threads = 1L,\n    seed = 1) %>%\n  set_mode(\"classification\")\n\n# mlr3\nlearner = lrn(\"classif.ranger\",\n  num.trees = 1000L,\n  num.threads = 1L,\n  seed = 1,\n  verbose = FALSE,\n  predict_type = \"prob\")\n```\n:::\n\n\n:::\n\nWe measure the runtime for the train functions within each framework.\nThe result of the train function is a trained model in both frameworks.\nIn addition, we invoke the base `rpart()` and `ranger()` functions to establish a baseline for the minimum achievable runtime.\nThis allows us to not only assess the efficiency of the train functions in each framework but also to understand how they perform relative to the base packages.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels train\nfit(resample_tm_mod, formula, data = data)\n\n# mlr3 train\nlearner$train(task)\n```\n:::\n\n\nWhen training an `rpart` model, `tidymodels` demonstrates superior speed, outperforming `mlr3` (@tbl-train-rpart).\nNotably, the `mlr3` package requires approximately twice the time compared to the base call.\n\nA key observation from our results is the significant relative overhead when using a framework for `rpart` model training.\nGiven that `rpart` inherently requires a shorter training time, the additional processing time introduced by the frameworks becomes more pronounced.\nThis aspect highlights the trade-off between the convenience offered by these frameworks and their impact on runtime for quicker tasks.\n\nConversely, when we shift our focus to training a `ranger` model, the scenario changes (@tbl-train-ranger).\nHere, the runtime performance of `ranger` is strikingly similar across both tidymodels and mlr3.\nThis equality in execution time can be attributed to the inherently longer training duration required by `ranger` models.\nAs a result, the relative overhead introduced by either framework becomes minimal, effectively diminishing in the face of the more time-intensive training process.\nThis pattern suggests that for more complex or time-consuming tasks, the choice of framework may have a less significant impact on overall runtime performance.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-train-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of training `rpart` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  | LQ| Median| UQ|\n|:----------|--:|------:|--:|\n|base       | 11|     11| 12|\n|mlr3       | 23|     23| 24|\n|tidymodels | 18|     18| 19|\n:::\n:::\n\n::: {#tbl-train-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of training `ranger` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |  LQ| Median|  UQ|\n|:----------|---:|------:|---:|\n|base       | 286|    322| 347|\n|mlr3       | 301|    335| 357|\n|tidymodels | 310|    342| 362|\n:::\n:::\n\n\n:::\n\n## Resample Sequential\n\nWe proceed to evaluate the runtime performance of the resampling functions within both frameworks, specifically under conditions without parallelization.\nThis step involves the generation of resampling splits, including 3-fold, 6-fold, and 9-fold cross-validation.\nAdditionally, we run a 100 times repeated 3-fold cross-validation.\n\nWe generate the same resampling splits for both frameworks.\nThis consistency is key to ensuring that any observed differences in runtime are attributable to the frameworks themselves, rather than variations in the resampling process.\n\nIn our pursuit of a fair and balanced comparison, we address certain inherent differences between the two frameworks.\nNotably, tidymodels inherently includes scoring of the resampling results as part of its process.\nTo align the comparison, we replicate this scoring step in mlr3, thus maintaining a level field for evaluation.\nFurthermore, mlr3 inherently saves predictions during the resampling process.\nTo match this, we activate the saving of the predictions in tidymodels.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels resample\ncontrol = control_grid(save_pred = TRUE)\nmetrics = metric_set(accuracy)\n\nfit_resamples(tidymodels_workflow, folds, metrics = metrics, control = control)\n\n# mlr3 resample\nmeasure = msr(\"classif.acc\")\n\nrr = resample(task, learner, resampling)\nrr$score(measure)\n```\n:::\n\n\nWhen resampling the fast-fitting rpart model, mlr3 demonstrates a notable edge in speed, as detailed in @tbl-resample-sequential-rpart.\nIn contrast, when it comes to resampling the more computationally intensive ranger models, the performance of tidymodels and mlr3 converges closely (@tbl-resample-sequential-ranger).\nThis parity in performance is particularly noteworthy, considering the differing internal mechanisms and optimizations of tidymodels and mlr3.\nA consistent trend observed across both frameworks is a linear increase in runtime proportional to the number of folds in cross-validation (@fig-resample-sequential).\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-sequential-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of `rpart` depending on the framework and resampling strategy.'}\n::: {.cell-output-display}\n|Framework  |Resampling |    LQ| Median|    UQ|\n|:----------|:----------|-----:|------:|-----:|\n|mlr3       |cv3        |   188|    196|   210|\n|tidymodels |cv3        |   233|    242|   257|\n|mlr3       |cv6        |   343|    357|   379|\n|tidymodels |cv6        |   401|    415|   436|\n|mlr3       |cv9        |   500|    520|   548|\n|tidymodels |cv9        |   568|    588|   616|\n|mlr3       |rcv100     | 15526|  16023| 16777|\n|tidymodels |rcv100     | 16409|  16876| 17527|\n:::\n:::\n\n::: {#tbl-resample-sequential-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of `ranger` depending on the framework and resampling strategy.'}\n::: {.cell-output-display}\n|Framework  |Resampling |    LQ| Median|    UQ|\n|:----------|:----------|-----:|------:|-----:|\n|mlr3       |cv3        |   923|   1004|  1062|\n|tidymodels |cv3        |   916|    981|  1023|\n|mlr3       |cv6        |  1990|   2159|  2272|\n|tidymodels |cv6        |  2089|   2176|  2239|\n|mlr3       |cv9        |  3074|   3279|  3441|\n|tidymodels |cv9        |  3260|   3373|  3453|\n|mlr3       |rcv100     | 85909|  88642| 91381|\n|tidymodels |rcv100     | 87828|  88822| 89843|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime in milliseconds of a cross-validation of `rpart` (left) and `ranger` (right) depending on the framework and number of folds.](index_files/figure-html/fig-resample-sequential-1.png){#fig-resample-sequential fig-align='center' width=672}\n:::\n:::\n\n\n## Resample Parallel\n\nWe conducted a second set of resampling function tests, this time incorporating parallelization to explore its impact on runtime efficiency.\nIn this phase, we utilized doFuture and doParallel as the primary parallelization packages for tidymodels, recognizing their robust support and compatibility.\nMeanwhile, for mlr3, the future package was employed to facilitate parallel processing.\n\nOur findings, as presented in the respective tables (@tbl-resample-parallel-mlr3-future-rpart and @tbl-resample-parallel-mlr3-future-ranger), reveal interesting dynamics about parallelization within the frameworks.\nWhen the number of folds in the resampling process is doubled, we observe only a marginal increase in the average runtime.\nThis pattern suggests a significant overhead associated with initializing the parallel workers, a factor that becomes particularly influential in the overall efficiency of the parallelization process.\n\nIn the case of the rpart model, the parallelization overhead appears to outweigh the potential speedup benefits, as illustrated in the left section of @fig-resample-parallel.\nThis result indicates that for less complex models like rpart, where individual training times are relatively short, the initialization cost of parallel workers may not be sufficiently offset by the reduced processing time per fold.\n\nConversely, for the ranger model, the utilization of parallelization demonstrates a clear advantage over the sequential version, as evidenced in the right section of @fig-resample-parallel.\nThis finding underscores that for more computationally intensive models like ranger, which have longer individual training times, the benefits of parallel processing significantly overcome the initial overhead of worker setup.\nThis differentiation highlights the importance of considering the complexity and inherent processing time of models when deciding to implement parallelization strategies in these frameworks.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-parallel-mlr3-future-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of mlr3 with future and `rpart` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |   LQ| Median|   UQ|\n|:----------|----:|------:|----:|\n|cv3        |  625|    655|  703|\n|cv6        |  738|    771|  817|\n|cv9        |  831|    875|  923|\n|rcv100     | 8620|   9043| 9532|\n:::\n:::\n\n::: {#tbl-resample-parallel-mlr3-future-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of mlr3 with future and `ranger` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |   836|    884|   943|\n|cv6        |  1200|   1249|  1314|\n|cv9        |  1577|   1634|  1706|\n|rcv100     | 32047|  32483| 33022|\n:::\n:::\n\n\n:::\n\nThe `tidymodels` package with `doFuture` is much slower than `mlr3` with `future` (@tbl-resample-parallel-tidymodels-future-rpart and @tbl-resample-parallel-tidymodels-future-ranger).\nWe observed that `tidymodels` exports more data to the workers than `mlr3`.\nThis might be the reason for the slower runtime.\n\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-parallel-tidymodels-future-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of tidymodels with doFuture and `rpart` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |   LQ| Median|   UQ|\n|:----------|----:|------:|----:|\n|cv3        | 2778|   2817| 3019|\n|cv6        | 2808|   2856| 3033|\n|cv9        | 2935|   2975| 3170|\n|rcv100     | 9154|   9302| 9489|\n:::\n:::\n\n::: {#tbl-resample-parallel-tidymodels-future-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of tidymodels with doFuture and `ranger` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |  2982|   3046|  3234|\n|cv6        |  3282|   3366|  3543|\n|cv9        |  3568|   3695|  3869|\n|rcv100     | 27546|  27843| 28166|\n:::\n:::\n\n\n:::\n\nThe `doParallel` package works better for these rather small resampling tasks.\nThe resampling is always a bit faster than `mlr3` but for `rpart` it can also not beat the sequential version (left @fig-resample-parallel).\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-parallel-tidymodels-parallel-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of tidymodels with doParallel and `rpart` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |   557|    649|   863|\n|cv6        |   602|    714|   910|\n|cv9        |   661|    772|   968|\n|rcv100     | 10609|  10820| 11071|\n:::\n:::\n\n::: {#tbl-resample-parallel-tidymodels-parallel-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of tidymodels with doParallel and `ranger` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |   684|    756|   948|\n|cv6        |  1007|   1099|  1272|\n|cv9        |  1360|   1461|  1625|\n|rcv100     | 31205|  31486| 31793|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime in milliseconds of a cross-validation of `rpart` (left) and `ranger` (right) depending on the framework, number of folds and parallelization.](index_files/figure-html/fig-resample-parallel-1.png){#fig-resample-parallel fig-align='center' width=672}\n:::\n:::\n\n\nFor repeated cross-validation, it is beneficial to use parallelization (@fig-resample-parallel-2).\nAll frameworks are faster with parallelization.\nFor these larger resamplings, the `doFuture` package is faster than `doParallel`.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime in seconds of a 100 times repeated 3-fold cross-validation of `rpart` (left) and `ranger` (right) depending on the framework and parallelization.](index_files/figure-html/fig-resample-parallel-2-1.png){#fig-resample-parallel-2 fig-align='center' width=672}\n:::\n:::\n\n\n## Tune Seuqential\n\nNow we measure the runtime of the tune functions.\nThe `tidymodels` package evaluates a predefined grid of points.\nWe use the `\"design_points\"` tuner of the `mlr3tuning` package to evaluate the same grid.\nThe grid contains 200 points for the `rpart` model and 200 points for the `ranger` model.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = decision_tree(\n  cost_complexity = tune()) %>%\n  set_engine(\"rpart\",\n    xval = 0) %>%\n  set_mode(\"classification\")\n\ntm_design = data.table(\n  cost_complexity = seq(0.1, 0.2, length.out = 200))\n\n# mlr3\nlearner = lrn(\"classif.rpart\",\n  xval = 0,\n  cp = to_tune())\n\nmlr3_design = data.table(\n  cp = seq(0.1, 0.2, length.out = 200))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = rand_forest(\n  trees = tune()) %>%\n  set_engine(\"ranger\",\n    num.threads = 1L,\n    seed = 1) %>%\n  set_mode(\"classification\")\n\ntm_design = data.table(\n  trees = seq(1000, 1199))\n\n# mlr3\nlearner = lrn(\"classif.ranger\",\n  num.trees = to_tune(1, 10000),\n  num.threads = 1L,\n  seed = 1,\n  verbose = FALSE,\n  predict_type = \"prob\")\n\nmlr3_design = data.table(\n  num.trees = seq(1000, 1199))\n```\n:::\n\n\n:::\n\nWe measure the runtime of the tune functions.\nBoth runs return the best hyperparameter configuration.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels tune\ntune::tune_grid(\n  tm_wf,\n  resamples = resamples,\n  grid = design,\n  metrics = metrics)\n\n# mlr3 tune\ntuner = tnr(\"design_points\", design = design, batch_size = nrow(design))\nmlr3tuning::tune(\n  tuner = tuner,\n  task = task,\n  learner = learner,\n  resampling = resampling,\n  measures = measure,\n  store_benchmark_result = FALSE)\n```\n:::\n\n\nRunning the tuning sequentially is faster with `mlr3` (@tbl-tune-sequential-rpart and @tbl-tune-sequential-ranger).\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-tune-sequential-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of `rpart` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  | LQ| Median| UQ|\n|:----------|--:|------:|--:|\n|mlr3       | 27|     27| 28|\n|tidymodels | 37|     37| 39|\n:::\n:::\n\n::: {#tbl-tune-sequential-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of `ranger` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |  LQ| Median|  UQ|\n|:----------|---:|------:|---:|\n|mlr3       | 167|    171| 175|\n|tidymodels | 194|    195| 196|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Tune Parallel\n\nFinally, we measure the runtime of the tune functions with parallelization.\nThe parallelization runs on 3 cores.\nWe use the largest possible chunk size for `mlr3`.\nThis means the workers get all points at once which minimizes the parallelization overhead.\nThe `tidymodels` package uses the same chunk size but sets it internally.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptions(\"mlr3.exec_chunk_size\" = 200)\n```\n:::\n\n\nThe runtimes of `mlr3` and `tidymodels` are very similar.\nThe `mlr3` package is slightly faster with `rpart` (@tbl-tune-parallel-mlr3-future-rpart) and slower with `ranger` (@tbl-tune-parallel-mlr3-future-ranger).\nSince we are using a 3-fold cross-validation, the `doParallel` package is faster than `doFuture` (@fig-tune-parallel).\nRegardless of the framework-backend combination, it is worth activating parallelization.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-tune-parallel-mlr3-future-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of `rpart` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |Backend    | LQ| Median| UQ|\n|:----------|:----------|--:|------:|--:|\n|mlr3       |future     | 11|     12| 12|\n|tidymodels |doFuture   | 17|     17| 17|\n|tidymodels |doParallel | 13|     13| 13|\n:::\n:::\n\n::: {#tbl-tune-parallel-mlr3-future-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of `ranger` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |Backend    | LQ| Median| UQ|\n|:----------|:----------|--:|------:|--:|\n|mlr3       |future     | 54|     55| 55|\n|tidymodels |doFuture   | 58|     58| 59|\n|tidymodels |doParallel | 54|     54| 55|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime in seconds of tuning 200 points of `rpart` (left) and `ranger` (right) depending on the framework and parallelization.](index_files/figure-html/fig-tune-parallel-1.png){#fig-tune-parallel fig-align='center' width=672}\n:::\n:::\n\n\n# Conclusion\n\nWe cannot identify a clear winner.\nBoth frameworks have a similar runtime for training, resampling and tuning.\nThe relative overhead of using a framework is high for `rpart` because the training time is short.\nFor slow-fitting models like `ranger`, the relative overhead becomes negligible.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}