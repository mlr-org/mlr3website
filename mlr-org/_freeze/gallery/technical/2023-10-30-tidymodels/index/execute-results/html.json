{
  "hash": "15e37b777206c4d4e0a0d79fe0c73a6f",
  "result": {
    "markdown": "---\ntitle: \"Analyzing the Runtime Performance of tidymodels and mlr3\"\ndescription: |\n  Compare the runtime performance of tidymodels and mlr3.\nauthor:\n  - name: Marc Becker\n    orcid: 0000-0002-8115-0400\n    url: https://github.com/be-marc\ndate: 2023-10-30\nbibliography: ../../bibliography.bib\nimage: cover.png\n---\n\n\n\n\n\n\n# Scope\n\nIn the realm of data science, machine learning frameworks play an important role in streamlining and accelerating the development of analytical workflows.\nAmong these, [tidymodels](https://cran.r-project.org/package=tidymodels) and [mlr3](https://mlr3.mlr-org.com) stand out as prominent tools within the R community.\nThey provide a unified interface for data preprocessing, model training, resampling and tuning.\nThe streamlined and accelerated development process, while efficient, typically results in a trade-off concerning runtime performance.\nThis article undertakes a detailed comparison of the runtime efficiency of `tidymodels` and `mlr3`, focusing on their performance in training, resampling, and tuning machine learning models.\nSpecifically, we assess the time efficiency of these frameworks in running the [`rpart::rpart()`](https://www.rdocumentation.org/packages/rpart/topics/rpart) and [`ranger::ranger()`](https://www.rdocumentation.org/packages/ranger/topics/ranger) models, using the [`Sonar`](https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html) dataset as a test case.\nAdditionally, the study delves into analyzing the runtime overhead of these frameworks by comparing their performance against training the models without a framework.\nThrough this comparative analysis, the article aims to provide valuable insights into the operational trade-offs of using these advanced machine learning frameworks in practical data science applications.\n\n# Setup\n\nWe employ the [microbenchmark](https://cran.r-project.org/package=microbenchmark) package to measure the time required for training, resampling, and tuning models.\nThis benchmarking process is applied to the `Sonar` dataset using the `rpart` and `ranger` algorithms.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\nlibrary(\"tidymodels\")\nlibrary(\"microbenchmark\")\n\ntask = tsk(\"sonar\")\ndata = task$data()\nformula = Class ~ .\n```\n:::\n\n\nTo ensure the robustness of our results, each function call within the benchmark is executed 100 times in a randomized sequence.\nThe microbenchmark package then provides us with detailed insights, including the median, lower quartile, and upper quartile of the runtimes.\nTo further enhance the reliability of our findings, we execute the benchmark on a cluster.\nEach run of `microbenchmark` is repeated 100 times, with different seeds applied for each iteration.\nResulting in a total of 10,000 function calls of each command.\nThe computing environment for each worker in the cluster consists of 3 cores and 12 GB of RAM.\nFor transparency and reproducibility, the examples of the code used for this experiment are provided as snippets in the article.\nThe complete code, along with all details of the experiment, is available in our public repository, [mlr-org/mlr-benchmark](https://github.com/mlr-org/mlr-benchmark/tree/main/tidymodels).\n\nIt's important to note that our cluster setup is not specifically optimized for single-core performance.\nConsequently, executing the same benchmark on a local machine with might yield faster results.\n\n# Benchmark\n\n## Train the Models\n\nOur benchmark starts with the fundamental task of model training.\nTo facilitate a direct comparison, we have structured our presentation into two distinct segments.\nOn the left, we demonstrate the initialization of the `rpart` model, employing both `mlr3` and `tidymodels` frameworks.\nThe `rpart` model is a decision tree classifier, which is a simple and fast-fitting algorithm for classification tasks.\nSimultaneously, on the right, we turn our attention to the initialization of the `ranger` model, known for its efficient implementation of the random forest algorithm.\nOur aim is to mirror the configuration as closely as possible across both frameworks, maintaining consistency in parameters and settings.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = decision_tree() %>%\n  set_engine(\"rpart\",\n    xval = 0L) %>%\n  set_mode(\"classification\")\n\n# mlr3\nlearner = lrn(\"classif.rpart\",\n  xval = 0L)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = rand_forest(trees = 1000L) %>%\n  set_engine(\"ranger\",\n    num.threads = 1L,\n    seed = 1) %>%\n  set_mode(\"classification\")\n\n# mlr3\nlearner = lrn(\"classif.ranger\",\n  num.trees = 1000L,\n  num.threads = 1L,\n  seed = 1,\n  verbose = FALSE,\n  predict_type = \"prob\")\n```\n:::\n\n\n:::\n\nWe measure the runtime for the train functions within each framework.\nThe result of the train function is a trained model in both frameworks.\nIn addition, we invoke the `rpart()` and `ranger()` functions to establish a baseline for the minimum achievable runtime.\nThis allows us to not only assess the efficiency of the train functions in each framework but also to understand how they perform relative to the base packages.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels train\nfit(tm_mod, formula, data = data)\n\n# mlr3 train\nlearner$train(task)\n```\n:::\n\n\nWhen training an `rpart` model, `tidymodels` demonstrates superior speed, outperforming `mlr3` (@tbl-train-rpart).\nNotably, the `mlr3` package requires approximately twice the time compared to the baseline.\n\nA key observation from our results is the significant relative overhead when using a framework for `rpart` model training.\nGiven that `rpart` inherently requires a shorter training time, the additional processing time introduced by the frameworks becomes more pronounced.\nThis aspect highlights the trade-off between the convenience offered by these frameworks and their impact on runtime for quicker tasks.\n\nConversely, when we shift our focus to training a `ranger` model, the scenario changes (@tbl-train-ranger).\nHere, the runtime performance of `ranger` is strikingly similar across both `tidymodels` and `mlr3`.\nThis equality in execution time can be attributed to the inherently longer training duration required by `ranger` models.\nAs a result, the relative overhead introduced by either framework becomes minimal, effectively diminishing in the face of the more time-intensive training process.\nThis pattern suggests that for more complex or time-consuming tasks, the choice of framework may have a less significant impact on overall runtime performance.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-train-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of training `rpart` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  | LQ| Median| UQ|\n|:----------|--:|------:|--:|\n|base       | 11|     11| 12|\n|mlr3       | 23|     23| 24|\n|tidymodels | 18|     18| 19|\n:::\n:::\n\n::: {#tbl-train-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of training `ranger` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |  LQ| Median|  UQ|\n|:----------|---:|------:|---:|\n|base       | 286|    322| 347|\n|mlr3       | 301|    335| 357|\n|tidymodels | 310|    342| 362|\n:::\n:::\n\n\n:::\n\n## Resample Sequential\n\nWe proceed to evaluate the runtime performance of the resampling functions within both frameworks, specifically under conditions without parallelization.\nThis step involves the generation of resampling splits, including 3-fold, 6-fold, and 9-fold cross-validation.\nAdditionally, we run a 100 times repeated 3-fold cross-validation.\n\nWe generate the same resampling splits for both frameworks.\nThis consistency is key to ensuring that any observed differences in runtime are attributable to the frameworks themselves, rather than variations in the resampling process.\n\nIn our pursuit of a fair and balanced comparison, we address certain inherent differences between the two frameworks.\nNotably, `tidymodels` inherently includes scoring of the resampling results as part of its process.\nTo align the comparison, we replicate this scoring step in `mlr3`, thus maintaining a level field for evaluation.\nFurthermore, `mlr3` inherently saves predictions during the resampling process.\nTo match this, we activate the saving of the predictions in `tidymodels`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels resample\ncontrol = control_grid(save_pred = TRUE)\nmetrics = metric_set(accuracy)\n\ntm_wf =\n  workflow() %>%\n  add_model(tm_mod) %>%\n  add_formula(formula)\n\nfit_resamples(tm_wf, folds, metrics = metrics, control = control)\n\n# mlr3 resample\nmeasure = msr(\"classif.acc\")\n\nrr = resample(task, learner, resampling)\nrr$score(measure)\n```\n:::\n\n\nWhen resampling the fast-fitting `rpart` model, `mlr3` demonstrates a notable edge in speed, as detailed in @tbl-resample-sequential-rpart.\nIn contrast, when it comes to resampling the more computationally intensive `ranger` models, the performance of `tidymodels` and `mlr3` converges closely (@tbl-resample-sequential-ranger).\nThis parity in performance is particularly noteworthy, considering the differing internal mechanisms and optimizations of `tidymodels` and `mlr3`.\nA consistent trend observed across both frameworks is a linear increase in runtime proportional to the number of folds in cross-validation (@fig-resample-sequential).\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-sequential-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of `rpart` depending on the framework and resampling strategy.'}\n::: {.cell-output-display}\n|Framework  |Resampling |    LQ| Median|    UQ|\n|:----------|:----------|-----:|------:|-----:|\n|mlr3       |cv3        |   188|    196|   210|\n|tidymodels |cv3        |   233|    242|   257|\n|mlr3       |cv6        |   343|    357|   379|\n|tidymodels |cv6        |   401|    415|   436|\n|mlr3       |cv9        |   500|    520|   548|\n|tidymodels |cv9        |   568|    588|   616|\n|mlr3       |rcv100     | 15526|  16023| 16777|\n|tidymodels |rcv100     | 16409|  16876| 17527|\n:::\n:::\n\n::: {#tbl-resample-sequential-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of `ranger` depending on the framework and resampling strategy.'}\n::: {.cell-output-display}\n|Framework  |Resampling |    LQ| Median|    UQ|\n|:----------|:----------|-----:|------:|-----:|\n|mlr3       |cv3        |   923|   1004|  1062|\n|tidymodels |cv3        |   916|    981|  1023|\n|mlr3       |cv6        |  1990|   2159|  2272|\n|tidymodels |cv6        |  2089|   2176|  2239|\n|mlr3       |cv9        |  3074|   3279|  3441|\n|tidymodels |cv9        |  3260|   3373|  3453|\n|mlr3       |rcv100     | 85909|  88642| 91381|\n|tidymodels |rcv100     | 87828|  88822| 89843|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime, measured in milliseconds, for cross-validations using `rpart` (displayed on the left) and `ranger` (on the right). The comparison encompasses variations across different frameworks and the number of folds in the cross-validation.](index_files/figure-html/fig-resample-sequential-1.png){#fig-resample-sequential fig-align='center' width=672}\n:::\n:::\n\n\n## Resample Parallel\n\nWe conducted a second set of resampling function tests, this time incorporating parallelization to explore its impact on runtime efficiency.\nIn this phase, we utilized `doFuture` and `doParallel`  as the primary parallelization packages for tidymodels, recognizing their robust support and compatibility.\nMeanwhile, for `mlr3`, the `future`  package was employed to facilitate parallel processing.\n\nOur findings, as presented in the respective tables (@tbl-resample-parallel-mlr3-future-rpart and @tbl-resample-parallel-mlr3-future-ranger), reveal interesting dynamics about parallelization within the frameworks.\nWhen the number of folds in the resampling process is doubled, we observe only a marginal increase in the average runtime.\nThis pattern suggests a significant overhead associated with initializing the parallel workers, a factor that becomes particularly influential in the overall efficiency of the parallelization process.\n\nIn the case of the `rpart` model, the parallelization overhead appears to outweigh the potential speedup benefits, as illustrated in the left section of @fig-resample-parallel.\nThis result indicates that for less complex models like `rpart`, where individual training times are relatively short, the initialization cost of parallel workers may not be sufficiently offset by the reduced processing time per fold.\n\nConversely, for the `ranger` model, the utilization of parallelization demonstrates a clear advantage over the sequential version, as evidenced in the right section of @fig-resample-parallel.\nThis finding underscores that for more computationally intensive models like `ranger`, which have longer individual training times, the benefits of parallel processing significantly overcome the initial overhead of worker setup.\nThis differentiation highlights the importance of considering the complexity and inherent processing time of models when deciding to implement parallelization strategies in these frameworks.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-parallel-mlr3-future-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of `mlr3` with `future` and `rpart` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |   LQ| Median|   UQ|\n|:----------|----:|------:|----:|\n|cv3        |  625|    655|  703|\n|cv6        |  738|    771|  817|\n|cv9        |  831|    875|  923|\n|rcv100     | 8620|   9043| 9532|\n:::\n:::\n\n::: {#tbl-resample-parallel-mlr3-future-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of `mlr3` with `future` and `ranger` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |   836|    884|   943|\n|cv6        |  1200|   1249|  1314|\n|cv9        |  1577|   1634|  1706|\n|rcv100     | 32047|  32483| 33022|\n:::\n:::\n\n\n:::\n\nWhen paired with doFuture, `tidymodels` exhibits significantly slower runtime compared to the `mlr3` package utilizing `future` (@tbl-resample-parallel-tidymodels-future-rpart and @tbl-resample-parallel-tidymodels-future-ranger).\nWe observed that `tidymodels` exports more data to the parallel workers, which notably exceeds that of `mlr3`.\nThis substantial difference in data export could plausibly account for the observed slower runtime when using `tidymodels` on small tasks.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-parallel-tidymodels-future-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of `tidymodels` with `doFuture` and `rpart` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |   LQ| Median|   UQ|\n|:----------|----:|------:|----:|\n|cv3        | 2778|   2817| 3019|\n|cv6        | 2808|   2856| 3033|\n|cv9        | 2935|   2975| 3170|\n|rcv100     | 9154|   9302| 9489|\n:::\n:::\n\n::: {#tbl-resample-parallel-tidymodels-future-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of `tidymodels` with `doFuture` and `ranger` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |  2982|   3046|  3234|\n|cv6        |  3282|   3366|  3543|\n|cv9        |  3568|   3695|  3869|\n|rcv100     | 27546|  27843| 28166|\n:::\n:::\n\n\n:::\n\nThe utilization of the `doParallel` package demonstrates a notable improvement in handling smaller resampling tasks.\nIn these scenarios, the resampling process consistently outperforms the `mlr3` framework in terms of speed.\nHowever, it's important to note that even with this enhanced performance, the `doParallel` package does not always surpass the efficiency of the sequential version, especially when working with the `rpart` model.\nThis specific observation is illustrated in the left section of @fig-resample-parallel.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-parallel-tidymodels-parallel-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of `tidymodels` with `doParallel`  and `rpart` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |   557|    649|   863|\n|cv6        |   602|    714|   910|\n|cv9        |   661|    772|   968|\n|rcv100     | 10609|  10820| 11071|\n:::\n:::\n\n::: {#tbl-resample-parallel-tidymodels-parallel-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of `tidymodels` with `doParallel`  and `ranger` depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |   684|    756|   948|\n|cv6        |  1007|   1099|  1272|\n|cv9        |  1360|   1461|  1625|\n|rcv100     | 31205|  31486| 31793|\n:::\n:::\n\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime, measured in milliseconds, for cross-validations using `rpart` (displayed on the left) and `ranger` (on the right). The comparison encompasses variations across different frameworks, the number of folds in the cross-validation, and the implementation of parallelization.](index_files/figure-html/fig-resample-parallel-1.png){#fig-resample-parallel fig-align='center' width=672}\n:::\n:::\n\n\nIn the context of repeated cross-validation, our findings underscore the efficacy of parallelization (@fig-resample-parallel-2). Across all frameworks tested, the adoption of parallel processing techniques yields a significant increase in speed.\nThis enhancement is particularly noticeable in larger resampling tasks, where the demands on computational resources are more substantial.\n\nInterestingly, within these more extensive resampling scenarios, the `doFuture` package emerges as a more efficient option compared to `doParallel`.\nThis distinction is important, as it highlights the relative strengths of different parallelization packages under varying workload conditions.\nWhile `doParallel` shows proficiency in smaller tasks, `doFuture` demonstrates its capability to handle larger, more complex resampling processes with greater speed and efficiency.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime, measured in seconds, of a 100 times repeated 3-fold cross-validation using `rpart` (displayed on the left) and `ranger` (on the right). The comparison encompasses variations across different frameworks and the implementation of parallelization.](index_files/figure-html/fig-resample-parallel-2-1.png){#fig-resample-parallel-2 fig-align='center' width=672}\n:::\n:::\n\n\n## Tune Sequential\n\nWe then shift our focus to assessing the runtime performance of the tuning functions.\nIn this phase, the `tidymodels` package is utilized to evaluate a predefined grid, comprising a specific set of hyperparameter configurations.\nTo ensure a balanced and comparable analysis, we employ the `\"design_points\"` tuner from the `mlr3tuning` package.\nThis approach allows us to evaluate the same grid within the `mlr3` framework, maintaining consistency across both platforms.\nThe grid used for this comparison contains 200 hyperparameter configurations each, for both the `rpart` and `ranger` models.\nThis approach helps us to understand how each framework handles the optimization of model hyperparameters, a key aspect of building effective and efficient machine learning models.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = decision_tree(\n  cost_complexity = tune()) %>%\n  set_engine(\"rpart\",\n    xval = 0) %>%\n  set_mode(\"classification\")\n\ntm_design = data.table(\n  cost_complexity = seq(0.1, 0.2, length.out = 200))\n\n# mlr3\nlearner = lrn(\"classif.rpart\",\n  xval = 0,\n  cp = to_tune())\n\nmlr3_design = data.table(\n  cp = seq(0.1, 0.2, length.out = 200))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = rand_forest(\n  trees = tune()) %>%\n  set_engine(\"ranger\",\n    num.threads = 1L,\n    seed = 1) %>%\n  set_mode(\"classification\")\n\ntm_design = data.table(\n  trees = seq(1000, 1199))\n\n# mlr3\nlearner = lrn(\"classif.ranger\",\n  num.trees = to_tune(1, 10000),\n  num.threads = 1L,\n  seed = 1,\n  verbose = FALSE,\n  predict_type = \"prob\")\n\nmlr3_design = data.table(\n  num.trees = seq(1000, 1199))\n```\n:::\n\n\n:::\n\nWe measure the runtime of the tune functions within each framework.\nBoth the `tidymodels` and `mlr3` frameworks are tasked with identifying the optimal hyperparameter configuration.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels tune\ntune::tune_grid(\n  tm_wf,\n  resamples = resamples,\n  grid = design,\n  metrics = metrics)\n\n# mlr3 tune\ntuner = tnr(\"design_points\", design = design, batch_size = nrow(design))\nmlr3tuning::tune(\n  tuner = tuner,\n  task = task,\n  learner = learner,\n  resampling = resampling,\n  measures = measure,\n  store_benchmark_result = FALSE)\n```\n:::\n\n\nIn our sequential tuning tests, `mlr3` demonstrates a notable advantage in terms of speed.\nThis finding is clearly evidenced in our results, as shown in Table @tbl-tune-sequential-rpart for the `rpart` model and Table @tbl-tune-sequential-ranger for the `ranger` model.\nThe faster performance of `mlr3` in these sequential runs highlights its efficiency in handling the tuning process without parallelization.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-tune-sequential-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of `rpart` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  | LQ| Median| UQ|\n|:----------|--:|------:|--:|\n|mlr3       | 27|     27| 28|\n|tidymodels | 37|     37| 39|\n:::\n:::\n\n::: {#tbl-tune-sequential-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of `ranger` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |  LQ| Median|  UQ|\n|:----------|---:|------:|---:|\n|mlr3       | 167|    171| 175|\n|tidymodels | 194|    195| 196|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Tune Parallel\n\nConcluding our analysis, we proceed to evaluate the runtime performance of the tune functions, this time implementing parallelization to enhance efficiency.\nFor these runs, parallelization is executed on 3 cores.\n\nIn the case of `mlr3`, we opt for the largest possible chunk size.\nThis strategic choice means that all points within the tuning grid are sent to the workers in a single batch, effectively minimizing the overhead typically associated with parallelization.\nThis approach is crucial in reducing the time spent in distributing tasks across multiple cores, thereby streamlining the tuning process.\nOn the other hand, the `tidymodels` package also operates with the same chunk size, but this setting is determined and managed internally within the framework.\n\nBy conducting these parallelization tests, we aim to provide a deeper understanding of how each framework handles the distribution and management of computational tasks during the tuning process, particularly in a parallel computing environment.\nThis final set of measurements is important in painting a complete picture of the runtime performance of the tune functions across both `tidymodels` and `mlr3` under different operational settings.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptions(\"mlr3.exec_chunk_size\" = 200)\n```\n:::\n\n\nOur analysis of the parallelized tuning functions reveals that the runtimes for `mlr3` and `tidymodels` are remarkably similar.\nHowever, subtle differences emerge upon closer inspection.\nFor instance, the `mlr3` package exhibits a slightly faster performance when tuning the `rpart` model, as indicated in @tbl-tune-parallel-mlr3-future-rpart.\nIn contrast, it falls marginally behind `tidymodels` in tuning the `ranger` model, as shown in @tbl-tune-parallel-mlr3-future-ranger.\n\nInterestingly, when considering the specific context of a 3-fold cross-validation, the `doParallel` package outperforms `doFuture` in terms of speed, as demonstrated in @fig-tune-parallel.\nThis outcome suggests that the choice of parallelization package can have a significant impact on tuning efficiency, particularly in scenarios with a smaller number of folds.\n\nA key takeaway from our study is the clear benefit of enabling parallelization, regardless of the chosen framework-backend combination.\nActivating parallelization consistently enhances performance, making it a highly recommended strategy for tuning machine learning models, especially in tasks involving extensive hyperparameter exploration or larger datasets.\nThis conclusion underscores the value of parallel processing in modern machine learning workflows, offering a practical solution for accelerating model tuning across various computational settings.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-tune-parallel-mlr3-future-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of `rpart` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |Backend    | LQ| Median| UQ|\n|:----------|:----------|--:|------:|--:|\n|mlr3       |future     | 11|     12| 12|\n|tidymodels |doFuture   | 17|     17| 17|\n|tidymodels |doParallel | 13|     13| 13|\n:::\n:::\n\n::: {#tbl-tune-parallel-mlr3-future-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of `ranger` depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |Backend    | LQ| Median| UQ|\n|:----------|:----------|--:|------:|--:|\n|mlr3       |future     | 54|     55| 55|\n|tidymodels |doFuture   | 58|     58| 59|\n|tidymodels |doParallel | 54|     54| 55|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime, measured in seconds, of a tuning 200 hyperparameter configurations of `rpart` (displayed on the left) and `ranger` (on the right). The comparison encompasses variations across different frameworks and the implementation of parallelization.](index_files/figure-html/fig-tune-parallel-1.png){#fig-tune-parallel fig-align='center' width=672}\n:::\n:::\n\n\n# Conclusion\n\nOur analysis reveals that both `tidymodels` and `mlr3` exhibit comparable runtimes across key processes such as training, resampling, and tuning, each displaying its own set of strengths and efficiencies.\n\nA notable observation is the relative overhead associated with using either framework, particularly when working with fast-fitting models like `rpart`.\nIn these cases, the additional processing time introduced by the frameworks is more pronounced due to the inherently short training time of `rpart` models.\nThis results in a higher relative overhead, reflecting the trade-offs between the convenience of a comprehensive framework and the directness of more basic approaches.\n\nConversely, when dealing with slower-fitting models such as `ranger`, the scenario shifts.\nFor these more time-intensive models, the relative overhead introduced by the frameworks diminishes significantly.\nIn such instances, the extended training times of the models absorb much of the frameworks' inherent overhead, rendering it relatively negligible.\n\nIn summary, while there is no outright winner in terms of overall performance, the decision to use `tidymodels` or `mlr3` should be informed by the specific requirements of the task at hand.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}