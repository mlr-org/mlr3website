{
  "hash": "d610e2bbdbc54249a21eb5dddca5f139",
  "result": {
    "markdown": "---\ntitle: \"Runtime Comparison of tidymodels and mlr3\"\ndescription: |\n  Benchmark the runtime of tidymodels and mlr3.\nauthor:\n  - name: Marc Becker\n    orcid: 0000-0002-8115-0400\n    url: https://github.com/be-marc\ndate: 2023-10-30\nbibliography: ../../bibliography.bib\n---\n\n\n\n\n\n\n# Scope\n\nMachine learning frameworks simplify and quicken the development of machine learning workflows.\nThe [tidymodels](https://cran.r-project.org/package=tidymodels) and [mlr3](https://mlr3.mlr-org.com) packages are two popular frameworks for R.\nThey provide a unified interface for data preprocessing, model training, resampling and tuning.\nThe faster development comes at the cost of runtime performance.\nIn this article, we compare the runtime performance of `tidymodels` and `mlr3`.\nWe measure the time it takes to train, resample and tune an [`rpart::rpart()`](https://www.rdocumentation.org/packages/rpart/topics/rpart) and [`ranger::ranger()`](https://www.rdocumentation.org/packages/ranger/topics/ranger) model on the [`Sonar`](https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html) data set.\nMoreover, we analyze the runtime overhead by comparing the runtime of the frameworks with the base R call.\n\n# Setup\n\nWe measure the time it takes to train, resample and tune a model with the [microbenchmark](https://cran.r-project.org/package=microbenchmark) package.\nAll function calls are repeated 100 times in random order.\nThe benchmark is performed on the [`Sonar`](https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html) data set with `rpart` and `ranger`.\nThe `microbenchmark` package returns the median and lower and upper quartile of the runtimes.\nWe run the benchmark on a cluster and repeat each run of `microbenchmark` 100 times with different seeds.\nEach worker has 3 cores and 12 GB of RAM.\nThis means each command is called 10,000 times.\nThe cluster is not optimized for single-core performance, so a run on a local machine might be faster.\nThe code chunks only show examples of the executed code.\nThe whole experiment can be found in our repository [mlr-org/mlr-benchmark](https://github.com/mlr-org/mlr-benchmark/tree/main/tidymodels).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\nlibrary(\"tidymodels\")\nlibrary(\"microbenchmark\")\n```\n:::\n\n\n# Benchmark\n\n## Train the Models\n\nWe start with the simplest operation, training a model.\nThe left side shows the initialization of `rpart` with `mlr3` and `tidymodels`.\nThe right side is the initialization of `ranger`.\nWe try to use the same parameters for both frameworks.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = decision_tree() %>%\n  set_engine(\"rpart\",\n    xval = 0L) %>%\n  set_mode(\"classification\")\n\n# mlr3\nlearner = lrn(\"classif.rpart\",\n  xval = 0L)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = rand_forest(trees = 1000L) %>%\n  set_engine(\"ranger\",\n    num.threads = 1L,\n    seed = 1) %>%\n  set_mode(\"classification\")\n\n# mlr3\nlearner = lrn(\"classif.ranger\",\n  num.trees = 1000L,\n  num.threads = 1L,\n  seed = 1,\n  verbose = FALSE,\n  predict_type = \"prob\")\n```\n:::\n\n\n:::\n\nWe measure the runtime of the train functions.\nAdditionally, the base R function is called to get a lower limit of the runtime.\nBoth frameworks return the trained model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels train\nfit(resample_tm_mod, formula, data = data)\n\n# mlr3 train\nlearner$train(task)\n```\n:::\n\n\nTraining an `rpart` model is faster with `tidymodels` (@tbl-train-rpart).\nThe `mlr3` package takes twice as long as the base R call.\nWe observe that the relative overhead of using a framework is high for `rpart` because the training time is short.\nThe runtime of `ranger` is similar for both frameworks (@tbl-train-ranger).\nThe relative overhead with `ranger` becomes negligible because the training time is longer.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-train-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of training rpart depending on the framework.'}\n::: {.cell-output-display}\n|Framework  | LQ| Median| UQ|\n|:----------|--:|------:|--:|\n|base       | 11|     11| 12|\n|mlr3       | 23|     23| 24|\n|tidymodels | 18|     18| 19|\n:::\n:::\n\n::: {#tbl-train-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of training ranger depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |  LQ| Median|  UQ|\n|:----------|---:|------:|---:|\n|base       | 286|    322| 347|\n|mlr3       | 301|    335| 357|\n|tidymodels | 310|    342| 362|\n:::\n:::\n\n\n:::\n\n## Resample Sequential\n\nNext, we measure the runtime of the resample functions without parallelization.\nWe generate resampling splits for a 3-, 6- and 9-fold cross-validation.\nIn addition, we ran a 100 times repeated 3-fold cross-validation.\nBoth frameworks use the same resampling splits.\nSince `tidymodels` always scores the resampling result, we do the same with `mlr3`.\nWe activate the saving of predictions in `tidymodels` because `mlr3` always saves them.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels resample\ncontrol = control_grid(save_pred = TRUE)\nmetrics = metric_set(accuracy)\n\nfit_resamples(tidymodels_workflow, folds, metrics = metrics, control = control)\n\n# mlr3 resample\nmeasure = msr(\"classif.acc\")\n\nrr = resample(task, learner, resampling)\nrr$score(measure)\n```\n:::\n\n\nThe resampling of the fast-fitting `rpart` model is faster with `mlr3` (@tbl-resample-sequential-rpart).\nBoth frameworks show a linear increase in runtime with the number of folds (@fig-resample-sequential).\nThe runtime of the `ranger` models is almost the same for both frameworks (@tbl-resample-sequential-ranger).\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-sequential-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of rpart depending on the framework and resampling strategy.'}\n::: {.cell-output-display}\n|Framework  |Resampling |    LQ| Median|    UQ|\n|:----------|:----------|-----:|------:|-----:|\n|mlr3       |cv3        |   188|    196|   210|\n|tidymodels |cv3        |   233|    242|   257|\n|mlr3       |cv6        |   343|    357|   379|\n|tidymodels |cv6        |   401|    415|   436|\n|mlr3       |cv9        |   500|    520|   548|\n|tidymodels |cv9        |   568|    588|   616|\n|mlr3       |rcv100     | 15526|  16023| 16777|\n|tidymodels |rcv100     | 16409|  16876| 17527|\n:::\n:::\n\n::: {#tbl-resample-sequential-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of ranger depending on the framework and resampling strategy.'}\n::: {.cell-output-display}\n|Framework  |Resampling |    LQ| Median|    UQ|\n|:----------|:----------|-----:|------:|-----:|\n|mlr3       |cv3        |   923|   1004|  1062|\n|tidymodels |cv3        |   916|    981|  1023|\n|mlr3       |cv6        |  1990|   2159|  2272|\n|tidymodels |cv6        |  2089|   2176|  2239|\n|mlr3       |cv9        |  3074|   3279|  3441|\n|tidymodels |cv9        |  3260|   3373|  3453|\n|mlr3       |rcv100     | 85909|  88642| 91381|\n|tidymodels |rcv100     | 87828|  88822| 89843|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime in milliseconds of a cross-validation of rpart (left) and ranger (right) depending on the framework and number of folds.](index_files/figure-html/fig-resample-sequential-1.png){#fig-resample-sequential fig-align='center' width=672}\n:::\n:::\n\n\n## Resample Parallel\n\nWe run the resampling functions again but this time with parallelization.\nThe `doFuture` and `doParallel` packages are the best-supported parallelization packages for `tidymodels`.\nThe `mlr3` package uses the `future` package for parallelization.\n\nThe average runtime of the `mlr3` resampling function with `future` parallelization is shown in @tbl-resample-parallel-mlr3-future-rpart and @tbl-resample-parallel-mlr3-future-ranger.\nThe runtime only increases slightly when the number of folds is doubled.\nThis means that we see a large parallelization overhead for starting the workers.\nFor `rpart`, the parallelization overhead exceeds the speedup (left @fig-resample-parallel).\nFor `ranger`, using parallelization is faster than running the sequential version (right @fig-resample-parallel).\n\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-parallel-mlr3-future-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of mlr3 with future and rpart depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |   LQ| Median|   UQ|\n|:----------|----:|------:|----:|\n|cv3        |  625|    655|  703|\n|cv6        |  738|    771|  817|\n|cv9        |  831|    875|  923|\n|rcv100     | 8620|   9043| 9532|\n:::\n:::\n\n::: {#tbl-resample-parallel-mlr3-future-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of mlr3 with future and ranger depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |   836|    884|   943|\n|cv6        |  1200|   1249|  1314|\n|cv9        |  1577|   1634|  1706|\n|rcv100     | 32047|  32483| 33022|\n:::\n:::\n\n\n:::\n\nThe `tidymodels` package with `doFuture` is much slower than `mlr3` with `future` (@tbl-resample-parallel-tidymodels-future-rpart and @tbl-resample-parallel-tidymodels-future-ranger).\nWe observed that `tidymodels` exports more data to the workers than `mlr3`.\nThis might be the reason for the slower runtime.\n\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-parallel-tidymodels-future-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of tidymodels with doFuture and rpart depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |   LQ| Median|   UQ|\n|:----------|----:|------:|----:|\n|cv3        | 2778|   2817| 3019|\n|cv6        | 2808|   2856| 3033|\n|cv9        | 2935|   2975| 3170|\n|rcv100     | 9154|   9302| 9489|\n:::\n:::\n\n::: {#tbl-resample-parallel-tidymodels-future-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of tidymodels with doFuture and ranger depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |  2982|   3046|  3234|\n|cv6        |  3282|   3366|  3543|\n|cv9        |  3568|   3695|  3869|\n|rcv100     | 27546|  27843| 28166|\n:::\n:::\n\n\n:::\n\nThe `doParallel` package works better for these rather small resampling tasks.\nThe resampling is always a bit faster than `mlr3` but for `rpart` it can also not beat the sequential version (left @fig-resample-parallel).\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-resample-parallel-tidymodels-parallel-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of tidymodels with doParallel and rpart depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |   557|    649|   863|\n|cv6        |   602|    714|   910|\n|cv9        |   661|    772|   968|\n|rcv100     | 10609|  10820| 11071|\n:::\n:::\n\n::: {#tbl-resample-parallel-tidymodels-parallel-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in milliseconds of tidymodels with doParallel and ranger depending on the resampling strategy.'}\n::: {.cell-output-display}\n|Resampling |    LQ| Median|    UQ|\n|:----------|-----:|------:|-----:|\n|cv3        |   684|    756|   948|\n|cv6        |  1007|   1099|  1272|\n|cv9        |  1360|   1461|  1625|\n|rcv100     | 31205|  31486| 31793|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime in milliseconds of a cross-validation of rpart (left) and ranger (right) depending on the framework, number of folds and parallelization.](index_files/figure-html/fig-resample-parallel-1.png){#fig-resample-parallel fig-align='center' width=672}\n:::\n:::\n\n\nFor repeated cross-validation, it is beneficial to use parallelization (@fig-resample-parallel-2).\nAll frameworks are faster with parallelization.\nFor these larger resamplings, the `doFuture` package is faster than `doParallel`.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime in seconds of a 100 times repeated 3-fold cross-validation of rpart (left) and ranger (right) depending on the framework and parallelization.](index_files/figure-html/fig-resample-parallel-2-1.png){#fig-resample-parallel-2 fig-align='center' width=672}\n:::\n:::\n\n\n## Tune Seuqential\n\nNow we measure the runtime of the tune functions.\nThe `tidymodels` package evaluates a predefined grid of points.\nWe use the `\"design_points\"` tuner of the `mlr3tuning` package to evaluate the same grid.\nThe grid contains 200 points for the `rpart` model and 200 points for the `ranger` model.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = decision_tree(\n  cost_complexity = tune()) %>%\n  set_engine(\"rpart\",\n    xval = 0) %>%\n  set_mode(\"classification\")\n\ntm_design = data.table(\n  cost_complexity = seq(0.1, 0.2, length.out = 200))\n\n# mlr3\nlearner = lrn(\"classif.rpart\",\n  xval = 0,\n  cp = to_tune())\n\nmlr3_design = data.table(\n  cp = seq(0.1, 0.2, length.out = 200))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels\ntm_mod = rand_forest(\n  trees = tune()) %>%\n  set_engine(\"ranger\",\n    num.threads = 1L,\n    seed = 1) %>%\n  set_mode(\"classification\")\n\ntm_design = data.table(\n  trees = seq(1000, 1199))\n\n# mlr3\nlearner = lrn(\"classif.ranger\",\n  num.trees = to_tune(1, 10000),\n  num.threads = 1L,\n  seed = 1,\n  verbose = FALSE,\n  predict_type = \"prob\")\n\nmlr3_design = data.table(\n  num.trees = seq(1000, 1199))\n```\n:::\n\n\n:::\n\nWe measure the runtime of the tune functions.\nBoth runs return the best hyperparameter configuration.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# tidymodels tune\ntune::tune_grid(\n  tm_wf,\n  resamples = resamples,\n  grid = design,\n  metrics = metrics)\n\n# mlr3 tune\ntuner = tnr(\"design_points\", design = design, batch_size = nrow(design))\nmlr3tuning::tune(\n  tuner = tuner,\n  task = task,\n  learner = learner,\n  resampling = resampling,\n  measures = measure,\n  store_benchmark_result = FALSE)\n```\n:::\n\n\nRunning the tuning sequentially is faster with `mlr3` (@tbl-tune-sequential-rpart and @tbl-tune-sequential-ranger).\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-tune-sequential-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of rpart depending on the framework.'}\n::: {.cell-output-display}\n|Framework  | LQ| Median| UQ|\n|:----------|--:|------:|--:|\n|mlr3       | 27|     27| 28|\n|tidymodels | 37|     37| 39|\n:::\n:::\n\n::: {#tbl-tune-sequential-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of ranger depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |  LQ| Median|  UQ|\n|:----------|---:|------:|---:|\n|mlr3       | 167|    171| 175|\n|tidymodels | 194|    195| 196|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## Tune Parallel\n\nFinally, we measure the runtime of the tune functions with parallelization.\nThe parallelization runs on 3 cores.\nWe use the largest possible chunk size for `mlr3`.\nThis means the workers get all points at once which minimizes the parallelization overhead.\nThe `tidymodels` package uses the same chunk size but sets it internally.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptions(\"mlr3.exec_chunk_size\" = 200)\n```\n:::\n\n\nThe runtimes of `mlr3` and `tidymodels` are very similar.\nThe `mlr3` package is slightly faster with `rpart` (@tbl-tune-parallel-mlr3-future-rpart) and slower with `ranger` (@tbl-tune-parallel-mlr3-future-ranger).\nSince we are using a 3-fold cross-validation, the `doParallel` package is faster than `doFuture` (@fig-tune-parallel).\nRegardless of the framework-backend combination, it is worth activating parallelization.\n\n:::{layout-ncol=\"2\"}\n\n\n::: {#tbl-tune-parallel-mlr3-future-rpart .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of rpart depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |Backend    | LQ| Median| UQ|\n|:----------|:----------|--:|------:|--:|\n|mlr3       |future     | 11|     12| 12|\n|tidymodels |doFuture   | 17|     17| 17|\n|tidymodels |doParallel | 13|     13| 13|\n:::\n:::\n\n::: {#tbl-tune-parallel-mlr3-future-ranger .cell layout-align=\"center\" tbl-cap='Average runtime in seconds of tuning 200 points of ranger depending on the framework.'}\n::: {.cell-output-display}\n|Framework  |Backend    | LQ| Median| UQ|\n|:----------|:----------|--:|------:|--:|\n|mlr3       |future     | 54|     55| 55|\n|tidymodels |doFuture   | 58|     58| 59|\n|tidymodels |doParallel | 54|     54| 55|\n:::\n:::\n\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Average runtime in seconds of tuning 200 points of rpart (left) and ranger (right) depending on the framework and parallelization.](index_files/figure-html/fig-tune-parallel-1.png){#fig-tune-parallel fig-align='center' width=672}\n:::\n:::\n\n\n# Conclusion\n\nWe cannot identify a clear winner.\nBoth frameworks have a similar runtime for training, resampling and tuning.\nThe relative overhead of using a framework is high for `rpart` because the training time is short.\nFor slow-fitting models like `ranger`, the relative overhead becomes negligible.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}