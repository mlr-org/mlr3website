{
  "hash": "206a337b8ccf4770c5416af93551e42d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Wrapper-based Ensemble Feature Selection\"\ndescription: |\n  Find the most stable and predictive features using multiple learners and resampling techniques.\nauthor:\n  - name: John Zobolas\n    orcid: 0000-0002-3609-8674\n    url: https://github.com/bblodfon\ndate: 2025-01-12\nbibliography: ../../bibliography.bib\n---\n\n\n\n\n\n\n\n## Intro\n\nSome papers from which we draw the ideas for this tutorial:\n\n- **Stability selection**, i.e. drawing multiple subsamples from a dataset and performing feature selection on each [@Meinshausen2010].\nStability selection helps ensure that the selected features are robust to variations in the training data, increasing the reliability of the feature selection process.\n- The **ensemble idea** for feature selection, i.e. using multiple methods or models to perform feature selection on a dataset [@Saeys2008].\nThis combines the strengths of different approaches to achieve more comprehensive results and alleviates biases that may arise from each individual approach for feature selection.\n\nIn this post we will show how we can use the `mlr3fselect` R package to perform *wrapped*-based ensemble feature selection on a given dataset.\nWrapper-based ensemble feature selection involves applying stability selection techniques (resampling of the data) to create robust feature subsets by leveraging multiple ML models in wrapper-based feature selection strategies.\n\n:::{.callout-note}\nWe also support *embedded-based* ensemble feature selection, see the function [`mlr3fselect::embedded_ensemble_fselect()`](https://mlr3fselect.mlr-org.com/reference/embedded_ensemble_fselect.html) for more details.\n:::\n\n## Libraries\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(fastVoteR) # for feature ranking\nlibrary(ggplot2)\nlibrary(future)\nlibrary(progressr)\n```\n:::\n\n\n\n## Dataset\n\nWe will use the `sonar` dataset, which is a binary classification task:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"sonar\")\ntask\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:sonar> (208 x 61): Sonar: Mines vs. Rocks\n* Target: Class\n* Properties: twoclass\n* Features (60):\n  - dbl (60): V1, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V2, V20, V21, V22, V23, V24, V25, V26,\n    V27, V28, V29, V3, V30, V31, V32, V33, V34, V35, V36, V37, V38, V39, V4, V40, V41, V42, V43, V44, V45,\n    V46, V47, V48, V49, V5, V50, V51, V52, V53, V54, V55, V56, V57, V58, V59, V6, V60, V7, V8, V9\n```\n\n\n:::\n:::\n\n\n\n## EFS Workflow\n\nThe **ensemble feature selection (EFS)** workflow is the following (in parentheses we provide the arguments for the [`mlr3fselect::ensemble_fselect()`](https://mlr3fselect.mlr-org.com/reference/ensemble_fselect.html) function that implements this process):\n\n1. Repeatedly split a dataset to **train/test sets** (`init_resampling`), e.g. by subsampling $B$ times.\n2. Choose $M$ **learners** (`learners`).  \n3. Perform **wrapped-based feature selection** on each train set from (1) using each of the models from (2).\nThis process results in a 'best' feature (sub)set and a final trained model using these best features, for each combination of train set and learner ($B \\times M$ combinations in total).\n4. Score the final models on the respective test sets.\n\nTo guide the feature selection process (3) we need to choose:\n\n- An optimization algorithm (`fselector`), e.g. Recursive Feature Elimination (RFE)\n- An inner resampling technique (`inner_resampling`), e.g. 5-fold cross-validation (CV)\n- An inner measure (`inner_measure`), e.g. classification error\n- A stopping criterion for the feature selection (`terminator`), i.e. how many iterations should the optimization algorithm run\n\n:::{.callout-note}\nThe `inner_measure` (used for finding the best feature subset in each train set) and `measure` (assesses performance on each test set) can be different.\n:::\n\n### Parallelization\n\nInternally, `ensemble_fselect()` performs a full `mlr3::benchmark()`, the results of which can be stored with the argument `store_benchmark_result`.\nThe process is fully parallelizable, where **every job is a (train set, learner) combination**.\nSo it's better to make sure that each RFE optimization (done via [mlr3fselect::auto_fselector](https://mlr3fselect.mlr-org.com/reference/auto_fselector.html)) is single-threaded.\n\nBelow we show the code that setups the configuration for the parallelization:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Logging\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\nlgr::get_logger(\"mlr3\" )$set_threshold(\"warn\")\n\n# Parallelization of XGBoost (use 1 core when training on a dataset)\nSys.setenv(OMP_NUM_THREADS = 1)\nSys.setenv(OMP_THREAD_LIMIT = 1)\nSys.setenv(MKL_NUM_THREADS = 1) # MKL is an Intel-specific thing\n# Package-specific settings\ntry(data.table::setDTthreads(1))\ntry(RhpcBLASctl::blas_set_num_threads(1))\ntry(RhpcBLASctl::omp_set_num_threads(1))\n\n# Progress bars\noptions(progressr.enable = TRUE)\nhandlers(global = TRUE)\nhandlers(\"progress\")\n\n# Parallelization for EFS: use 10 cores\nplan(\"multisession\", workers = 10)\n```\n:::\n\n\n\n### RFE\n\nFor each (train set, learner) combination we will run a Recursive Feature Elimination (RFE) optimization algorithm.\nWe configure the algorithm to start with all features of the `task`, remove the 80% less important features in each iteration, and stop when 2 features are reached.\nIn each RFE iteration, a 5-fold CV resampling of the given `task` takes place and a `learner` is trained and used for prediction on the test folds.\nThe outcome of each RFE iteration is the average CV error (performance estimate) and the feature importances (by default the average of the feature ranks from each fold).\nPractically, for the `sonar` dataset, we will have **15 RFE iterations**, with the following feature subset sizes:\n\n`60 48 38 30 24 19 15 12 10 8  6  5  4  3  2`\n\nThe best feature set will be chosen as the one with the **lowest 5-fold CV error**. i.e. the best performance estimate in the inner resampling.\n\nIn `mlr3` code, we specify the RFE `fselector` as:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrfe = fs(\"rfe\", n_features = 2, feature_fraction = 0.8)\n```\n:::\n\n\n\nSee [this gallery post](https://mlr-org.com/gallery/optimization/2023-02-07-recursive-feature-elimination/) for more details on RFE optimization.\n\n:::{.callout-note}\n- Using RFE as the feature selection optimization algorithm means that all `learners` need to have the `\"importance\"` property.\n:::\n\n### Learners\n\nWe define a `list()` with the following classification `learners` (parameters are set at default values): \n\n1. XGBoost with early stopping\n2. A tree\n3. A random forest (RF)\n4. A Support Vector Machine (SVM)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmax_nrounds = 500\n\nlearners = list(\n  lrn(\"classif.xgboost\", id = \"xgb\", nrounds = max_nrounds,\n      early_stopping_rounds = 20, validate = \"test\"),\n  lrn(\"classif.rpart\", id = \"tree\"),\n  lrn(\"classif.ranger\", id = \"rf\", importance = \"permutation\"),\n  lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\", kernel = \"linear\")\n)\n```\n:::\n\n\n\n:::{.callout-note}\nIt is possible to perform tuning while also performing wrapper-based feature selection. This practically means that we would use an `AutoTuner` learner with its own inner resampling scheme and tuning space in the above list.\nThe whole process would then be a double (nested) cross-validation with outer loop the $B$ subsample iterations, which is computationally taxing.\nModels that need minimum to no tuning (e.g. like Random Forests) are therefore ideal candidates for wrapper-based ensemble feature selection.\n:::\n\n### Callbacks\n\nSince SVM doesn't support `importance` scores by itself, we convert the coefficients of the trained linear SVM model to importance scores via a callback:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsvm_rfe = clbk(\"mlr3fselect.svm_rfe\")\nsvm_rfe\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<CallbackBatchFSelect:mlr3fselect.svm_rfe>: SVM-RFE Callback\n* Active Stages: on_optimization_begin\n```\n\n\n:::\n:::\n\n\n\n---\n\nAlso, since the XGBoost learner performs **internal tuning via early stopping**, where the test folds in the inner cross-validation resampling scheme act as validation sets, we need to define the following callback:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninternal_ss = ps(\n  nrounds = p_int(upper = max_nrounds, aggr = function(x) as.integer(mean(unlist(x))))\n)\n\nxgb_clbk = clbk(\"mlr3fselect.internal_tuning\", internal_search_space = internal_ss)\nxgb_clbk\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<CallbackBatchFSelect:mlr3fselect.internal_tuning>: Internal Tuning\n* Active Stages: on_auto_fselector_after_final_model, on_auto_fselector_before_final_model,\n  on_eval_before_archive, on_optimization_end\n```\n\n\n:::\n:::\n\n\n\nThis practically sets the boosting rounds of the final XGBoost model (after the RFE optimization is finished) as the average boosting rounds from each subsequent training fold (corresponding to the model trained with the 'best' feature subset).\nFor example, since we're performing a 5-fold inner CV, we would have 5 different early-stopped boosting `nrounds`, from which we will use the average value to train the final XGBoost model using the whole train set.\n\n---\n\nFor all learners we will prefer **sparser models during the RFE optimization process**.\nThis means that across all RFE iterations, we will choose as 'best' feature subset the one that has the minimum number of features and its performance is **within one standard error** of the feature set with the best performance (e.g. the lowest classification error).\nThis can be achieved with the following callback:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\none_se_clbk = clbk(\"mlr3fselect.one_se_rule\")\none_se_clbk\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<CallbackBatchFSelect:mlr3fselect.one_se_rule>: One Standard Error Rule Callback\n* Active Stages: on_optimization_end\n```\n\n\n:::\n:::\n\n\n\n## Execute EFS\n\nUsing the [`mlr3fselect::ensemble_fselect()`](https://mlr3fselect.mlr-org.com/reference/ensemble_fselect.html) function, we split the `sonar` task to $B = 50$ subsamples (each corresponding to a 80%/20% train/test set split) and perform RFE in each train set using each of the $M = 4$ learners.\n\nFor a particular (train set, learner) combination, the RFE process will evaluate the $15$ feature subsets mentioned above.\nUsing the inner 5-fold CV resampling scheme, the average CV classification error will be used to find the best feature subset.\nUsing only features from this best feature set, a final model will be trained using all the observations from each trained set.\nLastly, the performance of this final model will be assessed on the corresponding test set using the classification accuracy metric.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\nefs = ensemble_fselect(\n  fselector = rfe,\n  task = task,\n  learners = learners,\n  init_resampling = rsmp(\"subsampling\", repeats = 50, ratio = 0.8),\n  inner_resampling = rsmp(\"cv\", folds = 5),\n  inner_measure = msr(\"classif.ce\"),\n  measure = msr(\"classif.acc\"),\n  terminator = trm(\"none\"),\n  # following list must be named with the learners' ids\n  callbacks = list(\n    xgb  = list(one_se_clbk, xgb_clbk),\n    tree = list(one_se_clbk),\n    rf   = list(one_se_clbk),\n    svm  = list(one_se_clbk, svm_rfe)\n  ),\n  store_benchmark_result = FALSE\n)\n```\n:::\n\n\n\nThe result is stored in an [`EnsembleFSResult`](https://mlr3fselect.mlr-org.com/reference/ensemble_fs_result.html) object, which can use to visualize the results, rank the features and assess the stability of the ensemble feature selection process, among others.\n\n# Analyze EFS Results\n\n## Result Object \n\nPrinting the result object provides some initial information:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(efs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<EnsembleFSResult> with 4 learners and 50 initial resamplings\n     resampling_iteration    learner_id n_features\n                    <int>        <char>      <int>\n  1:                    1 xgb.fselector          8\n  2:                    2 xgb.fselector         30\n  3:                    3 xgb.fselector         60\n  4:                    4 xgb.fselector         19\n  5:                    5 xgb.fselector          6\n ---                                              \n196:                   46 svm.fselector         24\n197:                   47 svm.fselector         19\n198:                   48 svm.fselector         15\n199:                   49 svm.fselector         19\n200:                   50 svm.fselector          8\n```\n\n\n:::\n:::\n\n\n\nAs we can see, we have $M \\times B = 4 \\times 50 = 200$ (train set, learner) combinations.\nWe can inspect the actual `data.table` result:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nefs$result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        learner_id resampling_iteration classif.acc                    features n_features classif.ce_inner\n            <char>                <int>       <num>                      <list>      <int>            <num>\n  1: xgb.fselector                    1   0.6904762  V11,V12,V16,V21,V36,V4,...          8        0.1081996\n  2: xgb.fselector                    2   0.9285714  V1,V10,V11,V12,V13,V15,...         30        0.1627451\n  3: xgb.fselector                    3   0.8333333  V1,V10,V11,V12,V13,V14,...         60        0.1203209\n  4: xgb.fselector                    4   0.8571429 V11,V12,V15,V16,V21,V27,...         19        0.1447415\n  5: xgb.fselector                    5   0.7142857     V10,V11,V16,V31,V36,V45          6        0.1319073\n ---                                                                                                       \n196: svm.fselector                   46   0.7619048  V1,V11,V12,V14,V23,V25,...         24        0.1440285\n197: svm.fselector                   47   0.7380952 V11,V15,V23,V30,V31,V33,...         19        0.1331551\n198: svm.fselector                   48   0.7619048 V12,V17,V31,V32,V36,V37,...         15        0.1809269\n199: svm.fselector                   49   0.7142857 V11,V12,V14,V17,V20,V24,...         19        0.1864528\n200: svm.fselector                   50   0.7619048 V11,V14,V23,V36,V39,V40,...          8        0.1629234\n                            importance\n                                <list>\n  1:       6.8,6.8,6.4,3.8,3.6,3.4,...\n  2: 27.2,26.8,25.6,22.0,22.0,21.6,...\n  3: 60.0,57.6,53.8,52.0,51.4,49.6,...\n  4: 19.0,13.4,13.0,12.8,12.2,11.0,...\n  5:           4.4,4.2,4.2,3.8,2.2,2.2\n ---                                  \n196: 24.0,21.6,20.2,18.4,17.6,16.8,...\n197: 16.6,16.2,16.2,15.0,14.8,14.4,...\n198: 14.0,13.6,13.0,10.4, 9.8, 9.2,...\n199: 17.6,16.0,15.6,14.4,13.4,12.8,...\n200:       8.0,5.8,5.6,5.2,4.6,3.2,...\n```\n\n\n:::\n:::\n\n\n\nFor each learner (`\"learner_id\"`) and dataset subsample (`\"resampling_iteration\"`) we get:\n\n- The 'best' feature subsets (`\"features\"`)\n- The number of 'best' features (`\"nfeatures\"`)\n- The importances for these 'best' features (`\"importance\"`) - this output column we get only because RFE optimization was used\n- The inner optimization performance scores on the train sets (`\"classif.ce_inner\"`)\n- The performance scores on the test sets (`\"classif.acc\"`)\n\nSince there are two ways in this process to evaluate performance, we can always check which is the **active measure**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nefs$active_measure\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"outer\"\n```\n\n\n:::\n:::\n\n\n\nBy default the active measure is the `\"outer\"`, i.e. the measure used to evaluate each learner's performance in the test sets.\nIn our case that was the classification accuracy:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nefs$measure\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<MeasureClassifSimple:classif.acc>: Classification Accuracy\n* Packages: mlr3, mlr3measures\n* Range: [0, 1]\n* Minimize: FALSE\n* Average: macro\n* Parameters: list()\n* Properties: -\n* Predict type: response\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-note}\nIn the following sections we can use the inner optimization scores (i.e. `\"classif.ce_inner\"`) by executing `efs$set_active_measure(\"inner\")`.\nThis affects all methods and plots that use performance scores.\n:::\n\n## Performance\n\nWe can view the **performance scores of the different learners** used in the ensemble feature selection process.\nEach box represents the distribution of scores across different resampling iterations for a particular learner.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(efs, type = \"performance\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/efs-015-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nWe observe that RF has better classification accuracy on the test sets of the $50$ subsamples, followed by XGBoost, then the SVM and last the tree model.\n\n## Number of Selected Features\n\nContinuing, we can plot **the number of features selected by each learner** in the different resampling iterations:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(efs, type = \"n_features\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_y_continuous(breaks = seq(0, 60, 10))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/efs-016-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nWe observe that RF needed more features to achieve the best average performance, followed by SVM, then XGBoost and the tree model was the model using the least features (but with worst performance).\n\n## Pareto Plot\n\nBoth performance scores and number of features selected by the RFE optimization process can be visualized jointly in the Pareto plot.\nHere we also draw the **Pareto front**, i.e. the set of points that represent the trade-off between the number of features and performance (classification accuracy).\nAs we see below, these points are derived from multiple learners and resamplings:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(efs, type = \"pareto\", theme = theme_minimal(base_size = 14)) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Empirical Pareto front\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/efs-017-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nWe can also draw an **estimated Pareto front curve** by fitting a linear model with the inverse of the number of selected features ($1/x$) of the empirical Pareto front as input, and the associated performance scores as output:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(efs, type = \"pareto\", pareto_front = \"estimated\", \n         theme = theme_minimal(base_size = 14)) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Estimated Pareto front\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/efs-018-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Knee Point Identification\n\nNo matter the type of Pareto front that we chose, specialized methods are available to identify **knee points**, i.e. points of the Pareto front with an **optimal trade-off between performance and number of selected features**.\n\nBy default, we use the geometry-based *Normal-Boundary Intersection* (NBI) method.\nThis approach calculates the perpendicular distance of each point from the line connecting the first (worst performance, minimum number of features) and last (best performance, maximum number of features) point of the Pareto front.\nThe knee point is then identified as the point with the maximum distance from this line [@Das1999].\n\nUsing the empirical and estimated Pareto fronts, we observe that the optimal knee points correspond to different numbers of features:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nefs$knee_points()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   n_features classif.acc\n        <num>       <num>\n1:         10   0.9047619\n```\n\n\n:::\n\n```{.r .cell-code}\nefs$knee_points(type = \"estimated\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   n_features classif.acc\n        <int>       <num>\n1:          8   0.8597253\n```\n\n\n:::\n:::\n\n\n\n:::{.callout-tip title=\"Number of features cutoff\"}\nThe number of features at the identified knee point provides a cutoff for prioritizing features when working with a ranked feature list (see \"Feature Ranking\" section).\n:::\n\n## Stability\n\nThe `stabm` R package [@Bommert2021] implements many measures for the assessment of the **stability of feature selection**, i.e. the similarity between the selected feature sets (`\"features\"` column in the `EnsembleFSResult` object).\nWe can use these measures to assess and visualize the stability across all resampling iterations and learners (`global = \"TRUE\"`) or per each learner separately (`global = \"FALSE\"`).\n\nThe default stability measure is the **Jaccard Index**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nefs$stability(stability_measure = \"jaccard\", global = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2640504\n```\n\n\n:::\n:::\n\n\n\nStability per learner:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nefs$stability(stability_measure = \"jaccard\", global = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n xgb.fselector tree.fselector   rf.fselector  svm.fselector \n     0.3657964      0.3554681      0.4716744      0.3119381 \n```\n\n\n:::\n:::\n\n\n\nWe observe that the RF model was the most stable in identifying similar predictive features across the different subsamples of the dataset, while the SVM model the least stable.\n\nTo visualize stability, the following code generates a stability barplot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(efs, type = \"stability\", theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/efs-022-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\nAlternatively, the **Nogueira** stability measure can be used, which unlike the Jaccard Index, it's a chance-corrected similarity measure [@Nogueira2018]:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(efs, type = \"stability\", stability_measure = \"nogueira\", \n         stability_args = list(p = task$n_features), \n         theme = theme_minimal(base_size = 14)) +\n  scale_fill_brewer(palette = \"Set1\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/efs-023-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Feature Ranking\n\nUsing the Pareto method, we demonstrated how we can identify a reasonable cutoff for the number of selected features.\nNow we will focus on how to create a consensus ranked feature list based on the results of the ensemble feature selection.\n\nThe most straightforward ranking is obtained by counting how often each feature appears in the 'best' feature subsets (`\"features\"`).\nBelow we show the top 8 features, i.e. up to the cutoff derived from the knee point of the estimated Pareto front.\nThe column `\"score\"` represents these counts, while the column `\"norm_score\"` is the **feature selection frequency** or also known as **selection probability** [@Meinshausen2010]:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nefs$feature_ranking(method = \"av\", use_weights = FALSE, committee_size = 8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   feature score norm_score borda_score\n    <char> <num>      <num>       <num>\n1:     V12   179      0.895   1.0000000\n2:     V11   170      0.850   0.9830508\n3:      V9   123      0.615   0.9661017\n4:     V45   121      0.605   0.9491525\n5:     V16   118      0.590   0.9322034\n6:     V36   113      0.565   0.9152542\n7:     V49   104      0.520   0.8983051\n8:      V4    99      0.495   0.8813559\n```\n\n\n:::\n:::\n\n\n\nIn the language of Voting Theory, we call the method that generates these counts *approval voting* (`method = \"av\"`) [@Lackner2023].\nUsing this framework, learners act as *voters*, features act as *candidates* and voters select certain candidates (features).\nThe primary objective is to compile these selections into a consensus ranked list of features (a committee).\nThe `committee_size` specifies how many (top-ranked) features to return.\n\nInternally, `$feature_ranking()` uses the [`fastVoteR`](https://bblodfon.github.io/fastVoteR/) R package, which supports more advanced ranking methods.\nFor example, we can perform **weighted ranking**, by considering the varying performance (accuracy) of each learner.\nThis results in the same top 8 features but with slightly different ordering:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nefs$feature_ranking(method = \"av\", use_weights = TRUE, committee_size = 8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   feature     score norm_score borda_score\n    <char>     <num>      <num>       <num>\n1:     V12 134.78571  0.8995710   1.0000000\n2:     V11 127.33333  0.8498331   0.9830508\n3:     V45  94.45238  0.6303830   0.9661017\n4:      V9  93.71429  0.6254569   0.9491525\n5:     V16  89.76190  0.5990783   0.9322034\n6:     V36  87.97619  0.5871603   0.9152542\n7:     V49  80.64286  0.5382171   0.8983051\n8:      V4  76.64286  0.5115207   0.8813559\n```\n\n\n:::\n:::\n\n\n\nAdditionally, alternative ranking methods are supported.\nBelow, we use *satisfaction approval voting* (SAV), which ranks features by normalizing approval scores based on the number of features a model has selected.\nSpecifically, models that select more features distribute their \"approval\" across a larger set, reducing the contribution to each selected feature.\nConversely, **features chosen by models with fewer selected features receive higher weights**, as their selection reflects stronger individual importance.\nThis approach ensures that sparsely selected features are prioritized in the ranking, leading to a different set of top-ranked features compared to standard approval voting.\nFor instance, in the example above, the `\"V10\"` feature enters the top 8 features, replacing `\"V4\"`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nefs$feature_ranking(method = \"sav\", committee_size = 8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   feature     score norm_score borda_score\n    <char>     <num>      <num>       <num>\n1:     V11 15.353545  0.9100050   1.0000000\n2:     V12 14.107691  0.8361632   0.9830508\n3:     V16  7.698460  0.4562879   0.9661017\n4:     V45  6.811607  0.4037241   0.9491525\n5:      V9  6.443311  0.3818952   0.9322034\n6:     V36  6.060615  0.3592128   0.9152542\n7:     V10  5.955446  0.3529794   0.8983051\n8:     V49  4.741014  0.2810000   0.8813559\n```\n\n\n:::\n:::\n\n\n\n# EFS-based Feature Selection\n\nThe ultimate goal of the ensemble feature selection process is to identify predictive and stable features.\nBy combining the ranked feature list with the Pareto-derived cutoff, we can select the final set of features and subset the original dataset for further modeling:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn_features = efs$knee_points(type = \"estimated\")$n_features\nres = efs$feature_ranking(method = \"sav\", committee_size = n_features)\ntask$select(cols = res$feature)\ntask\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<TaskClassif:sonar> (208 x 9): Sonar: Mines vs. Rocks\n* Target: Class\n* Properties: twoclass\n* Features (8):\n  - dbl (8): V10, V11, V12, V16, V36, V45, V49, V9\n```\n\n\n:::\n:::\n\n\n# Session Information\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessioninfo::session_info(info = \"packages\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n═ Session info ═══════════════════════════════════════════════════════════════════════════════════════════════════════\n─ Packages ───────────────────────────────────────────────────────────────────────────────────────────────────────────\n ! package          * version     date (UTC) lib source\n P backports          1.5.0       2024-05-23 [?] CRAN (R 4.4.2)\n P bbotk              1.5.0       2024-12-17 [?] CRAN (R 4.4.2)\n P checkmate          2.3.2       2024-07-29 [?] CRAN (R 4.4.2)\n P class              7.3-22      2023-05-03 [?] CRAN (R 4.4.2)\n P cli                3.6.3       2024-06-21 [?] CRAN (R 4.4.2)\n P clue               0.3-66      2024-11-13 [?] CRAN (R 4.4.2)\n P cluster            2.1.6       2023-12-01 [?] CRAN (R 4.4.2)\n P codetools          0.2-20      2024-03-31 [?] CRAN (R 4.4.2)\n P colorspace         2.1-1       2024-07-26 [?] CRAN (R 4.4.2)\n P crayon             1.5.3       2024-06-20 [?] CRAN (R 4.4.2)\n P data.table       * 1.16.4      2024-12-06 [?] CRAN (R 4.4.2)\n P DEoptimR           1.1-3-1     2024-11-23 [?] CRAN (R 4.4.2)\n P digest             0.6.37      2024-08-19 [?] CRAN (R 4.4.2)\n P diptest            0.77-1      2024-04-10 [?] CRAN (R 4.4.2)\n P evaluate           1.0.1       2024-10-10 [?] CRAN (R 4.4.2)\n P farver             2.1.2       2024-05-13 [?] CRAN (R 4.4.2)\n P fastmap            1.2.0       2024-05-15 [?] CRAN (R 4.4.2)\n P fastVoteR        * 0.0.1       2024-11-27 [?] RSPM\n P flexmix            2.3-19      2023-03-16 [?] CRAN (R 4.4.2)\n P fpc                2.2-13      2024-09-24 [?] CRAN (R 4.4.2)\n P future           * 1.34.0      2024-07-29 [?] CRAN (R 4.4.2)\n P ggplot2          * 3.5.1       2024-04-23 [?] CRAN (R 4.4.2)\n P globals            0.16.3      2024-03-08 [?] CRAN (R 4.4.2)\n P glue               1.8.0       2024-09-30 [?] CRAN (R 4.4.2)\n P gtable             0.3.6       2024-10-25 [?] CRAN (R 4.4.2)\n P htmltools          0.5.8.1     2024-04-04 [?] CRAN (R 4.4.2)\n P jsonlite           1.8.9       2024-09-20 [?] CRAN (R 4.4.2)\n P kernlab            0.9-33      2024-08-13 [?] CRAN (R 4.4.2)\n P knitr              1.49        2024-11-08 [?] CRAN (R 4.4.2)\n P labeling           0.4.3       2023-08-29 [?] CRAN (R 4.4.2)\n P lattice            0.22-6      2024-03-20 [?] CRAN (R 4.4.2)\n P lgr                0.4.4       2022-09-05 [?] CRAN (R 4.4.2)\n P lifecycle          1.0.4       2023-11-07 [?] CRAN (R 4.4.2)\n P listenv            0.9.1       2024-01-29 [?] CRAN (R 4.4.2)\n P magrittr           2.0.3       2022-03-30 [?] CRAN (R 4.4.2)\n P MASS               7.3-61      2024-06-13 [?] CRAN (R 4.4.2)\n P Matrix             1.7-1       2024-10-18 [?] CRAN (R 4.4.2)\n P mclust             6.1.1       2024-04-29 [?] CRAN (R 4.4.2)\n P mlr3             * 0.22.1      2024-11-27 [?] CRAN (R 4.4.2)\n P mlr3cluster        0.1.10      2024-10-03 [?] CRAN (R 4.4.2)\n P mlr3data           0.9.0       2024-11-08 [?] CRAN (R 4.4.2)\n P mlr3filters        0.8.1       2024-11-08 [?] CRAN (R 4.4.2)\n P mlr3fselect        1.2.1.9000  2024-12-16 [?] Github (mlr-org/mlr3fselect@ab6360a)\n P mlr3hyperband      0.6.0       2024-06-29 [?] CRAN (R 4.4.2)\n P mlr3learners       0.9.0       2024-11-23 [?] CRAN (R 4.4.2)\n P mlr3mbo            0.2.8       2024-11-21 [?] CRAN (R 4.4.2)\n P mlr3measures       1.0.0       2024-09-11 [?] CRAN (R 4.4.2)\n P mlr3misc           0.16.0      2024-11-28 [?] CRAN (R 4.4.2)\n P mlr3pipelines      0.7.1       2024-11-14 [?] CRAN (R 4.4.2)\n P mlr3tuning         1.3.0       2024-12-17 [?] CRAN (R 4.4.2)\n P mlr3tuningspaces   0.5.2       2024-11-22 [?] CRAN (R 4.4.2)\n P mlr3verse        * 0.3.0       2024-06-30 [?] CRAN (R 4.4.2)\n P mlr3viz            0.10.0.9000 2024-12-30 [?] Github (mlr-org/mlr3viz@b96b886)\n P modeltools         0.2-23      2020-03-05 [?] CRAN (R 4.4.2)\n P munsell            0.5.1       2024-04-01 [?] CRAN (R 4.4.2)\n P nnet               7.3-19      2023-05-03 [?] CRAN (R 4.4.2)\n P palmerpenguins     0.1.1       2022-08-15 [?] CRAN (R 4.4.2)\n P paradox            1.0.1       2024-07-09 [?] CRAN (R 4.4.2)\n P parallelly         1.41.0      2024-12-18 [?] CRAN (R 4.4.2)\n P pillar             1.10.0      2024-12-17 [?] CRAN (R 4.4.2)\n P pkgconfig          2.0.3       2019-09-22 [?] CRAN (R 4.4.2)\n P prabclus           2.3-4       2024-09-24 [?] CRAN (R 4.4.2)\n P progressr        * 0.15.1      2024-11-22 [?] CRAN (R 4.4.2)\n P R6                 2.5.1       2021-08-19 [?] CRAN (R 4.4.2)\n P RColorBrewer       1.1-3       2022-04-03 [?] CRAN (R 4.4.2)\n P Rcpp               1.0.13-1    2024-11-02 [?] CRAN (R 4.4.2)\n   renv               1.0.11      2024-10-12 [1] CRAN (R 4.4.2)\n P rlang              1.1.4       2024-06-04 [?] CRAN (R 4.4.2)\n P rmarkdown          2.29        2024-11-04 [?] CRAN (R 4.4.2)\n P robustbase         0.99-4-1    2024-09-27 [?] CRAN (R 4.4.2)\n P scales             1.3.0       2023-11-28 [?] CRAN (R 4.4.2)\n P sessioninfo        1.2.2       2021-12-06 [?] CRAN (R 4.4.2)\n P spacefillr         0.3.3       2024-05-22 [?] CRAN (R 4.4.2)\n P stabm              1.2.2       2023-04-04 [?] CRAN (R 4.4.2)\n P tibble             3.2.1       2023-03-20 [?] CRAN (R 4.4.2)\n P uuid               1.2-1       2024-07-29 [?] CRAN (R 4.4.2)\n P vctrs              0.6.5       2023-12-01 [?] CRAN (R 4.4.2)\n P withr              3.0.2       2024-10-28 [?] CRAN (R 4.4.2)\n P xfun               0.49        2024-10-31 [?] CRAN (R 4.4.2)\n P yaml               2.3.10      2024-07-26 [?] RSPM\n\n [1] /home/john/repos/mlr3-packages/mlr3website/mlr-org/renv/library/linux-ubuntu-focal/R-4.4/x86_64-pc-linux-gnu\n [2] /home/john/.cache/R/renv/sandbox/linux-ubuntu-focal/R-4.4/x86_64-pc-linux-gnu/db5e602d\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n\n\n\n\n## References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}