{
  "hash": "5a71f848d35190ebd9fba7b8a4080852",
  "result": {
    "markdown": "---\ntitle: House Prices in King County\ncategories:\n  - regression\n  - feature engineering\n  - tuning\n  - resampling\nauthor:\n  - name: Florian Pfisterer\ndate: 01-30-2020\ndescription: |\n  Use case illustrating data preprocessing and model fitting via mlr3 on the \"King County House Prices\" dataset.\nimage: thumbnail.png\nknitr:\n  opts_chunk:\n    fig.align: center\n---\n\n\n\n\nThe use-case illustrated below touches on the following concepts:\n\n- Data preprocessing\n- [Task](https://mlr3book.mlr-org.com/tasks.html)\n- [Fitting a learner](https://mlr3book.mlr-org.com/train-predict.html)\n- [Resampling](https://mlr3book.mlr-org.com/resampling.html)\n- [Tuning](https://mlr3book.mlr-org.com/tuning.html)\n\nThe relevant sections in the `mlr3book` are linked to for the reader's convenience.\n\nThis use case shows how to model housing price data in King County.\nFollowing features are illustrated:\n\n* Summarizing the data set\n* Converting data to treat it as a numeric feature/factor\n* Generating new variables\n* Splitting data into train and test data sets\n* Computing a first model (decision tree)\n* Building many trees (random forest)\n* Visualizing price data across different region\n* Optimizing the baseline by implementing a tuner\n* Engineering features\n* Creating a sparser model\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com) package which pulls in the most important packages for this example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: mlr3\n```\n:::\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\n## House Price Prediction in King County {#use-case-regr-houses}\n\nWe use the `kc_housing` dataset contained in the package [mlr3data](https://mlr3data.mlr-org.com) in order to provide a use-case for the application of [mlr3](https://mlr3.mlr-org.com) on real-world data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(\"kc_housing\", package = \"mlr3data\")\n```\n:::\n\n\n### Exploratory Data Analysis\n\nIn order to get a quick impression of our data, we perform some initial *Exploratory Data Analysis*.\nThis helps us to get a first impression of our data and might help us arrive at additional features that can help with the prediction of the house prices.\n\nWe can get a quick overview using R's summary function:\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(kc_housing)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      date                           price            bedrooms        bathrooms      sqft_living       sqft_lot      \n Min.   :2014-05-02 00:00:00.0   Min.   :  75000   Min.   : 0.000   Min.   :0.000   Min.   :  290   Min.   :    520  \n 1st Qu.:2014-07-22 00:00:00.0   1st Qu.: 321950   1st Qu.: 3.000   1st Qu.:1.750   1st Qu.: 1427   1st Qu.:   5040  \n Median :2014-10-16 00:00:00.0   Median : 450000   Median : 3.000   Median :2.250   Median : 1910   Median :   7618  \n Mean   :2014-10-29 03:58:09.9   Mean   : 540088   Mean   : 3.371   Mean   :2.115   Mean   : 2080   Mean   :  15107  \n 3rd Qu.:2015-02-17 00:00:00.0   3rd Qu.: 645000   3rd Qu.: 4.000   3rd Qu.:2.500   3rd Qu.: 2550   3rd Qu.:  10688  \n Max.   :2015-05-27 00:00:00.0   Max.   :7700000   Max.   :33.000   Max.   :8.000   Max.   :13540   Max.   :1651359  \n                                                                                                                     \n     floors      waterfront           view          condition         grade          sqft_above   sqft_basement   \n Min.   :1.000   Mode :logical   Min.   :0.0000   Min.   :1.000   Min.   : 1.000   Min.   : 290   Min.   :  10.0  \n 1st Qu.:1.000   FALSE:21450     1st Qu.:0.0000   1st Qu.:3.000   1st Qu.: 7.000   1st Qu.:1190   1st Qu.: 450.0  \n Median :1.500   TRUE :163       Median :0.0000   Median :3.000   Median : 7.000   Median :1560   Median : 700.0  \n Mean   :1.494                   Mean   :0.2343   Mean   :3.409   Mean   : 7.657   Mean   :1788   Mean   : 742.4  \n 3rd Qu.:2.000                   3rd Qu.:0.0000   3rd Qu.:4.000   3rd Qu.: 8.000   3rd Qu.:2210   3rd Qu.: 980.0  \n Max.   :3.500                   Max.   :4.0000   Max.   :5.000   Max.   :13.000   Max.   :9410   Max.   :4820.0  \n                                                                                                  NA's   :13126   \n    yr_built     yr_renovated      zipcode           lat             long        sqft_living15    sqft_lot15    \n Min.   :1900   Min.   :1934    Min.   :98001   Min.   :47.16   Min.   :-122.5   Min.   : 399   Min.   :   651  \n 1st Qu.:1951   1st Qu.:1987    1st Qu.:98033   1st Qu.:47.47   1st Qu.:-122.3   1st Qu.:1490   1st Qu.:  5100  \n Median :1975   Median :2000    Median :98065   Median :47.57   Median :-122.2   Median :1840   Median :  7620  \n Mean   :1971   Mean   :1996    Mean   :98078   Mean   :47.56   Mean   :-122.2   Mean   :1987   Mean   : 12768  \n 3rd Qu.:1997   3rd Qu.:2007    3rd Qu.:98118   3rd Qu.:47.68   3rd Qu.:-122.1   3rd Qu.:2360   3rd Qu.: 10083  \n Max.   :2015   Max.   :2015    Max.   :98199   Max.   :47.78   Max.   :-121.3   Max.   :6210   Max.   :871200  \n                NA's   :20699                                                                                   \n```\n:::\n\n```{.r .cell-code}\ndim(kc_housing)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 21613    20\n```\n:::\n:::\n\n\nOur dataset has 21613 observations and 20 columns.\nThe variable we want to predict is `price`.\nIn addition to the price column, we have several other columns:\n\n* `id:` A unique identifier for every house.\n\n* `date`: A date column, indicating when the house was sold.\n  This column is currently not encoded as a `date` and requires some preprocessing.\n\n* `zipcode`: A column indicating the ZIP code.\n  This is a categorical variable with many factor levels.\n\n* `long, lat` The longitude and latitude of the house\n\n* `...` several other numeric columns providing information about the house, such as number of rooms, square feet etc.\n\nBefore we continue with the analysis,  we preprocess some features so that they are stored in the correct format.\n\nFirst we convert the `date` column to `numeric`.\nTo do so, we convert the date to the POSIXct date/time class with the [anytime](https://cran.r-project.org/package=anytime) package.\nNext, use `difftime()` to convert to days since the first day recorded in the data set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(anytime)\ndates = anytime(kc_housing$date)\nkc_housing$date = as.numeric(difftime(dates, min(dates), units = \"days\"))\n```\n:::\n\n\nAfterwards, we convert the zip code to a factor:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkc_housing$zipcode = as.factor(kc_housing$zipcode)\n```\n:::\n\n\nAnd add a new column **renovated** indicating whether a house was renovated at some point.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkc_housing$renovated = as.numeric(!is.na(kc_housing$yr_renovated))\nkc_housing$has_basement = as.numeric(!is.na(kc_housing$sqft_basement))\n```\n:::\n\n\nWe drop the id column which provides no information about the house prices:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkc_housing$id = NULL\n```\n:::\n\n\nAdditionally, we convert the price from Dollar to units of 1000 Dollar to improve readability.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkc_housing$price = kc_housing$price / 1000\n```\n:::\n\n\nAdditionally, for now we simply drop the columns that have missing values, as some of our learners can not deal with them.\nA better option to deal with missing values would be imputation, i.e. replacing missing values with valid ones.\nWe will deal with this in a separate article.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkc_housing$yr_renovated = NULL\nkc_housing$sqft_basement = NULL\n```\n:::\n\n\nWe can now plot the density of the **price** to get a first impression on its distribution.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nggplot(kc_housing, aes(x = price)) + geom_density()\n```\n\n::: {.cell-output-display}\n![](2020-01-30-house-prices-in-king-county_files/figure-html/2020-01-30-house-prices-in-king-county-013-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can see that the prices for most houses lie between 75.000 and 1.5 million dollars.\nThere are few extreme values of up to 7.7 million dollars.\n\nFeature engineering often allows us to incorporate additional knowledge about the data and underlying processes.\nThis can often greatly enhance predictive performance.\nA simple example: A house which has `yr_renovated == 0` means that is has not been renovated yet.\nAdditionally, we want to drop features which should not have any influence (`id column`).\n\nAfter those initial manipulations, we load all required packages and create a [`TaskRegr`](https://mlr3.mlr-org.com/reference/TaskRegr.html) containing our data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk = as_task_regr(kc_housing, target = \"price\")\n```\n:::\n\n\nWe can inspect associations between variables using [mlr3viz](https://mlr3viz.mlr-org.com)'s `autoplot` function in order to get some good first impressions for our data.\nNote, that this does in no way prevent us from using other powerful plot functions of our choice on the original data.\n\n#### Distribution of the price:\n\nThe outcome we want to predict is the **price** variable.\nThe `autoplot` function provides a good first glimpse on our data.\nAs the resulting object is a `ggplot2` object, we can use `faceting` and other functions from **ggplot2** in order to enhance plots.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(tsk) + facet_wrap(~renovated)\n```\n\n::: {.cell-output-display}\n![](2020-01-30-house-prices-in-king-county_files/figure-html/2020-01-30-house-prices-in-king-county-015-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can observe that renovated flats seem to achieve higher sales values, and this might thus be a relevant feature.\n\nAdditionally, we can for example look at the condition of the house.\nAgain, we clearly can see that the price rises with increasing condition.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(tsk) + facet_wrap(~condition)\n```\n\n::: {.cell-output-display}\n![](2020-01-30-house-prices-in-king-county_files/figure-html/2020-01-30-house-prices-in-king-county-016-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n#### Association between variables\n\nIn addition to the association with the target variable, the association between the features can also lead to interesting insights.\nWe investigate using variables associated with the quality and size of the house.\nNote that we use `$clone()` and `$select()` to clone the task and select only a subset of the features for the `autoplot` function, as `autoplot` per default uses all features.\nThe task is cloned before we select features in order to keep the original task intact.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Variables associated with quality\nautoplot(tsk$clone()$select(tsk$feature_names[c(3, 17)]), type = \"pairs\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](2020-01-30-house-prices-in-king-county_files/figure-html/2020-01-30-house-prices-in-king-county-017-1.png){fig-align='center' width=960}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(tsk$clone()$select(tsk$feature_names[c(9:12)]), type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](2020-01-30-house-prices-in-king-county_files/figure-html/2020-01-30-house-prices-in-king-county-018-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Splitting into train and test data\n\nIn `mlr3`, we do not create `train` and `test` data sets, but instead keep only a vector of train and test indices.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain.idx = sample(seq_len(tsk$nrow), 0.7 * tsk$nrow)\ntest.idx = setdiff(seq_len(tsk$nrow), train.idx)\n```\n:::\n\n\nWe can do the same for our task:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_train = tsk$clone()$filter(train.idx)\ntask_test = tsk$clone()$filter(test.idx)\n```\n:::\n\n\n### A first model: Decision Tree\n\nDecision trees cannot only be used as a powerful tool for predictive models but also for exploratory data analysis.\nIn order to fit a decision tree, we first get the `regr.rpart` learner from the `mlr_learners` dictionary by using the sugar function [`lrn`](https://mlr3.mlr-org.com/reference/mlr_sugar.html).\n\nFor now, we leave out the  `zipcode` variable, as we also have the `latitude` and `longitude` of each house.\nAgain, we use `$clone()`, so we do not change the original task.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk_nozip = task_train$clone()$select(setdiff(tsk$feature_names, \"zipcode\"))\n\n# Get the learner\nlrn = lrn(\"regr.rpart\")\n\n# And train on the task\nlrn$train(tsk_nozip, row_ids = train.idx)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(lrn$model)\ntext(lrn$model)\n```\n\n::: {.cell-output-display}\n![](2020-01-30-house-prices-in-king-county_files/figure-html/2020-01-30-house-prices-in-king-county-022-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nThe learned tree relies on several variables in order to distinguish between cheaper and pricier houses.\nThe features we split along are **grade**, **sqft_living**, but also some features related to the area (longitude and latitude).\nWe can visualize the price across different regions in order to get more info:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the ggmap package in order to visualize on a map\nlibrary(ggmap)\n\n# And create a quick plot for the price\nqmplot(long, lat, maptype = \"watercolor\", color = log(price),\n  data = kc_housing[train.idx[1:3000], ]) +\n  scale_colour_viridis_c()\n```\n\n::: {.cell-output-display}\n![](2020-01-30-house-prices-in-king-county_files/figure-html/2020-01-30-house-prices-in-king-county-023-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\n# And the zipcode\nqmplot(long, lat, maptype = \"watercolor\", color = zipcode,\n  data = kc_housing[train.idx[1:3000], ]) + guides(color = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\n```\n:::\n\n::: {.cell-output-display}\n![](2020-01-30-house-prices-in-king-county_files/figure-html/2020-01-30-house-prices-in-king-county-023-2.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can see that the price is clearly associated with the zipcode when comparing then two plots.\nAs a result, we might want to indeed use the **zipcode** column in our future endeavors.\n\n### A first baseline: Decision Tree\n\nAfter getting an initial idea for our data, we might want to construct a first baseline, in order to see what a simple model already can achieve.\n\nWe use [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) with `3-fold cross-validation` on our training data in order to get a reliable estimate of the algorithm's performance on future data.\nBefore we start with defining and training learners, we create a [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html) in order to make sure that we always compare on exactly the same data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv3 = rsmp(\"cv\", folds = 3)\n```\n:::\n\n\nFor the cross-validation we only use the **training data** by cloning the task and selecting only observations from the training set.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_rpart = lrn(\"regr.rpart\")\nres = resample(task = task_train, lrn_rpart, cv3)\nres$score(msr(\"regr.rmse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             task    task_id                learner learner_id         resampling resampling_id iteration\n1: <TaskRegr[47]> kc_housing <LearnerRegrRpart[38]> regr.rpart <ResamplingCV[20]>            cv         1\n2: <TaskRegr[47]> kc_housing <LearnerRegrRpart[38]> regr.rpart <ResamplingCV[20]>            cv         2\n3: <TaskRegr[47]> kc_housing <LearnerRegrRpart[38]> regr.rpart <ResamplingCV[20]>            cv         3\n             prediction regr.rmse\n1: <PredictionRegr[19]>  205.7541\n2: <PredictionRegr[19]>  205.6597\n3: <PredictionRegr[19]>  213.3846\n```\n:::\n\n```{.r .cell-code}\nsprintf(\"RMSE of the simple rpart: %s\", round(sqrt(res$aggregate()), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"RMSE of the simple rpart: 208.3\"\n```\n:::\n:::\n\n\n### Many Trees: Random Forest\n\nWe might be able to improve upon the **RMSE** using more powerful learners.\nWe first load the [mlr3learners](https://mlr3learners.mlr-org.com) package, which contains the [ranger](https://cran.r-project.org/package=ranger) learner (a package which implements the \"Random Forest\" algorithm).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3learners)\nlrn_ranger = lrn(\"regr.ranger\", num.trees = 15L)\nres = resample(task = task_train, lrn_ranger, cv3)\nres$score(msr(\"regr.rmse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             task    task_id                 learner  learner_id         resampling resampling_id iteration\n1: <TaskRegr[47]> kc_housing <LearnerRegrRanger[38]> regr.ranger <ResamplingCV[20]>            cv         1\n2: <TaskRegr[47]> kc_housing <LearnerRegrRanger[38]> regr.ranger <ResamplingCV[20]>            cv         2\n3: <TaskRegr[47]> kc_housing <LearnerRegrRanger[38]> regr.ranger <ResamplingCV[20]>            cv         3\n             prediction regr.rmse\n1: <PredictionRegr[19]>  142.2415\n2: <PredictionRegr[19]>  161.6461\n3: <PredictionRegr[19]>  138.2573\n```\n:::\n\n```{.r .cell-code}\nsprintf(\"RMSE of the simple ranger: %s\", round(sqrt(res$aggregate()), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"RMSE of the simple ranger: 147.74\"\n```\n:::\n:::\n\n\nOften tuning **RandomForest** methods does not increase predictive performances substantially.\nIf time permits, it can nonetheless lead to improvements and should thus be performed.\nIn this case, we resort to tune a different kind of model: **Gradient Boosted Decision Trees** from the package [xgboost](https://cran.r-project.org/package=xgboost).\n\n### A better baseline: `AutoTuner`\n\nTuning can often further improve the performance.\nIn this case, we *tune* the xgboost learner in order to see whether this can improve performance.\nFor the `AutoTuner` we have to specify a **Termination Criterion** (how long the tuning should run) a **Tuner** (which tuning method to use) and a **ParamSet** (which space we might want to search through).\nFor now, we do not use the **zipcode** column, as [xgboost](https://cran.r-project.org/package=xgboost) cannot naturally deal with categorical features.\nThe **AutoTuner** automatically performs nested cross-validation.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_xgb = lrn(\"regr.xgboost\")\n\n# Define the search space\nsearch_space = ps(\n  eta = p_dbl(lower = 0.2, upper = .4),\n  min_child_weight = p_dbl(lower = 1, upper = 20),\n  subsample = p_dbl(lower = .7, upper = .8),\n  colsample_bytree = p_dbl(lower = .9, upper = 1),\n  colsample_bylevel = p_dbl(lower = .5, upper = .7),\n  nrounds = p_int(lower = 1L, upper = 25))\n\nat = auto_tuner(\n  method = \"random_search\",\n  learner = lrn_xgb,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"regr.rmse\"),\n  search_space = search_space,\n  term_evals = 10,\n  batch_size = 40)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# And resample the AutoTuner\nres = resample(tsk_nozip, at, cv3, store_models = TRUE)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres$score(msr(\"regr.rmse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             task    task_id         learner         learner_id         resampling resampling_id iteration\n1: <TaskRegr[47]> kc_housing <AutoTuner[42]> regr.xgboost.tuned <ResamplingCV[20]>            cv         1\n2: <TaskRegr[47]> kc_housing <AutoTuner[42]> regr.xgboost.tuned <ResamplingCV[20]>            cv         2\n3: <TaskRegr[47]> kc_housing <AutoTuner[42]> regr.xgboost.tuned <ResamplingCV[20]>            cv         3\n             prediction regr.rmse\n1: <PredictionRegr[19]>  147.0554\n2: <PredictionRegr[19]>  136.4282\n3: <PredictionRegr[19]>  132.4484\n```\n:::\n\n```{.r .cell-code}\nsprintf(\"RMSE of the tuned xgboost: %s\", round(sqrt(res$aggregate()), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"RMSE of the tuned xgboost: 138.78\"\n```\n:::\n:::\n\n\nWe can obtain the resulting parameters in the respective splits by accessing the [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsapply(res$learners, function(x) x$learner$param_set$values)[-2, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   [,1]      [,2]      [,3]     \nnrounds            25        23        24       \nverbose            0         0         0        \nearly_stopping_set \"none\"    \"none\"    \"none\"   \neta                0.220869  0.3809271 0.2115116\nmin_child_weight   1.885109  7.472919  1.910491 \nsubsample          0.7542127 0.7884631 0.7000357\ncolsample_bytree   0.9032271 0.9675298 0.9120812\ncolsample_bylevel  0.5259713 0.6817893 0.630818 \n```\n:::\n:::\n\n\n**NOTE:** To keep runtime low, we only tune parts of the hyperparameter space of [xgboost](https://cran.r-project.org/package=xgboost) in this example.\nAdditionally, we only allow for $10$ random search iterations, which is usually too little for real-world applications.\nNonetheless, we are able to obtain an improved performance when comparing to the [ranger](https://cran.r-project.org/package=ranger) model.\n\nIn order to further improve our results we have several options:\n\n* Find or engineer better features\n* Remove Features to avoid overfitting\n* Obtain additional data (often prohibitive)\n* Try more models\n* Improve the tuning\n   * Increase the tuning budget\n   * Enlarge the tuning search space\n   * Use a more efficient tuning algorithm\n* Stacking and Ensembles\n\nBelow we will investigate some of those possibilities and investigate whether this improves performance.\n\n### Advanced: Engineering Features: Mutating ZIP-Codes\n\nIn order to better cluster the zip codes, we compute a new feature: **med_price**:\nIt computes the median price in each zip-code.\nThis might help our model to improve the prediction.\nThis is equivalent to **impact encoding**\n[more information](https://win-vector.com/2012/07/23/modeling-trick-impact-coding-of-categorical-variables-with-many-levels/):\n\nWe can equip a learner with impact encoding using **mlr3pipelines**. More information on **mlr3pipelines** can be obtained from other posts.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_impact = po(\"encodeimpact\", affect_columns = selector_name(\"zipcode\")) %>>% lrn(\"regr.ranger\")\n```\n:::\n\n\nAgain, we run [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) and compute the **RMSE**.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nres = resample(task = task_train, lrn_impact, cv3)\n```\n:::\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nres$score(msr(\"regr.rmse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             task    task_id            learner               learner_id         resampling resampling_id iteration\n1: <TaskRegr[47]> kc_housing <GraphLearner[38]> encodeimpact.regr.ranger <ResamplingCV[20]>            cv         1\n2: <TaskRegr[47]> kc_housing <GraphLearner[38]> encodeimpact.regr.ranger <ResamplingCV[20]>            cv         2\n3: <TaskRegr[47]> kc_housing <GraphLearner[38]> encodeimpact.regr.ranger <ResamplingCV[20]>            cv         3\n             prediction regr.rmse\n1: <PredictionRegr[19]>  119.4389\n2: <PredictionRegr[19]>  146.3303\n3: <PredictionRegr[19]>  125.4191\n```\n:::\n\n```{.r .cell-code}\nsprintf(\"RMSE of ranger with med_price: %s\", round(sqrt(res$aggregate()), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"RMSE of ranger with med_price: 130.9\"\n```\n:::\n:::\n\n\n###  Advanced: Obtaining a sparser model\n\nIn many cases, we might want to have a sparse model.\nFor this purpose we can use a [`mlr3filters::Filter`](https://mlr3filters.mlr-org.com/reference/Filter.html) implemented in [mlr3filters](https://mlr3filters.mlr-org.com).\nThis can prevent our learner from overfitting make it easier for humans to interpret models as fewer variables influence the resulting prediction.\n\nIn this example, we use `PipeOpFilter` (via `po(\"filter\", ...)`) to add a feature-filter before training the model.\nFor a more in-depth insight, refer to the sections on [mlr3pipelines](https://mlr3pipelines.mlr-org.com)  and `mlr3filters` in the **mlr3 book**: [Feature Selection](https://mlr3book.mlr-org.com/fs.html) and [Pipelines](https://mlr3book.mlr-org.com/pipelines.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfilter = flt(\"mrmr\")\n```\n:::\n\n\nThe resulting **RMSE** is slightly higher, and at the same time we only use $12$ features.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = po(\"filter\", filter, param_vals = list(filter.nfeat = 12)) %>>% po(\"learner\", lrn(\"regr.ranger\"))\nlrn_filter = as_learner(graph)\nres = resample(task = task_train, lrn_filter, cv3)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres$score(msr(\"regr.rmse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             task    task_id            learner       learner_id         resampling resampling_id iteration\n1: <TaskRegr[47]> kc_housing <GraphLearner[38]> mrmr.regr.ranger <ResamplingCV[20]>            cv         1\n2: <TaskRegr[47]> kc_housing <GraphLearner[38]> mrmr.regr.ranger <ResamplingCV[20]>            cv         2\n3: <TaskRegr[47]> kc_housing <GraphLearner[38]> mrmr.regr.ranger <ResamplingCV[20]>            cv         3\n             prediction regr.rmse\n1: <PredictionRegr[19]>  152.6046\n2: <PredictionRegr[19]>  156.7964\n3: <PredictionRegr[19]>  149.0130\n```\n:::\n\n```{.r .cell-code}\nsprintf(\"RMSE of ranger with filtering: %s\", round(sqrt(res$aggregate()), 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"RMSE of ranger with filtering: 152.84\"\n```\n:::\n:::\n\n\n## Summary:\n\nWe have seen different ways to improve models with respect to our criteria by:\n\n* Choosing a suitable algorithm\n* Choosing good hyperparameters (tuning)\n* Filtering features\n* Engineering new features\n\nA combination of all the above would most likely yield an even better model.\nThis is left as an exercise to the reader.\n\nThe best model we found in this example is the `ranger` model with the added `med_price` feature.\nIn a final step, we now want to assess the model's quality on the held-out data we stored in our `task_test`.\nIn order to do so, and to prevent data leakage, we can only add the median price from the training data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(data.table)\n\ndata = task_train$data(cols = c(\"price\", \"zipcode\"))\ndata[, med_price := median(price), by = \"zipcode\"]\ntest_data = task_test$data(cols = \"zipcode\")\ntest = merge(test_data, unique(data[, .(zipcode, med_price)]), all.x = TRUE)\ntask_test$cbind(test)\n```\n:::\n\n\nNow we can use the augmented `task_test` to predict on new data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlrn_ranger$train(task_train)\npred = lrn_ranger$predict(task_test)\npred$score(msr(\"regr.rmse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nregr.rmse \n 144.9944 \n```\n:::\n:::\n",
    "supporting": [
      "2020-01-30-house-prices-in-king-county_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}