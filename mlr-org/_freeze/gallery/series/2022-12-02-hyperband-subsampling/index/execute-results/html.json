{
  "hash": "e2701b8c93875d9b851a0b4068a14f9c",
  "result": {
    "markdown": "---\ntitle: \"Hyperband Series - Data Set Subsampling\"\ndescription: |\n  Optimize the hyperparameters of a Support Vector Machine with Hyperband.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2022-12-02\nbibliography: bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n---\n\n\n\n\n\n\n# Scope\n\nWe continue working with the *Hyperband* optimization algorithm [@li_2018].\nThe previous post used the number of boosting iterations of an XGBoost model as the resource.\nHowever, Hyperband is not limited to machine learning algorithms that are trained iteratively.\nThe resource can also be the number of features, the training time of a model, or the size of the training data set.\nIn this post, we will tune a support vector machine and use the size of the training data set as the fidelity parameter.\nThe time to train a support vector machine increases with the size of the data set.\nThis makes the data set size a suitable fidelity parameter for Hyperband.\n\nThis is the second part of the Hyperband series.\nThe other parts can be found here:\n\n* [Hyperband Series - Iterative Training](/gallery/series/2022-12-01-hyperband-xgboost)\n* Hyperband Series - Large-Scale Experiments\n\nIf you don't know much about Hyperband, check out the first post which explains the algorithm in detail.\nA little knowledge about [mlr3pipelines](https://mlr3pipelines.mlr-org.com) is beneficial but not necessary to understand the example.\n\n# Hyperparameter Optimization\n\nIn this post, we will optimize the hyperparameters of the support vector machine on the [`Sonar`](https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html) data set.\nWe begin by constructing a classification machine by setting `type` to `\"C-classification\"`.\nWe truncate the id of the learner to `\"svm\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\")\n```\n:::\n\n\nThe [mlr3pipelines](https://mlr3pipelines.mlr-org.com) package features a [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) for subsampling.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo(\"subsample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPipeOp: <subsample> (not trained)\nvalues: <frac=0.6321, stratify=FALSE, replace=FALSE>\nInput channels <name [train type, predict type]>:\n  input [Task,Task]\nOutput channels <name [train type, predict type]>:\n  output [Task,Task]\n```\n:::\n:::\n\n\nThe [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) controls the size of the training data set with the `frac` parameter.\nWe connect the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)  with the learner and get a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(\n  po(\"subsample\") %>>%\n  learner\n)\n```\n:::\n\n\nThe graph learner subsamples and then fits a support vector machine on the data subset.\nThe parameter set of the graph learner is a combination of the parameter set of the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) and learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(graph_learner$param_set)[, .(id, lower, upper, levels)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    id lower upper                             levels\n 1:     subsample.frac     0   Inf                                   \n 2: subsample.stratify    NA    NA                         TRUE,FALSE\n 3:  subsample.replace    NA    NA                         TRUE,FALSE\n 4:      svm.cachesize  -Inf   Inf                                   \n 5:  svm.class.weights    NA    NA                                   \n---                                                                  \n15:             svm.nu  -Inf   Inf                                   \n16:          svm.scale    NA    NA                                   \n17:      svm.shrinking    NA    NA                         TRUE,FALSE\n18:      svm.tolerance     0   Inf                                   \n19:           svm.type    NA    NA C-classification,nu-classification\n```\n:::\n:::\n\n\nNext, we create the search space.\nWe use [`TuneToken`](https://paradox.mlr-org.com/reference/to_tune.html) to indicate that a hyperparameter should be tuned.\nWe have to prefix the hyperparameters with the id of the [`PipeOps`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html).\nThe `subsample.frac` is the fidelity parameter that must be tagged with `\"budget\"` in the search space.\nThe data set size is increased from 12.5% to 100%.\nFor the other hyperparameters, we took the search space for support vector machines from the @kuehn_2018 article.\nThis search space works for a wide range of data sets.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner$param_set$set_values(\n  subsample.frac  = to_tune(p_dbl(3^-2, 1, tags = \"budget\")),\n  svm.kernel      = to_tune(c(\"linear\", \"polynomial\", \"radial\")),\n  svm.cost        = to_tune(1e-4, 1e3, logscale = TRUE),\n  svm.gamma       = to_tune(1e-4, 1e3, logscale = TRUE),\n  svm.tolerance   = to_tune(1e-4, 2, logscale = TRUE),\n  svm.degree      = to_tune(2, 5)\n)\n```\n:::\n\n\nLet's create the tuning instance.\nWe use the `\"none\"` terminator because Hyperband controls the termination itself.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = graph_learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:subsample.svm_on_sonar>\n* Search Space:\n               id    class      lower     upper nlevels\n1: subsample.frac ParamDbl  0.1111111 1.0000000     Inf\n2:       svm.cost ParamDbl -9.2103404 6.9077553     Inf\n3:     svm.degree ParamInt  2.0000000 5.0000000       4\n4:      svm.gamma ParamDbl -9.2103404 6.9077553     Inf\n5:     svm.kernel ParamFct         NA        NA       3\n6:  svm.tolerance ParamDbl -9.2103404 0.6931472     Inf\n* Terminator: <TerminatorNone>\n```\n:::\n:::\n\n\nWe load the Hyperband [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html) and set `eta = 2`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3hyperband\")\n\ntuner = tnr(\"hyperband\", eta = 2)\n```\n:::\n\n\nUsing `eta = 2` and a lower bound of 12.5% for the data set size, results in the following schedule.\nConfigurations with the same data set size are evaluated in parallel.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-622223a7c1e436b36f3a\" class=\"reactable html-widget\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-622223a7c1e436b36f3a\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"budget\":[0.125,0.25,0.25,0.5,0.5,0.5,1,1,1,1],\"bracket\":[3,3,2,3,2,1,3,2,1,0],\"stage\":[0,1,0,2,1,0,3,2,1,0],\"n\":[8,4,6,2,3,4,1,1,2,4]},\"columns\":[{\"accessor\":\"budget\",\"name\":\"Data Set Size\",\"type\":\"numeric\",\"cell\":[{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"    12.5%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"12.5%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      25%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      25%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      50%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      50%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      50%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"     100%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"     100%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"     100%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"     100%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]}]},{\"accessor\":\"bracket\",\"name\":\"Bracket\",\"type\":\"numeric\"},{\"accessor\":\"stage\",\"name\":\"Stage\",\"type\":\"numeric\"},{\"accessor\":\"n\",\"name\":\"# Configruations\",\"type\":\"numeric\"}],\"pagination\":false,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"96ee797d77413a93439babe90214805c\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nNow we are ready to start the tuning.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   subsample.frac svm.cost svm.degree svm.gamma svm.kernel svm.tolerance learner_param_vals  x_domain classif.ce\n1:              1 3.522733         NA -4.487177     radial      -2.34614          <list[8]> <list[5]>  0.1394755\n```\n:::\n:::\n\n\nThe best model is a support vector machine with a radial basis kernel.\nWe are now looking at the best configuration across the stages.\nThe configuration was sampled in the second bracket and thus started with a data set size of 25%.\nWe observe that the performance increases with the size of the data set.\nThe multi-fidelity idea can be seen here.\nThe evaluation on 25% of the data set is only an approximation for the final performance on the full data set.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[c(9, 26, 33), .(bracket, stage, subsample.frac, svm.cost, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   bracket stage subsample.frac  svm.cost classif.ce\n1:       2     0           0.25 -2.957623  0.2786749\n2:       2     1           0.50 -2.957623  0.2452036\n3:       2     2           1.00  3.522733  0.1394755\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\ndata = as.data.table(instance$archive)[, i := .GRP, by = \"svm.cost\"]\ndata[, i := as.factor(i)]\ndata = data[bracket == 3]\n\nggplot(data, aes(x = subsample.frac, y = classif.ce, group = i)) +\n  geom_line(aes(color=i))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata[, .(i, bracket, stage, subsample.frac, svm.cost, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    i bracket stage subsample.frac  svm.cost classif.ce\n 1: 1       3     0          0.125  4.791403  0.5049689\n 2: 2       3     0          0.125  5.971788  0.5191856\n 3: 3       3     0          0.125  4.398106  0.3559006\n 4: 4       3     0          0.125  0.401039  0.3026915\n 5: 5       3     0          0.125 -8.029552  0.5191856\n---                                                    \n11: 3       3     1          0.250  4.398106  0.2930297\n12: 6       3     1          0.250  5.389961  0.3512767\n13: 4       3     2          0.500  0.401039  0.2545204\n14: 3       3     2          0.500  4.398106  0.2497585\n15: 3       3     3          1.000  4.398106  0.1877157\n```\n:::\n:::\n\n\n\n\n# Conclusion\n\nSubsampling makes it possible to tune learners with Hyperband that lack a natural fidelity parameter.\nMoreover, we can reduce the time to optimize a learner on a large data set.\n\n# References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"../../../site_libs/react-17.0.0/react.min.js\"></script>\n<script src=\"../../../site_libs/react-17.0.0/react-dom.min.js\"></script>\n<script src=\"../../../site_libs/reactwidget-1.0.0/react-tools.js\"></script>\n<script src=\"../../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<script src=\"../../../site_libs/reactable-binding-0.3.0/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}