{
  "hash": "971d3e1c89ea4321004152dac973e57f",
  "result": {
    "markdown": "---\ntitle: \"Hyperband Series II - Subsampling\"\ndescription: |\n  Optimize the hyperparameters of learners that lack a natural fidelity parameter.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2022-12-02\nbibliography: bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n---\n\n\n\n\n\n\n# Scope\n\nIncreasingly large data sets and search spaces make optimization a very time-consuming task.\nHyperband [@li_2018] solves this problem by approximating the performance of a configuration on a small subset of the training data, with just a few training epochs in a neural network or with only a small number of trees in a gradient-boosting model.\nAfter sampling a random population of configurations, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones.\nThis type of optimization is called *multi-fidelity* optimization.\nThe fidelity parameter is part of the search space and influences the computational cost of fitting a model.\nIn this post, we will tune a support vector machine and use the size of the training data set as the fidelity parameter.\nThe time to train a support vector machine increases with the size of the data set.\nThis makes the data set size a suitable fidelity parameter for Hyperband.\nWe assume that you are already familiar with tuning in the mlr3 ecosystem.\nIf not, you should start with the [Hyperparameter Optimization on the Palmer Penguins Data Set](gallery/optimization/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins) post.\n\n# Hyperparameter Optimization\n\nIn this post, we will optimize the hyperparameters of the support vector machine on the [`Sonar`](https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html) data set.\nWe begin by constructing a classification machine by setting `type` to `\"C-classification\"`.\nWe truncate the id of the learner to `\"svm\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\")\n```\n:::\n\n\nThe [mlr3pipelines](https://mlr3pipelines.mlr-org.com) package features a [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) for subsampling.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo(\"subsample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPipeOp: <subsample> (not trained)\nvalues: <frac=0.6321, stratify=FALSE, replace=FALSE>\nInput channels <name [train type, predict type]>:\n  input [Task,Task]\nOutput channels <name [train type, predict type]>:\n  output [Task,Task]\n```\n:::\n:::\n\n\nThe [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) controls the size of the training data set with the `frac` parameter.\nWe connect the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)  with the learner and get a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(\n  po(\"subsample\") %>>%\n  learner\n)\n```\n:::\n\n\nThe graph learner subsamples and then fits a support vector machine on the data subset.\nThe parameter set of the graph learner is a combination of the parameter set of the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) and learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(graph_learner$param_set)[, .(id, lower, upper, levels)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    id lower upper                             levels\n 1:     subsample.frac     0   Inf                                   \n 2: subsample.stratify    NA    NA                         TRUE,FALSE\n 3:  subsample.replace    NA    NA                         TRUE,FALSE\n 4:      svm.cachesize  -Inf   Inf                                   \n 5:  svm.class.weights    NA    NA                                   \n---                                                                  \n15:             svm.nu  -Inf   Inf                                   \n16:          svm.scale    NA    NA                                   \n17:      svm.shrinking    NA    NA                         TRUE,FALSE\n18:      svm.tolerance     0   Inf                                   \n19:           svm.type    NA    NA C-classification,nu-classification\n```\n:::\n:::\n\n\nNext, we create the search space.\nWe use [`TuneToken`](https://paradox.mlr-org.com/reference/to_tune.html) to indicate that a hyperparameter should be tuned.\nWe have to prefix the hyperparameters with the id of the [`PipeOps`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html).\nThe `subsample.frac` is the fidelity parameter that must be tagged with `\"budget\"` in the search space.\nThe data set size is increased from 12.5% to 100%.\nFor the other hyperparameters, we took the search space for support vector machines from the @kuehn_2018 article.\nThis search space works for a wide range of data sets.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner$param_set$set_values(\n  subsample.frac  = to_tune(p_dbl(2^-3, 1, tags = \"budget\")),\n  svm.kernel      = to_tune(c(\"linear\", \"polynomial\", \"radial\")),\n  svm.cost        = to_tune(1e-4, 1e3, logscale = TRUE),\n  svm.gamma       = to_tune(1e-4, 1e3, logscale = TRUE),\n  svm.tolerance   = to_tune(1e-4, 2, logscale = TRUE),\n  svm.degree      = to_tune(2, 5)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSetCollection>\n                    id    class lower upper nlevels          default  parents                value\n 1:     subsample.frac ParamDbl     0   Inf     Inf   <NoDefault[3]>          <ObjectTuneToken[2]>\n 2:  subsample.replace ParamLgl    NA    NA       2   <NoDefault[3]>                         FALSE\n 3: subsample.stratify ParamLgl    NA    NA       2   <NoDefault[3]>                         FALSE\n 4:      svm.cachesize ParamDbl  -Inf   Inf     Inf               40                              \n 5:  svm.class.weights ParamUty    NA    NA     Inf                                               \n---                                                                                               \n15:             svm.nu ParamDbl  -Inf   Inf     Inf              0.5 svm.type                     \n16:          svm.scale ParamUty    NA    NA     Inf             TRUE                              \n17:      svm.shrinking ParamLgl    NA    NA       2             TRUE                              \n18:      svm.tolerance ParamDbl     0   Inf     Inf            0.001           <RangeTuneToken[2]>\n19:           svm.type ParamFct    NA    NA       2 C-classification              C-classification\n```\n:::\n:::\n\n\nLet's create the tuning instance.\nWe use the `\"none\"` terminator because Hyperband controls the termination itself.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = ti(\n  task = tsk(\"sonar\"),\n  learner = graph_learner,\n  resampling = rsmp(\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:subsample.svm_on_sonar>\n* Search Space:\n               id    class    lower     upper nlevels\n1: subsample.frac ParamDbl  0.12500 1.0000000     Inf\n2:       svm.cost ParamDbl -9.21034 6.9077553     Inf\n3:     svm.degree ParamInt  2.00000 5.0000000       4\n4:      svm.gamma ParamDbl -9.21034 6.9077553     Inf\n5:     svm.kernel ParamFct       NA        NA       3\n6:  svm.tolerance ParamDbl -9.21034 0.6931472     Inf\n* Terminator: <TerminatorNone>\n```\n:::\n:::\n\n\nWe load the Hyperband [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html) and set `eta = 2`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3hyperband\")\n\ntuner = tnr(\"hyperband\", eta = 2)\n```\n:::\n\n\nUsing `eta = 2` and a lower bound of 12.5% for the data set size, results in the following schedule.\nConfigurations with the same data set size are evaluated in parallel.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-a49a1bbbd210b19f05a9\" class=\"reactable html-widget\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-a49a1bbbd210b19f05a9\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"budget\":[0.125,0.25,0.25,0.5,0.5,0.5,1,1,1,1],\"bracket\":[3,3,2,3,2,1,3,2,1,0],\"stage\":[0,1,0,2,1,0,3,2,1,0],\"n\":[8,4,6,2,3,4,1,1,2,4]},\"columns\":[{\"accessor\":\"budget\",\"name\":\"Data Set Size\",\"type\":\"numeric\",\"cell\":[{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"    12.5%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"12.5%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      25%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      25%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      50%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      50%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"      50%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"     100%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"     100%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"     100%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"     100%\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]}]},{\"accessor\":\"bracket\",\"name\":\"Bracket\",\"type\":\"numeric\"},{\"accessor\":\"stage\",\"name\":\"Stage\",\"type\":\"numeric\"},{\"accessor\":\"n\",\"name\":\"# Configruations\",\"type\":\"numeric\"}],\"pagination\":false,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"96ee797d77413a93439babe90214805c\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nNow we are ready to start the tuning.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   subsample.frac svm.cost svm.degree svm.gamma svm.kernel svm.tolerance learner_param_vals  x_domain classif.ce\n1:            0.5 1.180603         NA        NA     linear       -1.1751          <list[7]> <list[4]>  0.2608696\n```\n:::\n:::\n\n\nThe best model is a support vector machine with a radial basis kernel.\nWe are now looking at the best configuration across the stages.\nThe configuration was sampled in the second bracket and thus started with a data set size of 25%.\nWe observe that the performance increases with the size of the data set.\nThe multi-fidelity idea can be seen here.\nThe evaluation on 25% of the data set is only an approximation for the final performance on the full data set.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[c(9, 26, 33), .(bracket, stage, subsample.frac, svm.cost, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   bracket stage subsample.frac  svm.cost classif.ce\n1:       2     0           0.25 -8.424640  0.5507246\n2:       2     1           0.50  5.439728  0.2898551\n3:       2     2           1.00  5.439728  0.2608696\n```\n:::\n:::\n\n\n# Conclusion\n\nSubsampling makes it possible to tune learners with Hyperband that lack a natural fidelity parameter.\nMoreover, we can reduce the time to optimize a learner on a large data set.\n\n# References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"../../../site_libs/react-17.0.0/react.min.js\"></script>\n<script src=\"../../../site_libs/react-17.0.0/react-dom.min.js\"></script>\n<script src=\"../../../site_libs/reactwidget-1.0.0/react-tools.js\"></script>\n<script src=\"../../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<script src=\"../../../site_libs/reactable-binding-0.3.0/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}