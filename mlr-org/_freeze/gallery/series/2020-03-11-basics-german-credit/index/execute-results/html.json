{
  "hash": "2678e01e0c11883db00882792acda1ef",
  "result": {
    "markdown": "---\ntitle: German Credit Series - Basics\ncategories:\n  - visualization\n  - classification\n  - feature importance\nauthor:\n  - name: Martin Binder\n  - name: Florian Pfisterer\n  - name: Michel Lang\ndate: 03-11-2020\ndescription: |\n  Train different models.\n---\n\n\n\n\n## Intro\n\nThis is the first part in a serial of tutorials.\nThe other parts of this series can be found here:\n\n- [Part II - Tuning](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3tuning-tutorial-german-credit/)\n- [Part III - Pipelines](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3pipelines-tutorial-german-credit/)\n\nWe will walk through this tutorial interactively. The text is kept short to be followed in real time.\n\n## Prerequisites\n\nEnsure all packages used in this tutorial are installed.\nThis includes the [mlr3verse](https://mlr3verse.mlr-org.com) package, as well as other packages for data handling, cleaning and visualization which we are going to use ([data.table](https://cran.r-project.org/package=data.table), [ggplot2](https://cran.r-project.org/package=ggplot2), [rchallenge](https://cran.r-project.org/package=rchallenge), and [skimr](https://cran.r-project.org/package=skimr)).\n\nThen, load the main packages we are going to use:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\nlibrary(\"mlr3learners\")\nlibrary(\"mlr3tuning\")\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\n\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n```\n:::\n\n\n## Machine Learning Use Case: German Credit Data\n\nThe German credit data was originally donated in 1994 by Prof. Dr. Hans Hoffman of the University of Hamburg.\nA description can be found at the [UCI repository](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).\nThe goal is to classify people by their credit risk (good or bad) using 20 personal, demographic and financial features:\n\n| Feature Name            | Description                                            |\n| ------------------------| ------------------------------------------------------ |\n| age                     | age in years                                           |\n| amount                  | amount asked by applicant                              |\n| credit_history          | past credit history of applicant at this bank          |\n| duration                | duration of the credit in months                       |\n| employment_duration     | present employment since                               |\n| foreign_worker          | is applicant foreign worker?                           |\n| housing                 | type of apartment rented, owned, for free / no payment |\n| installment_rate        | installment rate in percentage of disposable income    |\n| job                     | current job information                                |\n| number_credits          | number of existing credits at this bank                |\n| other_debtors           | other debtors/guarantors present?                      |\n| other_installment_plans | other installment plans the applicant is paying        |\n| people_liable           | number of people being liable to provide maintenance   |\n| personal_status_sex     | combination of sex and personal status of applicant    |\n| present_residence       | present residence since                                |\n| property                | properties that applicant has                          |\n| purpose                 | reason customer is applying for a loan                 |\n| savings                 | savings accounts/bonds at this bank                    |\n| status                  | status/balance of checking account at this bank        |\n| telephone               | is there any telephone registered for this customer?   |\n\n### Importing the Data\n\nThe dataset we are going to use is a transformed version of this German credit dataset, as provided by the [rchallenge](https://cran.r-project.org/package=rchallenge) package (this transformed dataset was proposed by Ulrike Gr√∂mping, with factors instead of dummy variables and corrected features):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(\"german\", package = \"rchallenge\")\n```\n:::\n\n\nFirst, we'll do a thorough investigation of the dataset.\n\n### Exploring the Data\n\nWe can get a quick overview of our dataset using R's summary function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndim(german)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1000   21\n```\n:::\n\n```{.r .cell-code}\nstr(german)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t1000 obs. of  21 variables:\n $ status                 : Factor w/ 4 levels \"no checking account\",..: 1 1 2 1 1 1 1 1 4 2 ...\n $ duration               : int  18 9 12 12 12 10 8 6 18 24 ...\n $ credit_history         : Factor w/ 5 levels \"delay in paying off in the past\",..: 5 5 3 5 5 5 5 5 5 3 ...\n $ purpose                : Factor w/ 11 levels \"others\",\"car (new)\",..: 3 1 10 1 1 1 1 1 4 4 ...\n $ amount                 : int  1049 2799 841 2122 2171 2241 3398 1361 1098 3758 ...\n $ savings                : Factor w/ 5 levels \"unknown/no savings account\",..: 1 1 2 1 1 1 1 1 1 3 ...\n $ employment_duration    : Factor w/ 5 levels \"unemployed\",\"< 1 yr\",..: 2 3 4 3 3 2 4 2 1 1 ...\n $ installment_rate       : Ord.factor w/ 4 levels \">= 35\"<\"25 <= ... < 35\"<..: 4 2 2 3 4 1 1 2 4 1 ...\n $ personal_status_sex    : Factor w/ 4 levels \"male : divorced/separated\",..: 2 3 2 3 3 3 3 3 2 2 ...\n $ other_debtors          : Factor w/ 3 levels \"none\",\"co-applicant\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ present_residence      : Ord.factor w/ 4 levels \"< 1 yr\"<\"1 <= ... < 4 yrs\"<..: 4 2 4 2 4 3 4 4 4 4 ...\n $ property               : Factor w/ 4 levels \"unknown / no property\",..: 2 1 1 1 2 1 1 1 3 4 ...\n $ age                    : int  21 36 23 39 38 48 39 40 65 23 ...\n $ other_installment_plans: Factor w/ 3 levels \"bank\",\"stores\",..: 3 3 3 3 1 3 3 3 3 3 ...\n $ housing                : Factor w/ 3 levels \"for free\",\"rent\",..: 1 1 1 1 2 1 2 2 2 1 ...\n $ number_credits         : Ord.factor w/ 4 levels \"1\"<\"2-3\"<\"4-5\"<..: 1 2 1 2 2 2 2 1 2 1 ...\n $ job                    : Factor w/ 4 levels \"unemployed/unskilled - non-resident\",..: 3 3 2 2 2 2 2 2 1 1 ...\n $ people_liable          : Factor w/ 2 levels \"3 or more\",\"0 to 2\": 2 1 2 1 2 1 2 1 2 2 ...\n $ telephone              : Factor w/ 2 levels \"no\",\"yes (under customer name)\": 1 1 1 1 1 1 1 1 1 1 ...\n $ foreign_worker         : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 1 1 1 1 1 2 2 ...\n $ credit_risk            : Factor w/ 2 levels \"bad\",\"good\": 2 2 2 2 2 2 2 2 2 2 ...\n```\n:::\n:::\n\n\nOur dataset has 1000 observations and 21 columns.\nThe variable we want to predict is `credit_risk` (either good or bad), i.e., we aim to classify people by their credit risk.\n\nWe also recommend the [skimr](https://cran.r-project.org/package=skimr) package as it creates very well readable and understandable overviews:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nskimr::skim(german)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |       |\n|:------------------------|:------|\n|Name                     |german |\n|Number of rows           |1000   |\n|Number of columns        |21     |\n|_______________________  |       |\n|Column type frequency:   |       |\n|factor                   |18     |\n|numeric                  |3      |\n|________________________ |       |\n|Group variables          |None   |\n\n\n**Variable type: factor**\n\n|skim_variable           | n_missing| complete_rate|ordered | n_unique|top_counts                             |\n|:-----------------------|---------:|-------------:|:-------|--------:|:--------------------------------------|\n|status                  |         0|             1|FALSE   |        4|...: 394, no : 274, ...: 269, 0<=: 63  |\n|credit_history          |         0|             1|FALSE   |        5|no : 530, all: 293, exi: 88, cri: 49   |\n|purpose                 |         0|             1|FALSE   |       10|fur: 280, oth: 234, car: 181, car: 103 |\n|savings                 |         0|             1|FALSE   |        5|unk: 603, ...: 183, ...: 103, 100: 63  |\n|employment_duration     |         0|             1|FALSE   |        5|1 <: 339, >= : 253, 4 <: 174, < 1: 172 |\n|installment_rate        |         0|             1|TRUE    |        4|< 2: 476, 25 : 231, 20 : 157, >= : 136 |\n|personal_status_sex     |         0|             1|FALSE   |        4|mal: 548, fem: 310, fem: 92, mal: 50   |\n|other_debtors           |         0|             1|FALSE   |        3|non: 907, gua: 52, co-: 41             |\n|present_residence       |         0|             1|TRUE    |        4|>= : 413, 1 <: 308, 4 <: 149, < 1: 130 |\n|property                |         0|             1|FALSE   |        4|bui: 332, unk: 282, car: 232, rea: 154 |\n|other_installment_plans |         0|             1|FALSE   |        3|non: 814, ban: 139, sto: 47            |\n|housing                 |         0|             1|FALSE   |        3|ren: 714, for: 179, own: 107           |\n|number_credits          |         0|             1|TRUE    |        4|1: 633, 2-3: 333, 4-5: 28, >= : 6      |\n|job                     |         0|             1|FALSE   |        4|ski: 630, uns: 200, man: 148, une: 22  |\n|people_liable           |         0|             1|FALSE   |        2|0 t: 845, 3 o: 155                     |\n|telephone               |         0|             1|FALSE   |        2|no: 596, yes: 404                      |\n|foreign_worker          |         0|             1|FALSE   |        2|no: 963, yes: 37                       |\n|credit_risk             |         0|             1|FALSE   |        2|goo: 700, bad: 300                     |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|    mean|      sd|  p0|    p25|    p50|     p75|  p100|hist  |\n|:-------------|---------:|-------------:|-------:|-------:|---:|------:|------:|-------:|-----:|:-----|\n|duration      |         0|             1|   20.90|   12.06|   4|   12.0|   18.0|   24.00|    72|‚ñá‚ñá‚ñÇ‚ñÅ‚ñÅ |\n|amount        |         0|             1| 3271.25| 2822.75| 250| 1365.5| 2319.5| 3972.25| 18424|‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ |\n|age           |         0|             1|   35.54|   11.35|  19|   27.0|   33.0|   42.00|    75|‚ñá‚ñÜ‚ñÉ‚ñÅ‚ñÅ |\n:::\n:::\n\n\nDuring an exploratory analysis meaningful discoveries could be:\n\n- Skewed distributions\n- Missing values\n- Empty / rare factor variables\n\nAn explanatory analysis is crucial to get a feeling for your data.\nOn the other hand the data can be validated this way.\nNon-plausible data can be investigated or outliers can be removed.\n\nAfter feeling confident with the data, we want to do modeling now.\n\n## Modeling\n\nConsidering how we are going to tackle the problem of classifying the credit risk relates closely to what [mlr3](https://mlr3.mlr-org.com) entities we will use.\n\nThe typical questions that arise when building a machine learning workflow are:\n\n- What is the problem we are trying to solve?\n- What are appropriate learning algorithms?\n- How do we evaluate \"good\" performance?\n\nMore systematically in [mlr3](https://mlr3.mlr-org.com) they can be expressed via five components:\n\n1. The [`Task`](https://mlr3.mlr-org.com/reference/Task.html) definition.\n2. The [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) definition.\n3. The training.\n4. The prediction.\n5. The evaluation via one or multiple [`Measures`](https://mlr3.mlr-org.com/reference/Measure.html).\n\n### Task Definition\n\nFirst, we are interested in the target which we want to model.\nMost supervised machine learning problems are **regression** or **classification** problems.\nHowever, note that other problems include unsupervised learning or time-to-event data (covered in [mlr3proba](https://mlr3proba.mlr-org.com)).\n\nWithin [mlr3](https://mlr3.mlr-org.com), to distinguish between these problems, we define [`Tasks`](https://mlr3.mlr-org.com/reference/Task.html).\nIf we want to solve a classification problem, we define a classification task -- [`TaskClassif`](https://mlr3.mlr-org.com/reference/TaskClassif.html).\nFor a regression problem, we define a regression task -- [`TaskRegr`](https://mlr3.mlr-org.com/reference/TaskRegr.html).\n\nIn our case it is clearly our objective to model or predict the binary `factor` variable `credit_risk`.\nThus, we define a [`TaskClassif`](https://mlr3.mlr-org.com/reference/TaskClassif.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_classif(german, id = \"GermanCredit\", target = \"credit_risk\")\n```\n:::\n\n\nNote that the German credit data is also given as an example task which ships with the [mlr3](https://mlr3.mlr-org.com) package.\nThus, you actually don't need to construct it yourself, just call `tsk(\"german_credit\")` to retrieve the object from the dictionary [`mlr_tasks`](https://mlr3.mlr-org.com/reference/mlr_tasks.html).\n\n### Learner Definition\n\nAfter having decided *what* should be modeled, we need to decide on *how*.\nThis means we need to decide which learning algorithms, or [`Learners`](https://mlr3.mlr-org.com/reference/Learner.html) are appropriate.\nUsing prior knowledge (e.g. knowing that it is a classification task or assuming that the classes are linearly separable) one ends up with one or more suitable [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html)s.\n\nMany learners can be obtained via the [mlr3learners](https://mlr3learners.mlr-org.com) package.\nAdditionally, many learners are provided via the [mlr3extralearners](https://mlr3extralearners.mlr-org.com) package, from GitHub.\nThese two resources combined account for a large fraction of standard learning algorithms.\nAs [mlr3](https://mlr3.mlr-org.com) usually only wraps learners from packages, it is even easy to create a formal [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) by yourself.\nYou may find the section about [extending mlr3](https://mlr3book.mlr-org.com/extending.html) in the [mlr3book](https://cran.r-project.org/package=mlr3book) very helpful.\nIf you happen to write your own [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) in [mlr3](https://mlr3.mlr-org.com), we would be happy if you share it with the [mlr3](https://mlr3.mlr-org.com) community.\n\nAll available [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html)s (i.e. all which you have installed from [mlr3](https://mlr3.mlr-org.com), `mlr3learners`, `mlr3extralearners`, or self-written ones) are registered in the dictionary [`mlr_learners`](https://mlr3.mlr-org.com/reference/mlr_learners.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlr_learners\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<DictionaryLearner> with 134 stored values\nKeys: classif.AdaBoostM1, classif.bart, classif.C50, classif.catboost, classif.cforest, classif.ctree,\n  classif.cv_glmnet, classif.debug, classif.earth, classif.featureless, classif.fnn, classif.gam,\n  classif.gamboost, classif.gausspr, classif.gbm, classif.glmboost, classif.glmer, classif.glmnet,\n  classif.IBk, classif.J48, classif.JRip, classif.kknn, classif.ksvm, classif.lda, classif.liblinear,\n  classif.lightgbm, classif.LMT, classif.log_reg, classif.lssvm, classif.mob, classif.multinom,\n  classif.naive_bayes, classif.nnet, classif.OneR, classif.PART, classif.qda, classif.randomForest,\n  classif.ranger, classif.rfsrc, classif.rpart, classif.svm, classif.xgboost, clust.agnes, clust.ap,\n  clust.cmeans, clust.cobweb, clust.dbscan, clust.diana, clust.em, clust.fanny, clust.featureless,\n  clust.ff, clust.hclust, clust.kkmeans, clust.kmeans, clust.MBatchKMeans, clust.mclust, clust.meanshift,\n  clust.pam, clust.SimpleKMeans, clust.xmeans, dens.kde_ks, dens.locfit, dens.logspline, dens.mixed,\n  dens.nonpar, dens.pen, dens.plug, dens.spline, regr.bart, regr.catboost, regr.cforest, regr.ctree,\n  regr.cubist, regr.cv_glmnet, regr.debug, regr.earth, regr.featureless, regr.fnn, regr.gam, regr.gamboost,\n  regr.gausspr, regr.gbm, regr.glm, regr.glmboost, regr.glmnet, regr.IBk, regr.kknn, regr.km, regr.ksvm,\n  regr.liblinear, regr.lightgbm, regr.lm, regr.lmer, regr.M5Rules, regr.mars, regr.mob, regr.nnet,\n  regr.randomForest, regr.ranger, regr.rfsrc, regr.rpart, regr.rsm, regr.rvm, regr.svm, regr.xgboost,\n  surv.akritas, surv.aorsf, surv.blackboost, surv.cforest, surv.coxboost, surv.coxtime, surv.ctree,\n  surv.cv_coxboost, surv.cv_glmnet, surv.deephit, surv.deepsurv, surv.dnnsurv, surv.flexible,\n  surv.gamboost, surv.gbm, surv.glmboost, surv.glmnet, surv.loghaz, surv.mboost, surv.nelson,\n  surv.obliqueRSF, surv.parametric, surv.pchazard, surv.penalized, surv.ranger, surv.rfsrc, surv.svm,\n  surv.xgboost\n```\n:::\n:::\n\n\nFor our problem, a suitable learner could be one of the following:\nLogistic regression, CART, random forest (or many more).\n\nA learner can be initialized with the [`lrn()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html) function and the name of the learner, e.g., `lrn(\"classif.xxx\")`.\nUse `?mlr_learners_xxx` to open the help page of a learner named `xxx`.\n\nFor example, a logistic regression can be initialized in the following manner (logistic regression uses R's `glm()` function and is provided by the `mlr3learners` package):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3learners\")\nlearner_logreg = lrn(\"classif.log_reg\")\nprint(learner_logreg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<LearnerClassifLogReg:classif.log_reg>\n* Model: -\n* Parameters: list()\n* Packages: mlr3, mlr3learners, stats\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: loglik, twoclass\n```\n:::\n:::\n\n\n### Training\n\nTraining is the procedure, where a model is fitted on the (training) data.\n\n#### Logistic Regression\n\nWe start with the example of the logistic regression.\nHowever, you will immediately see that the procedure generalizes to any learner very easily.\n\nAn initialized learner can be trained on data using `$train()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_logreg$train(task)\n```\n:::\n\n\nTypically, in machine learning, one does not use the full data which is available but a subset, the so-called training data.\n\nTo efficiently perform a split of the data one could do the following:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain_set = sample(task$row_ids, 0.8 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n```\n:::\n\n\n80 percent of the data is used for training.\nThe remaining 20 percent are used for evaluation at a subsequent later point in time.\n`train_set` is an integer vector referring to the selected rows of the original dataset:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(train_set)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 690 301 747 782 823 614\n```\n:::\n:::\n\n\nIn [mlr3](https://mlr3.mlr-org.com) the training with a subset of the data can be declared by the additional argument `row_ids = train_set`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_logreg$train(task, row_ids = train_set)\n```\n:::\n\n\nThe fitted model can be accessed via:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_logreg$model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:  stats::glm(formula = task$formula(), family = \"binomial\", data = data, \n    model = FALSE)\n\nCoefficients:\n                                              (Intercept)                                                        age  \n                                                1.5127522                                                 -0.0207415  \n                                                   amount     credit_historycritical account/other credits elsewhere  \n                                                0.0001248                                                 -0.0714159  \ncredit_historyno credits taken/all credits paid back duly     credit_historyexisting credits paid back duly till now  \n                                               -0.6681280                                                 -0.9100879  \n    credit_historyall credits at this bank paid back duly                                                   duration  \n                                               -1.7423617                                                  0.0275806  \n                                employment_duration< 1 yr                        employment_duration1 <= ... < 4 yrs  \n                                               -0.0388128                                                 -0.1412623  \n                      employment_duration4 <= ... < 7 yrs                                employment_duration>= 7 yrs  \n                                               -0.8248727                                                 -0.1095270  \n                                         foreign_workerno                                                housingrent  \n                                                0.9861260                                                 -0.2409026  \n                                               housingown                                         installment_rate.L  \n                                               -0.3362948                                                  0.6327691  \n                                       installment_rate.Q                                         installment_rate.C  \n                                                0.0395784                                                  0.0321240  \n                                  jobunskilled - resident                               jobskilled employee/official  \n                                                0.0531632                                                  0.0842291  \n            jobmanager/self-empl./highly qualif. employee                                           number_credits.L  \n                                                0.0151071                                                  1.1176306  \n                                         number_credits.Q                                           number_credits.C  \n                                                0.4436624                                                  0.3561489  \n                                other_debtorsco-applicant                                     other_debtorsguarantor  \n                                                0.2094952                                                 -1.0630499  \n                            other_installment_plansstores                                other_installment_plansnone  \n                                               -0.0119907                                                 -0.2509714  \n                                      people_liable0 to 2    personal_status_sexfemale : non-single or male : single  \n                                               -0.1806938                                                  0.1207635  \n                personal_status_sexmale : married/widowed                         personal_status_sexfemale : single  \n                                               -0.5024804                                                 -0.2640719  \n                                      present_residence.L                                        present_residence.Q  \n                                                0.3716018                                                 -0.4180157  \n                                      present_residence.C                                       propertycar or other  \n                                                0.3618467                                                  0.1598825  \n        propertybuilding soc. savings agr./life insurance                                        propertyreal estate  \n                                               -0.1110857                                                  0.4590346  \n                                         purposecar (new)                                          purposecar (used)  \n                                               -1.3325409                                                 -0.5761228  \n                               purposefurniture/equipment                                    purposeradio/television  \n                                               -0.7649856                                                 -0.6956411  \n                               purposedomestic appliances                                             purposerepairs  \n                                               -0.1394842                                                  0.4548465  \n                                          purposevacation                                          purposeretraining  \n                                               -1.0990717                                                 -0.4418095  \n                                          purposebusiness                                       savings... <  100 DM  \n                                               -2.1284440                                                 -0.3344746  \n                              savings100 <= ... <  500 DM                                savings500 <= ... < 1000 DM  \n                                               -0.0490406                                                 -1.0170624  \n                                    savings... >= 1000 DM                                           status... < 0 DM  \n                                               -1.2475963                                                 -0.5985399  \n                                   status0<= ... < 200 DM           status... >= 200 DM / salary for at least 1 year  \n                                               -0.9322632                                                 -1.7481642  \n                       telephoneyes (under customer name)  \n                                               -0.2000152  \n\nDegrees of Freedom: 799 Total (i.e. Null);  745 Residual\nNull Deviance:\t    974 \nResidual Deviance: 716.6 \tAIC: 826.6\n```\n:::\n:::\n\n\nThe stored object is a normal `glm` object and all its `S3` methods work as expected:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclass(learner_logreg$model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"glm\" \"lm\" \n```\n:::\n\n```{.r .cell-code}\nsummary(learner_logreg$model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nstats::glm(formula = task$formula(), family = \"binomial\", data = data, \n    model = FALSE)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0004  -0.6940  -0.3923   0.6727   2.7051  \n\nCoefficients:\n                                                            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                                                1.513e+00  1.312e+00   1.153 0.248756    \nage                                                       -2.074e-02  1.097e-02  -1.890 0.058767 .  \namount                                                     1.248e-04  5.163e-05   2.417 0.015650 *  \ncredit_historycritical account/other credits elsewhere    -7.142e-02  6.394e-01  -0.112 0.911071    \ncredit_historyno credits taken/all credits paid back duly -6.681e-01  4.940e-01  -1.352 0.176247    \ncredit_historyexisting credits paid back duly till now    -9.101e-01  5.341e-01  -1.704 0.088398 .  \ncredit_historyall credits at this bank paid back duly     -1.742e+00  4.956e-01  -3.516 0.000439 ***\nduration                                                   2.758e-02  1.078e-02   2.558 0.010534 *  \nemployment_duration< 1 yr                                 -3.881e-02  5.067e-01  -0.077 0.938937    \nemployment_duration1 <= ... < 4 yrs                       -1.413e-01  4.795e-01  -0.295 0.768305    \nemployment_duration4 <= ... < 7 yrs                       -8.249e-01  5.269e-01  -1.565 0.117491    \nemployment_duration>= 7 yrs                               -1.095e-01  4.896e-01  -0.224 0.822969    \nforeign_workerno                                           9.861e-01  6.391e-01   1.543 0.122822    \nhousingrent                                               -2.409e-01  2.763e-01  -0.872 0.383270    \nhousingown                                                -3.363e-01  5.539e-01  -0.607 0.543770    \ninstallment_rate.L                                         6.328e-01  2.410e-01   2.626 0.008636 ** \ninstallment_rate.Q                                         3.958e-02  2.184e-01   0.181 0.856182    \ninstallment_rate.C                                         3.212e-02  2.264e-01   0.142 0.887162    \njobunskilled - resident                                    5.316e-02  7.551e-01   0.070 0.943868    \njobskilled employee/official                               8.423e-02  7.273e-01   0.116 0.907803    \njobmanager/self-empl./highly qualif. employee              1.511e-02  7.459e-01   0.020 0.983841    \nnumber_credits.L                                           1.118e+00  9.754e-01   1.146 0.251869    \nnumber_credits.Q                                           4.437e-01  7.976e-01   0.556 0.578031    \nnumber_credits.C                                           3.561e-01  5.467e-01   0.651 0.514745    \nother_debtorsco-applicant                                  2.095e-01  4.821e-01   0.435 0.663868    \nother_debtorsguarantor                                    -1.063e+00  4.797e-01  -2.216 0.026690 *  \nother_installment_plansstores                             -1.199e-02  4.977e-01  -0.024 0.980778    \nother_installment_plansnone                               -2.510e-01  2.885e-01  -0.870 0.384427    \npeople_liable0 to 2                                       -1.807e-01  2.865e-01  -0.631 0.528175    \npersonal_status_sexfemale : non-single or male : single    1.208e-01  4.513e-01   0.268 0.789006    \npersonal_status_sexmale : married/widowed                 -5.025e-01  4.423e-01  -1.136 0.255955    \npersonal_status_sexfemale : single                        -2.641e-01  5.302e-01  -0.498 0.618470    \npresent_residence.L                                        3.716e-01  2.415e-01   1.539 0.123875    \npresent_residence.Q                                       -4.180e-01  2.259e-01  -1.851 0.064237 .  \npresent_residence.C                                        3.618e-01  2.271e-01   1.593 0.111124    \npropertycar or other                                       1.599e-01  2.851e-01   0.561 0.574982    \npropertybuilding soc. savings agr./life insurance         -1.111e-01  2.641e-01  -0.421 0.674034    \npropertyreal estate                                        4.590e-01  4.805e-01   0.955 0.339400    \npurposecar (new)                                          -1.333e+00  4.170e-01  -3.195 0.001396 ** \npurposecar (used)                                         -5.761e-01  2.952e-01  -1.952 0.050971 .  \npurposefurniture/equipment                                -7.650e-01  2.812e-01  -2.720 0.006528 ** \npurposeradio/television                                   -6.956e-01  8.332e-01  -0.835 0.403747    \npurposedomestic appliances                                -1.395e-01  5.848e-01  -0.239 0.811482    \npurposerepairs                                             4.548e-01  4.390e-01   1.036 0.300150    \npurposevacation                                           -1.099e+00  1.239e+00  -0.887 0.375130    \npurposeretraining                                         -4.418e-01  3.797e-01  -1.164 0.244560    \npurposebusiness                                           -2.128e+00  9.751e-01  -2.183 0.029048 *  \nsavings... <  100 DM                                      -3.345e-01  3.224e-01  -1.038 0.299468    \nsavings100 <= ... <  500 DM                               -4.904e-02  4.204e-01  -0.117 0.907136    \nsavings500 <= ... < 1000 DM                               -1.017e+00  5.571e-01  -1.826 0.067902 .  \nsavings... >= 1000 DM                                     -1.248e+00  3.161e-01  -3.946 7.94e-05 ***\nstatus... < 0 DM                                          -5.985e-01  2.492e-01  -2.402 0.016299 *  \nstatus0<= ... < 200 DM                                    -9.323e-01  4.176e-01  -2.232 0.025583 *  \nstatus... >= 200 DM / salary for at least 1 year          -1.748e+00  2.564e-01  -6.817 9.31e-12 ***\ntelephoneyes (under customer name)                        -2.000e-01  2.281e-01  -0.877 0.380571    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 973.97  on 799  degrees of freedom\nResidual deviance: 716.55  on 745  degrees of freedom\nAIC: 826.55\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n\n#### Random Forest\n\nJust like the logistic regression, we could train a random forest instead.\nWe use the fast implementation from the [ranger](https://cran.r-project.org/package=ranger) package.\nFor this, we first need to define the learner and then actually train it.\n\nWe now additionally supply the importance argument (`importance = \"permutation\"`).\nDoing so, we override the default and let the learner do feature importance determination based on permutation feature importance:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_rf = lrn(\"classif.ranger\", importance = \"permutation\")\nlearner_rf$train(task, row_ids = train_set)\n```\n:::\n\n\nWe can access the importance values using `$importance()`:\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_rf$importance()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 status                duration          credit_history                  amount                     age \n           2.907670e-02            1.847976e-02            1.371827e-02            1.180288e-02            6.346851e-03 \n                savings                property     employment_duration           other_debtors          number_credits \n           5.275841e-03            3.886640e-03            3.828736e-03            3.267589e-03            3.072534e-03 \n                purpose        installment_rate                 housing       present_residence                     job \n           2.385257e-03            2.257738e-03            1.454765e-03            1.175556e-03            8.880849e-04 \n    personal_status_sex               telephone          foreign_worker other_installment_plans           people_liable \n           7.865194e-04            4.485976e-04            1.600356e-04            4.673067e-05           -3.021458e-04 \n```\n:::\n:::\n\n\nIn order to obtain a plot for the importance values, we convert the importance to a [data.table](https://cran.r-project.org/package=data.table) and then process it with [ggplot2](https://cran.r-project.org/package=ggplot2):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimportance = as.data.table(learner_rf$importance(), keep.rownames = TRUE)\ncolnames(importance) = c(\"Feature\", \"Importance\")\nggplot(importance, aes(x = reorder(Feature, Importance), y = Importance)) +\n  geom_col() + coord_flip() + xlab(\"\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-03-11-basics-german-credit-016-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n### Prediction\n\nLet's see what the models predict.\n\nAfter training a  model, the model can be used for prediction.\nUsually, prediction is the main purpose of machine learning models.\n\nIn our case, the model can be used to classify new credit applicants w.r.t. their associated credit risk (good vs. bad) on the basis of the features.\nTypically, machine learning models predict numeric values.\nIn the regression case this is very natural.\nFor classification, most models predict scores or probabilities.\nBased on these values, one can derive class predictions.\n\n#### Predict Classes\n\nFirst, we directly predict classes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction_logreg = learner_logreg$predict(task, row_ids = test_set)\nprediction_rf = learner_rf$predict(task, row_ids = test_set)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction_logreg\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 200 observations:\n    row_ids truth response\n          3  good     good\n          8  good     good\n         22  good     good\n---                       \n        984   bad      bad\n        992   bad      bad\n        999   bad     good\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction_rf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 200 observations:\n    row_ids truth response\n          3  good     good\n          8  good     good\n         22  good     good\n---                       \n        984   bad      bad\n        992   bad      bad\n        999   bad     good\n```\n:::\n:::\n\n\nThe `$predict()` method returns  a [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html) object.\nIt can be converted to a [`data.table`](https://www.rdocumentation.org/packages/data.table/topics/data.table-package) if one wants to use it downstream.\n\nWe can also display the prediction results aggregated in a confusion matrix:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprediction_logreg$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        truth\nresponse bad good\n    bad   32   15\n    good  30  123\n```\n:::\n\n```{.r .cell-code}\nprediction_rf$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        truth\nresponse bad good\n    bad   27    9\n    good  35  129\n```\n:::\n:::\n\n\n#### Predict Probabilities\n\nMost learners may not only predict a class variable (\"response\"), but also their degree of \"belief\" / \"uncertainty\" in a given response.\nTypically, we achieve this by setting the `$predict_type` slot of a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) to `\"prob\"`.\nSometimes this needs to be done *before* the learner is trained.\nAlternatively, we can directly create the learner with this option:\n`lrn(\"classif.log_reg\", predict_type = \"prob\")`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_logreg$predict_type = \"prob\"\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_logreg$predict(task, row_ids = test_set)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 200 observations:\n    row_ids truth response   prob.bad prob.good\n          3  good     good 0.21519298 0.7848070\n          8  good     good 0.08866662 0.9113334\n         22  good     good 0.18817928 0.8118207\n---                                            \n        984   bad      bad 0.80063978 0.1993602\n        992   bad      bad 0.72086955 0.2791305\n        999   bad     good 0.05175198 0.9482480\n```\n:::\n:::\n\n\nNote that sometimes one needs to be cautious when dealing with the probability interpretation of the predictions.\n\n### Performance Evaluation\n\nTo measure the performance of a learner on new unseen data, we usually mimic the scenario of unseen data by splitting up the data into training and test set.\nThe training set is used for training the learner, and the test set is only used for predicting and evaluating the performance of the trained learner.\nNumerous resampling methods (cross-validation, bootstrap) repeat the splitting process in different ways.\n\nWithin [mlr3](https://mlr3.mlr-org.com), we need to specify the resampling strategy using the [`rsmp()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html) function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling = rsmp(\"holdout\", ratio = 2/3)\nprint(resampling)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResamplingHoldout>: Holdout\n* Iterations: 1\n* Instantiated: FALSE\n* Parameters: ratio=0.6667\n```\n:::\n:::\n\n\nHere, we use \"holdout\", a simple train-test split (with just one iteration).\nWe use the [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) function to undertake the resampling calculation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres = resample(task, learner = learner_logreg, resampling = resampling)\nres\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResampleResult> of 1 iterations\n* Task: GermanCredit\n* Learner: classif.log_reg\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n```\n:::\n:::\n\n\nThe default score of the measure is included in the `$aggregate()` slot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.ce \n 0.2312312 \n```\n:::\n:::\n\n\nThe default measure in this scenario is the [`classification error`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.ce.html).\nLower is better.\n\nWe can easily run different resampling strategies, e.g. repeated holdout (`\"subsampling\"`), or cross validation.\nMost methods perform repeated train/predict cycles on different data subsets and aggregate the result (usually as the mean).\nDoing this manually would require us to write loops.\n[mlr3](https://mlr3.mlr-org.com) does the job for us:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling = rsmp(\"subsampling\", repeats = 10)\nrr = resample(task, learner = learner_logreg, resampling = resampling)\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.ce \n 0.2567568 \n```\n:::\n:::\n\n\nInstead, we could also run cross-validation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling = resampling = rsmp(\"cv\", folds = 10)\nrr = resample(task, learner = learner_logreg, resampling = resampling)\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.ce \n      0.25 \n```\n:::\n:::\n\n\n[mlr3](https://mlr3.mlr-org.com) features scores for many more measures.\nHere, we apply [`mlr_measures_classif.fpr`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.fpr.html) for the false positive rate, and [`mlr_measures_classif.fnr`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.fnr.html) for the false negative rate.\nMultiple measures can be provided as a list of measures (which can directly be constructed via [`msrs()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# false positive rate\nrr$aggregate(msr(\"classif.fpr\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.fpr \n  0.1323901 \n```\n:::\n\n```{.r .cell-code}\n# false positive rate and false negative\nmeasures = msrs(c(\"classif.fpr\", \"classif.fnr\"))\nrr$aggregate(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.fpr classif.fnr \n  0.1323901   0.5242329 \n```\n:::\n:::\n\n\nThere are a few more resampling methods, and quite a few more measures (implemented in [mlr3measures](https://cran.r-project.org/package=mlr3measures)).\nThey are automatically registered in the respective dictionaries:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlr_resamplings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<DictionaryResampling> with 9 stored values\nKeys: bootstrap, custom, custom_cv, cv, holdout, insample, loo, repeated_cv, subsampling\n```\n:::\n\n```{.r .cell-code}\nmlr_measures\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<DictionaryMeasure> with 67 stored values\nKeys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier, classif.ce, classif.costs,\n  classif.dor, classif.fbeta, classif.fdr, classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,\n  classif.logloss, classif.mauc_au1p, classif.mauc_au1u, classif.mauc_aunp, classif.mauc_aunu,\n  classif.mbrier, classif.mcc, classif.npv, classif.ppv, classif.prauc, classif.precision, classif.recall,\n  classif.sensitivity, classif.specificity, classif.tn, classif.tnr, classif.tp, classif.tpr, clust.ch,\n  clust.db, clust.dunn, clust.silhouette, clust.wss, debug, oob_error, regr.bias, regr.ktau, regr.mae,\n  regr.mape, regr.maxae, regr.medae, regr.medse, regr.mse, regr.msle, regr.pbias, regr.rae, regr.rmse,\n  regr.rmsle, regr.rrse, regr.rse, regr.rsq, regr.sae, regr.smape, regr.srho, regr.sse, selected_features,\n  sim.jaccard, sim.phi, time_both, time_predict, time_train\n```\n:::\n:::\n\n\nTo get help on a resampling method, use `?mlr_resamplings_xxx`, for a measure do `?mlr_measures_xxx`.\nYou can also browse the [mlr3 reference](https://mlr3.mlr-org.com/reference/index.html) online.\n\nNote that some measures, for example [`AUC`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.auc.html), require the prediction of probabilities.\n\n### Performance Comparison and Benchmarks\n\nWe could compare [`Learners`](https://mlr3.mlr-org.com/reference/Learner.html) by evaluating [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) for each of them manually.\nHowever, [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) automatically performs resampling evaluations for multiple learners and tasks.\n[`benchmark_grid()`](https://mlr3.mlr-org.com/reference/benchmark_grid.html) creates fully crossed designs:\nMultiple [`Learners`](https://mlr3.mlr-org.com/reference/Learner.html) for multiple [`Tasks`](https://mlr3.mlr-org.com/reference/Task.html) are compared w.r.t. multiple [`Resamplings`](https://mlr3.mlr-org.com/reference/Resampling.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearners = lrns(c(\"classif.log_reg\", \"classif.ranger\"), predict_type = \"prob\")\ngrid = benchmark_grid(\n  tasks = task,\n  learners = learners,\n  resamplings = rsmp(\"cv\", folds = 10)\n)\nbmr = benchmark(grid)\n```\n:::\n\n\nCareful, large benchmarks may take a long time! This one should take less than a minute, however.\nIn general, we want to use parallelization to speed things up on multi-core machines.\nFor parallelization, [mlr3](https://mlr3.mlr-org.com) relies on the [future](https://cran.r-project.org/package=future) package:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# future::plan(\"multicore\") # uncomment for parallelization\n```\n:::\n\n\nIn the benchmark we can compare different measures.\nHere, we look at the [`misclassification rate`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.ce.html) and the [`AUC`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.auc.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"classif.ce\", \"classif.auc\"))\nperformances = bmr$aggregate(measures)\nperformances[, c(\"learner_id\", \"classif.ce\", \"classif.auc\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        learner_id classif.ce classif.auc\n1: classif.log_reg      0.251   0.7689745\n2:  classif.ranger      0.232   0.7958243\n```\n:::\n:::\n\n\nWe see that the two models perform very similarly.\n\n## Deviating from hyperparameters defaults\n\nThe previously shown techniques build the backbone of a [mlr3](https://mlr3.mlr-org.com)-featured machine learning workflow.\nHowever, in most cases one would never proceed in the way we did.\nWhile many R packages have carefully selected default settings, they will not perform optimally in any scenario.\nTypically, we can select the values of such hyperparameters.\nThe (hyper)parameters of a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) can be accessed and set via its [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) `$param_set`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_rf$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n                              id    class lower upper nlevels        default    parents       value\n 1:                        alpha ParamDbl  -Inf   Inf     Inf            0.5                       \n 2:       always.split.variables ParamUty    NA    NA     Inf <NoDefault[3]>                       \n 3:                class.weights ParamUty    NA    NA     Inf                                      \n 4:                      holdout ParamLgl    NA    NA       2          FALSE                       \n 5:                   importance ParamFct    NA    NA       4 <NoDefault[3]>            permutation\n 6:                   keep.inbag ParamLgl    NA    NA       2          FALSE                       \n 7:                    max.depth ParamInt     0   Inf     Inf                                      \n 8:                min.node.size ParamInt     1   Inf     Inf                                      \n 9:                     min.prop ParamDbl  -Inf   Inf     Inf            0.1                       \n10:                      minprop ParamDbl  -Inf   Inf     Inf            0.1                       \n11:                         mtry ParamInt     1   Inf     Inf <NoDefault[3]>                       \n12:                   mtry.ratio ParamDbl     0     1     Inf <NoDefault[3]>                       \n13:            num.random.splits ParamInt     1   Inf     Inf              1  splitrule            \n14:                  num.threads ParamInt     1   Inf     Inf              1                      1\n15:                    num.trees ParamInt     1   Inf     Inf            500                       \n16:                    oob.error ParamLgl    NA    NA       2           TRUE                       \n17:        regularization.factor ParamUty    NA    NA     Inf              1                       \n18:      regularization.usedepth ParamLgl    NA    NA       2          FALSE                       \n19:                      replace ParamLgl    NA    NA       2           TRUE                       \n20:    respect.unordered.factors ParamFct    NA    NA       3         ignore                       \n21:              sample.fraction ParamDbl     0     1     Inf <NoDefault[3]>                       \n22:                  save.memory ParamLgl    NA    NA       2          FALSE                       \n23: scale.permutation.importance ParamLgl    NA    NA       2          FALSE importance            \n24:                    se.method ParamFct    NA    NA       2        infjack                       \n25:                         seed ParamInt  -Inf   Inf     Inf                                      \n26:         split.select.weights ParamUty    NA    NA     Inf                                      \n27:                    splitrule ParamFct    NA    NA       3           gini                       \n28:                      verbose ParamLgl    NA    NA       2           TRUE                       \n29:                 write.forest ParamLgl    NA    NA       2           TRUE                       \n                              id    class lower upper nlevels        default    parents       value\n```\n:::\n\n```{.r .cell-code}\nlearner_rf$param_set$values = list(verbose = FALSE)\n```\n:::\n\n\nWe can choose parameters for our learners in two distinct manners.\nIf we have prior knowledge on how the learner should be (hyper-)parameterized, the way to go would be manually entering the parameters in the parameter set.\nIn most cases, however, we would want to tune the learner so that it can search \"good\" model configurations itself.\nFor now, we only want to compare a few models.\n\nTo get an idea on which parameters can be manipulated, we can investigate the parameters of the original package version or look into the parameter set of the learner:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## ?ranger::ranger\nas.data.table(learner_rf$param_set)[, .(id, class, lower, upper)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                              id    class lower upper\n 1:                        alpha ParamDbl  -Inf   Inf\n 2:       always.split.variables ParamUty    NA    NA\n 3:                class.weights ParamUty    NA    NA\n 4:                      holdout ParamLgl    NA    NA\n 5:                   importance ParamFct    NA    NA\n 6:                   keep.inbag ParamLgl    NA    NA\n 7:                    max.depth ParamInt     0   Inf\n 8:                min.node.size ParamInt     1   Inf\n 9:                     min.prop ParamDbl  -Inf   Inf\n10:                      minprop ParamDbl  -Inf   Inf\n11:                         mtry ParamInt     1   Inf\n12:                   mtry.ratio ParamDbl     0     1\n13:            num.random.splits ParamInt     1   Inf\n14:                  num.threads ParamInt     1   Inf\n15:                    num.trees ParamInt     1   Inf\n16:                    oob.error ParamLgl    NA    NA\n17:        regularization.factor ParamUty    NA    NA\n18:      regularization.usedepth ParamLgl    NA    NA\n19:                      replace ParamLgl    NA    NA\n20:    respect.unordered.factors ParamFct    NA    NA\n21:              sample.fraction ParamDbl     0     1\n22:                  save.memory ParamLgl    NA    NA\n23: scale.permutation.importance ParamLgl    NA    NA\n24:                    se.method ParamFct    NA    NA\n25:                         seed ParamInt  -Inf   Inf\n26:         split.select.weights ParamUty    NA    NA\n27:                    splitrule ParamFct    NA    NA\n28:                      verbose ParamLgl    NA    NA\n29:                 write.forest ParamLgl    NA    NA\n                              id    class lower upper\n```\n:::\n:::\n\n\nFor the random forest two meaningful parameters which steer model complexity are `num.trees` and `mtry`.\n`num.trees` defaults to `500` and `mtry` to `floor(sqrt(ncol(data) - 1))`, in our case 4.\n\nIn the following we aim to train three different learners:\n\n1. The default random forest.\n2. A random forest with **low** `num.trees` and **low** `mtry`.\n3. A random forest with **high** `num.trees` and **high** `mtry`.\n\nWe will benchmark their performance on the German credit dataset.\nFor this we construct the three learners and set the parameters accordingly:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrf_med = lrn(\"classif.ranger\", id = \"med\", predict_type = \"prob\")\n\nrf_low = lrn(\"classif.ranger\", id = \"low\", predict_type = \"prob\",\n  num.trees = 5, mtry = 2)\n\nrf_high = lrn(\"classif.ranger\", id = \"high\", predict_type = \"prob\",\n  num.trees = 1000, mtry = 11)\n```\n:::\n\n\nOnce the learners are defined, we can benchmark them:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearners = list(rf_low, rf_med, rf_high)\ngrid = benchmark_grid(\n  tasks = task,\n  learners = learners,\n  resamplings = rsmp(\"cv\", folds = 10)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr = benchmark(grid)\nprint(bmr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<BenchmarkResult> of 30 rows with 3 resampling runs\n nr      task_id learner_id resampling_id iters warnings errors\n  1 GermanCredit        low            cv    10        0      0\n  2 GermanCredit        med            cv    10        0      0\n  3 GermanCredit       high            cv    10        0      0\n```\n:::\n:::\n\n\nWe compare misclassification rate and AUC again:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"classif.ce\", \"classif.auc\"))\nperformances = bmr$aggregate(measures)\nperformances[, .(learner_id, classif.ce, classif.auc)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   learner_id classif.ce classif.auc\n1:        low      0.276   0.7264382\n2:        med      0.232   0.7984940\n3:       high      0.224   0.7931174\n```\n:::\n\n```{.r .cell-code}\nautoplot(bmr)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-03-11-basics-german-credit-038-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe \"low\" settings seem to underfit a bit, the \"high\" setting is comparable to the default setting \"med\".\n\n## Outlook\n\nThis tutorial was a detailed introduction to machine learning workflows within [mlr3](https://mlr3.mlr-org.com).\nHaving followed this tutorial you should be able to run your first models yourself.\nNext to that we spiked into performance evaluation and benchmarking.\nFurthermore, we showed how to customize learners.\n\nThe next parts of the tutorial will go more into depth into additional [mlr3](https://mlr3.mlr-org.com) topics:\n\n- [Part II - Tuning](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3tuning-tutorial-german-credit/) introduces you to the [mlr3tuning](https://mlr3tuning.mlr-org.com) package\n\n- [Part III - Pipelines](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3pipelines-tutorial-german-credit/) introduces you to the [mlr3pipelines](https://mlr3pipelines.mlr-org.com) package\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}