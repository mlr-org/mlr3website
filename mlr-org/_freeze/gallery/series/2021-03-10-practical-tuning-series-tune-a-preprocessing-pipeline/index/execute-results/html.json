{
  "hash": "1139c9b2e3f8e08a14bf2c789f062244",
  "result": {
    "markdown": "---\ntitle: Practical Tuning Series - Tune a Preprocessing Pipeline\ndescription: |\n  Build a simple preprocessing pipeline and tune it.\ncategories:\n  - tuning\n  - resampling\n  - mlr3pipelines\n  - classification\n  - practical tuning series\nauthor:\n  - name: Marc Becker\n  - name: Theresa Ullmann\n  - name: Michel Lang\n  - name: Bernd Bischl\n  - name: Jakob Richter\n  - name: Martin Binder\ndate: 2021-03-10\naliases:\n  - ../../../gallery/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline/index.html\n---\n\n\n\n\n\n\n# Scope\n\nThis is the second part of the practical tuning series.\nThe other parts can be found here:\n\n* [Part I - Tune a Support Vector Machine](https://mlr-org.com/gallery/series/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/)\n* [Part III - Build an Automated Machine Learning System](https://mlr-org.com/gallery/series/2021-03-11-practical-tuning-series-build-an-automated-machine-learning-system/)\n* [Part IV - Tuning and Parallel Processing](https://mlr-org.com/gallery/series/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/)\n\nIn this post, we build a simple preprocessing pipeline and tune it.\nFor this, we are using the [mlr3pipelines](https://mlr3pipelines.mlr-org.com) extension package.\nFirst, we start by imputing missing values in the [Pima Indians Diabetes data set](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html).\nAfter that, we encode a factor column to numerical dummy columns in the data set.\nNext, we combine both preprocessing steps to a [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) and create a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html).\nFinally, nested resampling is used to compare the performance of two imputation methods.\n\n# Prerequisites\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com)  package which pulls in the most important packages for this example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\nThe [`lgr`](https://mlr3book.mlr-org.com/logging.html) package is used for logging in all [mlr3](https://mlr3.mlr-org.com) packages.\nThe [mlr3](https://mlr3.mlr-org.com) logger prints the logging messages from the base package, whereas the [bbotk](https://bbotk.mlr-org.com)  logger is responsible for logging messages from the optimization packages (e.g. [mlr3tuning](https://mlr3tuning.mlr-org.com) ).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\nIn this example, we use the [Pima Indians Diabetes data set](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) which is used to predict whether or not a patient has diabetes.\nThe patients are characterized by 8 numeric features of which some have missing values.\nWe alter the data set by categorizing the feature `pressure` (blood pressure) into the categories `\"low\"`, `\"mid\"`, and `\"high\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# retrieve the task from mlr3\ntask = tsk(\"pima\")\n\n# create data frame with categorized pressure feature\ndata = task$data(cols = \"pressure\")\nbreaks = quantile(data$pressure, probs = c(0, 0.33, 0.66, 1), na.rm = TRUE)\ndata$pressure = cut(data$pressure, breaks, labels = c(\"low\", \"mid\", \"high\"))\n\n# overwrite the feature in the task\ntask$cbind(data)\n\n# generate a quick textual overview\nskimr::skim(task$data())\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |            |\n|:------------------------|:-----------|\n|Name                     |task$data() |\n|Number of rows           |768         |\n|Number of columns        |9           |\n|Key                      |NULL        |\n|_______________________  |            |\n|Column type frequency:   |            |\n|factor                   |2           |\n|numeric                  |7           |\n|________________________ |            |\n|Group variables          |None        |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts                   |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------------------|\n|diabetes      |         0|          1.00|FALSE   |        2|neg: 500, pos: 268           |\n|pressure      |        36|          0.95|FALSE   |        3|low: 282, mid: 245, hig: 205 |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|    p0|   p25|    p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|------:|-----:|-----:|------:|------:|------:|:-----|\n|age           |         0|          1.00|  33.24|  11.76| 21.00| 24.00|  29.00|  41.00|  81.00|▇▃▁▁▁ |\n|glucose       |         5|          0.99| 121.69|  30.54| 44.00| 99.00| 117.00| 141.00| 199.00|▁▇▇▃▂ |\n|insulin       |       374|          0.51| 155.55| 118.78| 14.00| 76.25| 125.00| 190.00| 846.00|▇▂▁▁▁ |\n|mass          |        11|          0.99|  32.46|   6.92| 18.20| 27.50|  32.30|  36.60|  67.10|▅▇▃▁▁ |\n|pedigree      |         0|          1.00|   0.47|   0.33|  0.08|  0.24|   0.37|   0.63|   2.42|▇▃▁▁▁ |\n|pregnant      |         0|          1.00|   3.85|   3.37|  0.00|  1.00|   3.00|   6.00|  17.00|▇▃▂▁▁ |\n|triceps       |       227|          0.70|  29.15|  10.48|  7.00| 22.00|  29.00|  36.00|  99.00|▆▇▁▁▁ |\n:::\n:::\n\n\nWe choose the xgboost algorithm from the [xgboost](https://cran.r-project.org/package=xgboost) package as learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.xgboost\", nrounds = 100, id = \"xgboost\", verbose = 0)\n```\n:::\n\n\n# Missing Values\n\nThe task has missing data in five columns.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(task$missings() / task$nrow, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure  triceps \n    0.00     0.00     0.01     0.49     0.01     0.00     0.00     0.05     0.30 \n```\n:::\n:::\n\n\nThe `xgboost` learner has an internal method for handling missing data but some learners cannot handle missing values.\nWe will try to beat the internal method in terms of predictive performance.\nThe [mlr3pipelines](https://mlr3pipelines.mlr-org.com) package offers various methods to impute missing values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlr_pipeops$keys(\"^impute\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"imputeconstant\" \"imputehist\"     \"imputelearner\"  \"imputemean\"     \"imputemedian\"   \"imputemode\"    \n[7] \"imputeoor\"      \"imputesample\"  \n```\n:::\n:::\n\n\nWe choose the [`PipeOpImputeOOR`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputeoor.html) that adds the new factor level `\".MISSING\".` to factorial features and imputes numerical features by constant values shifted below the minimum (default) or above the maximum.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimputer = po(\"imputeoor\")\nprint(imputer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPipeOp: <imputeoor> (not trained)\nvalues: <min=TRUE, offset=1, multiplier=1>\nInput channels <name [train type, predict type]>:\n  input [Task,Task]\nOutput channels <name [train type, predict type]>:\n  output [Task,Task]\n```\n:::\n:::\n\n\nAs the output suggests, the in- and output of this pipe operator is a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) for both the training and the predict step.\nWe can manually train the pipe operator to check its functionality:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_imputed = imputer$train(list(task))[[1]]\ntask_imputed$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndiabetes      age pedigree pregnant  glucose  insulin     mass pressure  triceps \n       0        0        0        0        0        0        0        0        0 \n```\n:::\n:::\n\n\nLet's compare an observation with missing values to the observation with imputed observation.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrbind(\n  task$data()[8,],\n  task_imputed$data()[8,]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   diabetes age glucose insulin mass pedigree pregnant pressure triceps\n1:      neg  29     115      NA 35.3    0.134       10     <NA>      NA\n2:      neg  29     115    -819 35.3    0.134       10 .MISSING     -86\n```\n:::\n:::\n\n\nNote that OOR imputation is in particular useful for tree-based models, but should not be used for linear models or distance-based models.\n\n# Factor Encoding\n\nThe `xgboost` learner cannot handle categorical features.\nTherefore, we must to convert factor columns to numerical dummy columns.\nFor this, we argument the `xgboost` learner with automatic factor encoding.\n\nThe [`PipeOpEncode`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_encode.html) encodes factor columns with one of six methods.\nIn this example, we use `one-hot` encoding which creates a new binary column for each factor level.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfactor_encoding = po(\"encode\", method = \"one-hot\")\n```\n:::\n\n\nWe manually trigger the encoding on the task.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfactor_encoding$train(list(task))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$output\n<TaskClassif:pima> (768 x 11): Pima Indian Diabetes\n* Target: diabetes\n* Properties: twoclass\n* Features (10):\n  - dbl (10): age, glucose, insulin, mass, pedigree, pregnant, pressure.high, pressure.low, pressure.mid,\n    triceps\n```\n:::\n:::\n\n\nThe factor column `pressure` has been converted to the three binary columns `\"pressure.low\"`, `\"pressure.mid\"`, and `\"pressure.high\"`.\n\n# Constructing the Pipeline\n\nWe created two preprocessing steps which could be used to create a new task with encoded factor variables and imputed missing values.\nHowever, if we do this before resampling, information from the test can leak into our training step which typically leads to overoptimistic performance measures.\nTo avoid this, we add the preprocessing steps to the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) itself, creating a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html).\nFor this, we create a [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) first.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = po(\"encode\") %>>%\n  po(\"imputeoor\") %>>%\n  learner\nplot(graph, html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline-013-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nWe use [`as_learner()`](https://mlr3.mlr-org.com/reference/as_learner.html) to  wrap the [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) into a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) with which allows us to use the graph like a normal learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(graph)\n\n# short learner id for printing\ngraph_learner$id = \"graph_learner\"\n```\n:::\n\n\nThe [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) can be trained and used for making predictions.\nInstead of calling `$train()` or `$predict()` manually, we will directly use it for resampling.\nWe choose a 3-fold cross-validation as the resampling strategy.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling = rsmp(\"cv\", folds = 3)\n\nrr = resample(task = task, learner = graph_learner, resampling = resampling)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$score()[, c(\"iteration\", \"task_id\", \"learner_id\", \"resampling_id\", \"classif.ce\"), with = FALSE]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   iteration task_id    learner_id resampling_id classif.ce\n1:         1    pima graph_learner            cv  0.2851562\n2:         2    pima graph_learner            cv  0.2460938\n3:         3    pima graph_learner            cv  0.2968750\n```\n:::\n:::\n\n\nFor each resampling iteration, the following steps are performed:\n\n1. The task is subsetted to the training indices.\n2. The factor encoder replaces factor features with dummy columns in the training task.\n3. The OOR imputer determines values to impute from the training task and then replaces all missing values with learned imputation values.\n4. The learner is applied on the modified training task and the model is stored inside the learner.\n\nNext is the predict step:\n\n1. The task is subsetted to the test indices.\n2. The factor encoder replaces all factor features with dummy columns in the test task.\n3. The OOR imputer replaces all missing values of the test task with the imputation values learned on the training set.\n4. The learner's predict method is applied on the modified test task.\n\nBy following this procedure, it is guaranteed that no information can leak from the training step to the predict step.\n\n# Tuning the Pipeline\n\nLet's have a look at the parameter set of the [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html).\nIt consists of the `xgboost` hyperparameters, and additionally, the parameter of the  [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) `encode` and `imputeoor`.\nAll hyperparameters are prefixed with the id of the respective [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) or learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(graph_learner$param_set)[, c(\"id\", \"class\", \"lower\", \"upper\", \"nlevels\"), with = FALSE]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                     id    class lower upper nlevels\n 1:                       encode.method ParamFct    NA    NA       5\n 2:               encode.affect_columns ParamUty    NA    NA     Inf\n 3:                       imputeoor.min ParamLgl    NA    NA       2\n 4:                    imputeoor.offset ParamDbl     0   Inf     Inf\n 5:                imputeoor.multiplier ParamDbl     0   Inf     Inf\n 6:            imputeoor.affect_columns ParamUty    NA    NA     Inf\n 7:                       xgboost.alpha ParamDbl     0   Inf     Inf\n 8:               xgboost.approxcontrib ParamLgl    NA    NA       2\n 9:                  xgboost.base_score ParamDbl  -Inf   Inf     Inf\n10:                     xgboost.booster ParamFct    NA    NA       3\n11:                   xgboost.callbacks ParamUty    NA    NA     Inf\n12:           xgboost.colsample_bylevel ParamDbl     0     1     Inf\n13:            xgboost.colsample_bynode ParamDbl     0     1     Inf\n14:            xgboost.colsample_bytree ParamDbl     0     1     Inf\n15: xgboost.disable_default_eval_metric ParamLgl    NA    NA       2\n16:       xgboost.early_stopping_rounds ParamInt     1   Inf     Inf\n17:          xgboost.early_stopping_set ParamFct    NA    NA       3\n18:                         xgboost.eta ParamDbl     0     1     Inf\n19:                 xgboost.eval_metric ParamUty    NA    NA     Inf\n20:            xgboost.feature_selector ParamFct    NA    NA       5\n21:                       xgboost.feval ParamUty    NA    NA     Inf\n22:                       xgboost.gamma ParamDbl     0   Inf     Inf\n23:                 xgboost.grow_policy ParamFct    NA    NA       2\n24:     xgboost.interaction_constraints ParamUty    NA    NA     Inf\n25:              xgboost.iterationrange ParamUty    NA    NA     Inf\n26:                      xgboost.lambda ParamDbl     0   Inf     Inf\n27:                 xgboost.lambda_bias ParamDbl     0   Inf     Inf\n28:                     xgboost.max_bin ParamInt     2   Inf     Inf\n29:              xgboost.max_delta_step ParamDbl     0   Inf     Inf\n30:                   xgboost.max_depth ParamInt     0   Inf     Inf\n31:                  xgboost.max_leaves ParamInt     0   Inf     Inf\n32:                    xgboost.maximize ParamLgl    NA    NA       2\n33:            xgboost.min_child_weight ParamDbl     0   Inf     Inf\n34:                     xgboost.missing ParamDbl  -Inf   Inf     Inf\n35:        xgboost.monotone_constraints ParamUty    NA    NA     Inf\n36:              xgboost.normalize_type ParamFct    NA    NA       2\n37:                     xgboost.nrounds ParamInt     1   Inf     Inf\n38:                     xgboost.nthread ParamInt     1   Inf     Inf\n39:                  xgboost.ntreelimit ParamInt     1   Inf     Inf\n40:           xgboost.num_parallel_tree ParamInt     1   Inf     Inf\n41:                   xgboost.objective ParamUty    NA    NA     Inf\n42:                    xgboost.one_drop ParamLgl    NA    NA       2\n43:                xgboost.outputmargin ParamLgl    NA    NA       2\n44:                 xgboost.predcontrib ParamLgl    NA    NA       2\n45:                   xgboost.predictor ParamFct    NA    NA       2\n46:             xgboost.predinteraction ParamLgl    NA    NA       2\n47:                    xgboost.predleaf ParamLgl    NA    NA       2\n48:               xgboost.print_every_n ParamInt     1   Inf     Inf\n49:                xgboost.process_type ParamFct    NA    NA       2\n50:                   xgboost.rate_drop ParamDbl     0     1     Inf\n51:                xgboost.refresh_leaf ParamLgl    NA    NA       2\n52:                     xgboost.reshape ParamLgl    NA    NA       2\n53:          xgboost.seed_per_iteration ParamLgl    NA    NA       2\n54:             xgboost.sampling_method ParamFct    NA    NA       2\n55:                 xgboost.sample_type ParamFct    NA    NA       2\n56:                   xgboost.save_name ParamUty    NA    NA     Inf\n57:                 xgboost.save_period ParamInt     0   Inf     Inf\n58:            xgboost.scale_pos_weight ParamDbl  -Inf   Inf     Inf\n59:                   xgboost.skip_drop ParamDbl     0     1     Inf\n60:                xgboost.strict_shape ParamLgl    NA    NA       2\n61:                   xgboost.subsample ParamDbl     0     1     Inf\n62:                       xgboost.top_k ParamInt     0   Inf     Inf\n63:                    xgboost.training ParamLgl    NA    NA       2\n64:                 xgboost.tree_method ParamFct    NA    NA       5\n65:      xgboost.tweedie_variance_power ParamDbl     1     2     Inf\n66:                     xgboost.updater ParamUty    NA    NA     Inf\n67:                     xgboost.verbose ParamInt     0     2       3\n68:                   xgboost.watchlist ParamUty    NA    NA     Inf\n69:                   xgboost.xgb_model ParamUty    NA    NA     Inf\n                                     id    class lower upper nlevels\n```\n:::\n:::\n\n\nWe will tune the encode method.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner$param_set$values$encode.method = to_tune(c(\"one-hot\", \"treatment\"))\n```\n:::\n\n\nWe define a tuning instance and use grid search since we want to try all encode methods.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  method = \"grid_search\",\n  task = task,\n  learner = graph_learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\")\n)\n```\n:::\n\n\nThe archive shows us the performance of the model with different encoding methods.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(instance$archive)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ArchiveTuning>\n   encode.method classif.ce runtime_learners              timestamp batch_nr warnings errors      resample_result\n1:       one-hot       0.27              1.2 2023-01-13 09:39:45.30        1        0      0 <ResampleResult[21]>\n2:     treatment       0.27              1.2 2023-01-13 09:39:46.64        2        0      0 <ResampleResult[21]>\n```\n:::\n:::\n\n\n# Nested Resampling\n\nWe create one [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) with `imputeoor` and test it against a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) that uses the internal imputation method of `xgboost`.\nApplying nested resampling ensures a fair comparison of the predictive performances.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_1 = po(\"encode\") %>>%\n  learner\ngraph_learner_1 = GraphLearner$new(graph_1)\n\ngraph_learner_1$param_set$values$encode.method = to_tune(c(\"one-hot\", \"treatment\"))\n\nat_1 = AutoTuner$new(\n  learner = graph_learner_1,\n  resampling = resampling,\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"none\"),\n  tuner = tnr(\"grid_search\"),\n  store_models = TRUE\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_2 = po(\"encode\") %>>%\n  po(\"imputeoor\") %>>%\n  learner\ngraph_learner_2 = GraphLearner$new(graph_2)\n\ngraph_learner_2$param_set$values$encode.method = to_tune(c(\"one-hot\", \"treatment\"))\n\nat_2 = AutoTuner$new(\n  learner = graph_learner_2,\n  resampling = resampling,\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"none\"),\n  tuner = tnr(\"grid_search\"),\n  store_models = TRUE\n)\n```\n:::\n\n\nWe run the benchmark.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nresampling_outer = rsmp(\"cv\", folds = 3)\ndesign = benchmark_grid(task, list(at_1, at_2), resampling_outer)\n\nbmr = benchmark(design, store_models = TRUE)\n```\n:::\n\n\nWe compare the aggregated performances on the outer test sets which give us an unbiased performance estimate of the [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html)s with the different encoding methods.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nr      resample_result task_id                     learner_id resampling_id iters classif.ce\n1:  1 <ResampleResult[21]>    pima           encode.xgboost.tuned            cv     3  0.2695312\n2:  2 <ResampleResult[21]>    pima encode.imputeoor.xgboost.tuned            cv     3  0.2682292\n```\n:::\n\n```{.r .cell-code}\nautoplot(bmr)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline-024-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nNote that in practice, it is required to tune preprocessing hyperparameters jointly with the hyperparameters of the learner.\nOtherwise, comparing preprocessing steps is not feasible and can lead to wrong conclusions.\n\nApplying nested resampling can be shortened by using the [`auto_tuner()`](https://mlr3tuning.mlr-org.com/reference/auto_tuner.html)-shortcut.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_1 = po(\"encode\") %>>% learner\ngraph_learner_1 = as_learner(graph_1)\ngraph_learner_1$param_set$values$encode.method = to_tune(c(\"one-hot\", \"treatment\"))\n\nat_1 = auto_tuner(\n  method = \"grid_search\",\n  learner = graph_learner_1,\n  resampling = resampling,\n  measure = msr(\"classif.ce\"),\n  store_models = TRUE)\n\ngraph_2 = po(\"encode\") %>>% po(\"imputeoor\") %>>% learner\ngraph_learner_2 = as_learner(graph_2)\ngraph_learner_2$param_set$values$encode.method = to_tune(c(\"one-hot\", \"treatment\"))\n\nat_2 = auto_tuner(\n  method = \"grid_search\",\n  learner = graph_learner_2,\n  resampling = resampling,\n  measure = msr(\"classif.ce\"),\n  store_models = TRUE)\n\ndesign = benchmark_grid(task, list(at_1, at_2), rsmp(\"cv\", folds = 3))\n\nbmr = benchmark(design, store_models = TRUE)\n```\n:::\n\n\n# Final Model\n\nWe train the chosen [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) with the [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) to get a final model with optimized hyperparameters.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat_2$train(task)\n```\n:::\n\n\nThe trained model can now be used to make predictions on new data `at_2$predict()`.\nThe pipeline ensures that the preprocessing is always a part of the train and predict step.\n\n# Resources\n\nThe [mlr3book](https://mlr3book.mlr-org.com/) includes chapters on [pipelines](https://mlr3book.mlr-org.com/pipelines.html) and [hyperparameter tuning](https://mlr3book.mlr-org.com/tuning.html).\nThe [mlr3cheatsheets](https://cheatsheets.mlr-org.com/) contain frequently used commands and workflows of mlr3.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}