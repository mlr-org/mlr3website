{
  "hash": "1b46012aab1d7f7ba085b90a39749f22",
  "result": {
    "markdown": "---\ntitle: \"Hyperband Series I - XGBoost\"\ndescription: |\n  Optimize the hyperparameters of an XGBoost model with Hyperband.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2022-12-01\nbibliography: bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n      datatable.print.trunc.cols: TRUE\n---\n\n\n\n\n\n\n# Scope\n\nIncreasingly large data sets and search spaces make hyperparameter optimization a very time-consuming task.\nHyperband [@li_2018] solves this problem by approximating the performance of a configuration on a small subset of the training data, with just a few training epochs in a neural network, or with only a small number of trees in a gradient-boosting model.\nAfter sampling random configurations, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones.\nThis type of optimization is called *multi-fidelity* optimization.\nThe fidelity parameter is part of the search space and influences the computational cost of fitting a model.\nIn the context of hyperband, the fidelity parameter is often called the budget parameter.\nIn this post, we will optimize XGBoost and use the number of boosting iterations as the fidelity parameter.\nThe time to train XGBoost increases with the number of boosting iterations.\n\nWe assume that you are already familiar with tuning in the mlr3 ecosystem.\nIf not, you should start with the [Hyperparameter Optimization on the Palmer Penguins Data Set](gallery/optimization/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins) post.\n\n# Hyperband\n\nHyperband is an advancement of the Successive Halving algorithm by @jamieson_2016.\nSuccessive Halving is initialized with the number of starting configurations $n$, the proportion of configurations discarded in each stage $\\eta$, and the minimum $r{_{min}}$ and maximum $r{_{max}}$ budget of a single evaluation.\nThe algorithm starts by sampling $n$ random configurations and allocating the minimum budget $r{_{min}}$ to them.\nThe configurations are evaluated and $\\frac{1}{\\eta}$ of the worst-performing configurations are discarded.\nThe remaining configurations are promoted to the next stage and evaluated on a larger budget.\nThis continues until one or more configurations are evaluated on the maximum budget $r{_{max}}$.\nThe number of stages is calculated so that each stage consumes approximately the same budget.\nThis sometimes results in the minimum budget having to be slightly adjusted by the algorithm.\nSuccessive Halving has the disadvantage that is not clear whether we should choose a large $n$ and try many configurations on a small budget or choose a small $n$ and train more configurations on the full budget.\n\nHyperband solves this problem by running Successive Halving with different numbers of stating configurations.\nThe algorithm is initialized with the same parameters as Successive Halving but without $n$.\nEach run of Successive Halving is called a bracket and starts with a different budget $r{_{i}}$.\nA smaller starting budget means that more configurations can be tried out.\nThe most explorative bracket $s = 3$ allocated the minimum budget $r{_{min}}$.\nThe next bracket increases the starting budget by a factor of $\\eta$.\nIn each bracket, the starting budget increases further until the last bracket $s = 0$ essentially performs a random search with the full budget $r{_{max}}$.\nThe number of configurations in the base stages is calculated so that each bracket uses approximately the same amount of budget.\n\n\n|     |     |           |   $s = 3$ |     |           |   $s = 2$ |     |           |   $s = 1$ |     |           |   $s = 0$ |\n| --: | --- | --------: | --------: | --- | --------: | --------: | --- | --------: | --------: | --- | --------: | --------: |\n| $i$ |     | $n{_{i}}$ | $r{_{i}}$ |     | $n{_{i}}$ | $r{_{i}}$ |     | $n{_{i}}$ | $r{_{i}}$ |     | $n{_{i}}$ | $r{_{i}}$ |\n|   0 |     |         8 |         1 |     |         6 |         2 |     |         4 |         4 |     |         8 |         4 |\n|   1 |     |         4 |         2 |     |         3 |         4 |     |         2 |         8 |     |           |           |\n|   2 |     |         2 |         4 |     |         1 |         8 |     |           |           |     |           |           |\n|   3 |     |         1 |         8 |     |           |           |     |           |           |     |           |           |\n\n: Hyperband schedule with $\\eta = 2$ , $r{_{min}} = 1$ and $r{_{max}} = 8$ {#tbl-schedule}\n\n# XGBoost\n\n\n# Optimization\n\nIn this practical example, we will optimize the hyperparameters of XGBoost on the [`Spam`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html) data set.\nWe begin by loading the [`XGBoost learner.`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.xgboost\")\n```\n:::\n\n\nThe next thing we do is define the search space.\nThe `nrounds` parameter controls the number of boosting iterations.\nWe set a range from 16 to 128 boosting iterations.\nThis is used as $r{_{min}}$ and $r{_{max}}$ by the Hyperband algorithm.\nWe need to tag the parameter with `\"budget\"` to identify it as a fidelity parameter.\nFor the other hyperparameters, we take the search space for XGBoost from the @bischl_hyperparameter_2021 article.\nThis search space works for a wide range of data sets.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$param_set$set_values(\n  nrounds           = to_tune(p_int(16, 128, tags = \"budget\")),\n  eta               = to_tune(1e-4, 1, logscale = TRUE),\n  max_depth         = to_tune(1, 20),\n  colsample_bytree  = to_tune(1e-1, 1),\n  colsample_bylevel = to_tune(1e-1, 1),\n  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),\n  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),\n  subsample         = to_tune(1e-1, 1)\n)\n```\n:::\n\n\nWe construct the tuning instance.\nWe use the `\"none\"` terminator because Hyperband terminates itself when all brackets are evaluated.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = ti(\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:classif.xgboost_on_spam>\n* Search Space:\n                  id    class     lower      upper nlevels\n1:           nrounds ParamInt 16.000000 128.000000     113\n2:               eta ParamDbl -9.210340   0.000000     Inf\n3:         max_depth ParamInt  1.000000  20.000000      20\n4:  colsample_bytree ParamDbl  0.100000   1.000000     Inf\n5: colsample_bylevel ParamDbl  0.100000   1.000000     Inf\n6:            lambda ParamDbl -6.907755   6.907755     Inf\n7:             alpha ParamDbl -6.907755   6.907755     Inf\n8:         subsample ParamDbl  0.100000   1.000000     Inf\n* Terminator: <TerminatorNone>\n```\n:::\n:::\n\n\nWe load the Hyperband [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html) and set `eta = 2`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3hyperband\")\n\ntuner = tnr(\"hyperband\", eta = 2)\n```\n:::\n\n\nThe Hyperband implementation in [mlr3hyperband](https://mlr3hyperband.mlr-org.com) evaluates configurations with the same budget in parallel.\nThis results in all brackets finishing at approximately the same time.\nYou can think of it as going diagonally through @tbl-schedule.\nUsing `eta = 2` and a lower bound of 1 results in the following schedule.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-91e0cefa9e2f72d8e0cc\" class=\"reactable html-widget\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-91e0cefa9e2f72d8e0cc\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"budget\":[16,32,32,64,64,64,128,128,128,128],\"bracket\":[3,3,2,3,2,1,3,2,1,0],\"stage\":[0,1,0,2,1,0,3,2,1,0],\"n\":[8,4,6,2,3,4,1,1,2,4]},\"columns\":[{\"accessor\":\"budget\",\"name\":\"Boosting Iterations\",\"type\":\"numeric\",\"cell\":[{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 16\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"12.5%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 32\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 32\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]}],\"width\":300},{\"accessor\":\"bracket\",\"name\":\"Bracket\",\"type\":\"numeric\"},{\"accessor\":\"stage\",\"name\":\"Stage\",\"type\":\"numeric\"},{\"accessor\":\"n\",\"name\":\"# Configruations\",\"type\":\"numeric\"}],\"pagination\":false,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"6ccf99bc0f5e7d34b4097399fd390c1c\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nNow we are ready to start the tuning.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nrounds       eta max_depth colsample_bytree colsample_bylevel    lambda     alpha subsample learner_param_vals\n1:      64 -1.982106        10        0.9173327         0.5936574 -4.110593 0.8630636 0.6726211         <list[11]>\n2 variables not shown: [x_domain, classif.ce]\n```\n:::\n:::\n\n\nThe result of a run is the configuration with the best performance.\nThis does not necessarily have to be a configuration evaluated with the highest budget.\n\nThe archive of a Hyperband run has the additional columns `\"bracket\"` and `\"stage\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, .(bracket, stage, classif.ce, eta, max_depth, colsample_bytree)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    bracket stage classif.ce        eta max_depth colsample_bytree\n 1:       3     0 0.07953064 -6.9232271        12        0.9706196\n 2:       3     0 0.09387223 -3.4583366        15        0.9763626\n 3:       3     0 0.09713168 -6.1591107        16        0.2186855\n 4:       3     0 0.09256845 -6.8366060        10        0.7162782\n 5:       3     0 0.07561930 -3.5035186        14        0.3044670\n---                                                               \n31:       0     0 0.08148631 -6.9057271         8        0.6810314\n32:       3     3 0.05801825 -3.3033174        19        0.8411304\n33:       2     2 0.06975228 -0.8263072         8        0.6614852\n34:       1     1 0.05932203 -1.9821061        10        0.9173327\n35:       1     1 0.06258149 -1.9860778        12        0.3976502\n```\n:::\n:::\n\n\n# Conclusion\n\nPerforming an optimization with hyperband in [mlr3tuning](https://mlr3tuning.mlr-org.com) hardly differs from using other methods.\nChoose a suitable range for the fidelity parameter and tag it with `\"budget\"`.\nLoad the Hyperband tuner and start your optimization.\nWe have tried to keep the runtime of the example low.\nFor your own optimization, you should use cross-validation and increase the maximum number of boosting rounds.\n@bischl_hyperparameter_2021 search space actually suggests 5000 boosting rounds.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"../../../site_libs/react-17.0.0/react.min.js\"></script>\n<script src=\"../../../site_libs/react-17.0.0/react-dom.min.js\"></script>\n<script src=\"../../../site_libs/reactwidget-1.0.0/react-tools.js\"></script>\n<script src=\"../../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<script src=\"../../../site_libs/reactable-binding-0.3.0/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}