{
  "hash": "1b46012aab1d7f7ba085b90a39749f22",
  "result": {
    "markdown": "---\ntitle: \"Hyperband Series I - XGBoost\"\ndescription: |\n  Optimize the hyperparameters of an XGBoost model with Hyperband.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2022-12-01\nbibliography: bibliography.bib\nknitr:\n  opts_chunk:\n    R.options:\n      datatable.print.nrows: 6\n      datatable.print.trunc.cols: TRUE\n---\n\n\n\n\n\n\n# Scope\n\nIncreasingly large data sets and search spaces make hyperparameter optimization a very time-consuming task.\nHyperband [@li_2018] solves this problem by approximating the performance of a configuration on a small subset of the training data, with just a few training epochs in a neural network, or with only a small number of trees in a gradient-boosting model.\nAfter sampling random configurations, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones.\nThis type of optimization is called *multi-fidelity* optimization.\nThe fidelity parameter is part of the search space and influences the computational cost of fitting a model.\nIn the context of hyperband, the fidelity parameter is often called the budget parameter.\nIn this post, we will optimize XGBoost and use the number of boosting iterations as the fidelity parameter.\nThe time to train XGBoost increases with the number of boosting iterations.\n\nWe assume that you are already familiar with tuning in the mlr3 ecosystem.\nIf not, you should start with the [Hyperparameter Optimization on the Palmer Penguins Data Set](gallery/optimization/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins) post.\n\n# Hyperband\n\nHyperband is an advancement of the Successive Halving algorithm by @jamieson_2016.\nSuccessive Halving is initialized with the number of starting configurations $n$, the proportion of configurations discarded in each stage $\\eta$, and the minimum $r{_{min}}$ and maximum $r{_{max}}$ budget of a single evaluation.\nThe algorithm starts by sampling $n$ random configurations and allocating the minimum budget $r{_{min}}$ to them.\nThe configurations are evaluated and $\\frac{1}{\\eta}$ of the worst-performing configurations are discarded.\nThe remaining configurations are promoted to the next stage and evaluated on a larger budget.\nThis continues until one or more configurations are evaluated on the maximum budget $r{_{max}}$.\nThe number of stages is calculated so that each stage consumes approximately the same budget.\nThis sometimes results in the minimum budget having to be slightly adjusted by the algorithm.\nSuccessive Halving has the disadvantage that is not clear whether we should choose a large $n$ and try many configurations on a small budget or choose a small $n$ and train more configurations on the full budget.\n\nHyperband solves this problem by running Successive Halving with different numbers of stating configurations.\nThe algorithm is initialized with the same parameters as Successive Halving but without $n$.\nEach run of Successive Halving is called a bracket and starts with a different budget $r{_{i}}$.\nA smaller starting budget means that more configurations can be tried out.\nThe most explorative bracket $s = 3$ allocated the minimum budget $r{_{min}}$.\nThe next bracket increases the starting budget by a factor of $\\eta$.\nIn each bracket, the starting budget increases further until the last bracket $s = 0$ essentially performs a random search with the full budget $r{_{max}}$.\nThe number of configurations in the base stages is calculated so that each bracket uses approximately the same amount of budget.\n\n\n|     |     |           |   $s = 3$ |     |           |   $s = 2$ |     |           |   $s = 1$ |     |           |   $s = 0$ |\n| --: | --- | --------: | --------: | --- | --------: | --------: | --- | --------: | --------: | --- | --------: | --------: |\n| $i$ |     | $n{_{i}}$ | $r{_{i}}$ |     | $n{_{i}}$ | $r{_{i}}$ |     | $n{_{i}}$ | $r{_{i}}$ |     | $n{_{i}}$ | $r{_{i}}$ |\n|   0 |     |         8 |         1 |     |         6 |         2 |     |         4 |         4 |     |         8 |         4 |\n|   1 |     |         4 |         2 |     |         3 |         4 |     |         2 |         8 |     |           |           |\n|   2 |     |         2 |         4 |     |         1 |         8 |     |           |           |     |           |           |\n|   3 |     |         1 |         8 |     |           |           |     |           |           |     |           |           |\n\n: Hyperband schedule with $\\eta = 2$ , $r{_{min}} = 1$ and $r{_{max}} = 8$ {#tbl-schedule}\n\n# XGBoost\n\n\n# Optimization\n\nIn this practical example, we will optimize the hyperparameters of XGBoost on the [`Spam`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html) data set.\nWe begin by loading the [`XGBoost learner.`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.xgboost\")\n```\n:::\n\n\nThe next thing we do is define the search space.\nThe `nrounds` parameter controls the number of boosting iterations.\nWe set a range from 16 to 128 boosting iterations.\nThis is used as $r{_{min}}$ and $r{_{max}}$ by the Hyperband algorithm.\nWe need to tag the parameter with `\"budget\"` to identify it as a fidelity parameter.\nFor the other hyperparameters, we take the search space for XGBoost from the @bischl_hyperparameter_2021 article.\nThis search space works for a wide range of data sets.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner$param_set$set_values(\n  nrounds           = to_tune(p_int(16, 128, tags = \"budget\")),\n  eta               = to_tune(1e-4, 1, logscale = TRUE),\n  max_depth         = to_tune(1, 20),\n  colsample_bytree  = to_tune(1e-1, 1),\n  colsample_bylevel = to_tune(1e-1, 1),\n  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),\n  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),\n  subsample         = to_tune(1e-1, 1)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n                        id    class lower upper nlevels        default   parents               value\n 1:                  alpha ParamDbl     0   Inf     Inf              0           <RangeTuneToken[2]>\n 2:          approxcontrib ParamLgl    NA    NA       2          FALSE                              \n 3:             base_score ParamDbl  -Inf   Inf     Inf            0.5                              \n 4:                booster ParamFct    NA    NA       3         gbtree                              \n 5:              callbacks ParamUty    NA    NA     Inf      <list[0]>                              \n---                                                                                                 \n59: tweedie_variance_power ParamDbl     1     2     Inf            1.5 objective                    \n60:                updater ParamUty    NA    NA     Inf <NoDefault[3]>                              \n61:                verbose ParamInt     0     2       3              1                             0\n62:              watchlist ParamUty    NA    NA     Inf                                             \n63:              xgb_model ParamUty    NA    NA     Inf                                             \n```\n:::\n:::\n\n\nWe construct the tuning instance.\nWe use the `\"none\"` terminator because Hyperband terminates itself when all brackets are evaluated.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = ti(\n  task = tsk(\"spam\"),\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measures = msr(\"classif.ce\"),\n  terminator = trm(\"none\")\n)\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:classif.xgboost_on_spam>\n* Search Space:\n                  id    class     lower      upper nlevels\n1:           nrounds ParamInt 16.000000 128.000000     113\n2:               eta ParamDbl -9.210340   0.000000     Inf\n3:         max_depth ParamInt  1.000000  20.000000      20\n4:  colsample_bytree ParamDbl  0.100000   1.000000     Inf\n5: colsample_bylevel ParamDbl  0.100000   1.000000     Inf\n6:            lambda ParamDbl -6.907755   6.907755     Inf\n7:             alpha ParamDbl -6.907755   6.907755     Inf\n8:         subsample ParamDbl  0.100000   1.000000     Inf\n* Terminator: <TerminatorNone>\n```\n:::\n:::\n\n\nWe load the Hyperband [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html) and set `eta = 2`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"mlr3hyperband\")\n\ntuner = tnr(\"hyperband\", eta = 2)\n```\n:::\n\n\nThe Hyperband implementation in [mlr3hyperband](https://mlr3hyperband.mlr-org.com) evaluates configurations with the same budget in parallel.\nThis results in all brackets finishing at approximately the same time.\nYou can think of it as going diagonally through @tbl-schedule.\nUsing `eta = 2` and a lower bound of 1 results in the following schedule.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-3004e11685b0668df1c9\" class=\"reactable html-widget\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-3004e11685b0668df1c9\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"budget\":[16,32,32,64,64,64,128,128,128,128],\"bracket\":[3,3,2,3,2,1,3,2,1,0],\"stage\":[0,1,0,2,1,0,3,2,1,0],\"n\":[8,4,6,2,3,4,1,1,2,4]},\"columns\":[{\"accessor\":\"budget\",\"name\":\"Boosting Iterations\",\"type\":\"numeric\",\"cell\":[{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 16\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"12.5%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 32\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 32\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"25%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\" 64\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"50%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]},{\"name\":\"div\",\"attribs\":{\"className\":\"bar-chart__bar-cell\"},\"children\":[{\"name\":\"span\",\"attribs\":{\"className\":\"bar-chart__number\"},\"children\":[\"128\"]},{\"name\":\"div\",\"attribs\":{\"style\":{\"marginRight\":\"0.375rem\"},\"className\":\"bar-chart\"},\"children\":[{\"name\":\"div\",\"attribs\":{\"style\":{\"width\":\"100%\"},\"className\":\"bar-chart__bar\"},\"children\":[]}]}]}],\"width\":300},{\"accessor\":\"bracket\",\"name\":\"Bracket\",\"type\":\"numeric\"},{\"accessor\":\"stage\",\"name\":\"Stage\",\"type\":\"numeric\"},{\"accessor\":\"n\",\"name\":\"# Configruations\",\"type\":\"numeric\"}],\"pagination\":false,\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"6ccf99bc0f5e7d34b4097399fd390c1c\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nNow we are ready to start the tuning.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nrounds       eta max_depth colsample_bytree colsample_bylevel   lambda     alpha subsample learner_param_vals\n1:     128 -1.278331         4        0.5880271         0.8760228 2.208718 -0.298236 0.3038533         <list[11]>\n2 variables not shown: [x_domain, classif.ce]\n```\n:::\n:::\n\n\nThe result of a run is the configuration with the best performance.\nThis does not necessarily have to be a configuration evaluated with the highest budget.\n\nThe archive of a Hyperband run has the additional columns `\"bracket\"` and `\"stage\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, .(bracket, stage, classif.ce, eta, max_depth, colsample_bytree)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    bracket stage classif.ce       eta max_depth colsample_bytree\n 1:       3     0 0.37874837 -5.353868        18        0.2077620\n 2:       3     0 0.23011734 -4.496589         2        0.2735785\n 3:       3     0 0.10299870 -8.710933        10        0.2290292\n 4:       3     0 0.08344198 -3.054917        18        0.7287955\n 5:       3     0 0.08279009 -7.428686         6        0.9277740\n---                                                              \n31:       0     0 0.07757497 -6.150785        12        0.9386196\n32:       3     3 0.05345502 -1.275317        14        0.9768313\n33:       2     2 0.05019557 -1.278331         4        0.5880271\n34:       1     1 0.05280313 -2.001107         6        0.6385301\n35:       1     1 0.09256845 -6.676612         7        0.2553367\n```\n:::\n:::\n\n\n# Conclusion\n\nPerforming an optimization with hyperband in [mlr3tuning](https://mlr3tuning.mlr-org.com) hardly differs from using other methods.\nChoose a suitable range for the fidelity parameter and tag it with `\"budget\"`.\nLoad the Hyperband tuner and start your optimization.\nWe have tried to keep the runtime of the example low.\nFor your optimization, you should use cross-validation and increase the maximum number of boosting rounds.\n@bischl_hyperparameter_2021 search space suggests 5000 boosting rounds.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/core-js-2.5.3/shim.min.js\"></script>\n<script src=\"../../../site_libs/react-17.0.0/react.min.js\"></script>\n<script src=\"../../../site_libs/react-17.0.0/react-dom.min.js\"></script>\n<script src=\"../../../site_libs/reactwidget-1.0.0/react-tools.js\"></script>\n<script src=\"../../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<script src=\"../../../site_libs/reactable-binding-0.3.0/reactable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}