{
  "hash": "92408ec402a4364d987f8fbd5121cb2f",
  "result": {
    "markdown": "---\ntitle: Tuning a Stacked Learner\ncategories:\n  - mlr3pipelines\n  - tuning\n  - resampling\n  - stacking\n  - classification\nauthor:\n  - name: Milan Dragicevic\n  - name: Giuseppe Casalicchio\ndate: 04-27-2020\ndescription: |\n  How to create and tune a multilevel stacking model using the mlr3pipelines package.\nimage: thumbnail.png\n---\n\n\n\n\n## Intro\n\nMultilevel stacking is an ensemble technique, where predictions of several learners are added as new features to extend the orginal data on different levels.\nOn each level, the extended data is used to train a new level of learners.\nThis can be repeated for several iterations until a final learner is trained.\nTo avoid overfitting, it is advisable to use test set (out-of-bag) predictions in each level.\n\nIn this post, a multilevel stacking example will be created using [mlr3pipelines](https://mlr3pipelines.mlr-org.com)  and tuned using [mlr3tuning](https://mlr3tuning.mlr-org.com) .\nA similar example is available in the [mlr3book](https://mlr3book.mlr-org.com/pipe-nonlinear.html#multilevel-stacking).\nHowever, we additionally explain how to tune the hyperparameters of the whole ensemble and each underlying learner jointly.\n\nIn our stacking example, we proceed as follows:\n\n1. **Level 0:** Based on the input data, we train three learners ([`rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html), [`glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.glmnet.html) and [`lda`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.lda.html)) on a sparser feature space obtained using different feature filter methods from [mlr3filters](https://mlr3filters.mlr-org.com) to obtain slightly decorrelated predictions.\nThe test set predictions of these learners are attached to the original data (used in level 0) and will serve as input for the learners in level 1.\n2. **Level 1:** We transform this extended data using PCA, on which we then train additional three learners ([`rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html), [`glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.glmnet.html) and [`lda`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.lda.html)).\nThe test set predictions of the level 1 learners are attached to input data used in level 1.\n3. Finally, we train a final [`ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) learner to the data extended by level 1. Note that the number of features selected by the feature filter method in level 0 and the number of principal components retained in level 1 will be jointly tuned with some other hyperparameters of the learners in each level.\n\n## Prerequisites\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com) package which pulls in the most important packages for this example.\nThe [mlr3learners](https://mlr3learners.mlr-org.com) package loads additional [`learners`](https://mlr3.mlr-org.com/reference/Learner.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3learners)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\nFor the stacking example, we use the [sonar classification task](https://mlr3.mlr-org.com/reference/mlr_tasks_sonar.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_sonar = tsk(\"sonar\")\ntask_sonar$col_roles$stratum = task_sonar$target_names # stratification\n```\n:::\n\n\n## Pipeline creation\n\n### Level 0\n\nAs mentioned, the level 0 learners are [`rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html), [`glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.glmnet.html) and [`lda`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.lda.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_rpart = lrn(\"classif.rpart\", predict_type = \"prob\")\nlearner_glmnet = lrn(\"classif.glmnet\", predict_type = \"prob\")\nlearner_lda = lrn(\"classif.lda\", predict_type = \"prob\")\n```\n:::\n\n\nTo create the learner out-of-bag predictions, we use [`PipeOpLearnerCV`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_learner_cv.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv1_rpart = po(\"learner_cv\", learner_rpart, id = \"rprt_1\")\ncv1_glmnet = po(\"learner_cv\", learner_glmnet, id = \"glmnet_1\")\ncv1_lda = po(\"learner_cv\", learner_lda, id = \"lda_1\")\n```\n:::\n\n\nA sparser representation of the input data in level 0 is obtained using the following filters:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova = po(\"filter\", flt(\"anova\"), id = \"filt1\")\nmrmr = po(\"filter\", flt(\"mrmr\"), id = \"filt2\")\nfind_cor = po(\"filter\", flt(\"find_correlation\"), id = \"filt3\")\n```\n:::\n\n\nTo summarize these steps into level 0, we use the [`gunion()`](https://mlr3pipelines.mlr-org.com/reference/gunion.html) function.\nThe out-of-bag predictions of all level 0 learners is attached using [`PipeOpFeatureUnion`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_featureunion.html) along with the original data passed via [`PipeOpNOP`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_nop.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlevel0 = gunion(list(\n  anova %>>% cv1_rpart,\n  mrmr %>>% cv1_glmnet,\n  find_cor %>>% cv1_lda,\n  po(\"nop\", id = \"nop1\"))) %>>%\n  po(\"featureunion\", id = \"union1\")\n```\n:::\n\n\nWe can have a look at the graph from level 0:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlevel0$plot(html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-04-27-tuning-stacking-009-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n### Level 1\n\nNow, we create the level 1 learners:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv2_rpart = po(\"learner_cv\", learner_rpart, id = \"rprt_2\")\ncv2_glmnet = po(\"learner_cv\", learner_glmnet, id = \"glmnet_2\")\ncv2_lda = po(\"learner_cv\", learner_lda, id = \"lda_2\")\n```\n:::\n\n\nAll level 1 learners will use [`PipeOpPCA`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_pca.html) transformed data as input:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlevel1 = level0 %>>%\n  po(\"copy\", 4) %>>%\n  gunion(list(\n    po(\"pca\", id = \"pca2_1\", param_vals = list(scale. = TRUE)) %>>% cv2_rpart,\n    po(\"pca\", id = \"pca2_2\", param_vals = list(scale. = TRUE)) %>>% cv2_glmnet,\n    po(\"pca\", id = \"pca2_3\", param_vals = list(scale. = TRUE)) %>>% cv2_lda,\n    po(\"nop\", id = \"nop2\"))) %>>%\n  po(\"featureunion\", id = \"union2\")\n```\n:::\n\n\nWe can have a look at the graph from level 1:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlevel1$plot(html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-04-27-tuning-stacking-012-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\nThe out-of-bag predictions of the level 1 learners are attached to the input data from level 1 and a final ranger learner will be trained:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nranger_lrn = lrn(\"classif.ranger\", predict_type = \"prob\")\n\nensemble = level1 %>>% ranger_lrn\nensemble$plot(html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-04-27-tuning-stacking-013-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n### Defining the tuning space\n\nIn order to tune the ensemble's hyperparameter jointly, we define the search space using [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) from the [paradox](https://paradox.mlr-org.com/) package:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space_ensemble = ps(\n  filt1.filter.nfeat = p_int(5, 50),\n  filt2.filter.nfeat = p_int(5, 50),\n  filt3.filter.nfeat = p_int(5, 50),\n  pca2_1.rank. = p_int(3, 50),\n  pca2_2.rank. = p_int(3, 50),\n  pca2_3.rank. = p_int(3, 20),\n  rprt_1.cp = p_dbl(0.001, 0.1),\n  rprt_1.minbucket = p_int(1, 10),\n  glmnet_1.alpha = p_dbl(0, 1),\n  rprt_2.cp = p_dbl(0.001, 0.1),\n  rprt_2.minbucket = p_int(1, 10),\n  glmnet_2.alpha = p_dbl(0, 1),\n  classif.ranger.mtry = p_int(1, 10),\n  classif.ranger.sample.fraction = p_dbl(0.5, 1),\n  classif.ranger.num.trees = p_int(50, 200))\n```\n:::\n\n\n### Performance comparison\n\nEven with a simple ensemble, there is quite a few things to setup.\nWe compare the performance of the ensemble with a simple tuned [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html).\n\nTo proceed, we convert the `ensemble` pipeline as a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_ensemble = as_learner(ensemble)\nlearner_ensemble$id = \"ensemble\"\nlearner_ensemble$predict_type = \"prob\"\n```\n:::\n\n\nWe define the search space for the simple ranger learner:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space_ranger = ps(\n  mtry = p_int(1, 10),\n  sample.fraction = p_dbl(0.5, 1),\n  num.trees = p_int(50, 200))\n```\n:::\n\n\nFor performance comparison, we use the [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) function that requires a design incorporating a list of learners and a list of tasks.\nHere, we have two learners (the simple ranger learner and the ensemble) and one task.\nSince we want to tune the simple ranger learner as well as the whole ensemble learner, we need to create an [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) for each learner to be compared.\nTo do so, we need to define a resampling strategy for the tuning in the inner loop (we use 3-fold cross-validation) and for the final evaluation (outer loop) use use holdout validation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninner_resampling = rsmp(\"cv\", folds = 3)\n\n# AutoTuner for the ensemble learner\nat_1 = auto_tuner(\n  method = \"random_search\",\n  learner = learner_ensemble,\n  resampling = inner_resampling,\n  measure = msr(\"classif.auc\"),\n  search_space = search_space_ensemble,\n  term_evals = 3) # to limit running time\n\n# AutoTuner for the simple ranger learner\nat_2 = auto_tuner(\n  method = \"random_search\",\n  learner = ranger_lrn,\n  resampling = inner_resampling,\n  measure = msr(\"classif.auc\"),\n  search_space = search_space_ranger,\n  term_evals = 3) # to limit running time\n\n# Define the list of learners\nlearners = list(at_1, at_2)\n\n# For benchmarking, we use a simple holdout\nouter_resampling = rsmp(\"holdout\")\nouter_resampling$instantiate(task_sonar)\n\ndesign = benchmark_grid(\n  tasks = task_sonar,\n  learners = learners,\n  resamplings = outer_resampling\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr = benchmark(design, store_models = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $train()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_1's $predict()\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Multiple lambdas have been fit. Lambda will be set to 0.01 (see parameter 's').\nThis happened PipeOp glmnet_2's $predict()\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr$aggregate(msr(\"classif.auc\"))[, .(nr, task_id, learner_id, resampling_id, iters, classif.auc)]\n```\n:::\n\n\n\nFor a more reliable comparison, the number of evaluation of the random search should be increased.\n\n## Conclusion\n\nThis example shows the versatility of [mlr3pipelines](https://mlr3pipelines.mlr-org.com).\nBy using more learners, varied representations of the data set as well as more levels, a powerful yet compute hungry pipeline can be created.\nIt is important to note that care should be taken to avoid name clashes of pipeline objects.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}