{
  "hash": "ac27fbc268693922eccdd4b2a128e7da",
  "result": {
    "markdown": "---\ntitle: Tuning Over Multiple Learners\ncategories:\n  - tuning\n  - resampling\n  - classification\nauthor:\n  - name: Jakob Richter\n  - name: Bernd Bischl\ndate: 02-01-2020\ndescription: |\n  This use case shows how to tune over multiple learners for a single task.\nimage: preview.png\n---\n\n\n\n\n\n\nThis use case shows how to tune over multiple learners for a single task.\nYou will learn the following:\n\n* Build a pipeline that can switch between multiple learners\n* Define the hyperparameter search space for the pipeline\n* Run a random or grid search (or any other tuner, always works the same)\n* Run nested resampling for unbiased performance estimates\n\nThis is an advanced use case. What should you know before:\n\n* [mlr3](https://mlr3.mlr-org.com) basics\n* [mlr3tuning](https://mlr3tuning.mlr-org.com) basics, especially [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html)\n* [mlr3pipelines](https://mlr3pipelines.mlr-org.com), especially branching\n\n# The Setup\n\nAssume, you are given some ML task and what to compare a couple of learners, probably because you want to select the best of them at the end of the analysis.\nThat's a super standard scenario, it actually sounds so common that you might wonder: Why an (advanced) blog post about this? With pipelines?\nWe will consider 2 cases: (a) Running the learners in their default, so without tuning, and (b) with tuning.\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com) package which pulls in the most important packages for this example.\nThe [mlr3learners](https://mlr3learners.mlr-org.com) package loads additional [`learners`](https://mlr3.mlr-org.com/reference/Learner.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3tuning)\nlibrary(mlr3learners)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\nLet's define our learners.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearners = list(\n  lrn(\"classif.xgboost\", id = \"xgb\", eval_metric = \"logloss\"),\n  lrn(\"classif.ranger\", id = \"rf\")\n)\nlearners_ids = sapply(learners, function(x) x$id)\n\ntask = tsk(\"sonar\") # some random data for this demo\ninner_cv2 = rsmp(\"cv\", folds = 2) # inner loop for nested CV\nouter_cv5 = rsmp(\"cv\", folds = 5) # outer loop for nested CV\n```\n:::\n\n\n# Default Parameters\n\n# The Benchmark-Table Approach\nAssume we don't want to perform tuning and or with running all learner in their respective defaults.\nSimply run benchmark on the learners and the tasks. That tabulates our results nicely and shows us what works best.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrid = benchmark_grid(task, learners, outer_cv5)\nbmr = benchmark(grid)\nbmr$aggregate(measures = msr(\"classif.ce\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nr      resample_result task_id learner_id resampling_id iters classif.ce\n1:  1 <ResampleResult[21]>   sonar        xgb            cv     5  0.2736353\n2:  2 <ResampleResult[21]>   sonar         rf            cv     5  0.1973287\n```\n:::\n:::\n\n\n# The Pipelines Approach\n\nOk, why would we ever want to change the simple approach above - and use pipelines / tuning for this?\nThree reasons:\n\n1. What we are doing with [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) is actually statistically flawed, insofar if we report the error of the numerically best method from the benchmark table as its estimated future performance.\n   If we do that we have \"optimized on the CV\" (we basically ran a grid search over our learners!) and we know that this is will produce optimistically biased results.\n   NB: This is a somewhat ridiculous criticism if we are going over only a handful of options, and the bias will be very small.\n   But it will be noticeable if we do this over hundreds of learners, so it is important to understand the underlying problem.\n   This is a somewhat subtle point, and this gallery post is more about technical hints for [mlr3](https://mlr3.mlr-org.com), so we will stop this discussion here.\n2. For some tuning algorithms, you might have a chance to more efficiently select from the set of algorithms than running the full benchmark.\n   Because of the categorical nature of the problem, you will not be able to learn stuff like \"If learner A works bad, I don't have to try learner B\", but you can potentially save some resampling iterations.\n   Assume you have so select from 100 candidates, experiments are expensive, and you use a 20-fold CV.\n   If learner A has super-bad results in the first 5 folds of the CV, you might already want to stop here.\n   \"Racing\" would be such a tuning algorithm.\n3. It helps us to foreshadow what comes later in this post where we tune the learners.\n\n\nThe pipeline just has a single purpose in this example:\nIt should allow us to switch between different learners, depending on a hyperparameter.\nThe pipe consists of three elements:\n\n* [`branch`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_branch.html) pipes incoming data to one of the following elements, on different data channels. We can name these channel on construction with `options`.\n* our learners (combined with [`gunion()`](https://mlr3pipelines.mlr-org.com/reference/gunion.html))\n* [`unbranch`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_unbranch.html) combines the forked paths at the end.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph =\n  po(\"branch\", options = learners_ids) %>>%\n  gunion(lapply(learners, po)) %>>%\n  po(\"unbranch\")\nplot(graph, html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-02-01-tuning-multiplexer-006-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe pipeline has now quite a lot of available hyperparameters.\nIt includes all hyperparameters from all contained learners.\nBut as we don't tune them here (yet), we don't care (yet).\nBut the first hyperparameter is special.\n`branch.selection` controls over which (named) branching channel our data flows.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph$param_set$ids()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"branch.selection\"                \"xgb.alpha\"                       \"xgb.approxcontrib\"              \n [4] \"xgb.base_score\"                  \"xgb.booster\"                     \"xgb.callbacks\"                  \n [7] \"xgb.colsample_bylevel\"           \"xgb.colsample_bynode\"            \"xgb.colsample_bytree\"           \n[10] \"xgb.disable_default_eval_metric\" \"xgb.early_stopping_rounds\"       \"xgb.early_stopping_set\"         \n[13] \"xgb.eta\"                         \"xgb.eval_metric\"                 \"xgb.feature_selector\"           \n[16] \"xgb.feval\"                       \"xgb.gamma\"                       \"xgb.grow_policy\"                \n[19] \"xgb.interaction_constraints\"     \"xgb.iterationrange\"              \"xgb.lambda\"                     \n[22] \"xgb.lambda_bias\"                 \"xgb.max_bin\"                     \"xgb.max_delta_step\"             \n[25] \"xgb.max_depth\"                   \"xgb.max_leaves\"                  \"xgb.maximize\"                   \n[28] \"xgb.min_child_weight\"            \"xgb.missing\"                     \"xgb.monotone_constraints\"       \n[31] \"xgb.normalize_type\"              \"xgb.nrounds\"                     \"xgb.nthread\"                    \n[34] \"xgb.ntreelimit\"                  \"xgb.num_parallel_tree\"           \"xgb.objective\"                  \n[37] \"xgb.one_drop\"                    \"xgb.outputmargin\"                \"xgb.predcontrib\"                \n[40] \"xgb.predictor\"                   \"xgb.predinteraction\"             \"xgb.predleaf\"                   \n[43] \"xgb.print_every_n\"               \"xgb.process_type\"                \"xgb.rate_drop\"                  \n[46] \"xgb.refresh_leaf\"                \"xgb.reshape\"                     \"xgb.seed_per_iteration\"         \n[49] \"xgb.sampling_method\"             \"xgb.sample_type\"                 \"xgb.save_name\"                  \n[52] \"xgb.save_period\"                 \"xgb.scale_pos_weight\"            \"xgb.skip_drop\"                  \n[55] \"xgb.strict_shape\"                \"xgb.subsample\"                   \"xgb.top_k\"                      \n[58] \"xgb.training\"                    \"xgb.tree_method\"                 \"xgb.tweedie_variance_power\"     \n[61] \"xgb.updater\"                     \"xgb.verbose\"                     \"xgb.watchlist\"                  \n[64] \"xgb.xgb_model\"                   \"rf.alpha\"                        \"rf.always.split.variables\"      \n[67] \"rf.class.weights\"                \"rf.holdout\"                      \"rf.importance\"                  \n[70] \"rf.keep.inbag\"                   \"rf.max.depth\"                    \"rf.min.node.size\"               \n[73] \"rf.min.prop\"                     \"rf.minprop\"                      \"rf.mtry\"                        \n[76] \"rf.mtry.ratio\"                   \"rf.num.random.splits\"            \"rf.num.threads\"                 \n[79] \"rf.num.trees\"                    \"rf.oob.error\"                    \"rf.regularization.factor\"       \n[82] \"rf.regularization.usedepth\"      \"rf.replace\"                      \"rf.respect.unordered.factors\"   \n[85] \"rf.sample.fraction\"              \"rf.save.memory\"                  \"rf.scale.permutation.importance\"\n[88] \"rf.se.method\"                    \"rf.seed\"                         \"rf.split.select.weights\"        \n[91] \"rf.splitrule\"                    \"rf.verbose\"                      \"rf.write.forest\"                \n```\n:::\n\n```{.r .cell-code}\ngraph$param_set$params$branch.selection\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 id    class lower upper levels        default\n1: branch.selection ParamFct    NA    NA xgb,rf <NoDefault[3]>\n```\n:::\n:::\n\n\nWe can now tune over this pipeline, and probably running grid search seems a good idea to \"touch\" every\navailable learner.\nNB: We have now written down in (much more complicated code) what we did before with `benchmark`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(graph)\ngraph_learner$id = \"g\"\n\nsearch_space = ps(\n  branch.selection = p_fct(c(\"rf\", \"xgb\"))\n)\n\ninstance = tune(\n  method = \"grid_search\",\n  task = task,\n  learner = graph_learner,\n  resampling = inner_cv2,\n  measure = msr(\"classif.ce\"),\n  search_space = search_space\n)\n\nas.data.table(instance$archive)[, list(branch.selection, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   branch.selection classif.ce\n1:               rf  0.1778846\n2:              xgb  0.3269231\n```\n:::\n:::\n\n\nBut: Via this approach we can now get unbiased performance results via nested resampling and using the [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) (which would make much more sense if we would select from 100 models and not 2).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nat = auto_tuner(\n  method = \"grid_search\",\n  learner = graph_learner,\n  resampling = inner_cv2,\n  measure = msr(\"classif.ce\"),\n  search_space = search_space\n)\n\nrr = resample(task, at, outer_cv5, store_models = TRUE)\n```\n:::\n\n\nAccess inner tuning result.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nextract_inner_tuning_results(rr)[, list(iteration, branch.selection, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   iteration branch.selection classif.ce\n1:         1               rf  0.2349398\n2:         2               rf  0.1626506\n3:         3               rf  0.3012048\n4:         4               rf  0.2813396\n5:         5               rf  0.2932444\n```\n:::\n:::\n\n\nAccess inner tuning archives.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nextract_inner_tuning_archives(rr)[, list(iteration, branch.selection, classif.ce, resample_result)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    iteration branch.selection classif.ce      resample_result\n 1:         1               rf  0.2349398 <ResampleResult[21]>\n 2:         1              xgb  0.2469880 <ResampleResult[21]>\n 3:         2               rf  0.1626506 <ResampleResult[21]>\n 4:         2              xgb  0.2530120 <ResampleResult[21]>\n 5:         3              xgb  0.3795181 <ResampleResult[21]>\n 6:         3               rf  0.3012048 <ResampleResult[21]>\n 7:         4              xgb  0.3714859 <ResampleResult[21]>\n 8:         4               rf  0.2813396 <ResampleResult[21]>\n 9:         5              xgb  0.3353414 <ResampleResult[21]>\n10:         5               rf  0.2932444 <ResampleResult[21]>\n```\n:::\n:::\n\n\n# Model-Selection and Tuning with Pipelines\n\nNow let's select from our given set of models and tune their hyperparameters.\nOne way to do this is to define a search space for each individual learner, wrap them all with the [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html), then call [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) on them.\nAs this is pretty standard, we will skip this here, and show an even neater option, where you can tune over models and hyperparameters in one go.\nIf you have quite a large space of potential learners and combine this with an efficient tuning algorithm, this can save quite some time in tuning as you can learn during optimization which options work best and focus on them.\nNB: Many AutoML systems work in a very similar way.\n\n# Define the Search Space\n\nRemember, that the pipeline contains a joint set of all contained hyperparameters.\nPrefixed with the respective PipeOp ID, to make names unique.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(graph$param_set)[, list(id, class, lower, upper, nlevels)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                 id    class lower upper nlevels\n 1:                branch.selection ParamFct    NA    NA       2\n 2:                       xgb.alpha ParamDbl     0   Inf     Inf\n 3:               xgb.approxcontrib ParamLgl    NA    NA       2\n 4:                  xgb.base_score ParamDbl  -Inf   Inf     Inf\n 5:                     xgb.booster ParamFct    NA    NA       3\n 6:                   xgb.callbacks ParamUty    NA    NA     Inf\n 7:           xgb.colsample_bylevel ParamDbl     0     1     Inf\n 8:            xgb.colsample_bynode ParamDbl     0     1     Inf\n 9:            xgb.colsample_bytree ParamDbl     0     1     Inf\n10: xgb.disable_default_eval_metric ParamLgl    NA    NA       2\n11:       xgb.early_stopping_rounds ParamInt     1   Inf     Inf\n12:          xgb.early_stopping_set ParamFct    NA    NA       3\n13:                         xgb.eta ParamDbl     0     1     Inf\n14:                 xgb.eval_metric ParamUty    NA    NA     Inf\n15:            xgb.feature_selector ParamFct    NA    NA       5\n16:                       xgb.feval ParamUty    NA    NA     Inf\n17:                       xgb.gamma ParamDbl     0   Inf     Inf\n18:                 xgb.grow_policy ParamFct    NA    NA       2\n19:     xgb.interaction_constraints ParamUty    NA    NA     Inf\n20:              xgb.iterationrange ParamUty    NA    NA     Inf\n21:                      xgb.lambda ParamDbl     0   Inf     Inf\n22:                 xgb.lambda_bias ParamDbl     0   Inf     Inf\n23:                     xgb.max_bin ParamInt     2   Inf     Inf\n24:              xgb.max_delta_step ParamDbl     0   Inf     Inf\n25:                   xgb.max_depth ParamInt     0   Inf     Inf\n26:                  xgb.max_leaves ParamInt     0   Inf     Inf\n27:                    xgb.maximize ParamLgl    NA    NA       2\n28:            xgb.min_child_weight ParamDbl     0   Inf     Inf\n29:                     xgb.missing ParamDbl  -Inf   Inf     Inf\n30:        xgb.monotone_constraints ParamUty    NA    NA     Inf\n31:              xgb.normalize_type ParamFct    NA    NA       2\n32:                     xgb.nrounds ParamInt     1   Inf     Inf\n33:                     xgb.nthread ParamInt     1   Inf     Inf\n34:                  xgb.ntreelimit ParamInt     1   Inf     Inf\n35:           xgb.num_parallel_tree ParamInt     1   Inf     Inf\n36:                   xgb.objective ParamUty    NA    NA     Inf\n37:                    xgb.one_drop ParamLgl    NA    NA       2\n38:                xgb.outputmargin ParamLgl    NA    NA       2\n39:                 xgb.predcontrib ParamLgl    NA    NA       2\n40:                   xgb.predictor ParamFct    NA    NA       2\n41:             xgb.predinteraction ParamLgl    NA    NA       2\n42:                    xgb.predleaf ParamLgl    NA    NA       2\n43:               xgb.print_every_n ParamInt     1   Inf     Inf\n44:                xgb.process_type ParamFct    NA    NA       2\n45:                   xgb.rate_drop ParamDbl     0     1     Inf\n46:                xgb.refresh_leaf ParamLgl    NA    NA       2\n47:                     xgb.reshape ParamLgl    NA    NA       2\n48:          xgb.seed_per_iteration ParamLgl    NA    NA       2\n49:             xgb.sampling_method ParamFct    NA    NA       2\n50:                 xgb.sample_type ParamFct    NA    NA       2\n51:                   xgb.save_name ParamUty    NA    NA     Inf\n52:                 xgb.save_period ParamInt     0   Inf     Inf\n53:            xgb.scale_pos_weight ParamDbl  -Inf   Inf     Inf\n54:                   xgb.skip_drop ParamDbl     0     1     Inf\n55:                xgb.strict_shape ParamLgl    NA    NA       2\n56:                   xgb.subsample ParamDbl     0     1     Inf\n57:                       xgb.top_k ParamInt     0   Inf     Inf\n58:                    xgb.training ParamLgl    NA    NA       2\n59:                 xgb.tree_method ParamFct    NA    NA       5\n60:      xgb.tweedie_variance_power ParamDbl     1     2     Inf\n61:                     xgb.updater ParamUty    NA    NA     Inf\n62:                     xgb.verbose ParamInt     0     2       3\n63:                   xgb.watchlist ParamUty    NA    NA     Inf\n64:                   xgb.xgb_model ParamUty    NA    NA     Inf\n65:                        rf.alpha ParamDbl  -Inf   Inf     Inf\n66:       rf.always.split.variables ParamUty    NA    NA     Inf\n67:                rf.class.weights ParamUty    NA    NA     Inf\n68:                      rf.holdout ParamLgl    NA    NA       2\n69:                   rf.importance ParamFct    NA    NA       4\n70:                   rf.keep.inbag ParamLgl    NA    NA       2\n71:                    rf.max.depth ParamInt     0   Inf     Inf\n72:                rf.min.node.size ParamInt     1   Inf     Inf\n73:                     rf.min.prop ParamDbl  -Inf   Inf     Inf\n74:                      rf.minprop ParamDbl  -Inf   Inf     Inf\n75:                         rf.mtry ParamInt     1   Inf     Inf\n76:                   rf.mtry.ratio ParamDbl     0     1     Inf\n77:            rf.num.random.splits ParamInt     1   Inf     Inf\n78:                  rf.num.threads ParamInt     1   Inf     Inf\n79:                    rf.num.trees ParamInt     1   Inf     Inf\n80:                    rf.oob.error ParamLgl    NA    NA       2\n81:        rf.regularization.factor ParamUty    NA    NA     Inf\n82:      rf.regularization.usedepth ParamLgl    NA    NA       2\n83:                      rf.replace ParamLgl    NA    NA       2\n84:    rf.respect.unordered.factors ParamFct    NA    NA       3\n85:              rf.sample.fraction ParamDbl     0     1     Inf\n86:                  rf.save.memory ParamLgl    NA    NA       2\n87: rf.scale.permutation.importance ParamLgl    NA    NA       2\n88:                    rf.se.method ParamFct    NA    NA       2\n89:                         rf.seed ParamInt  -Inf   Inf     Inf\n90:         rf.split.select.weights ParamUty    NA    NA     Inf\n91:                    rf.splitrule ParamFct    NA    NA       3\n92:                      rf.verbose ParamLgl    NA    NA       2\n93:                 rf.write.forest ParamLgl    NA    NA       2\n                                 id    class lower upper nlevels\n```\n:::\n:::\n\n\nWe decide to tune the `mtry` parameter of the random forest and the `nrounds` parameter of xgboost.\nAdditionally, we tune branching parameter that selects our learner.\n\nWe also have to reflect the hierarchical order of the parameter sets (admittedly, this is somewhat inconvenient).\nWe can only set the `mtry` value if the pipe is configured to use the random forest ([`ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html)).\nThe same applies for the xgboost parameter.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsearch_space = ps(\n  branch.selection = p_fct(c(\"rf\", \"xgb\")),\n  rf.mtry = p_int(1L, 20L, depends = branch.selection == \"rf\"),\n  xgb.nrounds = p_int(1, 500, depends = branch.selection == \"xgb\"))\n```\n:::\n\n\n# Tune the Pipeline with a Random Search\n\nVery similar code as before, we just swap out the search space.\nAnd now use random search.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  method = \"random_search\",\n  task = task,\n  learner = graph_learner,\n  resampling = inner_cv2,\n  measure = msr(\"classif.ce\"),\n  term_evals = 10,\n  search_space = search_space\n)\n\nas.data.table(instance$archive)[, list(branch.selection, xgb.nrounds, rf.mtry, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    branch.selection xgb.nrounds rf.mtry classif.ce\n 1:              xgb         292      NA  0.1875000\n 2:               rf          NA      19  0.2692308\n 3:               rf          NA       5  0.2307692\n 4:              xgb         229      NA  0.1875000\n 5:              xgb         301      NA  0.1875000\n 6:               rf          NA      20  0.2596154\n 7:               rf          NA       8  0.2355769\n 8:               rf          NA       2  0.2355769\n 9:               rf          NA       5  0.2500000\n10:               rf          NA      18  0.2451923\n```\n:::\n:::\n\n\nThe following shows a quick way to visualize the tuning results.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(instance, cols_x = c(\"xgb.nrounds\",\"rf.mtry\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-02-01-tuning-multiplexer-015-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nNested resampling, now really needed:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = tune_nested(\n  method = \"grid_search\",\n  task = task,\n  learner = graph_learner,\n  inner_resampling = inner_cv2,\n  outer_resampling = outer_cv5,\n  measure = msr(\"classif.ce\"),\n  term_evals = 10L,\n  search_space = search_space)\n```\n:::\n\n\nAccess inner tuning result.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nextract_inner_tuning_results(rr)[, list(iteration, branch.selection, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   iteration branch.selection classif.ce\n1:         1               rf  0.2108434\n2:         2               rf  0.1807229\n3:         3               rf  0.2289157\n4:         4              xgb  0.1915519\n5:         5               rf  0.1858147\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}