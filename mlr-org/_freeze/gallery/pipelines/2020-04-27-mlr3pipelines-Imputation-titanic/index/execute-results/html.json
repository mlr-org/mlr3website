{
  "hash": "e79b6207b61f97da7f780780e75e103b",
  "result": {
    "markdown": "---\ntitle: A Pipeline for the Titanic Data Set - Advanced\ncategories:\n  - imputation\n  - classification\n  - mlr3pipelines\n  - feature engineering\nauthor:\n  - name: Florian Pfisterer\ndescription: |\n  This post shows how to build a Graph using the mlr3pipelines package on the \"titanic\" dataset. Moreover, feature engineering, data imputation and benchmarking are covered.\ndate: 04-27-2020\nimage: thumbnail.png\n---\n\n\n\n\nThis is the second post of the titanic use case series.\nYou can find the first use case [here](https://mlr3gallery.mlr-org.com/posts/2020-03-12-intro-pipelines-titanic/).\n\nIn this section we will focus on more advanced usage of [mlr3pipelines](https://mlr3pipelines.mlr-org.com) .\nSpecifically, this section illustrates the different options when it comes to data imputation and feature engineering.\nFurthermore, the section shows how to **benchmark**, **feature engineer** and compare our results.\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com) package which pulls in the most important packages for this example.\nThe [mlr3learners](https://mlr3learners.mlr-org.com) package loads additional [`learners`](https://mlr3.mlr-org.com/reference/Learner.html).\nThe data is part of the [mlr3data](https://mlr3data.mlr-org.com)  package.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3learners)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\nAs in the basics chapter, we use the titanic data set.\nTo recap we have undertaken the following steps:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(\"titanic\", package = \"mlr3data\")\n\n# setting up the task\ntask = as_task_classif(titanic, target = \"survived\", positive = \"yes\")\ntask$set_row_roles(892:1309, \"holdout\")\ntask$select(cols = setdiff(task$feature_names, c(\"cabin\", \"name\", \"ticket\")))\n\n# setting up the learner\nlearner = lrn(\"classif.rpart\")\n\n#setting up our resampling method\nresampling = rsmp(\"cv\", folds = 3L)$instantiate(task)\n\nres = resample(task, learner, resampling, store_models = TRUE)\n```\n:::\n\n\n## Imputation\n\nA very simple way to do this to just impute a constant value for each feature.\nWe could i.e. impute every `character` or `factor` column  with `missing` and every numeric column with `-999`.\nAnd depending on the model, this might actually be fine.\nThis approach has a few drawbacks though:\n\n* `-999` could be a real value in the data.\n* imputing `-999` skews the distribution of the data, which might result in bad models.\n\nAs a result, instead of imputing a constant value, we will do two things:\n* Draw samples from each numeric features' histogram using `PipeOpImputeHist`\n* Add an additional column for each `variable` that indicates whether a value was missing or not.\n  If the information that a value was missing is important, this column contains this information.\n\nThis imputation scheme is called 'imputation with constants' and is already implemented in [mlr3pipelines](https://mlr3pipelines.mlr-org.com) .\nIt can be done using [`PipeOpImputeConstant`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputeconstant.html).\n\nRemember that we are trying to optimize our predictive power by using a random forest model ([`mlr_learners_classif.ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html)).\nNow, random forest models do not naturally handle missing values which is the reason why we need imputation.\nBefore imputation, our data looks as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsurvived      age embarked     fare    parch   pclass      sex   sib_sp \n       0      177        2        0        0        0        0        0 \n```\n:::\n:::\n\n\n\nLet's first deal with the categorical variables:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo_newlvl = po(\"imputeoor\")\ntask_newlvl = po_newlvl$train(list(task))[[1]]\n```\n:::\n\n\nNote that we use the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) in an unusual way, which is why the syntax does not look very clean.\nWe'll learn how to use a full graph below.\n\nFirst, let's look at the result:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_newlvl$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsurvived     fare    parch   pclass      sex   sib_sp      age embarked \n       0        0        0        0        0        0        0        0 \n```\n:::\n:::\n\n\nCool! `embarked` does not have missing values anymore.\nNote that `PipeOpImputeOOR` by default affects `character`, `factor` and `ordered` columns.\n\nFor the `numeric` features we want to do two things, impute values and add an indicator column.\nIn order to do this, we need a more complicated structure, a [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html).\n\nOur `po_indicator` creates the indicator column.\nWe tell it to only do this for `numeric` and `integer` columns via its `param_vals`, and additionally tell it to create a numeric column (0 = \"not missing\", 1 = \"missing\").\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npo_indicator = po(\"missind\",\n  affect_columns = selector_type(c(\"numeric\", \"integer\")), type = \"numeric\")\n```\n:::\n\n\nNow we can simultaneously impute features from the histogram and create indicator columns.\nThis can be achieved using the [`gunion`](https://mlr3pipelines.mlr-org.com/reference/gunion.html) function, which puts two operations in parallel:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = gunion(list(po_indicator, po(\"imputehist\")))\ngraph = graph %>>% po(\"featureunion\")\n```\n:::\n\n\nAfterwards, we `cbind` the resulting data using `po(\"featureunion\")`, connecting the different operations using our **graph connector**: `%>>%`.\nWe can now also connect the newlvl imputation:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = graph %>>% po(\"imputeoor\")\n```\n:::\n\n\nand see what happens when we now train the whole **Graph**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_imputed = graph$clone()$train(task)[[1]]\ntask_imputed$missings()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   survived missing_age      pclass         sex        fare       parch      sib_sp         age    embarked \n          0           0           0           0           0           0           0           0           0 \n```\n:::\n:::\n\n\nAwesome, now we do not have any missing values!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(task_imputed)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-04-27-mlr3pipelines-titanic-advanced-012-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe could now use `task_imputed` for resampling and see whether a **ranger** model does better.\nBut this is dangerous!\nIf we preprocess all training data at once, data could leak through the different cross-validation folds.\nIn order to do this properly, we have to process the training data in every fold separately.\nLuckily, this is automatically handled in our [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html), if we use it through a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html).\n\nWe can simply append a [`ranger learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) to the Graph and create a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) from this.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(graph$clone() %>>%\n  po(\"imputesample\") %>>%\n  po(\"fixfactors\") %>>%\n  po(learner))\n```\n:::\n\n\nWe needed to use the following commands for the Graph:\n* [`PipeOpFixFactors`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_fixfactors.html): Removes empty factor levels and removes factor levels that do not exist during training.\n* [`PipeOpImputeSample`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputesample.html): In some cases, if missing factor levels do not occur during training but only while predicting, [`PipeOpImputeOOR`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputeoor.html) does not create a new level. For those, we sample a random value.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr = resample(task, graph_learner, resampling, store_models = TRUE)\nrr$aggregate(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.acc \n  0.7934905 \n```\n:::\n:::\n\n\nSo our model has not improved heavily, currently it has an accuracy of  `0.79`.\n\n## Feature Engineering\n\nWe will do this using [`PipeOpMutate`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_mutate.html) in order to showcase the power of [mlr3pipelines](https://mlr3pipelines.mlr-org.com) .\nAdditionally, we will make use of the `character` columns.\nHence, we will re-select them:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask$col_roles$feature = c(task$feature_names, c(\"cabin\", \"name\", \"ticket\"))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(\"stringi\")\npo_ftextract = po(\"mutate\", mutation = list(\n  fare_per_person = ~ fare / (parch + sib_sp + 1),\n  deck = ~ factor(stri_sub(cabin, 1, 1)),\n  title = ~ factor(stri_match(name, regex = \", (.*)\\\\.\")[, 2]),\n  surname = ~ factor(stri_match(name, regex = \"(.*),\")[, 2]),\n  ticket_prefix = ~ factor(stri_replace_all_fixed(stri_trim(stri_match(ticket, regex = \"(.*) \")[, 2]), \".\", \"\"))\n))\n```\n:::\n\n\nQuickly checking what happens:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask_eng = po_ftextract$clone()$train(list(task))[[1]]\ntask_eng$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     survived age embarked    fare parch pclass    sex sib_sp cabin                                                name\n  1:       no  22        S  7.2500     0      3   male      1  <NA>                             Braund, Mr. Owen Harris\n  2:      yes  38        C 71.2833     0      1 female      1   C85 Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n  3:      yes  26        S  7.9250     0      3 female      0  <NA>                              Heikkinen, Miss. Laina\n  4:      yes  35        S 53.1000     0      1 female      1  C123        Futrelle, Mrs. Jacques Heath (Lily May Peel)\n  5:       no  35        S  8.0500     0      3   male      0  <NA>                            Allen, Mr. William Henry\n ---                                                                                                                   \n887:       no  27        S 13.0000     0      2   male      0  <NA>                               Montvila, Rev. Juozas\n888:      yes  19        S 30.0000     0      1 female      0   B42                        Graham, Miss. Margaret Edith\n889:       no  NA        S 23.4500     2      3 female      1  <NA>            Johnston, Miss. Catherine Helen \"Carrie\"\n890:      yes  26        C 30.0000     0      1   male      0  C148                               Behr, Mr. Karl Howell\n891:       no  32        Q  7.7500     0      3   male      0  <NA>                                 Dooley, Mr. Patrick\n               ticket fare_per_person deck title   surname ticket_prefix\n  1:        A/5 21171         3.62500 <NA>    Mr    Braund           A/5\n  2:         PC 17599        35.64165    C   Mrs   Cumings            PC\n  3: STON/O2. 3101282         7.92500 <NA>  Miss Heikkinen       STON/O2\n  4:           113803        26.55000    C   Mrs  Futrelle          <NA>\n  5:           373450         8.05000 <NA>    Mr     Allen          <NA>\n ---                                                                    \n887:           211536        13.00000 <NA>   Rev  Montvila          <NA>\n888:           112053        30.00000    B  Miss    Graham          <NA>\n889:       W./C. 6607         5.86250 <NA>  Miss  Johnston           W/C\n890:           111369        30.00000    C    Mr      Behr          <NA>\n891:           370376         7.75000 <NA>    Mr    Dooley          <NA>\n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(task_eng$clone()$select(c(\"sex\", \"age\")), type = \"pairs\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 177 rows containing non-finite values (`stat_boxplot()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 177 rows containing non-finite values (`stat_density()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 177 rows containing non-finite values (`stat_boxplot()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-04-27-mlr3pipelines-titanic-advanced-018-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nNow we can put everything together again, we concatenate our new [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) with the [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) created above and use [`PipeOpSelect`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_select.html) in order to de-select the `character` features we used for feature extraction.\nAdditionally, we collapse the 'surname', so only surnames that make up more than 0.6 \\% of the data are kept.\n\nIn summary, we do the following:\n\n* `mutate`: The `po_ftextract` we defined above extracts additional features from the data.\n* `collapsefactors`: Removes factor levels that make up less then 3 \\% of the data.\n* `select`: Drops `character` columns.\n* `gunion`: Puts two [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s in parallel.\n  * `missind`: `po_indicator` adds a column for each numeric with the info whether the value is NA or not.\n  * `imputehist`: Imputes numeric and integer columns by sampling from the histogram.\n* `featureunion`: Cbind's parallel data streams.\n* `imputeoor`: Imputes factor and ordered columns.\n* `fixfactors`: Removes empty factor levels and removes factor levels that do not exist during training.\n* `imputesample`: In some cases, if missing factor levels do not occur during training but only while predicting, `imputeoor` does not create a new level. For those, we sample a random value.\n* [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html): Appends a learner to the [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html).\n\nThe full graph we created is the following:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.ranger\", num.trees = 500, min.node.size = 4)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_final = po_ftextract %>>%\n  po(\"collapsefactors\", param_vals = list(no_collapse_above_prevalence = 0.03)) %>>%\n  po(\"select\", param_vals = list(selector = selector_invert(selector_type(\"character\")))) %>>%\n  gunion(list(po_indicator, po(\"imputehist\"))) %>>%\n  po(\"featureunion\") %>>%\n  po(\"imputeoor\") %>>%\n  po(\"fixfactors\") %>>%\n  po(\"imputesample\") %>>%\n  po(learner)\n```\n:::\n\n\n## Evaluation\n\nLet us see if things have improved:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(graph_final)\n\nrr = resample(task, graph_learner, resampling, store_models = TRUE)\n\nrr$aggregate(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.acc \n  0.8249158 \n```\n:::\n:::\n\n\nWe have improved even more!\n\n## Benchmarking\n\nTo undertake benchmarking, we need to set up a benchmarking design.\nThe first step is creating a list with the learners we used, namely the learners form the first and second part of this use case.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearners = list(\n  lrn(\"classif.rpart\", predict_type = \"prob\"),\n  lrn(\"classif.ranger\", predict_type = \"prob\")\n)\n```\n:::\n\n\nNow we can define our benchmark design.\nThis is done to ensure exhaustive and consistent resampling for all learners.\nThis step is needed to execute over the same train/test split for each task.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbm_design = benchmark_grid(task_imputed, learners, rsmp(\"cv\", folds = 10))\nbmr = benchmark(bm_design, store_models = TRUE)\nprint(bmr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<BenchmarkResult> of 20 rows with 2 resampling runs\n nr task_id     learner_id resampling_id iters warnings errors\n  1 titanic  classif.rpart            cv    10        0      0\n  2 titanic classif.ranger            cv    10        0      0\n```\n:::\n:::\n\n\nSo, where do we go from here?\nWe could for instance use a boxplot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bmr)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-04-27-mlr3pipelines-titanic-advanced-024-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nFurther we are able to compare sensitivity and specificity.\nHere we need to ensure that the benchmark results only contain a single Task:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(bmr$clone()$filter(task_id = \"titanic\"), type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-04-27-mlr3pipelines-titanic-advanced-025-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nMoreover, one can compare the precision-recall:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Precision vs Recall\nggplot2::autoplot(bmr, type = \"prc\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-04-27-mlr3pipelines-titanic-advanced-026-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nAs one can see, there are various options when it comes to benchmarking and visualizing.\nYou could have a look at some other use cases in our gallery for inspiration.\n\n## Future\n\nIn this case we have examined a number of different features, but there are many more things to explore!\nWe could extract even more information from the different features and see what happens.\nBut now you are left to yourself! There are many [kaggle kernels](https://www.kaggle.com/c/titanic) that treat the **Titanic Dataset**\navailable. This can be a great starter to find even better models.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}