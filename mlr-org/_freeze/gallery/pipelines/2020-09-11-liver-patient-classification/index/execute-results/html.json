{
  "hash": "3ce9d5a152b208eaccaa0a69a5cead94",
  "result": {
    "markdown": "---\ntitle: Liver Patient Classification Based on Diagnostic Measures\ncategories:\n  - tuning\n  - oversampling\n  - classification\nauthor:\n  - name: Julian Lange\n  - name: Jae-Eun Nam\n  - name: Viet Tran\n  - name: Simon Wiegrebe\n  - name: Henri Funk (Editor)\ndescription: |\n  Tune and benchmark pipelines.\ndate: 09-11-2020\nbibliography: bibliography.bib\nimage: ../../images/logo_color.png\naliases:\n  - ../../../gallery/2020-09-11-liver-patient-classification/index.html\n---\n\n\n\n\n# Preamble\n\nThe following examples were created as part of the Introduction to Machine Learning Lecture at LMU Munich.\nThe goal of the project was to create and compare one or several machine learning pipelines for the problem at hand together with exploratory analysis and an exposition of results.\nThe posts were contributed to the mlr3gallery by the authors and edited for better legibility by the editor.\nWe want to thank the authors for allowing us to publish their results.\nNote, that correctness of the results can not be guaranteed.\n\n## Prerequisites\n\nThis tutorial assumes familiarity with the basics of [mlr3tuning](https://mlr3tuning.mlr-org.com) and [mlr3pipelines](https://mlr3pipelines.mlr-org.com).\nConsult the [mlr3book](https://mlr3book.mlr-org.com/pipelines.html) if some aspects are not fully understandable.\nWe load the most important packages for this example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(DataExplorer)\nlibrary(ggplot2)\nlibrary(gridExtra)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\nNote, that expensive calculations are pre-saved in rds files in this tutorial to save computational time.\n\nMachine learning (ML), a branch of both computer science and statistics, in conjunction with new computing technologies has been transforming research and industries across the board over the past decade.\nA prime example for this is the healthcare industry, where applications of ML, as well as artificial intelligence in general, have become more and more popular in recent years.\nOne very frequently researched and applied use of ML in the medical field is the area of disease identification and diagnosis.\nML technologies have shown potential in detecting anomalies and diseases through pattern recognition, even though an entirely digital diagnosis by a computer is probably still something for the far future.\nHowever, suitable and reliable models estimating the risk of diseases could help real doctors make quicker and better decisions today already.\nIn this use case we examined machine learning algorithms and learners for the specific application of liver disease detection.\nThe task is therefore a binary classification task to predict whether a patient has liver disease or not based on some common diagnostic measurements.\nThis report is organized as follows.\nSection 1 introduces the data and section 2 provides more in-depth data exploration.\nSection 3 presents learners and their hyperparameter tuning while section 4, dealing with model fitting and benchmarking, presents results and conclusions.\n\n# Indian Liver Patient Dataset\n\nThe data set we used for our project is the \"Indian Liver Patient Dataset\" which was obtained from the [mlr3data](https://mlr3data.mlr-org.com) package. It was donated by three professors from India in 2012 @Dua2017.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Importing data\ndata(\"ilpd\", package = \"mlr3data\")\n```\n:::\n\n\nIt contains data for 583 patients collected in the north east of Andhra Pradesh, one of the 28 states of India.\nThe observations are divided into two classes based on the patient having liver disease or not.\nBesides the class variable, which is our target, ten, mostly numerical, features are provided.\nTo describe the features in more detail, the table below lists the variables included in the dataset.\n\n|Variable               |Description                                                  |\n|:----------------------|:------------------------------------------------------------|\n|age                    |Age of the patient (all patients above 89 are labelled as 90 |\n|gender                 |Sex of the patient (1 = female, 0 = male)                    |\n|total_bilirubin        |Total serum bilirubin (in mg/dL)                             |\n|direct_bilirubin       |Direct bilirubin level (in mg/dL)                            |\n|alkaline_phosphatase   |Serum alkaline phosphatase level (in U/L)                    |\n|alanine_transaminase   |Serum alanine transaminase level (in U/L)                    |\n|aspartate_transaminase |Serum aspartate transaminase level (in U/L)                  |\n|total_protein          |Total serum protein (in g/dL)                                |\n|albumin                |Serum albumin level (in g/dL)                                |\n|albumin_globulin_ratio |Albumin-to-globulin ratio                                    |\n|diseased               |Target variable (1 = liver disease, 0 = no liver disease)    |\n\nAs one can see, besides age and gender, the dataset contains eight additional numerical features.\nWhile the names and corresponding measurements look rather cryptic to the uninformed eye, they are all part of standard blood tests conducted to gather information about the state of a patient's liver, so-called liver function tests.\nAll of these measurements are frequently used markers for liver disease.\nFor the first five, measuring the chemical compound bilirubin and the three enzymes alkaline phosphatase, alanine transaminase and aspartate transaminase, elevated levels indicate liver disease @Gowda2009 @Oh2011.\nFor the remaining three, which concern protein levels, lower-than-normal values suggest a liver problem @Carvalho2018 @LabTests2019.\nLastly, one should note that some of the measurements are part of more than one variable. For example, the total serum bilirubin is simply the sum of both the direct and indirect bilirubin levels and the amount of albumin is used to calculate the values of the total serum protein as well as the albumin-to-globulin ratio. So, one might already suspect that some of the features are highly correlated to one another, but more on that kind of analysis in the following section.\n\n# Data exporation\n\n## Univariate distribution\n\nNext, we looked into the univariate distribution of each of the variables. We began with the target and the only discrete feature, gender, which are both binary.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/2020-09-11-liver-patient-classification-004-1.png){fig-align='center' width=768}\n:::\n:::\n\n\nThe distribution of the target variable is quite imbalanced, as the barplot shows: the number of patients with and without liver disease equals 416 and 167, respectively.\nThe underrepresentation of a class, in our case those without liver disease, might worsen the performance of ML models.\nIn order to examine this, we additionally fitted the models on a dataset where we randomly over-sampled the minority class, resulting in a perfectly balanced dataset.\nFurthermore, we applied stratified sampling to ensure the proportion of the classes is maintained during cross-validation.\n\nThe only discrete feature gender is quite imbalanced, too.\nAs one can see in the next section, this proportion is also observed within each target class. Prior to that, we looked into the distributions of the metric features.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/2020-09-11-liver-patient-classification-005-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nStrikingly, some of the metric features are extremely right-skewed and contain several extreme values.\nTo reduce the impact of outliers and since some models assume normality of features, we log-transformed these variables.\n\n## Features by class\n\nTo picture the relationship between the target and the features, we analysed the distributions of the features by class.\nFirst, we examined the discrete feature gender.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/2020-09-11-liver-patient-classification-006-1.png){fig-align='center' width=768}\n:::\n:::\n\n\nThe percentage of males in the \"disease\" class is slightly higher, but overall the difference is small.\nBesides that, the gender imbalance can be observed in both classes, as we mentioned before.\nTo see the differences in metric features, we compare the following boxplots, where right-skewed features are not log-transformed yet.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/2020-09-11-liver-patient-classification-007-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nExcept for the total amount of protein, for each feature we obtain differences between the median values of the two classes.\nNotably, in the case of strongly right-skewed features the \"disease\" class contains far more extreme values than the \"no disease\" class, which is probably because of its larger size.\nThis effect is weakened by log-transforming such features, as can be seen in the boxplots below.\nMoreover, the dispersion in the class \"disease\" is greater for these features, as the length of the boxes indicates.\nOverall, the features seem to be correlated to the target, so it makes sense to use them for this task and model their relationship with the target.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/2020-09-11-liver-patient-classification-008-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nNote, that the same result can be achieved more easily by using [`PipeOpMutate`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_mutate.html) from mlr3pipelines.\nThis PipeOp provides a smooth implementation to scale numeric features for mlr3 tasks.\n\n## Correlation\n\nAs we mentioned in the description of the data, there are features that are indirectly measured by another one.\nThis suggests that they are highly correlated.\nSome of the models we want to compare assume independent features or have problems with multicollinearity.\nTherefore, we checked for correlations between features.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in cor(data, use = method[1], method = method[2]): the standard deviation is zero\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-09-11-liver-patient-classification-009-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\nFor four of the pairs we obtained a very high correlation coefficient.\nLooking at these features, it is clear they affect each other.\nAs the complexity of the model should be minimized and due to multicollinearity concerns, we decided to take only one of each pair.\nWhen deciding on which features to keep, we chose those that are more specific and relevant regarding liver disease.\nTherefore, we chose albumin over the ratio between albumin and globulin and also over the total amount of protein.\nThe same argument applies to using the amount of direct bilirubin instead of the total amount of bilirubin.\nRegarding aspartate transaminase and alanine transaminase, it was not clear which one to use, especially since we have no given real world implementation for the task and no medical training.\nSince we did not notice any fundamental differences in the data for these two features, we arbitrarily chose aspartate transaminase.\n\n## Final Dataset\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Reducing, transforming and scaling dataset\nilpd = ilpd %>%\n  select(-total_bilirubin, -alanine_transaminase, -total_protein,\n         -albumin_globulin_ratio) %>%\n  mutate(\n    # Recode gender\n    gender = as.numeric(ifelse(gender == \"Female\", 1, 0)),\n     # Remove labels for class\n    diseased = factor(ifelse(diseased == \"yes\", 1, 0)),\n     # Log for features with skewed distributions\n    alkaline_phosphatase = log(alkaline_phosphatase),\n    aspartate_transaminase = log(aspartate_transaminase),\n    direct_bilirubin = log(direct_bilirubin)\n  )\n\npo_scale = po(\"scale\")\npo_scale$param_set$values$affect_columns =\n  selector_name(c(\"age\", \"direct_bilirubin\", \"alkaline_phosphatase\",\n  \"aspartate_transaminase\", \"albumin\"))\n```\n:::\n\n\nLastly, we standardized all metric features, as different ranges and units might weigh features.\nThis is especially important for the k-NN model.\nThe following table shows the final dataset and the transformations we applied.\n**Note**:\nDifferent from `log` or other transformation, `scaling` depends on the data themselves.\nScaling data before data are split leads to data leakage, were information of train and test set are shared.\nAs Data Leakage causes higher performance, scaling should always be applied in each data split induced by the ML workflow separately.\nTherefore we strongly recommend the usage of [`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html) in such cases.\n\n|Variable               |Transformation             |\n|:----------------------|:--------------------------|\n|age                    |scaled                     |\n|albumin                |scaled                     |\n|alkaline_phosphatase   |scaled and log-transformed |\n|aspartate_transaminase |scaled and log-transformed |\n|direct_bilirubin       |scaled and log-transformed |\n|diseased               |none                       |\n|gender                 |none                       |\n\n# Learners and tuning\n\nFirst, we need to define a task which contains the final dataset and some meta information.\nFurther we need to specify the positive class since the package takes the first one as the positive class by default.\nThe specification of the positive class has an impact on the evaluation later on.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Task definition\ntask_liver = as_task_classif(ilpd, target = \"diseased\", positive = \"1\")\n```\n:::\n\n\nIn the following we are going to evaluate logistic regression, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), naive Bayes, k-nearest neighbour (k-NN), classification trees (CART) and random forest on the binary target.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## Learner definition\n# Use predict type \"prob\" for the AUC score. Predict on train and test sets to\n# detect overfitting\n\nlearners = list(\n  learner_logreg = lrn(\"classif.log_reg\", predict_type = \"prob\",\n    predict_sets = c(\"train\", \"test\")),\n  learner_lda = lrn(\"classif.lda\", predict_type = \"prob\",\n    predict_sets = c(\"train\", \"test\")),\n  learner_qda = lrn(\"classif.qda\", predict_type = \"prob\",\n    predict_sets = c(\"train\", \"test\")),\n  learner_nb = lrn(\"classif.naive_bayes\", predict_type = \"prob\",\n    predict_sets = c(\"train\", \"test\")),\n  learner_knn = lrn(\"classif.kknn\", scale = FALSE,\n    predict_type = \"prob\"),\n  learner_rpart = lrn(\"classif.rpart\",\n    predict_type = \"prob\"),\n  learner_rf = lrn(\"classif.ranger\", num.trees = 1000,\n    predict_type = \"prob\")\n)\n```\n:::\n\n\nIn order to find optimal hyperparameters through tuning, we used random search to better cover the hyperparameter space.\nWe define the hyperparameters to tune.\nWe only tuned hyperparameters for k-NN, CART and random forest since the other methods have strong assumptions and serve as baseline.\nThe following table shows the assumptions of the methods we chose.\n\n| Learners                        | Assumption                                        |\n|---------------------------------|---------------------------------------------------|\n| Logistic regression             | No (or little) multicollinearity among features   |\n| Linear discriminant analysis    | Normality of classes, equal covariance (target)   |\n| Quadratic discriminant analysis | Normality of classes                              |\n| Naive Bayes                     | Conditional independence of features              |\n| CART                            | None                                              |\n| k-NN                            | None                                              |\n| Random forest                   | None                                              |\n\nThe following table shows the hyperparameters we tuned.\n\n| Learner       | Hyperparameters     |\n|---------------|---------------------|\n| k-NN          | k, distance, kernel |\n| CART          | minsplit, cp        |\n| Random forest | min.node.size, mtry |\n\nFor k-NN we chose 3 as the lower limit and 50 as the upper limit for `k` (number of neighbors).\nA too small k can lead to overfitting.\nWe also tried different distance measures (e.g. 1 for Manhattan distance, 2 for Euclidean distance) and kernels.\nFor CART we tuned the hyperparameters cp (complexity parameter) and minsplit (minimum number of observations in a node in order to attempt a split).\n`cp` controls the size of the tree: small values can result in overfitting while large values can cause underfitting. We also tuned parameters for the minimum size of terminal nodes and the number of variables randomly sampled as candidates at each split (from 1 to number of features) for random forest.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntune_ps_knn = ps(\n  k = p_int(lower = 3, upper = 50), # Number of neighbors considered\n  distance = p_dbl(lower = 1, upper = 3),\n  kernel = p_fct(levels = c(\"rectangular\", \"gaussian\", \"rank\", \"optimal\"))\n)\ntune_ps_rpart = ps(\n  # Minimum number of observations that must exist in a node in order for a\n  # split to be attempted\n  minsplit = p_int(lower = 10, upper = 40),\n  cp = p_dbl(lower = 0.001, upper = 0.1) # Complexity parameter\n)\ntune_ps_rf = ps(\n  # Minimum size of terminal nodes\n  min.node.size = p_int(lower = 10, upper = 50),\n  # Number of variables randomly sampled as candidates at each split\n  mtry = p_int(lower = 1, upper = 6)\n)\n```\n:::\n\n\nThe next step is to instantiate the AutoTuner from [mlr3tuning](https://mlr3tuning.mlr-org.com).\nWe employed 5-fold cross-validation for the inner loop of the nested resampling. The number of evaluations was set to 100 as the stopping criterion. As an evaluation metric we used AUC.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# AutoTuner for k-NN, CART and random forest\nlearners$learner_knn = auto_tuner(\n  method = \"random_search\",\n  learner = learners$learner_knn,\n  resampling = rsmp(\"cv\", folds = 5L),\n  measure = msr(\"classif.auc\"),\n  search_space = tune_ps_knn,\n  term_evals = 100,\n)\nlearners$learner_knn$predict_sets = c(\"train\", \"test\")\n\nlearners$learner_rpart = auto_tuner(\n  method = \"random_search\",\n  learner = learners$learner_rpart,\n  resampling = rsmp(\"cv\", folds = 5L),\n  measure = msr(\"classif.auc\"),\n  search_space = tune_ps_rpart,\n  term_evals = 100,\n)\nlearners$learner_rpart$predict_sets = c(\"train\", \"test\")\n\nlearners$learner_rf = auto_tuner(\n  method = \"random_search\",\n  learner = learners$learner_rf,\n  resampling = rsmp(\"cv\", folds = 5L),\n  measure = msr(\"classif.auc\"),\n  search_space = tune_ps_rf,\n  term_evals = 100,\n)\nlearners$learner_rf$predict_sets = c(\"train\", \"test\")\n```\n:::\n\n\nDuring our research we found that oversampling can potentially increase the performance of the learners.\nAs mentioned in section 2.2, we opted for perfectly balancing the classes. By using [mlr3pipelines](https://mlr3pipelines.mlr-org.com) we can apply the benchmark function later on.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Oversampling minority class to get perfectly balanced classes\npo_over = po(\"classbalancing\", id = \"oversample\", adjust = \"minor\",\n  reference = \"minor\", shuffle = FALSE, ratio = 416/167)\ntable(po_over$train(list(task_liver))$output$truth()) # Check class balance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  1   0 \n416 416 \n```\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Learners with balanced/oversampled data\nlearners_bal = lapply(learners, function(x) {\n  GraphLearner$new(po_scale %>>% po_over %>>% x)\n})\nlapply(learners_bal, function(x) x$predict_sets = c(\"train\", \"test\"))\n```\n:::\n\n\n# Model fitting and benchmarking\n\nWith the learners defined, the inner method of the nested resampling chosen and the tuners set up, we proceeded to choose the outer resampling method.\nWe opted for stratified 5-fold cross-validation to maintain the distribution of the target variable, independent of oversampling.\nHowever, it turned out that normal cross-validation without stratification yields very similar results.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# 5-fold cross-validation\nresampling_outer = rsmp(id = \"cv\", .key = \"cv\", folds = 5L)\n\n# Stratification\ntask_liver$col_roles$stratum = task_liver$target_names\n```\n:::\n\n\nTo rank the different learners and finally decide which one fits best for the task at hand, we used benchmarking.\nThe following code chunk executes our benchmarking with all learners.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndesign = benchmark_grid(\n  tasks = task_liver,\n  learners = c(learners, learners_bal),\n  resamplings = resampling_outer\n)\n\nbmr = benchmark(design, store_models = FALSE)\n```\n:::\n\n\nAs mentioned above, stratified 5-fold cross-validation was chosen.\nThis means that performance is determined as the average across five model evaluations with a train-test-split of 80% to 20%.\nFurthermore, the choice of performance metrics is crucial in ranking different learners.\nWhile each one of them has its specific use case, we opted for AUC, a performance metric taking into account both sensitivity and specificity, which we also used for hyperparameter tuning.\n\nWe first present a comparison of all learners by AUC, with and without oversampling, and for both training and test data.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                               learner_id auc_train  auc_test\n 1:                       classif.log_reg 0.7555646 0.7416606\n 2:                           classif.lda 0.7555611 0.7390708\n 3:                           classif.qda 0.7697367 0.7347738\n 4:                   classif.naive_bayes 0.7539943 0.7457096\n 5:                    classif.kknn.tuned 0.8876589 0.7323200\n 6:                   classif.rpart.tuned 0.8045344 0.6627003\n 7:                  classif.ranger.tuned 0.9586871 0.7439351\n 8:      scale.oversample.classif.log_reg 0.7556586 0.7434089\n 9:          scale.oversample.classif.lda 0.7547323 0.7434744\n10:          scale.oversample.classif.qda 0.7678794 0.7340827\n11:  scale.oversample.classif.naive_bayes 0.7537216 0.7441955\n12:   scale.oversample.classif.kknn.tuned 1.0000000 0.7026322\n13:  scale.oversample.classif.rpart.tuned 0.8611873 0.6250559\n14: scale.oversample.classif.ranger.tuned 1.0000000 0.7440514\n```\n:::\n:::\n\n\nAs can be seen in the results above, regardless of whether oversampling was applied or not, logistic regression, LDA, QDA, and naive Bayes have very similar performance on training and test data.\nOn the other hand, k-NN, CART and random forest predict much better on the training data, indicating overfitting.\n\nFurthermore, oversampling leaves AUC performance almost untouched for all learners.\n\nThe boxplots below graphically summarize AUC performance of all learners, with the blue dots indicating mean AUC performance.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-09-11-liver-patient-classification-020-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\nRandom forest is the learner with the best AUC performance, both with and without oversampling.\nWhereas mean AUC is roughly between 0.65 and 0.75 for all learners, the individual components of AUC might differ substantially.\n\nAs a first step towards \"AUC decomposition\", we consider the ROC curve, which provides valuable graphical insights into performance - even more so since AUC is directly derived from it.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nSubsequently, sensitivity, specificity, false negative rate (FNR), and false positive rate (FPR) for each learner are shown explicitly in the output below, next to AUC.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n                               learner_id classif.auc classif.sensitivity classif.specificity classif.fnr classif.fpr\n 1:                       classif.log_reg   0.7416606           0.8920252           0.2452763  0.10797476   0.7547237\n 2:                           classif.lda   0.7390708           0.9135972           0.1673797  0.08640275   0.8326203\n 3:                           classif.qda   0.7347738           0.6875215           0.6709447  0.31247849   0.3290553\n 4:                   classif.naive_bayes   0.7457096           0.6393574           0.7606061  0.36064257   0.2393939\n 5:                    classif.kknn.tuned   0.7323200           0.8339931           0.3896613  0.16600688   0.6103387\n 6:                   classif.rpart.tuned   0.6627003           0.8436317           0.1982175  0.15636833   0.8017825\n 7:                  classif.ranger.tuned   0.7439351           0.9422834           0.1376114  0.05771658   0.8623886\n 8:      scale.oversample.classif.log_reg   0.7434089           0.6202524           0.7609626  0.37974756   0.2390374\n 9:          scale.oversample.classif.lda   0.7434744           0.5865175           0.7848485  0.41348250   0.2151515\n10:          scale.oversample.classif.qda   0.7340827           0.5552209           0.8267380  0.44477912   0.1732620\n11:  scale.oversample.classif.naive_bayes   0.7441955           0.5407917           0.8386809  0.45920826   0.1613191\n12:   scale.oversample.classif.kknn.tuned   0.7026322           0.7281985           0.5254902  0.27180149   0.4745098\n13:  scale.oversample.classif.rpart.tuned   0.6250559           0.6032702           0.6060606  0.39672978   0.3939394\n14: scale.oversample.classif.ranger.tuned   0.7440514           0.7548480           0.5388592  0.24515204   0.4611408\n```\n:::\n:::\n\n\nAs it turned out, without oversampling logistic regression, LDA, k-NN, CART, and random forest score very high on sensitivity and rather low on specificity; QDA and naive Bayes, on the other hand, score relatively high on specificity, but not as high on sensitivity.\nBy definition, high sensitivity (specificity) results from a low false negative (positive) rate, which is also represented in the data.\n\nWith oversampling, specificity increases at the cost of sensitivity for all learners (even for those which already had high specificity), as can be seen in the two graphs below.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nFor a given learner, say random forest, the different performance metrics and their dependence upon target variable balance are shown in the following graph.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nTo get a look at performance from yet another angle, we next considered the confusion matrix for each learner, which simply contrasts the absolute numbers of predictions and true values by category, with and without oversampling.\nYou can have a look at all the confusion matrices, if you run the script.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\nThe confusion matrices confirm the above conclusions: without oversampling, all learners (except QDA and naive Bayes) display very high numbers of true positives, but also of false positives, implying high sensitivity and low specificity.\nAlso note that a trivial model classifying all individuals as \"1\" would cause fewer misclassifications than all of our models except random forest, casting doubt on the predictive power of the features in our dataset.\nRegarding learner performance with oversampling, the confusion matrices add another valuable insight:\n\n* the total number of misclassifications increases for all learners\n* the correct predictions become more balanced (by partly shifting from true positives to true negatives)\n* the misclassifications partly shift from false positives to false negatives\n\nThe final decision regarding which learner works best - and also whether oversampling should be used or not - strongly depends on the real world implications of sensitivity and specificity. One of the two might outweigh the other many times over in terms of practical importance.\nThink of the typical HIV rapid diagnostic test example, where high sensitivity at the cost of low specificity might cause an (unwarranted) shock but is otherwise not dangerous, whereas low sensitivity would be highly perilous.\nAs is usually the case, no black and white \"best model\" exists here - recall that, even with oversampling, none of our models perform well on both sensitivity and specificity.\nIn our case, we would need to ask ourselves: what would be the consequences of high specificity at the cost of low sensitivity, which implies telling many patients with a liver disease that they are healthy; versus what would be the consequences of high sensitivity at the cost of low specificity, which would mean telling many healthy patients they have a liver disease.\nIn absence of further topic-specific information, we can only state the best-performing learners for the particular performance metric chosen.\nAs mentioned above, random forest performs best based on AUC.\nRandom forest is furthermore the learner with the highest sensitivity score (and the lowest FNR), while naive Bayes is the one with the best specificity (and the lowest FPR)\nThese results - and the ranking of learners in general, independent of the performance metric - are not affected by oversampling.\n\nThe analysis we conducted is, however, by no means exhaustive.\nOn the feature level, while we focused almost exclusively on the machine learning and statistical analysis aspect during our analysis, one could also dig deeper into the actual topic (liver disease) and try to understand the variables as well as potential correlations and interactions more thoroughly.\nThis might also mean to consider already thrown out variables again.\nFurthermore, feature engineering as well as data preprocessing, for instance  using principal component analysis, could be applied to the dataset.\nRegarding hyperparameter tuning, different hyperparameters with larger hyperparameter spaces and numbers of evaluations could be considered.\nFurthermore, tuning could also be applied to some of those learners that we labeled as baseline learners, though to a lesser extent.\nFinally, we limited ourselves to those classifiers discussed in detail in the course.\nMore classifiers exist, however; in particular, gradient boosting and support vector machines could additionally be applied to this task and potentially yield better results.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}