{
  "hash": "b528b34a027968af88a9637818bb1598",
  "result": {
    "markdown": "---\ntitle: Target Transformations via Pipelines\ncategories:\n  - mlr3pipelines\n  - target transformation\n  - classification\nauthor:\n  - name: Lennart Schneider\ndate: 06-15-2020\ndescription: |\n  Transform the target variable.\nheader-includes:\n  - \\usepackage{amsmath}\nimage: thumbnail.svg\naliases:\n  - ../../../gallery/2020-06-15-target-transformations-via-pipelines/index.html\n---\n\n\n\n\nTransforming the target variable often can lead to predictive improvement and is a widely used tool. Typical transformations are for example the $\\log$ transformation of the target aiming at minimizing (right) skewness, or the Box Cox and Yeo-Johnson transformations being more flexible but having a similar goal.\n\nOne option to perform, e.g., a $\\log$ transformation would be to manually transform the target prior to training a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) (and also predicting from it) and then manually invert this transformation via $\\exp$ after predicting from the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html). This is quite cumbersome, especially if a transformation and inverse transformation require information about both the training and prediction data.\n\nIn this post, we show how to do various kinds of target transformations using [mlr3pipelines](https://mlr3pipelines.mlr-org.com) and explain the design of the target transformation and inversion [`PipeOps`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html).\n\nYou will:\n\n* learn how to do simple target transformations using [`PipeOpTargetMutate`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetmutate.html)\n\n* be introduced to the abstract base class to implement custom target transformations, [`PipeOpTargetTrafo`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTargetTrafo.html)\n\n* implement a custom target transformation PipeOp, `PipeOpTargetTrafoBoxCox`\n\nAs a prerequisite, you should be quite familiar with [mlr3pipelines](https://mlr3pipelines.mlr-org.com), i.e, know about the `$state` field of [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s, input and output channels, as well as [`Graphs`](https://mlr3pipelines.mlr-org.com/reference/Graph.html). We will start with a [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) for simple target transformations, [`PipeOpTargetMutate`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetmutate.html).\n\nWe load the most important packages for this example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3)\nlibrary(mlr3learners)\nlibrary(mlr3pipelines)\nlibrary(paradox)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n```\n:::\n\n\nIn all sections we will use the [`mtcars`](https://mlr3.mlr-org.com/reference/mlr_tasks_mtcars.html) regression [`task`](https://mlr3.mlr-org.com/reference/Task.html) with `mpg` being a numerical, positive target:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"mtcars\")\nsummary(task$data(cols = task$target_names))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90  \n```\n:::\n:::\n\n\nMoreover, as a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) we will use an [`ordinary linear regression learner`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.lm.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_lm = lrn(\"regr.lm\")\n```\n:::\n\n\n# Simple Target Transformations\n\nThe term *simple* refers to transformations that are given by a function of the target, relying on no other arguments (constants are of course allowed). The most prominent example is given by the $\\log$ transformation which we can later invert by applying the $\\exp$ transformation.\n\nIf you are only interested in doing such a transformation and you do not have the time to read more of this post, simply use the following syntactic sugar:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ng_ppl = ppl(\"targettrafo\", graph = learner_lm)\ng_ppl$param_set$values$targetmutate.trafo = function(x) log(x)\ng_ppl$param_set$values$targetmutate.inverter = function(x) list(response = exp(x$response))\n```\n:::\n\n\nThis constructs a [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) that will $\\log$ transform the target prior to training the linear regression learner (or predicting from it) and $\\exp$ transform the target after predicting from it.\nNote that you can supply any other [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) or even a whole [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) as the `graph` argument.\n\nNow, we will go into more detail about how this actually works:\n\nWe can perform a $\\log$ transformation of our numerical, positive target, `mpg`, using [`PipeOpTargetMutate`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetmutate.html) (by default, [`ppl(\"targettrafo\")`](https://mlr3pipelines.mlr-org.com/reference/mlr_graphs_targettrafo.html) uses this target transformation [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrafo = po(\"targetmutate\", param_vals = list(trafo = function(x) log(x)))\n```\n:::\n\n\nWe have to specify the `trafo` parameter as a function of `x` (which will internally be evaluated to be the target of the [`Task`](https://mlr3.mlr-org.com/reference/Task.html)): `trafo = function(x) log(x))`.\nIn principle, this is all that is needed to transform the target prior to training a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) (or predicting from it), i.e., if we now train this [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html), we see that the target is transformed as specified:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrafo$train(list(task))$output$data(cols = task$target_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         mpg\n 1: 3.044522\n 2: 3.044522\n 3: 3.126761\n 4: 3.063391\n 5: 2.928524\n 6: 2.895912\n 7: 2.660260\n 8: 3.194583\n 9: 3.126761\n10: 2.954910\n11: 2.879198\n12: 2.797281\n13: 2.850707\n14: 2.721295\n15: 2.341806\n16: 2.341806\n17: 2.687847\n18: 3.478158\n19: 3.414443\n20: 3.523415\n21: 3.068053\n22: 2.740840\n23: 2.721295\n24: 2.587764\n25: 2.954910\n26: 3.306887\n27: 3.258097\n28: 3.414443\n29: 2.760010\n30: 2.980619\n31: 2.708050\n32: 3.063391\n         mpg\n```\n:::\n:::\n\n\nAfter having predicted from the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) we could then proceed to apply the inverse transformation function in a similar manner. However, in [mlr3pipelines](https://mlr3pipelines.mlr-org.com), we decided to go with a more unified design of handling target transformations. In all target transformation [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s also the inverse transformation function of the target has to be specified. Therefore, in [`PipeOpTargetMutate`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetmutate.html), the parameter `inverter` also has to be correctly specified:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrafo$param_set$values$inverter = function(x) list(response = exp(x$response))\n```\n:::\n\n\nInternally, this function will be applied to the [`data.table`](https://www.rdocumentation.org/packages/data.table/topics/data.table-package) downstream of a [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html) object without the `$row_id` and `$truth` columns, and we specify that the `$response` column should be transformed. Note that applying the inverse transformation will typically only be done to the `$response` column, because transforming standard errors or probabilities is often not straightforward.\n\nTo actually carry out the inverse transformation function after predicting from the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html), we then rely on [`PipeOpTargetInvert`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetinvert.html). [`PipeOpTargetInvert`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetinvert.html) has an empty [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) and its sole purpose is to apply the inverse transformation function after having predicted from a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) (note that this whole design of target transformations may seem somewhat over-engineered at first glance, however, we will learn of its advantages when we later move to the advanced section).\n\n[`PipeOpTargetInvert`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetinvert.html) has two input channels named `\"fun\"` and `\"prediction\"`. During training, both take `NULL` as input (because this is what a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html)'s `\"output\"` output and [`PipeOpTargetMutate`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetmutate.html)'s `\"fun\"` output will return during training). During prediction, the `\"prediction\"` input takes a [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html), and the `\"fun\"` input takes the `\"fun\"` output from [`PipeOpTargetMutate`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetmutate.html) (you may have noticed already, that [`PipeOpTargetMutate`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetmutate.html) has actually two outputs, `\"fun\"` and `\"output\"`, with `\"fun\"` returning `NULL` during training and a function during prediction, while `\"output\"` always returns the transformed input [`Task`](https://mlr3.mlr-org.com/reference/Task.html)). We can see this, if we look at:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrafo$output\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     name train  predict\n1:    fun  NULL function\n2: output  Task     Task\n```\n:::\n\n```{.r .cell-code}\ntrafo$predict(list(task))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$fun\nfunction (inputs) \n{\n    assert_list(inputs, len = 1L, types = \"Prediction\")\n    list(private$.invert(inputs[[1L]], predict_phase_state))\n}\n<bytecode: 0x557f56857f78>\n<environment: 0x557f568576b8>\n\n$output\n<TaskRegr:mtcars> (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n```\n:::\n:::\n\n\nWe will talk more about such technical details in the advanced section. For now, to finally construct our target transformation pipeline, we build a [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ng = Graph$new()\ng$add_pipeop(trafo)\ng$add_pipeop(learner_lm)\ng$add_pipeop(po(\"targetinvert\"))\n```\n:::\n\n\nManually connecting the edges is quite cumbersome. First we connect the `\"output\"` output of `\"targetmutate\"` to the `\"input\"` input of `\"regr.lm\"`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ng$add_edge(src_id = \"targetmutate\", dst_id = \"regr.lm\",\n  src_channel = 2, dst_channel = 1)\n```\n:::\n\n\nThen we connect the `\"output\"` output of `\"regr.lm\"` to the `\"prediction\"` input of `\"targetinvert\"`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ng$add_edge(src_id = \"regr.lm\", dst_id = \"targetinvert\",\n  src_channel = 1, dst_channel = 2)\n```\n:::\n\n\nFinally, we connect the `\"fun\"` output of `\"targetmutate\"` to the `\"fun\"` input of `\"targetinvert\"`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ng$add_edge(src_id = \"targetmutate\", dst_id = \"targetinvert\",\n  src_channel = 1, dst_channel = 1)\n```\n:::\n\n\nThis graph (which is conceptually the same graph as constructed via the `ppl(\"targettrafo\")` syntactic sugar above) looks like the following:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ng$plot(html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-06-15-target-transformations-via-pipelines-014-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can then finally call `$train()` and `$predict()` (prior to this we wrap the [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) in a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html)):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngl = GraphLearner$new(g)\ngl$train(task)\ngl$state\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$model\n$model$targetmutate\nlist()\n\n$model$regr.lm\n$model$regr.lm$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n(Intercept)           am         carb          cyl         disp         drat         gear           hp         qsec  \n  2.776e+00    4.738e-02   -2.012e-02    7.657e-03    4.989e-05    2.220e-02    5.925e-02   -8.964e-04    3.077e-02  \n         vs           wt  \n -2.874e-03   -1.723e-01  \n\n\n$model$regr.lm$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$model$regr.lm$train_time\n[1] 0.009\n\n$model$regr.lm$param_vals\nnamed list()\n\n$model$regr.lm$task_hash\n[1] \"6ca8c90cdf732078\"\n\n$model$regr.lm$data_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$model$regr.lm$task_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$model$regr.lm$mlr3_version\n[1] '0.14.1'\n\n$model$regr.lm$train_task\n<TaskRegr:mtcars> (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\n\n$model$targetinvert\nlist()\n\n\n$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$train_time\n[1] 0.06\n\n$param_vals\n$param_vals$targetmutate.trafo\nfunction(x) log(x)\n<bytecode: 0x557f55aec0d8>\n\n$param_vals$targetmutate.inverter\nfunction(x) list(response = exp(x$response))\n\n\n$task_hash\n[1] \"58a137d2055e8406\"\n\n$data_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$task_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$mlr3_version\n[1] '0.14.1'\n\n$train_task\n<TaskRegr:mtcars> (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n```\n:::\n\n```{.r .cell-code}\ngl$predict(task)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionRegr> for 32 observations:\n    row_ids truth response\n          1  21.0 21.67976\n          2  21.0 21.10831\n          3  22.8 25.73690\n---                       \n         30  19.7 19.58533\n         31  15.0 14.11015\n         32  21.4 23.11105\n```\n:::\n:::\n\n\nand contrast this with `$train()` and `$predict()` of the naive linear regression learner (also look at the estimated coefficients of the linear regression contained in `$state$model`):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner_lm$train(task)\nlearner_lm$state\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n(Intercept)           am         carb          cyl         disp         drat         gear           hp         qsec  \n   12.30337      2.52023     -0.19942     -0.11144      0.01334      0.78711      0.65541     -0.02148      0.82104  \n         vs           wt  \n    0.31776     -3.71530  \n\n\n$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$train_time\n[1] 0.003\n\n$param_vals\nnamed list()\n\n$task_hash\n[1] \"58a137d2055e8406\"\n\n$data_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$task_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$mlr3_version\n[1] '0.14.1'\n\n$train_task\n<TaskRegr:mtcars> (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n```\n:::\n\n```{.r .cell-code}\nlearner_lm$predict(task)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionRegr> for 32 observations:\n    row_ids truth response\n          1  21.0 22.59951\n          2  21.0 22.11189\n          3  22.8 26.25064\n---                       \n         30  19.7 19.69383\n         31  15.0 13.94112\n         32  21.4 24.36827\n```\n:::\n:::\n\n\nYou should continue reading, if you are interested in more advanced target transformations, i.e., where the transformation and inverse transformation require information about both the training and prediction data.\n\nFirst we will introduce the abstract base class for doing target transformations, [`PipeOpTargetTrafo`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTargetTrafo.html), from which [`PipeOpTargetMutate`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetmutate.html) inherits.\n\n# Abstract Base Class: PipeOpTargetTrafo\n\nNo matter how \"complicated\" the actual target transformation and inverse transformation may be, applying the inverse transformation function after having predicted from a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) will always be done via [`PipeOpTargetInvert`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetinvert.html) (as already outlined above, [`PipeOpTargetInvert`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetinvert.html) has an empty `ParamSet` and its sole purpose is to apply the inverse transformation function after having predicted from a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html)). All [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html)s for doing target transformations will therefore look similar like the simple one above, i.e., a target transformation [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) followed by some [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) or a whole [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html), followed by [`PipeOpTargetInvert`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetinvert.html). Therefore, using `ppl(\"targettrafo\")` to construct such [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html)s is highly recommended.\n\nTo allow for more advanced target transformations, we now have a closer look at the abstract base class, `PipeOpTargetTrafo`:\n\n`PipeOpTargetTrafo` has one input channel, named `\"input\"` taking a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) both during training and prediction.\nIt's two output channels are named `\"fun\"` and `\"output\"`. During training `\"fun\"` returns `NULL` and during prediction `\"fun\"` returns a function that will be used by [`PipeOpTargetInvert`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetinvert.html) to perform the inverse target transformation on [`PipeOpTargetInvert`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetinvert.html)'s `\"prediction\"` input. `\"output\"` returns the modified input [`Task`](https://mlr3.mlr-org.com/reference/Task.html) both during training and prediction.\n\nSubclasses can overload up to four functions:\n\n* `.get_state()` takes the input [`Task`](https://mlr3.mlr-org.com/reference/Task.html) and returns a `list()` which will internally be used to set the `$state`. Typically it is sensible to make use of the `$state` during `.transform()` and `.train_invert()`. The base implementation returns `list()` and should be overloaded if setting the state is desired.\n\n* `.transform()` takes the input [`Task`](https://mlr3.mlr-org.com/reference/Task.html) and returns a modified [`Task`](https://mlr3.mlr-org.com/reference/Task.html) (i.e., the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) with the transformed target). This is the main function for doing the actual target transformation. Note that `.get_state()` is evaluated a single time during training right before `.transform()` and therefore, you can rely on the `$state` that has been set. To update the input [`Task`](https://mlr3.mlr-org.com/reference/Task.html) with respect to the transformed target, subclasses should make use of the [`convert_task()`](https://mlr3.mlr-org.com/reference/convert_task.html) function and drop the original target from the [`Task`](https://mlr3.mlr-org.com/reference/Task.html). `.transform()` also accepts a `phase` argument that will receive `\"train\"` during training and `\"predict\"` during prediction. This can be used to enable different behavior during training and prediction. `.transform()` should always be overloaded by subclasses.\n\n* `.train_invert()` takes the input [`Task`](https://mlr3.mlr-org.com/reference/Task.html) and returns a `predict_phase_state` object. This can be anything. Note that `.train_invert()` should not modify the input [`Task`](https://mlr3.mlr-org.com/reference/Task.html). The base implementation returns a list with a single argument, the `$truth` column of the input [`Task`](https://mlr3.mlr-org.com/reference/Task.html) and should be overloaded if a more training-phase-dependent state is desired.\n\n* `.invert()` takes a [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html) and a `predict_phase_state` object as inputs and returns a [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html). This is the main function for specifying the actual inverse target transformation that will later be carried out by [`PipeOpTargetInvert`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetinvert.html). Internally a private helper function , `.invert_help()` will construct the function that will be returned by the `\"fun\"` output of `PipeOpTargetTrafo` so that [`PipeOpTargetInvert`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_targetinvert.html) can later simply dispatch this inverse target transformation on its `\"prediction\"` input.\n\nThe supposed workflow of a class inherited from `PipeOpTargetTrafo` is given in the following figure:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](workflow.svg){fig-align='center' width=100%}\n:::\n:::\n\n\nTo solidify our understanding we will design a new target transformation [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) in the next section: `PipeOpTargetTrafoBoxCox`\n\n# Hands on: PipeOpTargetTrafoBoxCox\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(R6)\n```\n:::\n\n\nThe Box-Cox transformation of a target $y_{i}$ is given as:\n\n$$y_{i}(\\lambda) = \\begin{cases}\n\\frac{y_{i}^{\\lambda} - 1}{\\lambda} & \\text{if}~\\lambda \\neq 0; \\\\\n\\log(y_{i}) & \\text{if}~\\lambda = 0\n\\end{cases}$$\n\n[mlr3pipelines](https://mlr3pipelines.mlr-org.com) already supports the Box-Cox transformation for numerical, positive features, see `?PipeOpBoxCox`.\n\nHere we will design a [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) to apply the Box-Cox transformation as a target transformation. The $\\lambda$ parameter of the transformation is estimated during training and used for both the training and prediction transformation. After predicting from a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) we will as always apply the inverse transformation function. To do the actual transformation we will use [`bestNormalize::boxcox()`](https://www.rdocumentation.org/packages/bestNormalize/topics/boxcox).\n\nFirst, we inherit from `PipeOpTargetTrafo` and overload the `initialize()` function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nPipeOpTargetTrafoBoxCox = R6Class(\"PipeOpTargetTrafoBoxCox\",\n  inherit = PipeOpTargetTrafo,\n  public = list(\n    initialize = function(id = \"targettrafoboxcox\", param_vals = list()) {\n      param_set = ps(\n        standardize = p_lgl(default = TRUE, tags = c(\"train\", \"boxcox\")),\n        eps = p_dbl(default = 0.001, lower = 0, tags = c(\"train\", \"boxcox\")),\n        lower = p_dbl(default = -1L, tags = c(\"train\", \"boxcox\")),\n        upper = p_dbl(default = 2L, tags = c(\"train\", \"boxcox\"))\n      )\n      super$initialize(id = id, param_set = param_set, param_vals = param_vals,\n        packages = \"bestNormalize\", task_type_in = \"TaskRegr\",\n        task_type_out = \"TaskRegr\")\n    }\n  ),\n  private = list(\n\n    .get_state = function(task) {\n      ...\n    },\n\n    .transform = function(task, phase) {\n      ...\n    },\n\n    .train_invert = function(task) {\n      ...\n    },\n\n    .invert = function(prediction, predict_phase_state) {\n      ...\n    }\n  )\n)\n```\n:::\n\n\nAs parameters, we allow `\"standardize\"` (whether to center and scale the transformed values to attempt a standard normal distribution), `\"eps\"` (tolerance parameter to identify if the $\\lambda$ parameter is equal to zero), `\"lower\"` (lower value for the estimation of the $\\lambda$ parameter) and `\"upper\"` (upper value for the estimation of the $\\lambda$ parameter). Note that we set `task_type_in = \"TaskRegr\"` and `task_type_out = \"TaskRegr\"` to specify that this [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) only works for [`regression Tasks`](https://mlr3.mlr-org.com/reference/TaskRegr.html).\n\nSecond, we overload the four functions as mentioned above.\n\nWe start with `.get_state()`. We extract the target and apply the Box-Cox transformation to the target. This yields an object of class `\"boxcox\"` which we will wrap in a `list()` and set as the `$state` (`bc$x.t = NULL` and `bc$x = NULL` is done to save some memory because we do not need the transformed original data and original data later):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n    .get_state = function(task) {\n      target = task$data(cols = task$target_names)[[1L]]\n      bc = mlr3misc::invoke(bestNormalize::boxcox, target,\n        .args = self$param_set$get_values(tags = \"boxcox\"))\n      bc$x.t = NULL\n      bc$x = NULL\n      list(bc = bc)\n    },\n```\n:::\n\n\nNext, we tackle `.transform()`. This is quite straightforward, because objects of class `\"boxcox\"` have their own predict method which we can use here to carry out the actual Box-Cox transformation based on the learned $\\lambda$ parameter as stored in the `\"boxcox\"` object in the `$state` (both during training and prediction). We then rename the target, add it to the task and finally update the task with respect to this new target:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n    .transform = function(task, phase) {\n      target = task$data(cols = task$target_names)[[1L]]\n      new_target = as.data.table(predict(self$state$bc, newdata = target))\n      colnames(new_target) = paste0(task$target_names, \".bc\")\n      task$cbind(new_target)\n      convert_task(task, target = colnames(new_target),\n        drop_original_target = TRUE)\n    },\n```\n:::\n\n\nTime to overload `.train_invert()`. This is even more straightforward, because the prediction method for objects of class `\"boxcox\"` directly allows for inverting the transformation via setting the argument `inverse = TRUE`. Therefore, we only need the `\"boxcox\"` object stored in the `$state` along the `$truth` column of the input [`Task`](https://mlr3.mlr-org.com/reference/Task.html) (remember that this list will later be available as the `predict_phase_state` object):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n    .train_invert = function(task) {\n      list(truth = task$truth(), bc = self$state$bc)\n    },\n```\n:::\n\n\nFinally, we overload `.invert()`. We extract the truth from the `predict_phase_state` and the response from the [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html). We then apply the inverse Box-Cox transformation to the response based on the $\\lambda$ parameter and the mean and standard deviation learned during training, relying on the `predict_phase_state` object. Finally, we construct a new [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html) object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n    .invert = function(prediction, predict_phase_state) {\n      truth = predict_phase_state$truth\n      response = predict(predict_phase_state$bc, newdata = prediction$response,\n        inverse = TRUE)\n      PredictionRegr$new(row_ids = prediction$row_ids, truth = truth,\n        response = response)\n    }\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nNote that this [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) is ill-equipped to handle the case of `predict_type = \"se\"`, i.e., we always only return a `response` prediction (as outlined above, this is the case for most target transformations, because transforming standard errors or probabilities of a prediction is often not straightforward). We could of course check whether the `predict_type` is set to `\"se\"` and if this is the case, return `NA` as the standard errors.\n\nTo construct our final target transformation [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) with our linear regression learner, we again simply make use of `ppl(\"targettrafo\")`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ng_bc = ppl(\"targettrafo\", graph = learner_lm,\n  trafo_pipeop = PipeOpTargetTrafoBoxCox$new())\n```\n:::\n\n\nThe following plot should already look quite familiar:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ng_bc$plot(html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/2020-06-15-target-transformations-via-pipelines-026-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nFinally we `$train()` and `$predict()` on the task (again, we wrap the [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) in a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html)):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngl_bc = GraphLearner$new(g_bc)\ngl_bc$train(task)\ngl_bc$state\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$model\n$model$regr.lm\n$model$regr.lm$model\n\nCall:\nstats::lm(formula = task$formula(), data = task$data())\n\nCoefficients:\n(Intercept)           am         carb          cyl         disp         drat         gear           hp         qsec  \n -0.6272999    0.1670950   -0.0663126    0.0237529    0.0002376    0.0759944    0.1963335   -0.0030367    0.1043210  \n         vs           wt  \n -0.0080166   -0.5800635  \n\n\n$model$regr.lm$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$model$regr.lm$train_time\n[1] 0.007\n\n$model$regr.lm$param_vals\nnamed list()\n\n$model$regr.lm$task_hash\n[1] \"612ab4e0ad596159\"\n\n$model$regr.lm$data_prototype\nEmpty data.table (0 rows and 11 cols): mpg.bc,am,carb,cyl,disp,drat...\n\n$model$regr.lm$task_prototype\nEmpty data.table (0 rows and 11 cols): mpg.bc,am,carb,cyl,disp,drat...\n\n$model$regr.lm$mlr3_version\n[1] '0.14.1'\n\n$model$regr.lm$train_task\n<TaskRegr:mtcars> (32 x 11): Motor Trends\n* Target: mpg.bc\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\n\n$model$targettrafoboxcox\n$model$targettrafoboxcox$bc\nStandardized Box Cox Transformation with 32 nonmissing obs.:\n Estimated statistics:\n - lambda = 0.02955701 \n - mean (before standardization) = 3.092016 \n - sd (before standardization) = 0.324959 \n\n\n$model$targetinvert\nlist()\n\n\n$log\nEmpty data.table (0 rows and 3 cols): stage,class,msg\n\n$train_time\n[1] 0.285\n\n$param_vals\nnamed list()\n\n$task_hash\n[1] \"58a137d2055e8406\"\n\n$data_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$task_prototype\nEmpty data.table (0 rows and 11 cols): mpg,am,carb,cyl,disp,drat...\n\n$mlr3_version\n[1] '0.14.1'\n\n$train_task\n<TaskRegr:mtcars> (32 x 11): Motor Trends\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n```\n:::\n\n```{.r .cell-code}\ngl_bc$predict(task)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionRegr> for 32 observations:\n    row_ids truth response\n          1  21.0 21.70854\n          2  21.0 21.13946\n          3  22.8 25.75242\n---                       \n         30  19.7 19.58934\n         31  15.0 14.10658\n         32  21.4 23.15263\n```\n:::\n:::\n\n\nWe could now proceed to benchmark our different target transformations:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbg = benchmark_grid(list(task), learners = list(learner_lm, gl, gl_bc),\n  resamplings = list(rsmp(\"cv\", folds = 10)))\nbmr = benchmark(bg)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbmr$aggregate(msr(\"regr.mse\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nr      resample_result task_id                             learner_id resampling_id iters  regr.mse\n1:  1 <ResampleResult[21]>  mtcars                                regr.lm            cv    10 11.866071\n2:  2 <ResampleResult[21]>  mtcars      targetmutate.regr.lm.targetinvert            cv    10  7.793303\n3:  3 <ResampleResult[21]>  mtcars targettrafoboxcox.regr.lm.targetinvert            cv    10  8.230192\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}