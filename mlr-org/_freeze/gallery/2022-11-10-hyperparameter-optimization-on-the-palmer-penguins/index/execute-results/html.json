{
  "hash": "787ab8578f9415c89914505d655c7457",
  "result": {
    "markdown": "---\ntitle: \"Hyperparameter Optimization on the Palmer Penguins Data Set\"\ndescription: |\n  In this post, we optimize the hyperparameters of an rpart classification tree on the Palmer Penguins data set with only a few lines of code.\ncategories:\n  - tuning\n  - classification\nauthor:\n  - name: Marc Becker\n    url: https://github.com/be-marc\ndate: 2022-11-10\nbibliography: bibliography.bib\nimage: penguins.png\n---\n\n\n\n\n\n::: {.cell .column-body-outset layout-align=\"center\"}\n::: {.cell-output-display}\n![Artwork by @horst_palmer_2022.](penguins.png){fig-align='center' width=900}\n:::\n:::\n\n\n# Scope\n\nIn this post, we optimize the hyperparameters of a simple [`classification tree`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html) on the [`Palmer Penguins`](https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html) data set with only a few lines of code.\n\nFirst, we introduce tuning spaces and show the importance of transformation functions.\nNext, we execute the tuning and present the basic building blocks of tuning in mlr3.\nFinally, we fit a classification tree with optimized hyperparameters on the full data set.\n\n# Prerequistes\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com) package which pulls the most important packages for this example.\nAmong other packages, it loads the hyperparameter optimization package of the mlr3 ecosystem [mlr3tuning](https://mlr3tuning.mlr-org.com).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\nIn this example, we use the [`Palmer Penguins`](https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html) data set which classifies 344 penguins in three species.\nThe data set was collected from 3 islands in the Palmer Archipelago in Antarctica.\nIt includes the name of the island, the size (flipper length, body mass, and bill dimension), and the sex of the penguin.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntsk(\"penguins\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TaskClassif:penguins> (344 x 8): Palmer Penguins\n* Target: species\n* Properties: multiclass\n* Features (7):\n  - int (3): body_mass, flipper_length, year\n  - dbl (2): bill_depth, bill_length\n  - fct (2): island, sex\n```\n:::\n:::\n\n::: {.cell .column-body-outset layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(palmerpenguins)\nlibrary(ggplot2)\nggplot(data = penguins, aes(x = flipper_length_mm, y = bill_length_mm)) +\n  geom_point(aes(color = species, shape = species), size = 3, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, aes(color = species)) +\n  theme_minimal() +\n  scale_color_manual(values = c(\"darkorange\",\"purple\",\"cyan4\")) +\n  labs(x = \"Flipper length (mm)\", y = \"Bill length (mm)\",  color = \"Penguin species\", shape = \"Penguin species\") +\n  theme(\n    legend.position = c(0.85, 0.15),\n    legend.background = element_rect(fill = \"white\", color = NA),\n    text = element_text(size = 10))\n```\n\n::: {.cell-output-display}\n![Flipper and bill length dimensions for Adelie, Chinstrap, and Gentoo Penguins at Palmer Station [@horst_palmer_2022].](index_files/figure-html/hyperparameter-tuning-on-the-palmer-penguins-data-set-005-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n# Learner\n\nWe use the [`rpart classification tree`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html).\nA learner stores all information about its hyperparameters in the slot `$param_set`.\nNot all parameters are tunable.\nWe have to choose a subset of the hyperparameters we want to tune.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\")\nas.data.table(learner$param_set)[, list(id, class, lower, upper, nlevels)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                id    class lower upper nlevels\n 1:             cp ParamDbl     0     1     Inf\n 2:     keep_model ParamLgl    NA    NA       2\n 3:     maxcompete ParamInt     0   Inf     Inf\n 4:       maxdepth ParamInt     1    30      30\n 5:   maxsurrogate ParamInt     0   Inf     Inf\n 6:      minbucket ParamInt     1   Inf     Inf\n 7:       minsplit ParamInt     1   Inf     Inf\n 8: surrogatestyle ParamInt     0     1       2\n 9:   usesurrogate ParamInt     0     2       3\n10:           xval ParamInt     0   Inf     Inf\n```\n:::\n:::\n\n\n# Tuning Space\n\nThe package [mlr3tuningspaces](https://mlr3tuningspaces.mlr-org.com) is a collection of search spaces for hyperparameter tuning from peer-reviewed articles.\nWe use the search space from the @bischl_hyperparameter_2021 article.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlts(\"classif.rpart.default\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TuningSpace:classif.rpart.default>: Default Classification Tree\n          id lower upper levels logscale\n1:  minsplit 2e+00 128.0            TRUE\n2: minbucket 1e+00  64.0            TRUE\n3:        cp 1e-04   0.1            TRUE\n```\n:::\n:::\n\n\nThe classification tree is mainly influenced by three hyperparameters:\n\n* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.\n* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.\n* The `minbucket` hyperparameter that the minimum number of observations in any terminal node.\n\nWe argument the learner with the search space in one go.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlts(lrn(\"classif.rpart\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<LearnerClassifRpart:classif.rpart>: Classification Tree\n* Model: -\n* Parameters: xval=0, minsplit=<RangeTuneToken>, minbucket=<RangeTuneToken>, cp=<RangeTuneToken>\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features, twoclass, weights\n```\n:::\n:::\n\n\n## Transformations\n\nThe column `logscale` indicates that the hyperparameters are tuned on the logarithmic scale.\nThe tuning algorithm proposes hyperparameter values that are transformed with the exponential function before they are passed to the learner.\nFor example, the `cp` parameter is bounded between 0 and 1.\nThe tuning algorithm searches between `log(1e-04)` and `log(1e-01)` but the learner gets the transformed values between `1e-04` and `1e-01`.\nUsing the log transformation emphasizes smaller `cp` values but also creates large values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlts(\"classif.rpart.default\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TuningSpace:classif.rpart.default>: Default Classification Tree\n          id lower upper levels logscale\n1:  minsplit 2e+00 128.0            TRUE\n2: minbucket 1e+00  64.0            TRUE\n3:        cp 1e-04   0.1            TRUE\n```\n:::\n:::\n\n\n# Tuning\n\nThe [`tune()`](https://mlr3tuning.mlr-org.com/reference/tune.html) function controls and executes the tuning.\nThe `method` sets the optimization algorithm.\nThe mlr3 ecosystem offers various optimization algorithms e.g. [`Random Search`](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_random_search.html), [`GenSA`](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_gensa.html), and [`Hyperband`](https://mlr3hyperband.mlr-org.com/reference/mlr_tuners_hyperband.html).\nIn this example, we will use a simple grid search with a grid resolution of 5.\nOur three-dimensional grid consists of $5^3 = 125$ hyperparameter configurations.\nThe [`resampling strategy`](https://mlr3.mlr-org.com/reference/Resampling.html) and [`performance measure`](https://mlr3.mlr-org.com/reference/Measure.html) specify how the performance of a model is evaluated.\nWe choose a [`3-fold cross-validation`](https://mlr3.mlr-org.com/reference/mlr_resamplings_cv.html)  and use the [`classification error`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.ce.html).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  method = \"grid_search\",\n  task = tsk(\"penguins\"),\n  learner = lts(lrn(\"classif.rpart\")),\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  resolution = 5\n)\n```\n:::\n\n\nThe [`tune()`](https://mlr3tuning.mlr-org.com/reference/tune.html) function returns a tuning instance that includes an archive with all evaluated hyperparameter configurations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, list(minsplit, minbucket, cp, classif.ce, resample_result)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      minsplit minbucket        cp classif.ce      resample_result\n  1: 2.7764798  3.130790 -4.029524 0.06976862 <ResampleResult[21]>\n  2: 2.7764798  2.087194 -5.756463 0.06397152 <ResampleResult[21]>\n  3: 3.8181461  2.087194 -9.210340 0.06397152 <ResampleResult[21]>\n  4: 0.6931472  3.130790 -9.210340 0.06976862 <ResampleResult[21]>\n  5: 1.7348135  3.130790 -7.483402 0.06976862 <ResampleResult[21]>\n ---                                                              \n121: 0.6931472  2.087194 -5.756463 0.06397152 <ResampleResult[21]>\n122: 1.7348135  4.174387 -4.029524 0.12196796 <ResampleResult[21]>\n123: 1.7348135  3.130790 -5.756463 0.06976862 <ResampleResult[21]>\n124: 4.8598124  3.130790 -7.483402 0.06976862 <ResampleResult[21]>\n125: 3.8181461  1.043597 -5.756463 0.04363082 <ResampleResult[21]>\n```\n:::\n:::\n\n\nThe best configuration and the corresponding measured performance can be retrieved from the tuning instance.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    minsplit minbucket        cp learner_param_vals  x_domain classif.ce\n1: 0.6931472  1.043597 -7.483402          <list[4]> <list[3]> 0.03493516\n```\n:::\n:::\n\n\nThe `$result_learner_param_vals` field contains the best hyperparameter setting on the learner scale.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance$result_learner_param_vals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$xval\n[1] 0\n\n$minsplit\n[1] 2\n\n$minbucket\n[1] 2\n\n$cp\n[1] 0.0005623413\n```\n:::\n:::\n\n\n# Final Model\n\nThe learner we use to make predictions on new data is called the final model.\nThe final model is trained on the full data set.\nWe add the optimized hyperparameters to the learner and train the learner on the full dataset.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\")\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(tsk(\"penguins\"))\n```\n:::\n\n\nThe trained model can now be used to predict new, external data.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}