{
  "hash": "c2d646b227dc3709449db9af1058c1f5",
  "result": {
    "markdown": "---\ntitle: Tuning a Complex Graph\ncategories:\n  - tuning\n  - mlr3pipelines\n  - classification\nauthor:\n  - name: Lennart Schneider\ndate: 02-03-2021\ndescription: |\n  We show how to tune a complex graph for a single task.\nbibliography: bibliography.bib\nimage: thumbnail.png\n---\n\n\n\n\nIn this use case we show how to tune a rather complex graph consisting of different preprocessing steps and different learners where each preprocessing step and learner itself has parameters that can be tuned.\nYou will learn the following:\n\n* Build a [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) that consists of two common preprocessing steps, then switches between two dimensionality reduction techniques followed by a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) vs. no dimensionality reduction followed by another [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html)\n* Define the search space for tuning that handles inter-dependencies between pipeline steps and hyperparameters\n* Run a [`grid search`](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_grid_search.html) to find an optimal choice of preprocessing steps and hyperparameters.\n\nIdeally you already had a look at how to tune over [multiple learners](https://mlr3gallery.mlr-org.com/posts/2020-02-01-tuning-multiplexer/).\n\nFirst, we load the packages we will need:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\nlibrary(mlr3learners)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\nThe [`lgr`](https://mlr3book.mlr-org.com/logging.html) package is used for logging in all [mlr3](https://mlr3.mlr-org.com) packages.\nThe [mlr3](https://mlr3.mlr-org.com) logger prints the logging messages from the base package, whereas the [bbotk](https://bbotk.mlr-org.com)  logger is responsible for logging messages from the optimization packages (e.g. [mlr3tuning](https://mlr3tuning.mlr-org.com) ).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\n## Data and Task\n\nWe are going to work with some gene expression data included as a supplement in the [bst](https://cran.r-project.org/package=bst) package.\nThe data consists of 2308 gene profiles in 63 training and 20 test samples.\nThe following data preprocessing steps are done analogously as in `vignette(\"khan\", package = \"bst\")`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndatafile = system.file(\"extdata\", \"supplemental_data\", package = \"bst\")\ndat0 = read.delim(datafile, header = TRUE, skip = 1)[, -(1:2)]\ndat0 = t(dat0)\ndat = data.frame(dat0[!(rownames(dat0) %in%\n  c(\"TEST.9\", \"TEST.13\", \"TEST.5\", \"TEST.3\", \"TEST.11\")), ])\ndat$class = as.factor(\n  c(substr(rownames(dat)[1:63], start = 1, stop = 2),\n    c(\"NB\", \"RM\", \"NB\", \"EW\", \"RM\", \"BL\", \"EW\", \"RM\", \"EW\", \"EW\", \"EW\", \"RM\",\n      \"BL\", \"RM\", \"NB\", \"NB\", \"NB\", \"NB\", \"BL\", \"EW\")\n  )\n)\n```\n:::\n\n\nWe then construct our training and test [`Task`](https://mlr3.mlr-org.com/reference/Task.html) :\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = as_task_classif(dat, target = \"class\", id = \"SRBCT\")\ntask_train = task$clone(deep = TRUE)\ntask_train$filter(1:63)\ntask_test = task$clone(deep = TRUE)\ntask_test$filter(64:83)\n```\n:::\n\n\n## Workflow\n\nOur graph will start with log transforming the features, followed by scaling them.\nThen, either a [`PCA`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_pca.html) or [`ICA`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_ica.html) is applied to extract principal / independent components followed by fitting a [`LDA`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.lda.html) or a [`ranger random forest`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) is fitted without any preprocessing (the log transformation and scaling should most likely affect the `LDA` more than the `ranger random forest`).\nRegarding the `PCA` and `ICA`, both the number of principal / independent components are tuning parameters.\nRegarding the `LDA`, we can further choose different methods for estimating the mean and variance and regarding the `ranger`, we want to tune the `mtry` and `num.tree` parameters.\nNote that the `PCA-LDA` combination has already been successfully applied in different cancer diagnostic contexts when the feature space is of high dimensionality [@morais2018].\n\nTo allow for switching between the `PCA` / `ICA`-`LDA` and `ranger` we can either use branching or proxy pipelines, i.e., [`PipeOpBranch`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_branch.html) and [`PipeOpUnbranch`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_unbranch.html) or [`PipeOpProxy`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_proxy.html).\nWe will first cover branching in detail and later show how the same can be done using `PipeOpProxy`.\n\n## Baseline\n\nFirst, we have a look at the baseline [`classification accuracy`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.acc.html) of the `LDA` and `ranger` on the training task:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbase = benchmark(benchmark_grid(\n  task_train,\n  learners = list(lrn(\"classif.lda\"), lrn(\"classif.ranger\")),\n  resamplings = rsmp(\"cv\", folds = 3)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in lda.default(x, grouping, ...): variables are collinear\n\nWarning in lda.default(x, grouping, ...): variables are collinear\n\nWarning in lda.default(x, grouping, ...): variables are collinear\n```\n:::\n\n```{.r .cell-code}\nbase$aggregate(measures = msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nr      resample_result task_id     learner_id resampling_id iters classif.acc\n1:  1 <ResampleResult[21]>   SRBCT    classif.lda            cv     3   0.6666667\n2:  2 <ResampleResult[21]>   SRBCT classif.ranger            cv     3   0.9206349\n```\n:::\n:::\n\n\nThe out-of-the-box `ranger` appears to already have good performance on the training task.\nRegarding the `LDA`, we do get a warning message that some features are colinear.\nThis strongly suggests to reduce the dimensionality of the feature space.\nLet's see if we can get some better performance, at least for the `LDA`.\n\n## Branching\n\nOur graph starts with log transforming the features (we explicitly use base 10 only for better interpretability when inspecting the model later), using [`PipeOpColApply`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_colapply.html), followed by scaling the features using [`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html).\nThen, the first branch allows for switching between the `PCA` / `ICA`-`LDA` and `ranger`, and within `PCA` / `ICA`-`LDA`, the second branch allows for switching between `PCA` and `ICA`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph1 =\n  po(\"colapply\", applicator = function(x) log(x, base = 10)) %>>%\n  po(\"scale\") %>>%\n  # pca / ica followed by lda vs. ranger\n  po(\"branch\", id = \"branch_learner\", options = c(\"pca_ica_lda\", \"ranger\")) %>>%\n  gunion(list(\n    po(\"branch\", id = \"branch_preproc_lda\", options = c(\"pca\", \"ica\")) %>>%\n      gunion(list(\n        po(\"pca\"), po(\"ica\")\n      )) %>>%\n      po(\"unbranch\", id = \"unbranch_preproc_lda\") %>>%\n      lrn(\"classif.lda\"),\n    lrn(\"classif.ranger\")\n  )) %>>%\n  po(\"unbranch\", id = \"unbranch_learner\")\n```\n:::\n\n\nNote that the names of the options within each branch are arbitrary, but ideally they describe what is happening.\nTherefore we go with `\"pca_ica_lda\"` / `\"ranger`\" and `\"pca\"` / `\"ica\"`.\nFinally, we also could have used the `branch` [`ppl`](https://mlr3pipelines.mlr-org.com/reference/ppl.html) to make branching easier (we will come back to this in the [Proxy](#Proxy) section).\nThe graph looks like the following:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph1$plot(html = FALSE)\n```\n\n::: {.cell-output-display}\n![](tuning-a-complex-graph_files/figure-html/tuning-a-complex-graph-009-1.png){fig-align='center' width=960}\n:::\n:::\n\nWe can inspect the parameters of the [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) of the graph to see which parameters can be set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph1$param_set$ids()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"colapply.applicator\"                         \"colapply.affect_columns\"                    \n [3] \"scale.center\"                                \"scale.scale\"                                \n [5] \"scale.robust\"                                \"scale.affect_columns\"                       \n [7] \"branch_learner.selection\"                    \"branch_preproc_lda.selection\"               \n [9] \"pca.center\"                                  \"pca.scale.\"                                 \n[11] \"pca.rank.\"                                   \"pca.affect_columns\"                         \n[13] \"ica.n.comp\"                                  \"ica.alg.typ\"                                \n[15] \"ica.fun\"                                     \"ica.alpha\"                                  \n[17] \"ica.method\"                                  \"ica.row.norm\"                               \n[19] \"ica.maxit\"                                   \"ica.tol\"                                    \n[21] \"ica.verbose\"                                 \"ica.w.init\"                                 \n[23] \"ica.affect_columns\"                          \"classif.lda.dimen\"                          \n[25] \"classif.lda.method\"                          \"classif.lda.nu\"                             \n[27] \"classif.lda.predict.method\"                  \"classif.lda.predict.prior\"                  \n[29] \"classif.lda.prior\"                           \"classif.lda.tol\"                            \n[31] \"classif.ranger.alpha\"                        \"classif.ranger.always.split.variables\"      \n[33] \"classif.ranger.class.weights\"                \"classif.ranger.holdout\"                     \n[35] \"classif.ranger.importance\"                   \"classif.ranger.keep.inbag\"                  \n[37] \"classif.ranger.max.depth\"                    \"classif.ranger.min.node.size\"               \n[39] \"classif.ranger.min.prop\"                     \"classif.ranger.minprop\"                     \n[41] \"classif.ranger.mtry\"                         \"classif.ranger.mtry.ratio\"                  \n[43] \"classif.ranger.num.random.splits\"            \"classif.ranger.num.threads\"                 \n[45] \"classif.ranger.num.trees\"                    \"classif.ranger.oob.error\"                   \n[47] \"classif.ranger.regularization.factor\"        \"classif.ranger.regularization.usedepth\"     \n[49] \"classif.ranger.replace\"                      \"classif.ranger.respect.unordered.factors\"   \n[51] \"classif.ranger.sample.fraction\"              \"classif.ranger.save.memory\"                 \n[53] \"classif.ranger.scale.permutation.importance\" \"classif.ranger.se.method\"                   \n[55] \"classif.ranger.seed\"                         \"classif.ranger.split.select.weights\"        \n[57] \"classif.ranger.splitrule\"                    \"classif.ranger.verbose\"                     \n[59] \"classif.ranger.write.forest\"                \n```\n:::\n:::\n\n\nThe `id`'s are prefixed by the respective [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) they belong to, e.g., `pca.rank.` refers to the `rank.` parameter of [`PipeOpPCA`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_pca.html).\n\n## Search Space\n\nOur graph either fits a `LDA` after applying `PCA` or `ICA`, or alternatively a `ranger` with no preprocessing.\nThese two **options** each define selection parameters that we can tune.\nMoreover, within the respective [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)'s we want to tune the following parameters:\n`pca.rank.`, `ica.n.comp`, `classif.lda.method`, `classif.ranger.mtry`, and `classif.ranger.num.trees`.\nThe first two parameters are integers that in-principal could range from 1 to the number of features.\nHowever, for `ICA`, the upper bound must not exceed the number of observations and as we will later use `3-fold` [`cross-validation`](https://mlr3.mlr-org.com/reference/mlr_resamplings_cv.html) as the resampling method for the tuning, we just set the upper bound to 30 (and do the same for `PCA`).\nRegarding the `classif.lda.method` we will only be interested in `\"moment\"` estimation vs. minimum volume ellipsoid covariance estimation (`\"mve\"`).\nMoreover, we set the lower bound of `classif.ranger.mtry` to 200 (which is around the number of features divided by 10) and the upper bound to 1000.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntune_ps1 = ps(\n  branch_learner.selection =\n    p_fct(c(\"pca_ica_lda\", \"ranger\")),\n  branch_preproc_lda.selection =\n    p_fct(c(\"pca\", \"ica\"), depends = branch_learner.selection == \"pca_ica_lda\"),\n  pca.rank. =\n    p_int(1, 30, depends = branch_preproc_lda.selection == \"pca\"),\n  ica.n.comp =\n    p_int(1, 30, depends = branch_preproc_lda.selection == \"ica\"),\n  classif.lda.method =\n    p_fct(c(\"moment\", \"mve\"), depends = branch_preproc_lda.selection == \"ica\"),\n  classif.ranger.mtry =\n    p_int(200, 1000, depends = branch_learner.selection == \"ranger\"),\n  classif.ranger.num.trees =\n    p_int(500, 2000, depends = branch_learner.selection == \"ranger\"))\n```\n:::\n\n\nThe parameter `branch_learner.selection` defines whether we go down the left (`PCA` / `ICA` followed by `LDA`) or the right branch (`ranger`).\nThe parameter `branch_preproc_lda.selection` defines whether a `PCA` or `ICA` will be applied prior to the `LDA`.\nThe other parameters directly belong to the `ParamSet` of the `PCA` / `ICA` / `LDA` / `ranger`.\nNote that it only makes sense to switch between `PCA` / `ICA` if the `\"pca_ica_lda\"` branch was selected beforehand.\nWe have to specify this via the `depends` parameter.\n\nFinally, we also could have proceeded to tune the numeric parameters on a log scale.\nI.e., looking at `pca.rank.` the performance difference between rank 1 and 2 is probably much larger than between rank 29 and rank 30.\nThe [mlr3tuning Tutorial](https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3tuning-tutorial-german-credit/#random-search-and-transformation) covers such transformations.\n\n## Tuning\n\nWe can now tune the parameters of our graph as defined in the search space with respect to a measure.\nWe will use the [`classification accuracy`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.acc.html).\nAs a resampling method we use [`3-fold cross-validation`](https://mlr3.mlr-org.com/reference/mlr_resamplings_cv.html).\nWe will use the [`TerminatorNone`](https://bbotk.mlr-org.com/reference/mlr_terminators_none.html) (i.e., no early termination) for terminating the tuning because we will apply a [`grid search`](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_grid_search.html) (we use a `grid search` because it gives nicely plottable and understandable results but if there were much more parameters, [`random search`](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_random_search.html) or more intelligent optimization methods would be preferred to a `grid search`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntune1 = TuningInstanceSingleCrit$new(\n  task_train,\n  learner = graph1,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\"),\n  search_space = tune_ps1,\n  terminator = trm(\"none\")\n)\n```\n:::\n\n\nWe then perform a `grid search` using a resolution of 4 for the numeric parameters.\nThe grid being used will look like the following (note that the dependencies we specified above are handled automatically):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngenerate_design_grid(tune_ps1, resolution = 4)\n```\n:::\n\n::: {.cell .column-page layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-ea17b0379ba6180cc546\" style=\"width:100%;height:auto;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-ea17b0379ba6180cc546\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\"],[\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\"],[\"pca\",\"pca\",\"pca\",\"pca\",\"ica\",\"ica\",\"ica\",\"ica\",\"ica\",\"ica\",\"ica\",\"ica\",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[1,10,20,30,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,1,1,10,10,20,20,30,30,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,\"moment\",\"mve\",\"moment\",\"mve\",\"moment\",\"mve\",\"moment\",\"mve\",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],[null,null,null,null,null,null,null,null,null,null,null,null,200,200,200,200,466,466,466,466,733,733,733,733,1000,1000,1000,1000],[null,null,null,null,null,null,null,null,null,null,null,null,500,1000,1500,2000,500,1000,1500,2000,500,1000,1500,2000,500,1000,1500,2000]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>branch_learner.selection<\\/th>\\n      <th>branch_preproc_lda.selection<\\/th>\\n      <th>pca.rank.<\\/th>\\n      <th>ica.n.comp<\\/th>\\n      <th>classif.lda.method<\\/th>\\n      <th>classif.ranger.mtry<\\/th>\\n      <th>classif.ranger.num.trees<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[3,4,6,7]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nWe trigger the tuning.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuner_gs = tnr(\"grid_search\", resolution = 4, batch_size = 10)\ntuner_gs$optimize(tune1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   branch_learner.selection branch_preproc_lda.selection pca.rank. ica.n.comp classif.lda.method classif.ranger.mtry\n1:              pca_ica_lda                          ica        NA         10                mve                  NA\n   classif.ranger.num.trees learner_param_vals  x_domain classif.acc\n1:                       NA          <list[8]> <list[4]>    0.984127\n```\n:::\n:::\n\n\nNow, we can inspect the results ordered by the `classification accuracy`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(tune1$archive)[order(classif.acc), ]\n```\n:::\n\n::: {.cell .column-page layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-fe8ee9badd6991289a71\" style=\"width:100%;height:auto;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-fe8ee9badd6991289a71\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\"],[\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"ranger\",\"pca_ica_lda\",\"pca_ica_lda\",\"ranger\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\",\"pca_ica_lda\"],[\"ica\",\"ica\",\"pca\",\"ica\",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,\"pca\",\"ica\",null,\"ica\",\"pca\",\"ica\",\"pca\",\"ica\",\"ica\"],[null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,10,null,null,null,30,null,20,null,null],[1,1,null,20,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,10,null,30,null,30,null,20,10],[\"mve\",\"moment\",null,\"mve\",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,\"moment\",null,\"moment\",null,\"mve\",null,\"moment\",\"mve\"],[null,null,null,null,200,466,733,733,1000,200,466,733,733,1000,466,1000,1000,200,466,null,null,200,null,null,null,null,null,null],[null,null,null,null,500,1000,500,1500,500,1000,1500,1000,2000,1000,2000,1500,2000,2000,500,null,null,1500,null,null,null,null,null,null],[0.253968253968254,0.317460317460317,0.317460317460317,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.920634920634921,0.936507936507937,0.936507936507937,0.936507936507937,0.936507936507937,0.936507936507937,0.952380952380952,0.952380952380952,0.952380952380952,0.968253968253968,0.968253968253968,0.984126984126984],[219.366,218.868,3.18000000000001,220.595,4.337,6.798,5.907,10.885,6.75,5.08899999999998,8.33900000000001,8.34199999999998,13.281,10.173,12.096,15.215,18.484,6.85100000000001,5.167,5.129,210.115,8.07799999999999,217.421,5.29299999999999,216.956,3.19000000000001,211.887,219.918],[\"2022-11-17T11:23:13Z\",\"2022-11-17T11:23:13Z\",\"2022-11-17T11:23:21Z\",\"2022-11-17T11:23:13Z\",\"2022-11-17T11:23:13Z\",\"2022-11-17T11:23:13Z\",\"2022-11-17T11:23:13Z\",\"2022-11-17T11:23:13Z\",\"2022-11-17T11:23:13Z\",\"2022-11-17T11:23:21Z\",\"2022-11-17T11:23:21Z\",\"2022-11-17T11:23:21Z\",\"2022-11-17T11:23:21Z\",\"2022-11-17T11:23:21Z\",\"2022-11-17T11:24:36Z\",\"2022-11-17T11:24:36Z\",\"2022-11-17T11:24:36Z\",\"2022-11-17T11:23:21Z\",\"2022-11-17T11:23:21Z\",\"2022-11-17T11:24:36Z\",\"2022-11-17T11:24:36Z\",\"2022-11-17T11:24:36Z\",\"2022-11-17T11:23:13Z\",\"2022-11-17T11:24:36Z\",\"2022-11-17T11:24:36Z\",\"2022-11-17T11:23:21Z\",\"2022-11-17T11:24:36Z\",\"2022-11-17T11:23:13Z\"],[1,1,2,1,1,1,1,1,1,2,2,2,2,2,3,3,3,2,2,3,3,3,1,3,3,2,3,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>branch_learner.selection<\\/th>\\n      <th>branch_preproc_lda.selection<\\/th>\\n      <th>pca.rank.<\\/th>\\n      <th>ica.n.comp<\\/th>\\n      <th>classif.lda.method<\\/th>\\n      <th>classif.ranger.mtry<\\/th>\\n      <th>classif.ranger.num.trees<\\/th>\\n      <th>classif.acc<\\/th>\\n      <th>runtime_learners<\\/th>\\n      <th>timestamp<\\/th>\\n      <th>batch_nr<\\/th>\\n      <th>warnings<\\/th>\\n      <th>errors<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[3,4,6,7,8,9,11,12,13]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nWe achieve very good accuracy using `ranger`, more or less regardless how `mtry` and `num.trees` are set. However, the `LDA` also shows very good accuracy when combined with `PCA` or `ICA` retaining 30 components.\n\nFor now, we decide to use `ranger` with `mtry` set to 200 and `num.trees` set to 1000.\n\nSetting these parameters manually in our graph, then training on the training task and predicting on the test task yields an accuracy of:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph1$param_set$values$branch_learner.selection = \"ranger\"\ngraph1$param_set$values$classif.ranger.mtry = 200\ngraph1$param_set$values$classif.ranger.num.trees = 1000\ngraph1$train(task_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$unbranch_learner.output\nNULL\n```\n:::\n\n```{.r .cell-code}\ngraph1$predict(task_test)[[1L]]$score(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.acc \n          1 \n```\n:::\n:::\n\n\nNote that we also could have wrapped our graph in a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) and proceeded to use this as a learner in an [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html).\n\n## Proxy\n\nInstead of using branches to split our graph with respect to the learner and preprocessing options, we can also use [`PipeOpProxy`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_proxy.html).\n`PipeOpProxy` accepts a single `content` parameter that can contain any other [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) or [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html).\nThis is extremely flexible in the sense that we do not have to specify our options during construction.\nHowever, the parameters of the contained [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) or [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) are no longer directly contained in the `ParamSet` of the resulting graph.\nTherefore, when tuning the graph, we do have to make use of a `trafo` function.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph2 =\n  po(\"colapply\", applicator = function(x) log(x, base = 10)) %>>%\n  po(\"scale\") %>>%\n  po(\"proxy\")\n```\n:::\n\n\nThis graph now looks like the following:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph2$plot(html = FALSE)\n```\n\n::: {.cell-output-display}\n![](tuning-a-complex-graph_files/figure-html/tuning-a-complex-graph-022, graph2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nAt first, this may look like a linear graph. However, as the `content` parameter of `PipeOpProxy` can be tuned and set to contain any other [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) or [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html), this will allow for a similar non-linear graph as when doing branching.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph2$param_set$ids()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"colapply.applicator\"     \"colapply.affect_columns\" \"scale.center\"            \"scale.scale\"            \n[5] \"scale.robust\"            \"scale.affect_columns\"    \"proxy.content\"          \n```\n:::\n:::\n\n\nWe can tune the graph by using the same search space as before. However, here the `trafo` function is of central importance to actually set our options and parameters:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntune_ps2 = tune_ps1$clone(deep = TRUE)\n```\n:::\n\n\nThe `trafo` function does all the work, i.e., selecting either the `PCA` / `ICA`-`LDA` or `ranger` as the `proxy.content` as well as setting the parameters of the respective preprocessing [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s and [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html)s.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nproxy_options = list(\n  pca_ica_lda =\n    ppl(\"branch\", graphs = list(pca = po(\"pca\"), ica = po(\"ica\"))) %>>%\n      lrn(\"classif.lda\"),\n  ranger = lrn(\"classif.ranger\")\n)\n```\n:::\n\n\nAbove, we made use of the `branch` [`ppl`](https://mlr3pipelines.mlr-org.com/reference/ppl.html) allowing us to easily construct a branching graph.\nOf course we also could have use another nested `PipeOpProxy` to specify the preprocessing options (`\"pca\"` vs. `\"ica\"`) within `proxy_options` if for some reason we do not want to do branching at all.\nThe `trafo` function below selects one of the `proxy_options` from above and sets the respective parameters for the `PCA`, `ICA`, `LDA` and `ranger`.\nHere, the argument `x` is a list which will contain sampled / selected parameters from our `ParamSet` (in our case, `tune_ps2`).\nThe return value is a list only including the appropriate `proxy.content` parameter.\nIn each tuning iteration, the `proxy.content` parameter of our graph will be set to this value.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntune_ps2$trafo = function(x, param_set) {\n  proxy.content = proxy_options[[x$branch_learner.selection]]\n  if (x$branch_learner.selection == \"pca_ica_lda\") {\n    # pca_ica_lda\n    proxy.content$param_set$values$branch.selection = x$branch_preproc_lda.selection\n    if (x$branch_preproc_lda.selection == \"pca\") {\n      proxy.content$param_set$values$pca.rank. = x$pca.rank.\n    } else {\n      proxy.content$param_set$values$ica.n.comp = x$ica.n.comp\n    }\n    proxy.content$param_set$values$classif.lda.method = x$classif.lda.method\n  } else {\n    # ranger\n    proxy.content$param_set$values$mtry = x$classif.ranger.mtry\n    proxy.content$param_set$values$num.trees = x$classif.ranger.num.trees\n  }\n  list(proxy.content = proxy.content)\n}\n```\n:::\n\n\nI.e., suppose that the following parameters will be selected from our `ParamSet`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx = list(\n  branch_learner.selection = \"ranger\",\n  classif.ranger.mtry = 200,\n  classif.ranger.num.trees = 500)\n```\n:::\n\n\nThe `trafo` function will then return:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntune_ps2$trafo(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$proxy.content\n<LearnerClassifRanger:classif.ranger>\n* Model: -\n* Parameters: num.threads=1, mtry=200, num.trees=500\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error, twoclass, weights\n```\n:::\n:::\n\n\nTuning can be carried out analogously as done above:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntune2 = TuningInstanceSingleCrit$new(\n  task_train,\n  learner = graph2,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.acc\"),\n  search_space = tune_ps2,\n  terminator = trm(\"none\")\n)\ntuner_gs$optimize(tune2)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(tune2$archive)[order(classif.acc), ]\n```\n:::\n",
    "supporting": [
      "tuning-a-complex-graph_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/datatables-binding-0.25/datatables.js\"></script>\n<script src=\"../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../../site_libs/dt-core-1.11.3/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/dt-core-1.11.3/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/dt-core-1.11.3/js/jquery.dataTables.min.js\"></script>\n<link href=\"../../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}