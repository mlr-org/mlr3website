{
  "hash": "3e23bfcc6d8ae5dba381b34e12dbc557",
  "result": {
    "markdown": "---\ntitle: Practical Tuning Series - Build an Automated Machine Learning System\ndescription: |\n  We implement a simple automated machine learning (AutoML) system which includes preprocessing, a switch between multiple learners and hyperparameter tuning.\ncategories:\n  - tuning\n  - resampling\n  - mlr3pipelines\n  - automl\n  - classification\n  - practical tuning series\nauthor:\n  - name: Marc Becker\n  - name: Theresa Ullmann\n  - name: Michel Lang\n  - name: Bernd Bischl\n  - name: Jakob Richter\n  - name: Martin Binder\ndate: 03-11-2021\nimage: preview.png\n---\n\n\n\n\n# Scope\n\nThis is the third part of the practical tuning series.\nThe other parts can be found here:\n\n* [Part I - Tune a Support Vector Machine](https://mlr3gallery.mlr-org.com/posts/2021-03-09-practical-tuning-series-tune-a-support-vector-machine/)\n* [Part II - Tune a Preprocessing Pipeline](https://mlr3gallery.mlr-org.com/posts/2021-03-10-practical-tuning-series-tune-a-preprocessing-pipeline/)\n* [Part IV - Tuning and Parallel Processing](https://mlr3gallery.mlr-org.com/posts/2021-03-12-practical-tuning-series-tuning-and-parallel-processing/)\n\nIn this post, we implement a simple automated machine learning (AutoML) system which includes preprocessing, a switch between multiple learners and hyperparameter tuning.\nFor this, we build a pipeline with the [mlr3pipelines](https://mlr3pipelines.mlr-org.com) extension package.\nAdditionally, we use nested resampling to get an unbiased performance estimate of our AutoML system.\n\n# Prerequisites\n\nWe load the [mlr3verse](https://mlr3verse.mlr-org.com)  package which pulls in the most important packages for this example.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr3verse)\n```\n:::\n\n\nWe initialize the random number generator with a fixed seed for reproducibility, and decrease the verbosity of the logger to keep the output clearly represented.\nThe [`lgr`](https://mlr3book.mlr-org.com/logging.html) package is used for logging in all [mlr3](https://mlr3.mlr-org.com) packages.\nThe [mlr3](https://mlr3.mlr-org.com) logger prints the logging messages from the base package, whereas the [bbotk](https://bbotk.mlr-org.com)  logger is responsible for logging messages from the optimization packages (e.g. [mlr3tuning](https://mlr3tuning.mlr-org.com) ).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(7832)\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n```\n:::\n\n\nIn this example, we use the [Pima Indians Diabetes data set](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) which is used to to predict whether or not a patient has diabetes.\nThe patients are characterized by 8 numeric features and some have missing values.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\n```\n:::\n\n\n# Branching\n\nWe use three popular machine learning algorithms: k-nearest-neighbors, support vector machines and random forests.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearners = list(\n  lrn(\"classif.kknn\", id = \"kknn\"),\n  lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\"),\n  lrn(\"classif.ranger\", id = \"ranger\")\n)\n```\n:::\n\n\nThe [`PipeOpBranch`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_branch.html) allows us to specify multiple alternatives paths.\nIn this graph, the paths lead to the different learner models.\nThe `selection` hyperparameter controls which path is executed i.e., which learner is used to fit a model.\nIt is important to use the [`PipeOpBranch`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_branch.html) after the branching so that the outputs are merged into one result object.\nWe visualize the graph with branching below.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph =\n  po(\"branch\", options = c(\"kknn\", \"svm\", \"ranger\")) %>>%\n  gunion(lapply(learners, po)) %>>%\n  po(\"unbranch\")\ngraph$plot(html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/practical-tuning-series-build-an-automated-machine-learning-system-006-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nAlternatively, we can use the [`ppl()`](https://mlr3pipelines.mlr-org.com/reference/ppl.html)-shortcut to load a predefined graph from the [`mlr_graphs`](https://mlr3pipelines.mlr-org.com/reference/mlr_graphs.html) dictionary.\nFor this, the learner list must be named.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearners = list(\n  kknn = lrn(\"classif.kknn\", id = \"kknn\"),\n  svm = lrn(\"classif.svm\", id = \"svm\", type = \"C-classification\"),\n  ranger = lrn(\"classif.ranger\", id = \"ranger\")\n)\n\ngraph = ppl(\"branch\", lapply(learners, po))\n```\n:::\n\n\n# Preprocessing\n\nThe task has missing data in five columns.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(task$missings() / task$nrow, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndiabetes      age  glucose  insulin     mass pedigree pregnant pressure  triceps \n    0.00     0.00     0.01     0.49     0.01     0.00     0.00     0.05     0.30 \n```\n:::\n:::\n\n\nThe pipeline [`\"robustify\"`](https://mlr3pipelines.mlr-org.com/reference/mlr_graphs_robustify.html) function creates a preprocessing pipeline based on our task.\nThe resulting pipeline imputes missing values with [`PipeOpImputeHist`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_imputehist.html) and creates a dummy column ([`PipeOpMissInd`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_missind.html)) which indicates the imputed missing values.\nInternally, this creates two paths and the results are combined with [`PipeOpFeatureUnion`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_featureunion.html).\nIn contrast to [`PipeOpBranch`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_branch.html), both paths are executed.\nAdditionally, `\"robustify\"` adds [`PipeOpEncode`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_encode.html) to encode factor columns and [`PipeOpRemoveConstants`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_removeconstants.html) to remove features with a constant value.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph = ppl(\"robustify\", task = task, factors_to_numeric = TRUE) %>>%\n  graph\nplot(graph, html = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/practical-tuning-series-build-an-automated-machine-learning-system-009-1.png){fig-align='center' width=960}\n:::\n:::\n\n\nWe could also create the preprocessing pipeline manually.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngunion(list(po(\"imputehist\"),\n  po(\"missind\", affect_columns = selector_type(c(\"numeric\", \"integer\"))))) %>>%\n  po(\"featureunion\") %>>%\n  po(\"encode\") %>>%\n  po(\"removeconstants\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGraph with 5 PipeOps:\n              ID         State        sccssors          prdcssors\n      imputehist <<UNTRAINED>>    featureunion                   \n         missind <<UNTRAINED>>    featureunion                   \n    featureunion <<UNTRAINED>>          encode imputehist,missind\n          encode <<UNTRAINED>> removeconstants       featureunion\n removeconstants <<UNTRAINED>>                             encode\n```\n:::\n:::\n\n\n# Graph Learner\n\nWe use [`as_learner()`](https://mlr3.mlr-org.com/reference/as_learner.html) to create a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html) which encapsulates the pipeline and can be used like a learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(graph)\n```\n:::\n\n\nThe parameter set of the graph learner includes all hyperparameters from all contained learners.\nThe hyperparameter ids are prefixed with the corresponding learner ids.\nThe hyperparameter `branch.selection` controls which learner is used.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nas.data.table(graph_learner$param_set)[, .(id, class, lower, upper, nlevels)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                              id    class lower upper nlevels\n 1:           removeconstants_prerobustify.ratio ParamDbl     0     1     Inf\n 2:         removeconstants_prerobustify.rel_tol ParamDbl     0   Inf     Inf\n 3:         removeconstants_prerobustify.abs_tol ParamDbl     0   Inf     Inf\n 4:       removeconstants_prerobustify.na_ignore ParamLgl    NA    NA       2\n 5:  removeconstants_prerobustify.affect_columns ParamUty    NA    NA     Inf\n 6:                    imputehist.affect_columns ParamUty    NA    NA     Inf\n 7:                                missind.which ParamFct    NA    NA       2\n 8:                                 missind.type ParamFct    NA    NA       4\n 9:                       missind.affect_columns ParamUty    NA    NA     Inf\n10:                  imputesample.affect_columns ParamUty    NA    NA     Inf\n11:                                encode.method ParamFct    NA    NA       5\n12:                        encode.affect_columns ParamUty    NA    NA     Inf\n13:          removeconstants_postrobustify.ratio ParamDbl     0     1     Inf\n14:        removeconstants_postrobustify.rel_tol ParamDbl     0   Inf     Inf\n15:        removeconstants_postrobustify.abs_tol ParamDbl     0   Inf     Inf\n16:      removeconstants_postrobustify.na_ignore ParamLgl    NA    NA       2\n17: removeconstants_postrobustify.affect_columns ParamUty    NA    NA     Inf\n18:                                       kknn.k ParamInt     1   Inf     Inf\n19:                                kknn.distance ParamDbl     0   Inf     Inf\n20:                                  kknn.kernel ParamFct    NA    NA      10\n21:                                   kknn.scale ParamLgl    NA    NA       2\n22:                                 kknn.ykernel ParamUty    NA    NA     Inf\n23:                             kknn.store_model ParamLgl    NA    NA       2\n24:                                svm.cachesize ParamDbl  -Inf   Inf     Inf\n25:                            svm.class.weights ParamUty    NA    NA     Inf\n26:                                    svm.coef0 ParamDbl  -Inf   Inf     Inf\n27:                                     svm.cost ParamDbl     0   Inf     Inf\n28:                                    svm.cross ParamInt     0   Inf     Inf\n29:                          svm.decision.values ParamLgl    NA    NA       2\n30:                                   svm.degree ParamInt     1   Inf     Inf\n31:                                  svm.epsilon ParamDbl     0   Inf     Inf\n32:                                   svm.fitted ParamLgl    NA    NA       2\n33:                                    svm.gamma ParamDbl     0   Inf     Inf\n34:                                   svm.kernel ParamFct    NA    NA       4\n35:                                       svm.nu ParamDbl  -Inf   Inf     Inf\n36:                                    svm.scale ParamUty    NA    NA     Inf\n37:                                svm.shrinking ParamLgl    NA    NA       2\n38:                                svm.tolerance ParamDbl     0   Inf     Inf\n39:                                     svm.type ParamFct    NA    NA       2\n40:                                 ranger.alpha ParamDbl  -Inf   Inf     Inf\n41:                ranger.always.split.variables ParamUty    NA    NA     Inf\n42:                         ranger.class.weights ParamUty    NA    NA     Inf\n43:                               ranger.holdout ParamLgl    NA    NA       2\n44:                            ranger.importance ParamFct    NA    NA       4\n45:                            ranger.keep.inbag ParamLgl    NA    NA       2\n46:                             ranger.max.depth ParamInt     0   Inf     Inf\n47:                         ranger.min.node.size ParamInt     1   Inf     Inf\n48:                              ranger.min.prop ParamDbl  -Inf   Inf     Inf\n49:                               ranger.minprop ParamDbl  -Inf   Inf     Inf\n50:                                  ranger.mtry ParamInt     1   Inf     Inf\n51:                            ranger.mtry.ratio ParamDbl     0     1     Inf\n52:                     ranger.num.random.splits ParamInt     1   Inf     Inf\n53:                           ranger.num.threads ParamInt     1   Inf     Inf\n54:                             ranger.num.trees ParamInt     1   Inf     Inf\n55:                             ranger.oob.error ParamLgl    NA    NA       2\n56:                 ranger.regularization.factor ParamUty    NA    NA     Inf\n57:               ranger.regularization.usedepth ParamLgl    NA    NA       2\n58:                               ranger.replace ParamLgl    NA    NA       2\n59:             ranger.respect.unordered.factors ParamFct    NA    NA       3\n60:                       ranger.sample.fraction ParamDbl     0     1     Inf\n61:                           ranger.save.memory ParamLgl    NA    NA       2\n62:          ranger.scale.permutation.importance ParamLgl    NA    NA       2\n63:                             ranger.se.method ParamFct    NA    NA       2\n64:                                  ranger.seed ParamInt  -Inf   Inf     Inf\n65:                  ranger.split.select.weights ParamUty    NA    NA     Inf\n66:                             ranger.splitrule ParamFct    NA    NA       3\n67:                               ranger.verbose ParamLgl    NA    NA       2\n68:                          ranger.write.forest ParamLgl    NA    NA       2\n69:                             branch.selection ParamFct    NA    NA       3\n                                              id    class lower upper nlevels\n```\n:::\n:::\n\n\n# Tune the pipeline\n\nWe will only tune one hyperparameter for each learner in this example.\nAdditionally, we tune the branching parameter which selects one of the three learners.\nWe have to specify that a hyperparameter is only valid for a certain learner by using `depends = branch.selection == <learner_id>`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# branch\ngraph_learner$param_set$values$branch.selection =\n  to_tune(c(\"kknn\", \"svm\", \"ranger\"))\n\n# kknn\ngraph_learner$param_set$values$kknn.k =\n  to_tune(p_int(3, 50, logscale = TRUE, depends = branch.selection == \"kknn\"))\n\n# svm\ngraph_learner$param_set$values$svm.cost =\n  to_tune(p_dbl(-1, 1, trafo = function(x) 10^x, depends = branch.selection == \"svm\"))\n\n# ranger\ngraph_learner$param_set$values$ranger.mtry =\n  to_tune(p_int(1, 8, depends = branch.selection == \"ranger\"))\n\n# short learner id for printing\ngraph_learner$id = \"graph_learner\"\n```\n:::\n\n\nWe define a tuning instance and select a random search which is stopped after 20 evaluated configurations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstance = tune(\n  method = \"random_search\",\n  task = task,\n  learner = graph_learner,\n  resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 20\n)\n```\n:::\n\n\nThe following shows a quick way to visualize the tuning results.\n\n\n::: {.cell .column-page layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(instance, type = \"marginal\",\n  cols_x = c(\"x_domain_kknn.k\", \"x_domain_svm.cost\", \"ranger.mtry\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/practical-tuning-series-build-an-automated-machine-learning-system-018-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n# Final Model\n\nWe add the optimized hyperparameters to the graph learner and train the learner on the full dataset.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlearner = as_learner(graph)\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n```\n:::\n\n\nThe trained model can now be used to make predictions on new data.\nA common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (`instance$result_y`) as the model's performance.\nInstead we have to use nested resampling to get an unbiased performance estimate.\n\n# Nested Resampling\n\nWe use nested resampling to get an unbiased estimate of the predictive performance of our graph learner.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(graph)\ngraph_learner$param_set$values$branch.selection =\n  to_tune(c(\"kknn\", \"svm\", \"ranger\"))\ngraph_learner$param_set$values$kknn.k =\n  to_tune(p_int(3, 50, logscale = TRUE, depends = branch.selection == \"kknn\"))\ngraph_learner$param_set$values$svm.cost =\n  to_tune(p_dbl(-1, 1, trafo = function(x) 10^x, depends = branch.selection == \"svm\"))\ngraph_learner$param_set$values$ranger.mtry =\n  to_tune(p_int(1, 8, depends = branch.selection == \"ranger\"))\ngraph_learner$id = \"graph_learner\"\n\ninner_resampling = rsmp(\"cv\", folds = 3)\nat = AutoTuner$new(\n  learner = graph_learner,\n  resampling = inner_resampling,\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"evals\", n_evals = 10),\n  tuner = tnr(\"random_search\")\n)\n\nouter_resampling = rsmp(\"cv\", folds = 3)\nrr = resample(task, at, outer_resampling, store_models = TRUE)\n```\n:::\n\n\nWe check the inner tuning results for stable hyperparameters.\nThis means that the selected hyperparameters should not vary too much.\nWe might observe unstable models in this example because the small data set and the low number of resampling iterations might introduce too much randomness.\nUsually, we aim for the selection of stable hyperparameters for all outer training sets.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nextract_inner_tuning_results(rr)\n```\n:::\n\n::: {.cell .column-page layout-align=\"center\"}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-ac1ee7d5c55489f85af5\" style=\"width:100%;height:auto;\" class=\"datatables html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-ac1ee7d5c55489f85af5\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\"],[1,2,3],[null,null,null],[-0.131603902525644,null,null],[null,3,2],[\"svm\",\"ranger\",\"ranger\"],[0.232427473913542,0.228494438711157,0.259717922256622],[\"pima\",\"pima\",\"pima\"],[\"graph_learner.tuned\",\"graph_learner.tuned\",\"graph_learner.tuned\"],[\"cv\",\"cv\",\"cv\"]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>iteration<\\/th>\\n      <th>kknn.k<\\/th>\\n      <th>svm.cost<\\/th>\\n      <th>ranger.mtry<\\/th>\\n      <th>branch.selection<\\/th>\\n      <th>classif.ce<\\/th>\\n      <th>task_id<\\/th>\\n      <th>learner_id<\\/th>\\n      <th>resampling_id<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,6]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nNext, we want to compare the predictive performances estimated on the outer resampling to the inner resampling.\nSignificantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$score()[, .(iteration, task_id, learner_id, resampling_id, classif.ce)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   iteration task_id          learner_id resampling_id classif.ce\n1:         1    pima graph_learner.tuned            cv  0.2304688\n2:         2    pima graph_learner.tuned            cv  0.2578125\n3:         3    pima graph_learner.tuned            cv  0.2070312\n```\n:::\n:::\n\n\nThe aggregated performance of all outer resampling iterations is essentially the unbiased performance of the graph learner with optimal hyperparameter found by random search.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.ce \n 0.2317708 \n```\n:::\n:::\n\n\nApplying nested resampling can be shortened by using the [`tune_nested()`](https://mlr3tuning.mlr-org.com/reference/tune_nested.html)-shortcut.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngraph_learner = as_learner(graph)\ngraph_learner$param_set$values$branch.selection =\n  to_tune(c(\"kknn\", \"svm\", \"ranger\"))\ngraph_learner$param_set$values$kknn.k =\n  to_tune(p_int(3, 50, logscale = TRUE, depends = branch.selection == \"kknn\"))\ngraph_learner$param_set$values$svm.cost =\n  to_tune(p_dbl(-1, 1, trafo = function(x) 10^x, depends = branch.selection == \"svm\"))\ngraph_learner$param_set$values$ranger.mtry =\n  to_tune(p_int(1, 8, depends = branch.selection == \"ranger\"))\ngraph_learner$id = \"graph_learner\"\n\nrr = tune_nested(\n  method = \"random_search\",\n  task = task,\n  learner = graph_learner,\n  inner_resampling = rsmp(\"cv\", folds = 3),\n  outer_resampling = rsmp(\"cv\", folds = 3),\n  measure = msr(\"classif.ce\"),\n  term_evals = 10,\n)\n```\n:::\n\n\n# Resources\n\nThe [mlr3book](https://mlr3book.mlr-org.com/) includes chapters on [pipelines](https://mlr3book.mlr-org.com/pipelines.html) and [hyperparameter tuning](https://mlr3book.mlr-org.com/tuning.html).\nThe [mlr3cheatsheets](https://cheatsheets.mlr-org.com/) contain frequently used commands and workflows of mlr3.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<link href=\"../../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/datatables-binding-0.25/datatables.js\"></script>\n<script src=\"../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../../site_libs/dt-core-1.11.3/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../../site_libs/dt-core-1.11.3/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/dt-core-1.11.3/js/jquery.dataTables.min.js\"></script>\n<link href=\"../../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}