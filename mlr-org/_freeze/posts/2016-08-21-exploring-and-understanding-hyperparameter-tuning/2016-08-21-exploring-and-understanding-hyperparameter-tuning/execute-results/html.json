{
  "hash": "87f4f138aa451c5637187a0f40679b66",
  "result": {
    "markdown": "---\ntitle: \"Exploring and Understanding Hyperparameter Tuning\"\nauthor: [\"Mason Gallo\"]\ndate: '2016-08-21'\ntags:\n- hyperparameter\n- tuning\n- rstats\ncategories:\n- R\n- r-bloggers\n\ndescription: \"Hyperparameter optimization in mlr\"\n---\n\n\n\n\nLearners use hyperparameters to achieve better performance on particular\ndatasets. When we use a machine learning package to choose the best hyperparmeters,\nthe relationship between changing the hyperparameter and performance might not\nbe obvious. [mlr](http://github.com/mlr-org/mlr) provides several new\nimplementations to better understand what happens when we tune hyperparameters\nand to help us optimize our choice of hyperparameters.\n\n# Background\n\nLet's say you have a dataset, and you're getting ready to flex your machine\nlearning muscles. Maybe you want to do classification, or regression, or\nclustering. You get your dataset together and pick a few learners to evaluate.\n\nThe majority of learners that you might use for any of these tasks\nhave hyperparameters that the user must tune. Hyperparameters may be able to take\non a lot of possible values, so it's typically left to the user to specify the\nvalues. If you're using a popular machine learning library like [sci-kit learn](http://scikit-learn.org/),\nthe library will take care of this for you via cross-validation: auto-generating\nthe optimal values for your hyperparameters. We'll then take these best-performing\nhyperparameters and use those values for our learner. Essentially, we treat the\noptimization of hyperparameters as a black box.\n\nIn [mlr](http://github.com/mlr-org/mlr), we want to open up that black box, so\nthat you can make better decisions. Using the functionality built-in, we can\nanswer questions like:\n\n- How does varying the value of a hyperparameter change the performance of the machine learning algorithm?\n- On a related note: where's an ideal range to search for optimal hyperparameters?\n- How did the optimization algorithm (prematurely) converge?\n- What's the relative importance of each hyperparameter?\n\nSome of the users who might see benefit from \"opening\" the black box of hyperparameter\noptimization:\n\n- researchers that want to better understand learners in practice\n- engineers that want to maximize performance or minimize run time\n- teachers that want to demonstrate what happens when tuning hyperparameters\n\nWe'll use [Pima Indians](https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes) dataset in this blog post, where we want to\npredict whether or not someone has diabetes, so we'll perform classification,\nbut the methods we discuss also work for regression and clustering.\n\nPerhaps we decide we want to try [kernlab's svm](http://www.rdocumentation.org/packages/kernlab/versions/0.9-24) for our\nclassification task. Knowing that svm has several hyperparameters to tune, we\ncan ask mlr to list the hyperparameters to refresh our memory:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr)\nlibrary(ggplot2)\n# to make sure our results are replicable we set the seed\nset.seed(7)\ngetParamSet(\"classif.ksvm\")\n##                        Type  len    Def\n## scaled              logical    -   TRUE\n## type               discrete    -  C-svc\n## kernel             discrete    - rbfdot\n## C                   numeric    -      1\n## nu                  numeric    -    0.2\n## epsilon             numeric    -    0.1\n## sigma               numeric    -      -\n## degree              integer    -      3\n## scale               numeric    -      1\n## offset              numeric    -      1\n## order               integer    -      1\n## tol                 numeric    -  0.001\n## shrinking           logical    -   TRUE\n## class.weights numericvector <NA>      -\n## fit                 logical    -   TRUE\n## cache               integer    -     40\n##                                                 Constr Req Tunable Trafo\n## scaled                                               -   -    TRUE     -\n## type              C-svc,nu-svc,C-bsvc,spoc-svc,kbb-svc   -    TRUE     -\n## kernel        vanilladot,polydot,rbfdot,tanhdot,lap...   -    TRUE     -\n## C                                             0 to Inf   Y    TRUE     -\n## nu                                            0 to Inf   Y    TRUE     -\n## epsilon                                    -Inf to Inf   Y    TRUE     -\n## sigma                                         0 to Inf   Y    TRUE     -\n## degree                                        1 to Inf   Y    TRUE     -\n## scale                                         0 to Inf   Y    TRUE     -\n## offset                                     -Inf to Inf   Y    TRUE     -\n## order                                      -Inf to Inf   Y    TRUE     -\n## tol                                           0 to Inf   -    TRUE     -\n## shrinking                                            -   -    TRUE     -\n## class.weights                                 0 to Inf   -    TRUE     -\n## fit                                                  -   -   FALSE     -\n## cache                                         1 to Inf   -    TRUE     -\n```\n:::\n\n\nNoting that we have default values for each of the hyperparameters, we could\nsimply accept the defaults for each of the hyperparameters and evaluate our\n`mmce` performance using 3-fold cross validation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrdesc = makeResampleDesc(\"CV\", iters = 3)\nr = resample(\"classif.ksvm\", pid.task, rdesc)\nprint(r)\n## Resample Result\n## Task: PimaIndiansDiabetes-example\n## Learner: classif.ksvm\n## Aggr perf: mmce.test.mean=0.2434896\n## Runtime: 0.875401\n```\n:::\n\n\nWhile this result may seem decent, we have a nagging doubt: what if we chose\nhyperparameter values different from the defaults? Would we get better results?\n\nMaybe we believe that the default of `kernel = \"rbfdot\"` will work well based\non our prior knowledge of the dataset, but we want to try altering our\nregularization to get better performance. For [kernlab's svm](http://www.rdocumentation.org/packages/kernlab/versions/0.9-24), regularization\nis represented using the `C` hyperparameter. Calling `getParamSet` again to\nrefresh our memory, we see that `C` defaults to 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetParamSet(\"classif.ksvm\")\n##                        Type  len    Def\n## scaled              logical    -   TRUE\n## type               discrete    -  C-svc\n## kernel             discrete    - rbfdot\n## C                   numeric    -      1\n## nu                  numeric    -    0.2\n## epsilon             numeric    -    0.1\n## sigma               numeric    -      -\n## degree              integer    -      3\n## scale               numeric    -      1\n## offset              numeric    -      1\n## order               integer    -      1\n## tol                 numeric    -  0.001\n## shrinking           logical    -   TRUE\n## class.weights numericvector <NA>      -\n## fit                 logical    -   TRUE\n## cache               integer    -     40\n##                                                 Constr Req Tunable Trafo\n## scaled                                               -   -    TRUE     -\n## type              C-svc,nu-svc,C-bsvc,spoc-svc,kbb-svc   -    TRUE     -\n## kernel        vanilladot,polydot,rbfdot,tanhdot,lap...   -    TRUE     -\n## C                                             0 to Inf   Y    TRUE     -\n## nu                                            0 to Inf   Y    TRUE     -\n## epsilon                                    -Inf to Inf   Y    TRUE     -\n## sigma                                         0 to Inf   Y    TRUE     -\n## degree                                        1 to Inf   Y    TRUE     -\n## scale                                         0 to Inf   Y    TRUE     -\n## offset                                     -Inf to Inf   Y    TRUE     -\n## order                                      -Inf to Inf   Y    TRUE     -\n## tol                                           0 to Inf   -    TRUE     -\n## shrinking                                            -   -    TRUE     -\n## class.weights                                 0 to Inf   -    TRUE     -\n## fit                                                  -   -   FALSE     -\n## cache                                         1 to Inf   -    TRUE     -\n```\n:::\n\n\nLet's tell [mlr](http://github.com/mlr-org/mlr) to randomly pick `C` values\nbetween `2^-5` and `2^5`, evaluating `mmce` using 3-fold cross validation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create the C parameter in continuous space: 2^-5 : 2^5\nps = makeParamSet(\n  makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x)\n)\n# random search in the space with 100 iterations\nctrl = makeTuneControlRandom(maxit = 100L)\n# 3-fold CV\nrdesc = makeResampleDesc(\"CV\", iters = 2L)\n# run the hyperparameter tuning process\nres = tuneParams(\"classif.ksvm\", task = pid.task, control = ctrl,\n  resampling = rdesc, par.set = ps, show.info = FALSE)\nprint(res)\n## Tune result:\n## Op. pars: C=0.506\n## mmce.test.mean=0.2213542\n```\n:::\n\n\n[mlr](http://github.com/mlr-org/mlr) gives us the best performing value for `C`,\nand we can see that we've improved our results vs. just accepting the default\nvalue for `C`. This functionality is available in other machine learning packages, like\nsci-kit learn's [random search](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.RandomizedSearchCV.html), but this functionality is essentially treating our\nchoice of `C` as a black box method: we give a search strategy and just accept\nthe optimal value. What if we wanted to get a sense of the relationship between\n`C` and `mmce`? Maybe the relationship is linear in a certain range and we can\nexploit this to get better even performance! [mlr](http://github.com/mlr-org/mlr)\nprovides 2 methods to help answer this question: `generateHyperParsEffectData` to\ngenerate the resulting data and `plotHyperParsEffect` providing many options\nbuilt-in for the user to plot the data.\n\nLet's investigate the results from before where we tuned `C`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata = generateHyperParsEffectData(res)\nplotHyperParsEffect(data, x = \"C\", y = \"mmce.test.mean\")\n```\n\n::: {.cell-output-display}\n![](2016-08-21-exploring-and-understanding-hyperparameter-tuning_files/figure-html/first_chart-1.png){width=672}\n:::\n:::\n\n\nFrom the scatterplot, it appears our optimal performance is somewhere in the\nregion between `2^-2.5` and `2^-1.75`. This could provide us a region to further\nexplore if we wanted to try to get even better performance!\n\nWe could also evaluate how \"long\" it takes us to find that optimal value:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotHyperParsEffect(data, x = \"iteration\", y = \"mmce.test.mean\")\n```\n\n::: {.cell-output-display}\n![](2016-08-21-exploring-and-understanding-hyperparameter-tuning_files/figure-html/second_chart-1.png){width=672}\n:::\n:::\n\n\nBy default, the plot only shows the global optimum, so we can see that we found\nthe \"best\" performance in less than 25 iterations!\n\nBut wait, I hear you saying. I also want to tune `sigma`, the inverse kernel\nwidth of the radial basis kernel function. So now we have 2 hyperparameters that\nwe want to simultaneously tune: `C` and `sigma`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create the C and sigma parameter in continuous space: 2^-5 : 2^5\nps = makeParamSet(\n  makeNumericParam(\"C\", lower = -5, upper = 5, trafo = function(x) 2^x),\n  makeNumericParam(\"sigma\", lower = -5, upper = 5, trafo = function(x) 2^x)\n)\n# random search in the space with 100 iterations\nctrl = makeTuneControlRandom(maxit = 100L)\n# 3-fold CV\nrdesc = makeResampleDesc(\"CV\", iters = 2L)\n# run the hyperparameter tuning process\nres = tuneParams(\"classif.ksvm\", task = pid.task, control = ctrl,\n  resampling = rdesc, par.set = ps, show.info = FALSE)\nprint(res)\n## Tune result:\n## Op. pars: C=0.709; sigma=0.068\n## mmce.test.mean=0.2330729\n# collect the hyperparameter data\ndata = generateHyperParsEffectData(res)\n```\n:::\n\n\nWe can use `plotHyperParsEffect` to easily create a heatmap with both hyperparameters.\nWe get tons of functionality for free here. For example, [mlr](http://github.com/mlr-org/mlr)\nwill automatically interpolate the grid to get an estimate for values we didn't\neven test! All we need to do is pass a regression learner to the `interpolate`\nargument:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotHyperParsEffect(data, x = \"C\", y = \"sigma\", z = \"mmce.test.mean\",\n  plot.type = \"heatmap\", interpolate = \"regr.earth\")\n```\n\n::: {.cell-output-display}\n![](2016-08-21-exploring-and-understanding-hyperparameter-tuning_files/figure-html/third_chart-1.png){width=672}\n:::\n:::\n\n\nIf we use the `show.experiments` argument, we can see which points were\nactually tested and which were interpolated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotHyperParsEffect(data, x = \"C\", y = \"sigma\", z = \"mmce.test.mean\",\n  plot.type = \"heatmap\", interpolate = \"regr.earth\",\n  show.experiments = TRUE)\n```\n\n::: {.cell-output-display}\n![](2016-08-21-exploring-and-understanding-hyperparameter-tuning_files/figure-html/fourth_chart-1.png){width=672}\n:::\n:::\n\n\n`plotHyperParsEffect` returns a `ggplot2` object, so we can always customize it\nto better fit our needs downstream:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplt = plotHyperParsEffect(data, x = \"C\", y = \"sigma\", z = \"mmce.test.mean\",\n  plot.type = \"heatmap\", interpolate = \"regr.earth\",\n  show.experiments = TRUE)\nmin_plt = min(plt$data$mmce.test.mean, na.rm = TRUE)\nmax_plt = max(plt$data$mmce.test.mean, na.rm = TRUE)\nmean_plt = mean(c(min_plt, max_plt))\nplt + scale_fill_gradient2(breaks = seq(min_plt, max_plt, length.out = 4),\n  low = \"red\", mid = \"white\", high = \"blue\", midpoint = mean_plt)\n```\n\n::: {.cell-output-display}\n![](2016-08-21-exploring-and-understanding-hyperparameter-tuning_files/figure-html/fifth_chart-1.png){width=672}\n:::\n:::\n\n\nNow we can get a good sense of where the separation happens for each of the\nhyperparameters: in this particular example, we want lower values for `sigma`\nand values around 1 for `C`.\n\nThis was just a taste of mlr's hyperparameter tuning visualization capabilities. For the full tutorial, check out the [mlr tutorial](https://mlr.mlr-org.com/articles/tutorial/devel/hyperpar_tuning_effects.html).\n\nSome features coming soon:\n\n- \"Prettier\" plot defaults\n- Support for more than 2 hyperparameters\n- Direct support for hyperparameter \"importance\"\n\nThanks to the generous sponsorship from [GSoC](https://summerofcode.withgoogle.com/), and many thanks to my mentors Bernd Bischl and Lars Kotthoff!\n",
    "supporting": [
      "2016-08-21-exploring-and-understanding-hyperparameter-tuning_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}