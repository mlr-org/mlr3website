{
  "hash": "69c842ef1bcba53245a493430ab9f647",
  "result": {
    "markdown": "---\ntitle: \"Stepwise Bayesian Optimization with mlrMBO\"\nauthors: [\"Jakob Richter\"]\ndate: 2018-01-10\ncategories: [\"R\", \"r-bloggers\"]\ntags: [\"mlrMBO\", \"Bayesian\", \"optimization\", \"tuning\", \"stepwise\"]\ndescription: \"Tutorial on stepwise Bayesian Optimization with mlrMBO\"\n\n---\n\n\n\n\nWith the release of the new version of [mlrMBO](https://mlrMBO.mlr-org.com/) we added some minor fixes and added a practical feature called *[Human-in-the-loop MBO](https://mlrMBO.mlr-org.com/articles/supplementary/human_in_the_loop_MBO.html)*.\nIt enables you to sequentially\n\n* visualize the state of the surrogate model,\n* obtain the suggested parameter configuration for the next iteration and\n* update the surrogate model with arbitrary evaluations.\n\nIn the following we will demonstrate this feature on a simple example.\n\nFirst we need an objective function we want to optimize.\nFor this post a simple function will suffice but note that this function could also be an external process as in this mode **mlrMBO** does not need to access the objective function as you will only have to pass the results of the function to **mlrMBO**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlrMBO)\nlibrary(ggplot2)\nset.seed(1)\n\nfun = function(x) {\n  x^2 + sin(2 * pi * x) * cos(0.3 * pi * x)\n}\n```\n:::\n\n\nHowever we still need to define the our search space.\nIn this case we look for a real valued value between -3 and 3.\nFor more hints about how to define ParamSets you can look [here](http://jakob-r.de/mlrHyperopt/articles/working_with_parconfigs_and_paramsets.html#the-basics-of-a-paramset) or in the [help of ParamHelpers](https://rdrr.io/cran/ParamHelpers/man/makeParamSet.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nps = makeParamSet(\n  makeNumericParam(\"x\", lower = -3, upper = 3)\n)\n```\n:::\n\n\nWe also need some initial evaluations to start the optimization.\nThe design has to be passed as a `data.frame` with one column for each dimension of the search space and one column `y` for the outcomes of the objective function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndes = generateDesign(n = 3, par.set = ps)\ndes$y = apply(des, 1, fun)\ndes\n##            x         y\n## 1 -1.1835844 0.9988801\n## 2 -0.5966361 0.8386779\n## 3  2.7967794 8.6592973\n```\n:::\n\n\nWith these values we can initialize our sequential MBO object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl = makeMBOControl()\nctrl = setMBOControlInfill(ctrl, crit = crit.ei)\nopt.state = initSMBO(\n  par.set = ps,\n  design = des,\n  control = ctrl,\n  minimize = TRUE,\n  noisy = FALSE)\n```\n:::\n\n\nThe `opt.state` now contains all necessary information for the optimization.\nWe can even plot it to see how the Gaussian process models the objective function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(opt.state)\n```\n\n::: {.cell-output-display}\n![](2018-01-10-stepwise-bayesian-optimization-with-mlrmbo_files/figure-html/optstate1-1.png){width=672}\n:::\n:::\n\n\nIn the first panel the *expected improvement* ($EI = E(y_{min}-\\hat{y})$) (see [Jones et.al.](http://www.ressources-actuarielles.net/EXT/ISFA/1226.nsf/0/f84f7ac703bf5862c12576d8002f5259/$FILE/Jones98.pdf)) is plotted over the search space.\nThe maximum of the *EI* indicates the point that we should evaluate next.\nThe second panel shows the mean prediction of the surrogate model, which is the Gaussian regression model aka *Kriging* in this example.\nThe third panel shows the uncertainty prediction of the surrogate.\nWe can see, that the *EI* is high at points, where the mean prediction is low and/or the uncertainty is high.\n\nTo obtain the specific configuration suggested by mlrMBO for the next evaluation of the objective we can run:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprop = proposePoints(opt.state)\nprop\n## $prop.points\n##             x\n## 798 -2.999998\n## \n## $propose.time\n## [1] 0.209\n## \n## $prop.type\n## [1] \"infill_ei\"\n## \n## $crit.vals\n##            [,1]\n## [1,] -0.3733869\n## \n## $crit.components\n##         se     mean\n## 1 2.889951 3.031346\n## \n## $errors.model\n## [1] NA\n## \n## attr(,\"class\")\n## [1] \"Proposal\" \"list\"\n```\n:::\n\n\nWe will execute our objective function with the suggested value for `x` and feed it back to mlrMBO:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny = fun(prop$prop.points$x)\ny\n## [1] 8.999972\nupdateSMBO(opt.state, x = prop$prop.points, y = y)\n```\n:::\n\n\nThe nice thing about the *human-in-the-loop* mode is, that you don't have to stick to the suggestion.\nIn other words we can feed the model with values without receiving a proposal.\nLet's assume we have an expert who tells us to evaluate the values $x=-1$ and $x=1$ we can easily do so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncustom.prop = data.frame(x = c(-1,1))\nys = apply(custom.prop, 1, fun)\nupdateSMBO(opt.state, x = custom.prop, y = as.list(ys))\nplot(opt.state, scale.panels = TRUE)\n## Warning: Removed 1 rows containing missing values (geom_point).\n```\n\n::: {.cell-output-display}\n![](2018-01-10-stepwise-bayesian-optimization-with-mlrmbo_files/figure-html/feedmanual-1.png){width=672}\n:::\n:::\n\n\nWe can also automate the process easily:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreplicate(3, {\n  prop = proposePoints(opt.state)\n  y = fun(prop$prop.points$x)\n  updateSMBO(opt.state, x = prop$prop.points, y = y)\n})\n```\n:::\n\n\n*Note:* We suggest to use the normal mlrMBO if you are only doing this as mlrMBO has more advanced logging, termination and handling of errors etc.\n\nLet's see how the surrogate models the true objective function after having seen seven configurations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(opt.state, scale.panels = TRUE)\n## Warning: Removed 1 rows containing missing values (geom_point).\n```\n\n::: {.cell-output-display}\n![](2018-01-10-stepwise-bayesian-optimization-with-mlrmbo_files/figure-html/optstate2-1.png){width=672}\n:::\n:::\n\n\nYou can convert the `opt.state` object from this run to a normal mlrMBO result object like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres = finalizeSMBO(opt.state)\nres\n## Recommended parameters:\n## x=-0.22\n## Objective: y = -0.913\n## \n## Optimization path\n## 3 + 6 entries in total, displaying last 10 (or less):\n##            x          y dob eol error.message exec.time         ei error.model\n## 1 -1.1835844  0.9988801   0  NA          <NA>        NA         NA        <NA>\n## 2 -0.5966361  0.8386779   0  NA          <NA>        NA         NA        <NA>\n## 3  2.7967794  8.6592973   0  NA          <NA>        NA         NA        <NA>\n## 4 -2.9999977  8.9999724   1  NA          <NA>        NA -0.3733869        <NA>\n## 5 -1.0000000  1.0000000   2  NA          <NA>        NA -0.3136183        <NA>\n## 6  1.0000000  1.0000000   2  NA          <NA>        NA -0.1366681        <NA>\n## 7  0.3009402  1.0018545   3  NA          <NA>        NA -0.7751006        <NA>\n## 8 -0.2198973 -0.9127938   4  NA          <NA>        NA -0.1568373        <NA>\n## 9 -2.2260548  5.4527615   5  NA          <NA>        NA -0.2965027        <NA>\n##   train.time  prop.type propose.time        se      mean\n## 1         NA initdesign           NA        NA        NA\n## 2         NA initdesign           NA        NA        NA\n## 3         NA initdesign           NA        NA        NA\n## 4          0     manual           NA 2.8899512 3.0313463\n## 5          0     manual           NA 0.5709681 0.6836897\n## 6         NA       <NA>           NA 3.3578427 5.3792305\n## 7          0     manual           NA 1.2338757 0.3493772\n## 8          0     manual           NA 0.4512026 0.8870804\n## 9          0     manual           NA 3.5929994 2.6982361\n```\n:::\n\n\n*Note:* You can always run the *human-in-the-loop MBO* on `res$final.opt.state`.\n\nFor the curious, let's see how our original function actually looks like and which points we evaluated during our optimization:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fun, -3, 3)\npoints(x = getOptPathX(res$opt.path)$x, y = getOptPathY(res$opt.path))\n```\n\n::: {.cell-output-display}\n![](2018-01-10-stepwise-bayesian-optimization-with-mlrmbo_files/figure-html/plottrue-1.png){width=672}\n:::\n:::\n\n\nWe can see, that we got pretty close to the global optimum and that the surrogate in the previous plot models the objective quite accurate.\n\nFor more in-depth information look at the [Vignette for Human-in-the-loop MBO](https://mlrMBO.mlr-org.com/articles/supplementary/human_in_the_loop_MBO.html) and check out the other topics of our [mlrMBO page](https://mlrMBO.mlr-org.com/).\n",
    "supporting": [
      "2018-01-10-stepwise-bayesian-optimization-with-mlrmbo_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}