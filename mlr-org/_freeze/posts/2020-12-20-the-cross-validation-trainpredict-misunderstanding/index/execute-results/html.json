{
  "hash": "58dc8704fcac6c7f6b871f914f0523ba",
  "result": {
    "markdown": "---\ntitle: \"The Cross-Validation - Train/Predict Misunderstanding\"\ndescription: |\n  Over the past years I've seen multiple posts on Stackoverflow and our GitHub issues which suffer from a conceptual misunderstanding: cross-validation (CV) vs. train/predict.\n  Cover photo by [Brett Jordan](https://unsplash.com/es/@brett_jordan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).\nauthor:\n  - name: Patrick Schratz\n    url: https://github.com/pat-s\ndate: 2020-12-20\ncategories:\n    - R\nimage: preview.jpg\n---\n\n\n\n\n## Introduction\n\nOver the past years I've seen multiple posts on Stackoverflow and our GitHub issues which suffer from a conceptual misunderstanding: cross-validation (CV) vs. train/predict.\n\nBecause train/predict is an essential part of cross-validation, the point might not be so obvious.\nI'll try to make it more clear by providing some exemplary questions:\n\n- \"I've done cross-validation. How do I decided which model to use for prediction?\"\n\n- \"I've used the `resample()` function. How do I decided which hyperparameters are best?\"\n\n- \"I've benchmarked some algorithms. How do I find the most important features?\"\n\nAll of these questions have a common problem: users trying to extract something out of the CV to work with it afterwards.\nThey want to use the \"best model\" or find the \"most important features\" or similar.\nThe thoughtful reader might already infer by now that doing so is probably problematic.\n\nAnd yes, it is.\nOne should not try to extract a single component (be it a model, a set of hyperparameters or a set of predictors) out of a CV.\n\n## What happens in a CV, stays in a CV\n\nHere's why:\n\nEvery model fit and evaluation in a CV happens on a subset of the main dataset.\n\n1. The dataset is split by a specific method (e.g. randomly) into pre-selected partitions (e.g. 5).\n1. (Optional) Optimal hyperparameters are searched and feature selection is performed (in another inner CV cycle)\n1. The model is fit on the training set and predicts on the test set\n1. The performance of this prediction is evaluated (because \"truth\" (= test set) is known)\n\nThis is done multiple times.\nEvery time, the dataset is different.\nEvery time, different hyperparameters are found.\nEvery time - ok you got it by now.\n\nNone of these training/test set combinations represent overall \"the best\" choice - they only operate in their specific data split setting.\nThere is also no way to find a model (or similar) within a CV with respect to these criteria.\n\nThe **main reason** for this is that in all cases the fitted model was trained on only a subset of the data available.\nThis was done to evaluate the performance on a subset of the data - because \"truth\" is known for the hold back data.\nOtherwise there would be no need to hide precious data from model fitting.\n\n## Train & Predict\n\nThe main purpose of fitting a model is make predictions with it.\nFor this, you want to use all available data to fit the most robust model possible.\nAnd this is exactly what you should do: take all your data, optimize your hyperparameters, eventually conduct feature selection, and then fit the model.\n\nYes, do it again - by using the `train()` and `predict()` functions (and their tuning wrappers/pipelines) directly.\nDo not use `resample()` or `benchmark()` - these are for CV purposes!\n\nThen, take this one model and predict into **unknown space**.\nIn this scenario, you cannot know how good your predictions will be because there is no \"truth\" to evaluate against.\nBut this is perfectly fine.\nThis is why a CV was done (beforehand): to have a somewhat unbiased estimate of how your model will perform under different conditions.\nYou can also analyse the hyperparameters or evaluate the results of a feature selection from this model[^1].\n\nBe careful though: your final model which you fit and predicted into unknown space might exactly have this performance - but it might also be completely off.\nYou will never find out unless you eventually collect data at some point which you can compare against the predictions made by your model.\nYou can only state that, with a given uncertainty, your model will have a performance of X when predicting.\n\n[^1]: While it might be ok to look at feature selections of single model fits, it is usually recommended to fit multiple models and look at the average variable important outcomes - e.g. by doing a permutation-based features selection.\n\n## Summary\n\nTo make if fully clear again: CV and train/predict are two separate things.\nThink of them as two different buckets with no relation to each other.\n\n- CV is done to get an estimate of a model's performance.\n- Train/predict is done to create the final predictions (which your boss might use to make some decisions on).\n\nCV is used to explain the performance of your fitted model (which is a single fit of the chosen algorithm on all of your data points).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}