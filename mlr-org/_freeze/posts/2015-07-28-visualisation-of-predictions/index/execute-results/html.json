{
  "hash": "212e6091c7bdc1f0b0b477e10e9aac2e",
  "result": {
    "markdown": "---\ntitle: \"Visualization of predictions\"\ndescription: |\n  Tutorial on the visualization of predictions using mlr.\nauthors: [\"Jakob Richter\"]\ndate: 2015-07-28\ncategories: [\"R\", \"r-bloggers\"]\ntags: [\"visualization\", \"prediction\", \"rstats\"]\n\n---\n\n\n\n\nIn this post I want to shortly introduce you to the great visualization possibilities of `mlr`.\nWithin the last months a lot of work has been put into that field.\nThis post is not a [tutorial](https://mlr.mlr-org.com/) but more a demonstration of how little code you have to write with `mlr` to get some nice plots showing the prediction behaviors for different learners.\n\n<!--more-->\n\nFirst we define a list containing all the [learners](https://mlr.mlr-org.com/articles/tutorial/devel/integrated_learners.html) we want to visualize.\nNotice that most of the `mlr` methods are able to work with just the string (i.e. `\"classif.svm\"`) to know what learner you mean.\nNevertheless you can define the learner more precisely with `makeLearner()` and set some parameters such as the `kernel` in this example.\n\nFirst we define the list of learners we want to visualize.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mlr)\nlearners = list(\n  makeLearner(\"classif.svm\", kernel = \"linear\"),\n  makeLearner(\"classif.svm\", kernel = \"polynomial\"),\n  makeLearner(\"classif.svm\", kernel = \"radial\"),\n  \"classif.qda\",\n  \"classif.randomForest\",\n  \"classif.knn\"\n  )\n```\n:::\n\n\n## Support Vector Machines\nNow lets have a look at the different results and lets start with the SVM with a *linear kernel*.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotLearnerPrediction(learner = learners[[1]], task = iris.task)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/linear-svm-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can see clearly that in fact the decision boundary is indeed linear.\nFurthermore the misclassified items are highlighted and a 10-fold cross validation to obtain the mean missclassification error is executed.\n\nFor the *polynomial* and the *radial kernel* the decision boundaries already look a bit more sophisticated:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotLearnerPrediction(learner = learners[[2]], task = iris.task)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/polynomial-radial-svm-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nplotLearnerPrediction(learner = learners[[3]], task = iris.task)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/polynomial-radial-svm-2.png){fig-align='center' width=672}\n:::\n:::\n\n\nNote that the intensity of the colors also indicates the certainty of the prediction and that this example is probably a rare case where the linear kernel performs best. although this is likely only the case because we didn't optimize the parameters for the radial kernel.\n\n## Quadratic Discriminant Analysis\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotLearnerPrediction(learner = learners[[4]], task = iris.task)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/qda-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nA well known classificator from the basic course of statistics delivers a similar performance as the SVMs.\n\n## Random Forest\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotLearnerPrediction(learner = learners[[5]], task = iris.task)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/randomforest-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nA completely different picture is generated by the random forest.\nHere you can see that the whole data set is used to generate the model and as a result it looks like it gives a perfect fit but obviously you wouldn't use the train data to evaluate your model.\nAnd the results of the 10-fold cross validation indicate that the random forest is actually not better then the others.\n\n## Nearest Neighbour\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotLearnerPrediction(learner = learners[[6]], task = iris.task)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/knn-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nIn the default setting knn just look for 'k=1' neighbor and as a result the classifier does not return probabilities but only the class labels.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}