{
  "hash": "db255d171486c4ad7d71834b229c55d5",
  "result": {
    "markdown": "---\ntitle: \"Parameter tuning with mlrHyperopt\"\nauthors: [\"Jakob Richter\"]\ndate: 2017-07-19\ncategories: [\"R\", \"r-bloggers\"]\ntags: [\"tuning\", \"hyperparameter\", \"optimization\", \"mlrHyperopt\", \"rstats\"]\ndescription: \"Tutorial on mlrHyperopt\"\n\n---\n\n\n\n\nHyperparameter tuning with [**mlr**](https://github.com/mlr-org/mlr#-machine-learning-in-r) is rich in options as they are multiple tuning methods:\n\n* Simple Random Search\n* Grid Search\n* Iterated F-Racing (via [**irace**](http://iridia.ulb.ac.be/irace/))\n* Sequential Model-Based Optimization (via [**mlrMBO**](https://mlrMBO.mlr-org.com/))\n\nAlso the search space is easily definable and customizable for each of the [60+ learners of mlr](https://mlr.mlr-org.com/articles/tutorial/devel/integrated_learners.html) using the ParamSets from the [**ParamHelpers**](https://github.com/berndbischl/ParamHelpers) Package.\n\nThe only drawback and shortcoming of **mlr** in comparison to [**caret**](http://topepo.github.io/caret/index.html) in this regard is that **mlr** itself does not have defaults for the search spaces.\nThis is where [**mlrHyperopt**](https://github.com/jakob-r/mlrHyperopt) comes into play.\n\n**mlrHyperopt** offers\n\n* default search spaces for the most important learners in **mlr**,\n* parameter tuning in one line of code,\n* and an API to add and access custom search spaces from the **mlrHyperopt Database**\n\n### Installation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# version >= 1.11 needed.\ndevtools::install_github(\"berndbischl/ParamHelpers\")\ndevtools::install_github(\"jakob-r/mlrHyperopt\", dependencies = TRUE)\n```\n:::\n\n\n### Tuning in one line\n\nTuning can be done in one line relying on the defaults.\nThe default will automatically minimize the _missclassification rate_.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlrHyperopt)\nres = hyperopt(iris.task, learner = \"classif.svm\")\nres\n## Tune result:\n## Op. pars: cost=23; gamma=0.0304\n## mmce.test.mean=0.0333333\n```\n:::\n\n\nWe can find out what `hyperopt` did by inspecting the `res` object.\n\nDepending on the parameter space **mlrHyperopt** will automatically decide for a suitable tuning method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres$opt.path$par.set\n##          Type len Def    Constr Req Tunable Trafo\n## cost  numeric   -   0 -15 to 15   -    TRUE     Y\n## gamma numeric   -  -2 -15 to 15   -    TRUE     Y\nres$control\n## Tune control: TuneControlMBO\n## Same resampling instance: TRUE\n## Imputation value: 1\n## Start: <NULL>\n## \n## Tune threshold: FALSE\n## Further arguments: list()\n```\n:::\n\n\nAs the search space defined in the ParamSet is only numeric, sequential Bayesian optimization was chosen.\nWe can look into the evaluated parameter configurations and we can visualize the optimization run.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntail(as.data.frame(res$opt.path))\n##         cost      gamma mmce.test.mean dob eol error.message exec.time\n## 20  4.523485  -5.041227     0.03333333  20  NA          <NA>     0.101\n## 21 14.998631 -13.678416     0.03333333  21  NA          <NA>     0.093\n## 22  4.047901  -4.921606     0.04000000  22  NA          <NA>     0.083\n## 23 10.492025  -8.308426     0.05333333  23  NA          <NA>     0.084\n## 24 12.620900 -11.388249     0.03333333  24  NA          <NA>     0.094\n## 25 14.999114 -12.467912     0.05333333  25  NA          <NA>     0.092\nplotOptPath(res$opt.path)\n## Warning: `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\n## `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\n## `guides(<scale> = FALSE)` is deprecated. Please use `guides(<scale> = \"none\")` instead.\n```\n\n::: {.cell-output-display}\n![](2017-07-19-parameter-tuning-with-mlrhyperopt_files/figure-html/resObjectOptPath-1.png){width=672}\n:::\n:::\n\n\nThe upper left plot shows the distribution of the tried settings in the search space and contour lines indicate where regions of good configurations are located.\nThe lower right plot shows the value of the objective (the miss-classification rate) and how it decreases over the time.\nThis also shows nicely that wrong settings can lead to bad results.\n\n### Using the mlrHyperopt API with mlr\n\nIf you just want to use **mlrHyperopt** to access the default parameter search spaces from the\nOften you don't want to rely on the default procedures of **mlrHyperopt** and just incorporate it into your **mlr**-workflow.\nHere is one example how you can use the default search spaces for an easy benchmark:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrns = c(\"classif.xgboost\", \"classif.nnet\")\nlrns = makeLearners(lrns)\ntsk = pid.task\nrr = makeResampleDesc('CV', stratify = TRUE, iters = 10)\nlrns.tuned = lapply(lrns, function(lrn) {\n  if (getLearnerName(lrn) == \"xgboost\") {\n    # for xgboost we download a custom ParConfig from the Database\n    pcs = downloadParConfigs(learner.name = getLearnerName(lrn))\n    pc = pcs[[1]]\n  } else {\n    pc = getDefaultParConfig(learner = lrn)\n  }\n  ps = getParConfigParSet(pc)\n  # some parameters are dependend on the data (eg. the number of columns)\n  ps = evaluateParamExpressions(ps,\n    dict = mlrHyperopt::getTaskDictionary(task = tsk))\n  lrn = setHyperPars(lrn, par.vals = getParConfigParVals(pc))\n  ctrl = makeTuneControlRandom(maxit = 20)\n  makeTuneWrapper(learner = lrn, resampling = rr, par.set = ps,\n                  control = ctrl)\n})\nres = benchmark(learners = c(lrns, lrns.tuned), tasks = tsk,\n                resamplings = cv10)\nplotBMRBoxplots(res)\n```\n:::\n\n\nAs we can see we were able to improve the performance of xgboost and the nnet without any additional knowledge on what parameters we should tune.\nEspecially for nnet improved performance is noticable.\n\n### Additional Information\n\nSome recommended additional reads\n\n* [Vignette](http://jakob-r.de/mlrHyperopt/articles/mlrHyperopt.html) on getting started and also how to contribute by uploading alternative or additional ParConfigs.\n* [How to work with ParamSets](http://jakob-r.de/mlrHyperopt/articles/working_with_parconfigs_and_paramsets.html#the-basics-of-a-paramset) as part of the [Vignette](http://jakob-r.de/mlrHyperopt/articles/working_with_parconfigs_and_paramsets.html).\n* The [slides of the useR 2017 Talk](https://github.com/jakob-r/mlrHyperopt/raw/master/meta/useR2017/beamer/jakob_richter_mlrHyperopt.pdf) on **mlrHyperopt**.\n",
    "supporting": [
      "2017-07-19-parameter-tuning-with-mlrhyperopt_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}