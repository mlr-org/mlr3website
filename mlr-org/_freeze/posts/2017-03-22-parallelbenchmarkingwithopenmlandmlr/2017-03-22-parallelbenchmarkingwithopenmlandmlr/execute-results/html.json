{
  "hash": "5e2e413b072ed010bc8ef611d42a2714",
  "result": {
    "markdown": "---\ntitle: \"Parallel benchmarking with OpenML and mlr\"\nauthors: [\"Heidi Seibold\"]\ndate: 2017-03-22\ncategories: [\"R\"]\ndraft: true\ntags: [\"OpenML\", \"benchmark\", \"parallelization\", \"rstats\"]\n\ndescription: \"Tutorial on parallel benchmarking with OpenML and mlr\"\n---\n\n\n\n\nWith this post I want to show you how to benchmark several learners (or learners with different parameter settings) using several data sets in a structured and parallelized fashion.\nFor this we want to use [`batchtools`](https://mllg.github.io/batchtools/).\n\nThe data that we will use here is stored on the open machine learning platform [openml.org](https://www.openml.org/) and we can download it together with information on what to do with it in form of a task.\n\nIf you have a small project and don't need to parallelize, you might want to just look at the previous blog post called [mlr loves OpenML](https://mlr-org.com/docs/2016-09-09-mlr-loves-openml/).\n\nThe following packages are needed for this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"OpenML\")\nlibrary(\"mlr\")\nlibrary(\"batchtools\")\nlibrary(\"ggplot2\")\n```\n:::\n\n\nNow we download five OpenML-tasks from OpenML:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2017)\n\n## get useful tasks\ntask_infos = listOMLTasks(tag = \"study_14\")\n\n## take a sample of 5 tasks from these\ntask_ids = sample(task_infos$task.id, size = 5)\ntasks = lapply(task_ids, getOMLTask)\n```\n:::\n\n\nIn a next step we need to create the so called registry.\nWhat this basically does is to create a folder with a certain subfolder structure.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## create the experiment registry\nreg = makeExperimentRegistry(\n  file.dir = \"parallel_benchmarking_blogpost\",\n  packages = c(\"mlr\", \"OpenML\", \"party\"),\n  seed = 123)\nnames(reg)\nreg$cluster.functions\n\n## allow for parallel computing, for other options see ?makeClusterFunctions\n# to save ressources, we just use 2 cores here\nreg$cluster.functions = makeClusterFunctionsMulticore(2)\n```\n:::\n\n\nNow you should have a new folder in your working directory with the name `parallel_benchmarking_blogpost` and the following subfolders / files:\n\n```{}\nparallel_benchmarking_blogpost/\n├── algorithms\n├── exports\n├── external\n├── jobs\n├── logs\n├── problems\n├── registry.rds\n├── results\n└── updates\n```\n\nIn the next step we get to the interesting point.\nWe need to define...\n\n- the **problems**, which in our case are simply the OpenML tasks we downloaded.\n- the **algorithm**, which with mlr and OpenML is quite simply achieved using `makeLearner` and `runTaskMlr`.\nWe do not have to save the run results (result of applying the learner to the task), but we can directly upload it to OpenML where the results are automatically evaluated.\n- the machine learning **experiment**, i.e. in our case which parameters do we want to set for which learner.\nAs an example here, we will look at the _ctree_ algorithm from the [_party_](https://cran.r-project.org/package=party) package and see whether Bonferroni correction (correction for multiple testing) helps getting better predictions and also we want to check whether we need a tree that has more than two leaf nodes (`stump = FALSE`) or if a small tree is enough (`stump = TRUE`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## add the problem, in our case the tasks from OpenML\nfor (task in tasks) {\n  addProblem(name = paste(\"omltask\", task$task.id, sep = \"_\"), data = task)\n}\n\n##' Function that takes the task (data) and the learner, runs the learner on\n##' the task, uploads the run and returns the run ID.\n##'\n##' @param job required argument for addAlgorithm\n##' @param instance required argument for addAlgorithm\n##' @param data the task\n##' @param learner the string that defines the learner, see listLearners()\nrunTask_uploadRun = function(job, instance, data, learner, ...) {\n\n  learner = makeLearner(learner, par.vals = list(...))\n  run = runTaskMlr(data, learner)\n\n  run_id = uploadOMLRun(run, tag = \"test\", confirm.upload = FALSE)\n  return(run_id)\n\n}\n\n## add the algorithm\naddAlgorithm(name = \"mlr\", fun = runTask_uploadRun)\n\n## what versions of the algorithm do we want to compute\nalgo.design = list(mlr = expand.grid(\n  learner = \"classif.ctree\",\n  testtype = c(\"Bonferroni\", \"Univariate\"),\n  stump = c(FALSE, TRUE),\n  stringsAsFactors = FALSE))\nalgo.design$mlr\n\naddExperiments(algo.designs = algo.design, repls = 1)\n\n## get an overview of what we will submit\nsummarizeExperiments()\n```\n:::\n\n\nNow we can simply run our experiment:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubmitJobs()\n```\n:::\n\n\nWhile your job is running, you can check the progress using `getStatus()`.\nAs soon as `getStatus()` tells us that all our runs are done, we can collect the results of our experiment from OpenML.\nTo be able to do this we need to collect the run IDs from the uploaded runs we did during the experiment.\nAlso we want to add the info of the parameters used (`getJobPars()`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults0 = reduceResultsDataTable()\njob.pars = getJobPars()\nresults = cbind(run.id = results0$V1, job.pars)\n```\n:::\n\n\nWith the run ID information we can now grab the evaluations from OpenML and plot for example the parameter settings against the predictive accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrun.evals0 = listOMLRunEvaluations(run.id = results$run.id)\nrun.evals = merge(results, run.evals0, by = \"run.id\")\n\nggplot(run.evals, aes(\n  x = interaction(testtype, stump),\n  y = predictive.accuracy,\n  group = data.name,\n  color = interaction(task.id, data.name))) +\n  geom_point() + geom_line()\n```\n:::\n\n\nWe see that the only data set where a stump is good enough is the pc1 data set.\nFor the madelon data set Bonferroni correction helps.\nFor the others it does not seem to matter.\nYou can check out the results online by going to the task websites (e.g. for task 9976 for the madelon data set go to [openml.org/t/9976](https://www.openml.org/t/9976)) or the run websites (e.g. [openml.org/r/1852889](https://www.openml.org/r/1852889)).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}