{
  "hash": "6aa60d6f34011243b72b09da7aa5cd91",
  "result": {
    "markdown": "---\ntitle: \"Multilabel Classification with mlr\"\nauthors: [\"Quay Au\"]\ndate: 2017-03-28\ncategories: [\"R\"]\ntags: [\"classification\", \"multilabel\", \"rstats\"]\ndescription: \"Introducting to multilabel classification in mlr\"\n\n---\n\n\n\n\nMultilabel classification has lately gained growing interest in the research community.\nWe implemented several methods, which make use of the standardized mlr framework. Every available binary learner can be used for multilabel problem transformation methods.\nSo if you're interested in using several multilabel algorithms and want to know how to use them in the mlr framework, then this post is for you!\n\n### 1) Introduction to multilabel classification\n\nFirst, let me introduce you to multilabel classification. This is a classification problem, where every instance can have more than one label. Let's have a look at a typical multilabel dataset (which I, of course, download from the [OpenML server](https://www.openml.org/search?q=2016_multilabel_r_benchmark_paper)):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mlr)\nlibrary(OpenML)\nsetOMLConfig(apikey = \"c1994bdb7ecb3c6f3c8f3b35f4b47f1f\") # api key\noml.id = listOMLDataSets(tag = \"2016_multilabel_r_benchmark_paper\")$data.id\nscene = getOMLDataSet(data.id = oml.id[8])\ntarget = scene$target.features\nfeats = setdiff(colnames(scene$data), target)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(scene$data[, c(feats[1], feats[2], target)])\n##       Att1     Att2 Beach Sunset FallFoliage Field Mountain Urban\n## 0 0.646467 0.666435  TRUE  FALSE       FALSE FALSE     TRUE FALSE\n## 1 0.770156 0.767255  TRUE  FALSE       FALSE FALSE    FALSE  TRUE\n## 2 0.793984 0.772096  TRUE  FALSE       FALSE FALSE    FALSE FALSE\n## 3 0.938563 0.949260  TRUE  FALSE       FALSE FALSE    FALSE FALSE\n## 4 0.512130 0.524684  TRUE  FALSE       FALSE FALSE    FALSE FALSE\n## 5 0.824623 0.886845  TRUE  FALSE       FALSE FALSE    FALSE FALSE\n```\n:::\n\n\nHere I took the [*scene*](http://www.sciencedirect.com/science/article/pii/S0031320304001074) dataset, where the features represent color information of pictures and the targets could be objects like *beach*, *sunset*, and so on.\n\nAs you can see above, one defining property of a multilabel dataset is, that the target variables (which are called *labels*) are binary. If you want to use your own data set, make sure to encode these variables in *logical*, where *TRUE* indicates the relevance of a label.\n\nThe basic idea behind many multilabel classification algorithms is to make use of possible correlation between labels. Maybe a learner is very good at predicting label 1, but rather bad at predicting label 2. If label 1 and label 2 are highly correlated, it may be beneficial to predict label 1 first and use this prediction as a feature for predicting label 2.\n\nThis approach is the main concept behind the so called *problem transformation methods*. The multilabel problem is transformed into binary classification problems, one for each label. Predicted labels are used as features for predicting other labels.\n\nWe implemented the following problem transformation methods:\n\n* Classifier chains\n* Nested stacking\n* Dependent binary relevance\n* Stacking\n\nHow these methods are defined, can be read in the [mlr tutorial](https://mlr.mlr-org.com/articles/tutorial/multilabel.html) or in more detail in our [paper](https://arxiv.org/pdf/1703.08991.pdf). Enough theory now, let's apply these methods on our dataset.\n\n### 2) Let's Train and Predict!\n\nFirst we need to create a multilabel task.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1729)\ntarget\n## [1] \"Beach\"       \"Sunset\"      \"FallFoliage\" \"Field\"       \"Mountain\"   \n## [6] \"Urban\"\nscene.task = makeMultilabelTask(data = scene$data, target = target)\n```\n:::\n\n\nWe set a seed, because the classifier chain wrapper uses a random chain order.\nNext, we train a learner. I chose the classifier chain approach together with a decision tree for the binary classification problems.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbinary.learner = makeLearner(\"classif.rpart\")\nlrncc = makeMultilabelClassifierChainsWrapper(binary.learner)\n```\n:::\n\n\nNow let's train and predict on our dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn = getTaskSize(scene.task)\ntrain.set = seq(1, n, by = 2)\ntest.set = seq(2, n, by = 2)\n\nscene.mod.cc = train(lrncc, scene.task, subset = train.set)\nscene.pred.cc = predict(scene.mod.cc, task = scene.task, subset = test.set)\n```\n:::\n\n\nWe also implemented common multilabel performance measures. Here is a list with [available multilabel performance measures](https://mlr.mlr-org.com/articles/tutorial/measures.html#multilabel-classification):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlistMeasures(\"multilabel\")\n##  [1] \"featperc\"            \"multilabel.tpr\"      \"multilabel.hamloss\" \n##  [4] \"multilabel.subset01\" \"timeboth\"            \"timetrain\"          \n##  [7] \"timepredict\"         \"multilabel.ppv\"      \"multilabel.f1\"      \n## [10] \"multilabel.acc\"\n```\n:::\n\n\nHere is how the classifier chains method performed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nperformance(scene.pred.cc, measures = list(multilabel.hamloss,\n  multilabel.subset01, multilabel.f1, multilabel.acc))\n##  multilabel.hamloss multilabel.subset01       multilabel.f1      multilabel.acc \n##           0.1318925           0.4887781           0.5785259           0.5613743\n```\n:::\n\n\n### 3) Comparison Binary Relevance vs. Classifier Chains\n\nNow let's see if it can be beneficial to use predicted labels as features for other labels. Let us compare the performance of the classifier chains method with the binary relevance method (this method does not use predicted labels as features).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlrnbr = makeMultilabelBinaryRelevanceWrapper(binary.learner)\n\nscene.mod.br = train(lrnbr, scene.task, subset = train.set)\nscene.pred.br = predict(scene.mod.br, task = scene.task, subset = test.set)\n\nperformance(scene.pred.br, measures = list(multilabel.hamloss,\n  multilabel.subset01, multilabel.f1, multilabel.acc))\n##  multilabel.hamloss multilabel.subset01       multilabel.f1      multilabel.acc \n##           0.1305071           0.5719036           0.5357163           0.5083818\n```\n:::\n\n\nAs can be seen here, it could indeed make sense to use more elaborate methods for multilabel classification, since classifier chains beat the binary relevance methods in all of these measures (Note, that hamming loss and subset01 are loss measures!).\n\n### 4) Resampling\n\nHere I'll show you how to use resampling methods in the multilabel setting. Resampling methods are key for assessing the performance of a learning algorithm. To read more about resampling, see the page on our [tutorial](https://mlr.mlr-org.com/articles/tutorial/devel/resample.html).\n\nFirst, we need to define a resampling strategy. I chose subsampling, which is also called Monte-Carlo cross-validation. The dataset is split into training and test set at a predefined ratio. The learner is trained on the training set, the performance is evaluated with the test set. This whole process is repeated many times and the performance values are averaged. In mlr this is done the following way:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrdesc = makeResampleDesc(\"Subsample\", iters = 10, split = 2/3)\n```\n:::\n\n\nNow we can choose a measure, which shall be resampled. All there is left to do is to run the resampling:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr = resample(lrncc, scene.task, rdesc, measures = multilabel.subset01)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nr\n## Resample Result\n## Task: scene$data\n## Learner: multilabel.classifierChains.classif.rpart\n## Aggr perf: multilabel.subset01.test.mean=0.4912827\n## Runtime: 13.3857\n```\n:::\n\n\nIf you followed the mlr tutorial or if you are already familiar with mlr, you most likely saw, that using resampling in the multilabel setting isn't any different than generally using resampling in mlr.\nMany methods, which are available in mlr, like [preprocessing](https://mlr.mlr-org.com/articles/tutorial/preproc.html), [tuning](https://mlr.mlr-org.com/articles/tutorial/tune.html) or [benchmark experiments](https://mlr.mlr-org.com/articles/tutorial/benchmark_experiments.html) can also be used for multilabel datasets and the good thing here is: the syntax stays the same!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}