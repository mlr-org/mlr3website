{
  "hash": "bba8fd14456c4892ef97955fdbc0db27",
  "result": {
    "markdown": "---\ntitle: \"Benchmarking mlr learners on OpenML\"\ndescription: |\n  Benchmarking mlr learners on the OpenML platform.\nauthor:\n  - name: Florian Pfisterer\n    url: https://github.com/pfistfl\ndate: 2016-08-11\n---\n\n\n\n\nThere are already some benchmarking studies about different classification algorithms out there. The probably most well known and\nmost extensive one is the\n[Do we Need Hundreds of Classifers to Solve Real World Classication Problems?](http://www.jmlr.org/papers/volume15/delgado14a/source/delgado14a.pdf)\npaper. They use different software and also different tuning processes to compare 179 learners on more than 121 datasets, mainly\nfrom the [UCI](https://archive.ics.uci.edu/ml/datasets.html) site. They exclude different datasets, because their dimension\n(number of observations or number of features) are too high, they are not in a proper format or because of other reasons.\nThere are also summarized some criticism about the representability of the datasets and the generability of benchmarking results.\nIt remains a bit unclear if their tuning process is done also on the test data or only on the training data (page 3154).\nThey reported the random forest algorithms to be the best one (in general) for multiclass classification datasets and\nthe support vector machine (svm) the second best one. On binary class classification tasks neural networks also perform\ncompetitively. They recommend the R library **caret** for choosing a classifier.\n\nOther benchmarking studies use much less datasets and are much less extensive (e.g. the\n[Caruana Paper](https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml06.pdf)). Computational power was also not the same\non these days.\n\nIn my first approach for benchmarking different learners I follow a more standardized approach, that can be easily\nredone in future when new learners or datasets are added to the analysis.\nI use the R package **OpenML** for getting access to OpenML datasets and the R package **mlr** (similar to caret, but more extensive) to have a standardized interface to machine learning algorithms in R.\nFurthermore the experiments are done with the help of the package [**batchtools**](https://github.com/mllg/batchtools),\nin order to parallelize the experiments (Installation via **devtools** package: devtools::install_github(\"mllg/batchtools\")).\n\nThe first step is to choose some datasets of the OpenML dataplatform. This is done in the [datasets.R](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/code/datasets.R)\nfile. I want to evaluate classification learners as well as regression learners, so I choose datasets for both tasks.\nThe choosing date was 28.01.2016 so probably nowadays there are more available. I applied several exclusion criteria:\n\n1. No datasets with missing values (this can be omitted in future with some imputation technique)\n2. Each dataset only once (although there exist several tasks for some)\n3. Exclusion of datasets that were obviously artificially created (e.g. some artificial datasets created by Friedman)\n4. Only datasets with number of observations and number of features smaller than 1000 (this is done to get a first fast analysis;\nbigger datasets are added later)\n5. In the classification case: only datasets where the target is a factor\n\nOf course this exclusion criteria change the representativeness of the datasets.\n\nThese exclusion criteria provide 184 classification datasets and 98 regression datasets.\n\nThe benchmark file on these datasets can be found\n[here](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/code/benchmark.R).\nFor the classification datasets all available classification learners in mlr, that\ncan handle multiclass problems, provide probability estimations and can handle factor features, are used. \"boosting\" of the\n**adabag** package is excluded, because it took too long on our test dataset.\n\nFor the regression datasets only regression learners that can handle factor features are included.\nThe learners \"btgp\", \"btgpllm\" and \"btlm\" are excluded, because their training time was too long.\n\nIn this preliminary study all learners are used with their default hyperparameter settings without tuning.\nThe evaluation technique is 10-fold crossvalidation, 10 times repeated and it is executed by the resample function\nin mlr. The folds are the same for all the learners. The evaluation measures are the accuracy, the balanced error rate,\nthe (multiclass) auc, the (multiclass) brier score and the logarithmic loss for the classification and\nthe mean square error, mean of absolute error, median of square error and median of absolute error. Additionally the\ntraining time is recorded.\n\nOn 12 cores it took me around 4 days for all datasets.\n\nI evaluate the results with help of the **data.table** package, which is good for handling big datasets and fast calculation\nof subset statistics. Graphics were produced with help of the **ggplot** package.\n\nFor comparison, the learners are ranked on each dataset. (see [benchmark_analysis.R](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/code/benchmark_analysis.R))\nThere are a few datasets where some of the learners provide errors.\nIn the first approach these were treated as having the worst performance and so all learners providing errors get the worst rank.\nIf there were several learners they get all the *averaged* worst rank.\n\n## Classification\n\nThe results in the classification case, regarding the accuracy are summarized in the following barplot graphic:\n\n![](1_best_algo_classif_with_na_rank.png)\n\nIt depicts the average rank regarding accuracy over all classification dataset of each learner.\n\nClearly the random forest implementations outperform the other. None of the three available packages is clearly better than the other. **svm**, **glmnet** and **cforest** follow. One could probably get better results for **svm** and **xgboost** and some other learners with proper tuning.\n\nThe results for the other measures are quite similar and can be seen [here](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/results/best_algo_classif_rank.pdf).\nIn the case of the brier score, **svm** gets the second place and in the logarithmic loss case even the first place. SVM seems to be better suited for these probability measures.\n\nRegarding training time, **kknn**, **randomForestSRCSyn**, **naiveBayes** and **lda** gets the best results.\n\nInstead of taking all datasets one could exclude datasets, where some of the learners got errors. The [results](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/results/best_algo_classif_rank.pdf) are quite similar.\n\n## Regression\n\nMore interestingly are probably the results of the regression tasks, as there is no available comprehensive regression benchmark study to the best of my knowledge.\n\nIf an algorithm provided an error it was ranked with the worst rank like in the classification case.\n\nThe results for the mean squared error can be seen here:\n\n![](1_best_algo_regr_with_na_rank.png)\n\nIt depicts the average rank regarding mean square error over all regression dataset of each learner.\n\nSurprisingly the **bartMachine** algorithm performs best! The standard random forest implementations are also all under the top 4.\n**cubist**, **glmnet** and **kknn** also perform very good. The standard linear model (**lm**) is \"unter ferner liefen\".\n\n[bartMachine](https://arxiv.org/pdf/1312.2171.pdf) and [cubist](https://cran.r-project.org/web/packages/Cubist/vignettes/cubist.pdf) are tree based methods combined with an ensembling method like random forest.\n\nOnce again, if tuning is performed, the ranking would change for algorithms like **svm** and **xgboost**.\n\nResults for the other measures can be seen [here](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/results/best_algo_regr_with_na_rank.pdf).\nThe average rank of **cubist** gets much better when regarding the mean of absolute error and even gots best, when regarding the median of squared error and median of absolute error. It seems to be a very robust method.\n\n**kknn** also gets better for the median of squared and absolute error. Regarding the training time it is once again the unbeaten number one. **randomForestSRCSyn** is also much faster than the other random forest implementations. **lm** is also under the best regarding training time.\n\nWhen omitting datasets where some of the learners produced errors, only 26 regression datasets remain. **bartMachine** remains best for the mean squared error. The results for the other learners change slightly. See [here](https://github.com/PhilippPro/benchmark-mlr-openml/blob/master/results/best_algo_regr_rank.pdf).\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}