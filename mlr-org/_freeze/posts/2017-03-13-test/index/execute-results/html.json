{
  "hash": "a2a5aa541ee52851c988c2e1fd7564c1",
  "result": {
    "markdown": "---\ntitle: \"First release of mlrMBO - the toolbox for Bayesian Block Box Optimization\"\ndescription: |\n  A short description of the post.\nauthor:\n  - name: Jakob Richter\ndate: 2017-03-13\n---\n\n\n\n\nWe are happy to finally announce the first release of [**mlrMBO** on cran](https://cran.r-project.org/package=mlrMBO) after a quite long development time.\nFor the theoretical background and a nearly complete overview of mlrMBOs capabilities you can check our [paper on **mlrMBO** that we presubmitted to arxiv](https://arxiv.org/abs/1703.03373).\n\nThe key features of **mlrMBO** are:\n\n* Global optimization of expensive Black-Box functions.\n* Multi-Criteria Optimization.\n* Parallelization through multi-point proposals.\n* Support for optimization over categorical variables using random forests as a surrogate.\n\nFor examples covering different scenarios we have Vignettes that are also available as an [online documentation](https://mlrMBO.mlr-org.com/).\nFor **mlr** users **mlrMBO** is especially interesting for hyperparameter optimization.\n\n<!--more-->\n\n**mlrMBO** for **mlr** hyperparameter tuning was already used in [an earlier blog post](/How-to-win-a-drone-in-20-lines-of-R-code).\nNonetheless we want to provide a small toy example to demonstrate the work flow of **mlrMBO** in this post.\n\n### Example\n\nFirst, we define an objective function that we are going to minimize:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nlibrary(mlrMBO)\n## Loading required package: mlr\n## Loading required package: ParamHelpers\n## Warning message: 'mlr' is in 'maintenance-only' mode since July 2019.\n## Future development will only happen in 'mlr3'\n## (<https://mlr3.mlr-org.com>). Due to the focus on 'mlr3' there might be\n## uncaught bugs meanwhile in {mlr} - please consider switching.\n## Loading required package: smoof\n## Loading required package: checkmate\n## Warning: no DISPLAY variable so Tk is not available\nfun = makeSingleObjectiveFunction(\n  name = \"SineMixture\",\n  fn = function(x) sin(x[1]) * cos(x[2]) / 2 + 0.04 * sum(x^2),\n  par.set = makeNumericParamSet(id = \"x\", len = 2, lower = -5, upper = 5)\n)\n```\n:::\n\n\nTo define the objective function we use `makeSingleObjectiveFunction` from the neat package [**smoof**](https://github.com/jakobbossek/smoof), which gives us the benefit amongst others to be able to directly visualize the function.\n_If you happen to be in need of functions to optimize and benchmark your optimization algorithm I recommend you to have a look at the package!_\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plot3D)\nplot3D(fun, contour = TRUE, lightning = TRUE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plotObjectiveFunction-1.png){width=672}\n:::\n:::\n\n\nLet's start with the configuration of the optimization:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# In this simple example we construct the control object with the defaults:\nctrl = makeMBOControl()\n# For this numeric optimization we are going to use the Expected\n# Improvement as infill criterion:\nctrl = setMBOControlInfill(ctrl, crit = crit.ei)\n# We will allow for exactly 25 evaluations of the objective function:\nctrl = setMBOControlTermination(ctrl, max.evals = 25L)\n```\n:::\n\n\nThe optimization has to so start with an initial design.\n**mlrMBO** can automatically create one but here we are going to use a randomly sampled LHS design of our own:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ndes = generateDesign(n = 8L, par.set = getParamSet(fun),\n  fun = lhs::randomLHS)\nautoplot(fun, render.levels = TRUE) + geom_point(data = des)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/design-1.png){width=672}\n:::\n:::\n\n\nThe points demonstrate how the initial design already covers the search space but is missing the area of the global minimum.\nBefore we can start the Bayesian optimization we have to set the surrogate learner to *Kriging*.\nTherefore we use an *mlr* regression learner.\nIn fact, with *mlrMBO* you can use any regression learner integrated in *mlr* as a surrogate allowing for many special optimization applications.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsur.lrn = makeLearner(\"regr.km\", predict.type = \"se\",\n  config = list(show.learner.output = FALSE))\n```\n:::\n\n\n_Note:_ **mlrMBO** can automatically determine a good surrogate learner based on the search space defined for the objective function.\nFor a purely numeric domain it would have chosen *Kriging* as well with some slight modifications to make it a bit more stable against numerical problems that can occur during optimization.\n\nFinally, we can start the optimization run:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres = mbo(fun = fun, design = des, learner = sur.lrn, control = ctrl,\n  show.info = TRUE)\n## Computing y column(s) for design. Not provided.\n## [mbo] 0: x=0.897,-2.51 : y = -0.0312 : 0.0 secs : initdesign\n## [mbo] 0: x=-4.52,-0.278 : y = 1.29 : 0.0 secs : initdesign\n## [mbo] 0: x=-2.58,-2.23 : y = 0.63 : 0.0 secs : initdesign\n## [mbo] 0: x=-1.69,1.41 : y = 0.112 : 0.0 secs : initdesign\n## [mbo] 0: x=4.08,4.23 : y = 1.57 : 0.0 secs : initdesign\n## [mbo] 0: x=1.27,-4.52 : y = 0.792 : 0.0 secs : initdesign\n## [mbo] 0: x=-0.163,0.425 : y = -0.0656 : 0.0 secs : initdesign\n## [mbo] 0: x=3.1,3.25 : y = 0.788 : 0.0 secs : initdesign\n## [mbo] 1: x=0.483,-1.18 : y = 0.153 : 0.0 secs : infill_ei\n## [mbo] 2: x=-0.0918,1.51 : y = 0.0885 : 0.0 secs : infill_ei\n## [mbo] 3: x=-0.856,0.593 : y = -0.27 : 0.0 secs : infill_ei\n## [mbo] 4: x=-1.05,-0.239 : y = -0.375 : 0.0 secs : infill_ei\n## [mbo] 5: x=-0.694,-2.25 : y = 0.423 : 0.0 secs : infill_ei\n## [mbo] 6: x=-1.34,0.00144 : y = -0.415 : 0.0 secs : infill_ei\n## [mbo] 7: x=2.3,-2.06 : y = 0.206 : 0.0 secs : infill_ei\n## [mbo] 8: x=-1.55,-0.343 : y = -0.37 : 0.0 secs : infill_ei\n## [mbo] 9: x=1.84,1.01 : y = 0.433 : 0.0 secs : infill_ei\n## [mbo] 10: x=-0.408,4.41 : y = 0.844 : 0.0 secs : infill_ei\n## [mbo] 11: x=5,-2.85 : y = 1.78 : 0.0 secs : infill_ei\n## [mbo] 12: x=-1.29,-0.0751 : y = -0.412 : 0.0 secs : infill_ei\n## [mbo] 13: x=-2.01,-0.0272 : y = -0.291 : 0.0 secs : infill_ei\n## [mbo] 14: x=-5,5 : y = 2.14 : 0.0 secs : infill_ei\n## [mbo] 15: x=-5,-5 : y = 2.14 : 0.0 secs : infill_ei\n## [mbo] 16: x=1.21,3.12 : y = -0.0186 : 0.0 secs : infill_ei\n## [mbo] 17: x=-1.28,0.0491 : y = -0.413 : 0.0 secs : infill_ei\nres$x\n## $x\n## [1] -1.342495094  0.001436926\nres$y\n## [1] -0.4149338\n```\n:::\n\n\nWe can see that we have found the global optimum of $y = -0.414964$ at $x = (-1.35265,0)$ quite sufficiently.\nLet's have a look at the points mlrMBO evaluated.\nTherefore we can use the `OptPath` which stores all information about all evaluations during the optimization run:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopdf = as.data.frame(res$opt.path)\nautoplot(fun, render.levels = TRUE, render.contours = FALSE) +\n  geom_text(data = opdf, aes(label = dob))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/mboPoints-1.png){width=672}\n:::\n:::\n\n\nIt is interesting to see, that for this run the algorithm first went to the local minimum on the top right in the 6th and 7th iteration but later, thanks to the explorative character of the _Expected Improvement_, found the real global minimum.\n\n### Comparison\n\nThat is all good, but how do other optimization strategies perform?\n\n#### Grid Search\n\nGrid search is seldom a good idea.\nBut especially for hyperparameter tuning it is still used.\nProbably because it kind of gives you the feeling that you know what is going on and have not left out any important area of the search space.\nIn reality the grid is usually so sparse that it leaves important areas untouched as you can see in this example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid.des = generateGridDesign(par.set = getParamSet(fun), resolution = 5)\ngrid.des$y = apply(grid.des, 1, fun)\ngrid.des[which.min(grid.des$y), ]\n##      x1 x2           y\n## 12 -2.5  0 -0.04923607\nautoplot(fun, render.levels = TRUE, render.contours = FALSE) +\n  geom_point(data = grid.des)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/gridSeach-1.png){width=672}\n:::\n:::\n\n\nIt is no surprise, that the grid search could not cover the search space well enough and we only reach a bad result.\n\n#### What about a simple random search?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrandom.des = generateRandomDesign(par.set = getParamSet(fun), n = 25L)\nrandom.des$y = apply(random.des, 1, fun)\nrandom.des[which.min(random.des$y), ]\n##          x1       x2           y\n## 20 1.609746 -2.43721 -0.03946573\nautoplot(fun, render.levels = TRUE, render.contours = FALSE) +\n  geom_point(data = random.des)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/randomSearch-1.png){width=672}\n:::\n:::\n\n\nWith the random search you could always be lucky but in average the optimum is not reached if smarter optimization strategies work well.\n\n#### A fair comarison\n\n... for stochastic optimization algorithms can only be achieved by repeating the runs.\n**mlrMBO** is stochastic as the initial design is generated randomly and the fit of the Kriging surrogate is also not deterministic.\nFurthermore we should include other optimization strategies like a genetic algorithm and direct competitors like `rBayesOpt`.\nAn extensive benchmark is available in [our **mlrMBO** paper](https://arxiv.org/abs/1703.03373).\nThe examples here are just meant to demonstrate the package.\n\n### Engage\n\nIf you want to contribute to [**mlrMBO**](https://github.com/mlr-org/mlrMBO) we ware always open to suggestions and pull requests on github.\nYou are also invited to fork the repository and build and extend your own optimizer based on our toolbox.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}