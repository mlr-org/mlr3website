{
  "hash": "fb7cb912d4741676383eaef89ed52839",
  "result": {
    "markdown": "---\ntitle: \"Being successful on Kaggle using mlr\"\nauthors: [\"Giuseppe Casalicchio\"]\ndate: 2017-03-09\ncategories: [\"R\", \"r-bloggers\"]\ntags: [\"kaggle\", \"mlr\", \"rstats\"]\ndescription: \"Tutorial on how to be successful on Kaggle using mlr\"\n\n---\n\n\n\n\nAchieving a good score on a Kaggle competition is typically quite difficult.\nThis blog post outlines 7 tips for beginners to improve their ranking on the Kaggle leaderboards.\nFor this purpose, I also created a [*Kernel*](https://www.kaggle.com/casalicchio/bike-sharing-demand/tuning-with-mlr)\nfor the [*Kaggle bike sharing competition*](https://www.kaggle.com/c/bike-sharing-demand)\nthat shows how the R package, `mlr`, can be used to tune a xgboost model with random search in parallel (using 16 cores). The R script scores rank 90 (of 3251) on the Kaggle leaderboard.\n\n## 7 Rules\n\n  1. Use good software\n  2. Understand the objective\n  3. Create and select features\n  4. Tune your model\n  5. Validate your model\n  6. Ensemble different models\n  7. Track your progress\n\n### 1. Use good software\n\nWhether you choose R, Python or another language to work on Kaggle, you will\nmost likely need to leverage quite a few packages to follow best practices in\nmachine learning. To save time, you should use 'software'\nthat offers a standardized and well-tested interface for the important steps\nin your workflow:\n\n  - Benchmarking different machine learning algorithms (learners)\n  - Optimizing hyperparameters of learners\n  - Feature selection, feature engineering and dealing with missing values\n  - Resampling methods for validation of learner performance\n  - Parallelizing the points above\n\nExamples of 'software' that implement the steps above and more:\n\n  - For python: scikit-learn (<http://scikit-learn.org/stable/auto_examples>).\n  - For R: `mlr` (<https://mlr.mlr-org.com/index.html>) or `caret`.\n\n\n### 2. Understand the objective\n\nTo develop a good understanding of the Kaggle challenge, you should:\n\n  - Understand the problem domain:\n    - Read the description and try to understand the aim of the competition.\n    - Keep reading the forum and looking into scripts/kernels of others, learn from them!\n    - Domain knowledge might help you (i.e., read publications about the topic, wikipedia is also ok).\n    - Use external data if allowed (e.g., google trends, historical weather data).\n\n  - Explore the dataset:\n    - Which features are numerical, categorical, ordinal or time dependent?\n    - Decide how to handle [*missing values*](https://mlr.mlr-org.com/articles/tutorial/impute.html). Some options:\n        - Impute missing values with the mean, median or with values that are out of range (for numerical features).\n        - Interpolate missing values if the feature is time dependent.\n        - Introduce a new category for the missing values or use the mode (for categorical features).\n    - Do exploratory data analysis (for the lazy: wait until someone else uploads an EDA kernel).\n    - Insights you learn here will inform the rest of your workflow (creating new features).\n\nMake sure you choose an approach that directly optimizes the measure of interest!\nExample:\n\n  - The **median** minimizes the mean absolute error **(MAE)** and\n  the **mean** minimizes the mean squared error **(MSE)**.\n  - By default, many regression algorithms predict the expected **mean** but there\n  are counterparts that predict the expected **median**\n  (e.g., linear regression vs. quantile regression).\n  <!-- - Some measures use a (log-)transformation of the target  -->\n  <!-- (e.g. the **RMSLE**, see [*bike sharing competition*](https://www.kaggle.com/c/bike-sharing-demand/details/evaluation)). \\newline -->\n  <!-- $\\rightarrow$ transform the target in the same way before modeling. -->\n  - For strange measures: Use algorithms where you can implement your own objective\n  function, see e.g.\n      - [*tuning parameters of a custom objective*](https://www.kaggle.com/casalicchio/allstate-claims-severity/tuning-the-parameter-of-a-custom-objective-1120) or\n      - [*customize loss function, and evaluation metric*](https://github.com/tqchen/xgboost/tree/master/demo#features-walkthrough).\n\n\n### 3. Create and select features:\n\nIn many kaggle competitions, finding a \"magic feature\" can dramatically increase your ranking.\nSometimes, better data beats better algorithms!\nYou should therefore try to introduce new features containing valuable information\n(which can't be found by the model) or remove noisy features (which can decrease model performance):\n\n  - Concat several columns\n  - Multiply/Add several numerical columns\n  - Count NAs per row\n  - Create dummy features from factor columns\n  -  For time series, you could try\n      - to add the weekday as new feature\n      - to use rolling mean or median of any other numerical feature\n      - to add features with a lag...\n  - Remove noisy features: [*Feature selection / filtering*](https://mlr.mlr-org.com/articles/tutorial/feature_selection.html)\n\n### 4. Tune your model\n\nTypically you can focus on a single model (e.g. [*xgboost*](https://xgboost.readthedocs.io/en/latest)) and tune its hyperparameters for optimal performance.\n\n  - Aim:\n  Find the best hyperparameters that, for the given data set, optimize the pre-defined performance measure.\n  - Problem:\n  Some models have many hyperparameters that can be tuned.\n  - Possible solutions:\n    - [*Grid search or random search*](https://mlr.mlr-org.com/articles/tutorial/devel/tune.html)\n    - Advanced procedures such as [*irace*](https://mlr.mlr-org.com/articles/tutorial/devel/advanced_tune.html)\n    or [*mbo (bayesian optimization)*](https://mlrMBO.mlr-org.com/articles/mlrMBO.html)\n\n### 5. Validate your model\n\nGood machine learning models not only work on the data they were trained on, but\nalso on unseen (test) data that was not used for training the model. When you use training data\nto make any kind of decision (like feature or model selection, hyperparameter tuning, ...),\nthe data becomes less valuable for generalization to unseen data. So if you just use the public\nleaderboard for testing, you might overfit to the public leaderboard and lose many ranks once the private\nleaderboard is revealed.\nA better approach is to use validation to get an estimate of performane on unseen data:\n\n  - First figure out how the Kaggle data was split into train and test data. Your resampling strategy should follow the same method if possible. So if kaggle uses, e.g. a feature for splitting the data, you should not use random samples for creating cross-validation folds.\n  - Set up a [*resampling procedure*](https://mlr.mlr-org.com/articles/tutorial/devel/resample.html), e.g., cross-validation (CV) to measure your model performance\n  - Improvements on your local CV score should also lead to improvements on the leaderboard.\n  - If this is not the case, you can try\n      - several CV folds (e.g., 3-fold, 5-fold, 8-fold)\n      - repeated CV (e.g., 3 times 3-fold, 3 times 5-fold)\n      - stratified CV\n  - `mlr` offers nice [*visualizations to benchmark*](https://mlr.mlr-org.com/articles/tutorial/devel/benchmark_experiments.html#benchmark-analysis-and-visualization) different algorithms.\n\n### 6. Ensemble **different** models (see, e.g. [*this guide*](http://mlwave.com/kaggle-ensembling-guide)):\n\nAfter training many different models, you might want to ensemble them into one strong model using one of these methods:\n\n  - simple averaging or voting\n  - finding optimal weights for averaging or voting\n  - stacking\n\n### 7. Track your progress\n\nA kaggle project might get quite messy very quickly, because you might try and prototype\nmany different ideas. To avoid getting lost, make sure to keep track of:\n\n  - What preprocessing steps were used to create the data\n  - What model was used for each step\n  - What values were predicted in the test file\n  - What local score did the model achieve\n  - What public score did the model achieve\n\nIf you do not want to use a tool like git, at least make sure you create subfolders\nfor each prototype. This way you can later analyse which models you might want to ensemble\nor use for your final commits for the competition.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}