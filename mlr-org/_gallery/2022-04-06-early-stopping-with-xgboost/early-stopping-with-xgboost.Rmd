---
title: "Early Stopping with XGBoost"
description: |
  We show how to use early stopping to reduce overfitting when training XGBoost models.
author:
  - name: Marc Becker
date: 2022-04-06
output:
  distill::distill_article:
    toc: true
    self_contained: false
    highlight_downlit: true
params:
  eval_all: FALSE
---

```{r early-stopping-with-xgboost-001, include=FALSE}
 knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 120)
)

library(mlr3website)

set.seed(7832)
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
```

# Scope

In this post, we use early stopping to reduce overfitting when training an `r ref("mlr_learners_classif.xgboost", "XGBoost model")`.
We start with a short recap on early stopping and overfitting.
After that, we use the early stopping mechanism of XGBoost and train a model on the `r ref("mlr_tasks_spam", "Spam Classification")` data set.
Finally, we tune the hyperparameters of an XGBoost model and use early stopping at once.

# Early Stopping

Early stopping is a technique used to reduce overfitting when fitting a model in an iterative process.
Overfitting occurs when a model fits increasingly to the training data but the performance on unseen data decreases.
When using early stopping, the performance is monitored on a validation set, and the training stops when performance decreases in a specific number of iterations.

# Resampling

We load the `r mlr_pkg("mlr3verse")` which provides all functions required for this example.

```{r early-stopping-with-xgboost-002, message=FALSE}
library(mlr3verse)
```

When training an XGBoost learner, we can use early stopping to find the optimal number of trees.
We have to define a separate early stopping set so that the learner monitors its performance while training.
Additionally, we need to define the range in which the performance must increase with `early_stopping_rounds` and the maximum number of boosting rounds with `nrounds`.
In this example, the training is stopped when the classification error is not decreasing for 100 rounds or 1000 rounds are reached.

```{r early-stopping-with-xgboost-003}
learner = lrn("classif.xgboost",
  nrounds = 1000,
  early_stopping_rounds = 100,
  eval_metric = "error"
)
```

Before we can train the learner, we have to define the early stopping set.
The `r ref("partition()")` function splits the observations of the task into two disjoint sets.
We use 80% of observations to train the model and the remaining 20% as the early stopping set.

```{r early-stopping-with-xgboost-004}
task = tsk("spam")
split = partition(task, ratio = 0.8)
task$set_row_roles(split$test, "early_stopping")
```

We train the learner with early stopping.

```{r early-stopping-with-xgboost-005}
learner$train(task)
```

The `$evaluation_log` of the model stores the performance scores on the training and early stopping set.
Figure 1 shows that the classification error on the training set decreases, whereas the error on the early stopping set increases after 20 rounds.

```{r early-stopping-with-xgboost-006, code_folding=TRUE, fig.cap="Comparison between train and early stopping set classification error."}
library(ggplot2)
library(data.table)

data = melt(
  learner$model$evaluation_log,
  id.vars = "iter",
  variable.name = "set",
  value.name = "error"
)

ggplot(data, aes(x = iter, y = error, group = set)) +
  geom_line(aes(color = set)) +
  geom_vline(aes(xintercept = learner$model$best_iteration), color = "grey") +
  scale_colour_manual(values=c("#f8766d", "#00b0f6"), labels = c("Train", "Early Stopping")) +
  labs(x = "Rounds", y = "Classification Error", color = "Set") +
  theme_minimal()
```

We can check the optimal number of trees by accessing `$best_iteration`.

```{r early-stopping-with-xgboost-007}
learner$model$best_iteration
```

# Tuning

Next, we want to tune the hyperparameters of an XGBoost learner and find the optimal number of trees in one go.
First, we load a predefined tuning space from the `r mlr_pkg("mlr3tuningspaces")` package.

```{r early-stopping-with-xgboost-010, results=FALSE}
tuning_space = lts("classif.xgboost.default")
as.data.table(tuning_space)
```

```{r early-stopping-with-xgboost-011, echo=FALSE}
table_responsive(as.data.table(tuning_space))
```

We copy the `r ref("TuningSpace", "tuning space")` to the parameter set of the learner and add the additional early stopping parameters.

```{r early-stopping-with-xgboost-012}
learner$param_set$values = tuning_space$values
learner$param_set$values = mlr3misc::insert_named(learner$param_set$values, list(
  nrounds = 1000,
  early_stopping_rounds = 100,
  eval_metric = "error"
))
```

The `r ref("AutoTuner")` automates the tuning of a learner.
It tunes the hyperparameters and fits a final model on the complete data set.
The early stopping set allows the use of early stopping in both steps.

```{r early-stopping-with-xgboost-013}
task = tsk("spam")
split = partition(task, ratio = 0.8)
task$set_row_roles(split$test, roles = "early_stopping")
```

The training of the `r ref("AutoTuner")` starts the tuning.
For each configuration, early stopping estimates the optimal number of trees.
After completion, the learner is trained on the best hyperparameter configuration, and early stopping is used one last time.

```{r early-stopping-with-xgboost-014, eval=params$eval_all}
at = auto_tuner(
  method = "random_search",
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 10
)

at$train(task)
```

```{r early-stopping-with-xgboost-016, include=FALSE, eval=params$eval_all}
saveRDS(at, "data/at.rda")
```

```{r early-stopping-with-xgboost-017, include=FALSE}
at = readRDS("data/at.rda")
```

The `r ref("AutoTuner")` can now be used to make predictions on new data
