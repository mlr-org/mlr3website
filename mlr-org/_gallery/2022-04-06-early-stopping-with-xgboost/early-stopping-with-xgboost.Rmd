---
title: "Early Stopping with XGBoost"
description: |
  We show how to use early stopping to reduce overfitting when training XGBoost models.
author:
  - name: Marc Becker
date: 2022-04-06
output:
  distill::distill_article:
    toc: true
    self_contained: false
    highlight_downlit: true
params:
  eval_all: FALSE
---

```{r early-stopping-with-xgboost-001, include=FALSE}
 knitr::opts_chunk$set(
  echo = TRUE,
  R.options = list(width = 120)
)

library(mlr3website)

set.seed(7832)
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
```

# Scope

In this post, we use early stopping to reduce overfitting when training an XGBoost model.
We start with a short recap on early stopping and overfitting.
After that, we use the early stopping mechanism of XGBoost and train a model on the spam data set.
Finally, we tune the hyperparameters of an XGBoost learner and use early stopping at the same time.

# Early Stopping

Early stopping is a technique used to reduce overfitting when fitting a model in an iterative process.
Overfitting occurs when a model fits increasingly to the training data but the performance on unseen data decreases.
When using early stopping, the performance of the model is monitored on a test set and the training is stopped when performance is decreasing in a specific number of iterations.

# Resampling

We load the `r mlr_pkg("mlr3verse")`  package which pulls in the most important packages for this example.

```{r early-stopping-with-xgboost-002, message=FALSE}
library(mlr3verse)
library(mlr3misc)
```

When training an XGBoost learner, we can use early stopping to find the optimal number of trees.
For this, we have to define a separate early stopping set so that the performance of the model is monitored while training.
Additionally, we need to define the range in which the performance must increase with `early_stopping_rounds` and the maximum number of boosting rounds with `nrounds`.
In this example, the training is stopped when the classification error is not decreasing for 100 rounds or 1000 rounds are reached.

```{r early-stopping-with-xgboost-003}
learner = lrn("classif.xgboost",
  nrounds = 1000,
  early_stopping_rounds = 100,
  eval_metric = "error"
)
```

Before we can train the learner, we have to define the early stopping set,
The `partition()` function splits the rows ids of the task into a training and test set.
We use 80% of observations to fit the model and the remaining 20% as the early stopping set.

```{r early-stopping-with-xgboost-004}
task = tsk("spam")
split = partition(task, ratio = 0.8)
task$set_row_roles(split$test, "early_stopping")
```

Finally, we can train the learner with early stopping.

```{r early-stopping-with-xgboost-005}
learner$train(task)
```

The performance scores on the train and early stopping set are stored in the `$evaluation_log` of the model.
Figure 1 shows that the classification error on the training set decreases continuously whereas the error on the early stopping set increases after 20 rounds.

```{r early-stopping-with-xgboost-006, code_folding=TRUE, fig.cap="Clasification Error"}
library(ggplot2)
library(data.table)

data = melt(
  learner$model$evaluation_log,
  id.vars = "iter",
  variable.name = "set",
  value.name = "error"
)

ggplot(data, aes(x = iter, y = error, group = set)) +
  geom_line(aes(color = set)) +
  geom_vline(aes(xintercept = learner$model$best_iteration), color = "grey") +
  scale_colour_manual(values=c("#f8766d", "#00b0f6"), labels = c("Train", "Early Stopping")) +
  labs(x = "Rounds", y = "Classification Error", color = "Set") +
  theme_minimal()
```

We can check the optimal number of trees by accessing `$best_iteration`.

```{r early-stopping-with-xgboost-007}
learner$model$best_iteration
```

# Tuning

Next we want to tune the hyperparameters of an XGBoost learner and find the optimal number of trees in one go.
First we load a predefined tuning space from the `r mlr_pkg("mlr3tuningspaces")` package.

```{r early-stopping-with-xgboost-010, result=FALSE}
tuning_space = lts("classif.xgboost.default")
tuning_space
```

```{r early-stopping-with-xgboost-011, echo=FALSE}
table_responsive(as.data.table(tuning_space))
```

We copy the tuning space to the parameter set of the learner and add the additional early stopping parameters.

```{r early-stopping-with-xgboost-012}
learner$param_set$values = tuning_space$values
learner$param_set$values = insert_named(learner$param_set$values, list(
  nrounds = 1000,
  early_stopping_rounds = 100,
  eval_metric = "error"
))
```

The `r ref("AutoTuner")` tunes the hyperparameters of a learner and fits a final model with the best hyperparameter configuration.
Early stopping is used in both tuning of the hyperparameters and training of the final model.
We the early stopping set.

```{r early-stopping-with-xgboost-013}
task = tsk("spam")
split = partition(task, ratio = 0.8)
task$set_row_roles(split$test, roles = "early_stopping")
```

The `r ref("auto_tuner()")` function constructs the `r ref("AutoTuner")`.

```{r early-stopping-with-xgboost-014}
at = auto_tuner(
  method = "random_search",
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  term_evals = 10,
  batch_size = 10
)
```

Finally, we run the `r ref("AutoTuner")`.

```{r early-stopping-with-xgboost-015, eval=params$eval_all}
at$train(task)
```

```{r early-stopping-with-xgboost-016, eval=params$eval_all}
saveRDS(at, "data/at.rda")
```

```{r early-stopping-with-xgboost-017, echo=FALSE}
at = readRDS("data/at.rda")
```

Lets have a look at the best hyperparameter configuration.

```{r early-stopping-with-xgboost-018, eval=FALSE}
at$tuning_instance$result
```

```{r early-stopping-with-xgboost-019, echo=FALSE, layout="l-page"}
table_responsive(at$tuning_instance$result[, c(at$tuning_instance$archive$cols_x, at$tuning_instance$archive$cols_y), with = FALSE])
```

The corresponding optimal number of trees is stored in the XGBoost model.

```{r early-stopping-with-xgboost-020}
at$model$learner$model$best_iteration
```
