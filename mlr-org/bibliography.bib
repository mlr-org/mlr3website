
@article{bischl_hyperparameter_2021,
	title = {Hyperparameter {Optimization}: {Foundations}, {Algorithms}, {Best} {Practices} and {Open} {Challenges}},
	shorttitle = {Hyperparameter {Optimization}},
	url = {http://arxiv.org/abs/2107.05847},
	abstract = {Most machine learning algorithms are configured by one or several hyperparameters that must be carefully chosen and often considerably impact performance. To avoid a time consuming and unreproducible manual trial-and-error process to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods, e.g., based on resampling error estimation for supervised machine learning, can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods such as grid or random search, evolutionary algorithms, Bayesian optimization, Hyperband and racing. It gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with ML pipelines, runtime improvements, and parallelization.},
	urldate = {2021-11-09},
	journal = {arXiv:2107.05847 [cs, stat]},
	author = {Bischl, Bernd and Binder, Martin and Lang, Michel and Pielok, Tobias and Richter, Jakob and Coors, Stefan and Thomas, Janek and Ullmann, Theresa and Becker, Marc and Boulesteix, Anne-Laure and Deng, Difan and Lindauer, Marius},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.05847},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Bischl et al. - 2021 - Hyperparameter Optimization Foundations, Algorith.pdf:/home/marc/Zotero/storage/DEAE5HJG/Bischl et al. - 2021 - Hyperparameter Optimization Foundations, Algorith.pdf:application/pdf;arXiv.org Snapshot:/home/marc/Zotero/storage/YMYEAGWY/2107.html:text/html},
}

@misc{li_2020,
      title={A System for Massively Parallel Hyperparameter Tuning},
      author={Liam Li and Kevin Jamieson and Afshin Rostamizadeh and Ekaterina Gonina and Moritz Hardt and Benjamin Recht and Ameet Talwalkar},
      year={2020},
      eprint={1810.05934},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1810.05934},
}

@misc{egele_2023,
      title={Asynchronous Decentralized Bayesian Optimization for Large Scale Hyperparameter Optimization},
      author={Romain Egele and Isabelle Guyon and Venkatram Vishwanath and Prasanna Balaprakash},
      year={2023},
      eprint={2207.00479},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.00479},
}

@article{bergstra_2012,
  author  = {James Bergstra and Yoshua Bengio},
  title   = {Random Search for Hyper-Parameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {10},
  pages   = {281--305},
  url     = {http://jmlr.org/papers/v13/bergstra12a.html}
}
