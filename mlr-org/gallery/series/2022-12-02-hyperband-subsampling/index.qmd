---
title: "Hyperband Series - Data Set Subsampling"
description: |
  Optimize the hyperparameters of a Support Vector Machine with Hyperband.
categories:
  - tuning
  - classification
author:
  - name: Marc Becker
    url: https://github.com/be-marc
date: 2022-12-02
bibliography: bibliography.bib
knitr:
  opts_chunk:
    R.options:
      datatable.print.nrows: 6
---

{{< include ../_setup.qmd >}}

```{r}
#| include: false
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

future::plan("multisession")
```

# Scope

We continue working with the *Hyperband* optimization algorithm [@li_2018].
The previous post used the number of boosting iterations of an XGBoost model as the resource.
However, Hyperband is not limited to machine learning algorithms that are trained iteratively.
The resource can also be the number of features, the training time of a model, or the size of the training data set.
In this post, we will tune a support vector machine and use the size of the training data set as the fidelity parameter.
The time to train a support vector machine increases with the size of the data set.
This makes the data set size a suitable fidelity parameter for Hyperband.

This is the second part of the Hyperband series.
The other parts can be found here:

* [Hyperband Series - Iterative Training](/gallery/series/2022-12-01-hyperband-xgboost)
* Hyperband Series - Large-Scale Experiments

If you don't know much about Hyperband, check out the first post which explains the algorithm in detail.
A little knowledge about `r ref_pkg("mlr3pipelines")` is beneficial but not necessary to understand the example.

# Hyperparameter Optimization

In this post, we will optimize the hyperparameters of the support vector machine on the `r ref("mlr_tasks_sonar", "Sonar")` data set.
We begin by constructing a classification machine by setting `type` to `"C-classification"`.
We truncate the id of the learner to `"svm"`.

```{r}
#| message: false
library("mlr3verse")

learner = lrn("classif.svm", id = "svm", type = "C-classification")
```

The `r ref_pkg("mlr3pipelines")` package features a `r ref("PipeOp")` for subsampling.

```{r}
po("subsample")
```

The `r ref("PipeOp")` controls the size of the training data set with the `frac` parameter.
We connect the `r ref("PipeOp")`  with the learner and get a `r ref("GraphLearner")`.

```{r}
graph_learner = as_learner(
  po("subsample") %>>%
  learner
)
```

The graph learner subsamples and then fits a support vector machine on the data subset.
The parameter set of the graph learner is a combination of the parameter set of the `r ref("PipeOp")` and learner.

```{r}
as.data.table(graph_learner$param_set)[, .(id, lower, upper, levels)]
```

Next, we create the search space.
We use `r ref("TuneToken")` to indicate that a hyperparameter should be tuned.
We have to prefix the hyperparameters with the id of the `r ref("PipeOp", "PipeOps")`.
The `subsample.frac` is the fidelity parameter that must be tagged with `"budget"` in the search space.
The data set size is increased from 12.5% to 100%.
For the other hyperparameters, we took the search space for support vector machines from the @kuehn_2018 article.
This search space works for a wide range of data sets.

```{r}
graph_learner$param_set$set_values(
  subsample.frac  = to_tune(p_dbl(3^-3, 1, tags = "budget")),
  svm.kernel      = to_tune(c("linear", "polynomial", "radial")),
  svm.cost        = to_tune(1e-4, 1e3, logscale = TRUE),
  svm.gamma       = to_tune(1e-4, 1e3, logscale = TRUE),
  svm.tolerance   = to_tune(1e-4, 2, logscale = TRUE),
  svm.degree      = to_tune(2, 5)
)
```

Let's create the tuning instance.
We use the `"none"` terminator because Hyperband controls the termination itself.

```{r}
instance = ti(
  task = tsk("sonar"),
  learner = graph_learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("classif.ce"),
  terminator = trm("none")
)
instance
```

We load the Hyperband `r ref("Tuner")` and set `eta = 2`.

```{r}
#| message: false
library("mlr3hyperband")

tuner = tnr("hyperband", eta = 3)
```

Using `eta = 2` and a lower bound of 12.5% for the data set size, results in the following schedule.
Configurations with the same data set size are evaluated in parallel.

```{r}
#| echo: false
library(htmltools)
library(reactable)

data = hyperband_schedule(r_min = 2^-3, r_max = 1, eta = 2)
setorder(data, cols = "budget")


reactable(data[, .(budget, bracket, stage, n)],
  columns = list(
    budget = colDef(
      name = "Data Set Size",
      cell = function(value) {
        width = paste0(value * 100, "%")
        value = format(width, width = 9, justify = "right")
        bar = div(
          class = "bar-chart",
          style = list(marginRight = "0.375rem"),
          div(class = "bar-chart__bar", style = list(width = width))
        )
        div(class = "bar-chart__bar-cell", span(class = "bar-chart__number", value), bar)
      }
    ),
    bracket = colDef(name = "Bracket"),
    stage = colDef(name = "Stage"),
    n = colDef(name = "# Configruations")
  ),
  pagination = FALSE
  )
```

Now we are ready to start the tuning.

```{r}
tuner$optimize(instance)
```

The best model is a support vector machine with a radial basis kernel.
We are now looking at the best configuration across the stages.
The configuration was sampled in the second bracket and thus started with a data set size of 25%.
We observe that the performance increases with the size of the data set.
The multi-fidelity idea can be seen here.
The evaluation on 25% of the data set is only an approximation for the final performance on the full data set.

```{r}
as.data.table(instance$archive)[c(9, 26, 33), .(bracket, stage, subsample.frac, svm.cost, classif.ce)]
```

```{r}
library(ggplot2)

data = as.data.table(instance$archive)[, i := .GRP, by = "svm.cost"]
data[, i := as.factor(i)]
#data = data[bracket == 2]

ggplot(data, aes(x = subsample.frac, y = classif.ce, group = i)) +
  geom_line(aes(color=i)) +
  geom_point(aes(color=i), size = 5)
```
```{r}
data[, .(i, bracket, stage, subsample.frac, svm.cost, classif.ce)]
```



# Conclusion

Subsampling makes it possible to tune learners with Hyperband that lack a natural fidelity parameter.
Moreover, we can reduce the time to optimize a learner on a large data set.

# References
