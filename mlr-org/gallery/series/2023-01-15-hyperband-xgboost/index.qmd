---
title: "Hyperband Series - Iterative Training"
description: |
  Optimize the hyperparameters of an XGBoost model with Hyperband.
categories:
  - tuning
  - classification
author:
  - name: Marc Becker
    url: https://github.com/be-marc
date: 2023-01-15
bibliography: bibliography.bib
knitr:
  opts_chunk:
    R.options:
      datatable.print.nrows: 6
      datatable.print.trunc.cols: TRUE
aliases:
  - ../../../gallery/series/2022-12-01-hyperband-xgboost/index.html
---

{{< include ../_setup.qmd >}}

```{r 2023-01-15-hyperband-xgboost-001}
#| include: false
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

future::plan("multisession")

set.seed(1)
```

# Scope

Increasingly large data sets and search spaces make hyperparameter optimization a time-consuming task.
*Hyperband* [@li_2018] solves this by approximating the performance of a configuration on a simplified version of the problem such as a small subset of the training data, with just a few training epochs in a neural network, or with only a small number of iterations in a gradient-boosting model.
After starting randomly sampled configurations, Hyperband iteratively allocates more resources to promising configurations and terminates low-performing ones.
This type of optimization is called *multi-fidelity* optimization.
The fidelity parameter is part of the search space and controls the tradeoff between the runtime and accuracy of the performance approximation.
In this post, we will optimize XGBoost and use the number of boosting iterations as the fidelity parameter.
This means Hyperband will allocate more boosting iterations to well-performing configurations.
The number of boosting iterations increases the time to train a model and improves the performance until the model is overfitting to the training data.
It is therefore a suitable fidelity parameter.
We assume that you are already familiar with tuning in the mlr3 ecosystem.
If not, you should start with the [book chapter on optimization](https://mlr3book.mlr-org.com/optimization.html) or the [Hyperparameter Optimization on the Palmer Penguins Data Set](/gallery/optimization/2022-11-10-hyperparameter-optimization-on-the-palmer-penguins) post.
This is the first part of the Hyperband series.
The second part can be found here [Hyperband Series - Data Set Subsampling](/gallery/series/2023-01-16-hyperband-subsampling).

# Hyperband

Hyperband is an advancement of the Successive Halving algorithm by @jamieson_2016.
Successive Halving is initialized with the number of starting configurations $n$, the proportion of configurations discarded in each stage $\eta$, and the minimum $r{_{min}}$ and maximum $r{_{max}}$ budget of a single evaluation.
The algorithm starts by sampling $n$ random configurations and allocating the minimum budget $r{_{min}}$ to them.
The configurations are evaluated and $\frac{1}{\eta}$ of the worst-performing configurations are discarded.
The remaining configurations are promoted to the next stage and evaluated on a larger budget.
This continues until one or more configurations are evaluated on the maximum budget $r{_{max}}$ and the best performing configuration is selected.
The number of stages is calculated so that each stage consumes approximately the same budget.
This sometimes results in the minimum budget having to be slightly adjusted by the algorithm.
Successive Halving has the disadvantage that is not clear whether we should choose a large $n$ and try many configurations on a small budget or choose a small $n$ and train more configurations on the full budget.

Hyperband solves this problem by running Successive Halving with different numbers of stating configurations.
The algorithm is initialized with the same parameters as Successive Halving but without $n$.
Each run of Successive Halving is called a bracket and starts with a different budget $r{_{0}}$.
A smaller starting budget means that more configurations can be tried out.
The most explorative bracket allocated the minimum budget $r{_{min}}$.
The next bracket increases the starting budget by a factor of $\eta$.
In each bracket, the starting budget increases further until the last bracket $s = 0$ essentially performs a random search with the full budget $r{_{max}}$.
The number of brackets $s{_{max}} + 1$ is calculated with $s{_{max}} = {\log_\eta \frac{r{_{max}} }{r{_{min}}}}$.
Under the condition that $r{_{0}}$ increases by $\eta$ with each bracket, $r{_{min}}$ sometimes has to be adjusted slightly in order not to use more than $r{_{max}}$ resources in the last bracket.
The number of configurations in the base stages is calculated so that each bracket uses approximately the same amount of budget.
The following table shows a full run of the Hyperband algorithm.
The bracket $s = 3$ is the most explorative bracket and $s = 0$ performance a random search on the full budget.

```{r 2023-01-15-hyperband-xgboost-002}
#| echo: false
#| fig-cap-location: top
#| label: fig-schedule
#| fig-cap: Hyperband schedule with $\eta = 2$ , $r{_{min}} = 1$ and $r{_{max}} = 8$
#| column: body-outset
library(reactable)
library(colorspace)
library(htmltools)

data = mlr3misc::rowwise_table(
  ~i, ~ni_3,  ~ri_3,  ~ni_2,        ~ri_2,        ~ni_1,        ~ri_1,        ~ni_0,        ~ri_0,
  0,  8,      1,      6,            2,            4,            4,            4,            8,
  1,  4,      2,      3,            4,            2,            8,            NA_integer_,  NA_integer_,
  2,  2,      4,      1,            8,            NA_integer_,  NA_integer_,  NA_integer_,  NA_integer_,
  3,  1,      8,      NA_integer_,  NA_integer_,  NA_integer_,  NA_integer_,  NA_integer_,  NA_integer_
)

colors = sequential_hcl(4, palette = "Teal", rev = TRUE)
colors = c(colors, rep("#fff", 4))

reactable(data,
  columns = list(
    ni_3 = colDef(
      style = function(value, index) {
        list(background = colors[index])
      },
      header = function(value, name) {
        tags$b("n", tags$sub("i"))
      }
    ),
    ri_3 = colDef(
      style = function(value, index) {
        list(background = colors[index])
      },
      header = function(value, name) {
        tags$b("r", tags$sub("i"))
      }
    ),
    ni_2 = colDef(
      style = function(value, index) {
        list(background = colors[index + 1])
      },
      header = function(value, name) {
        tags$b("n", tags$sub("i"))
      }
    ),
    ri_2 = colDef(
      style = function(value, index) {
        list(background = colors[index + 1])
      },
      header = function(value, name) {
        tags$b("r", tags$sub("i"))
      }
    ),
    ni_1 = colDef(
      style = function(value, index) {
        list(background = colors[index + 2])
      },
      header = function(value, name) {
        tags$b("n", tags$sub("i"))
      }
    ),
    ri_1 = colDef(
      style = function(value, index) {
        list(background = colors[index + 2])
      },
      header = function(value, name) {
        tags$b("r", tags$sub("i"))
      }
    ),
    ni_0 = colDef(
      style = function(value, index) {
        list(background = colors[index + 3])
      },
      header = function(value, name) {
        tags$b("n", tags$sub("i"))
      }
    ),
    ri_0 = colDef(
      style = function(value, index) {
        list(background = colors[index + 3])
      },
      header = function(value, name) {
        tags$b("r", tags$sub("i"))
      }
    )
  ),
  columnGroups = list(
    colGroup(name = "s = 3", columns = c("ni_3", c("ri_3"))),
    colGroup(name = "s = 2", columns = c("ni_2", c("ri_2"))),
    colGroup(name = "s = 1", columns = c("ni_1", c("ri_1"))),
    colGroup(name = "s = 0", columns = c("ni_0", c("ri_0")))
  ),
  borderless = TRUE
)
```

The Hyperband implementation in `r ref_pkg("mlr3hyperband")` evaluates configurations with the same budget in parallel.
This results in all brackets finishing at approximately the same time.
The colors in @fig-schedule indicate batches that are evaluated in parallel.

# Hyperparameter Optimization

In this practical example, we will optimize the hyperparameters of XGBoost on the `r ref("mlr_tasks_spam", "Spam")` data set.
We begin by loading the `r ref("mlr_learners_classif.xgboost", "XGBoost learner.")`.

```{r 2023-01-15-hyperband-xgboost-003}
#| message: false
library("mlr3verse")

learner = lrn("classif.xgboost")
```

The next thing we do is define the search space.
The `nrounds` parameter controls the number of boosting iterations.
We set a range from 16 to 128 boosting iterations.
This is used as $r{_{min}}$ and $r{_{max}}$ by the Hyperband algorithm.
We need to tag the parameter with `"budget"` to identify it as a fidelity parameter.
For the other hyperparameters, we take the search space for XGBoost from the @bischl_hyperparameter_2021 article.
This search space works for a wide range of data sets.

```{r 2023-01-15-hyperband-xgboost-004}
learner$param_set$set_values(
  nrounds           = to_tune(p_int(16, 128, tags = "budget")),
  eta               = to_tune(1e-4, 1, logscale = TRUE),
  max_depth         = to_tune(1, 20),
  colsample_bytree  = to_tune(1e-1, 1),
  colsample_bylevel = to_tune(1e-1, 1),
  lambda            = to_tune(1e-3, 1e3, logscale = TRUE),
  alpha             = to_tune(1e-3, 1e3, logscale = TRUE),
  subsample         = to_tune(1e-1, 1)
)
```

We construct the tuning instance.
We use the `"none"` terminator because Hyperband terminates itself when all brackets are evaluated.

```{r 2023-01-15-hyperband-xgboost-005}
instance = ti(
  task = tsk("spam"),
  learner = learner,
  resampling = rsmp("holdout"),
  measures = msr("classif.ce"),
  terminator = trm("none")
)
instance
```

We load the `r ref("TunerHyperband", "Hyperband tuner")` and set `eta = 2`.
Hyperband can start from the beginning when the last bracket is evaluated.
We control the number of Hyperband runs with the `repetition` argument.
The setting `repetition = Inf` is useful when a terminator should stop the optimization.

```{r 2023-01-15-hyperband-xgboost-006}
#| message: false
library("mlr3hyperband")

tuner = tnr("hyperband", eta = 2, repetitions = 1)
```

The Hyperband implementation in `r ref_pkg("mlr3hyperband")` evaluates configurations with the same budget in parallel.
This results in all brackets finishing at approximately the same time.
You can think of it as going diagonally through @fig-schedule.
Using `eta = 2` and a range from 16 to 128 boosting iterations results in the following schedule.

```{r 2023-01-15-hyperband-xgboost-007}
#| echo: false
library(htmltools)
library(reactable)

data = hyperband_schedule(r_min = 16, r_max = 128, eta = 2)
setorder(data, cols = "budget")

reactable(data[, .(budget, bracket, stage, n)],
  columns = list(
    budget = colDef(
      name = "Boosting Iterations",
      width = 300,
      cell = function(value) {
        width = paste0(value/128 * 100, "%")
        value = format(value, width = 3, justify = "right")
        bar = div(
          class = "bar-chart",
          style = list(marginRight = "0.375rem"),
          div(class = "bar-chart__bar", style = list(width = width))
        )
        div(class = "bar-chart__bar-cell", span(class = "bar-chart__number", value), bar)
      }
    ),
    bracket = colDef(name = "Bracket"),
    stage = colDef(name = "Stage"),
    n = colDef(name = "# Configruations")
  ),
  pagination = FALSE
  )
```

Now we are ready to start the tuning.

```{r 2023-01-15-hyperband-xgboost-008}
#| output: false
tuner$optimize(instance)
```

The result of a run is the configuration with the best performance.
This does not necessarily have to be a configuration evaluated with the highest budget since we can overfit the data with too many boosting iterations.

```{r 2023-01-15-hyperband-xgboost-009}
instance$result[, .(nrounds, eta, max_depth, colsample_bytree, colsample_bylevel, lambda, alpha, subsample)]
```

The archive of a Hyperband run has the additional columns `"bracket"` and `"stage"`.

```{r 2023-01-15-hyperband-xgboost-010}
as.data.table(instance$archive)[, .(bracket, stage, classif.ce, eta, max_depth, colsample_bytree)]
```

# Conclusion

The handling of Hyperband in mlr3tuning is very similar to that of other tuners.
We only have to select an additional fidelity parameter and tag it with `"budget"`.
We have tried to keep the runtime of the example low.
For your optimization, you should use cross-validation and increase the maximum number of boosting rounds.
The @bischl_hyperparameter_2021 search space suggests 5000 boosting rounds.
Check out our [next post](/gallery/series/2023-01-16-hyperband-subsampling) on Hyperband which uses the size of the training data set as the fidelity parameter.


