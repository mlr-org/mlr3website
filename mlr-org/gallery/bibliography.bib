
@article{wu_controlling_2007,
	title = {Controlling {Variable} {Selection} by the {Addition} of {Pseudovariables}},
	volume = {102},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214506000000843},
	doi = {10.1198/016214506000000843},
	number = {477},
	urldate = {2023-01-19},
	journal = {Journal of the American Statistical Association},
	author = {Wu, Yujun and Boos, Dennis D and Stefanski, Leonard A},
	month = mar,
	year = {2007},
	pages = {235--243}
}

@article{thomas_probing_2017,
	title = {Probing for Sparse and Fast Variable Selection with Model-Based Boosting},
	volume = {2017},
	issn = {1748-670X},
	url = {https://www.hindawi.com/journals/cmmm/2017/1421409/},
	doi = {10.1155/2017/1421409},
	pages = {e1421409},
	journaltitle = {Computational and Mathematical Methods in Medicine},
	author = {Thomas, Janek and Hepp, Tobias and Mayr, Andreas and Bischl, Bernd},
	urldate = {2023-01-31},
	date = {2017-07-31},
	langid = {english},
	note = {Publisher: Hindawi}
}

@article{guyon_gene_2002,
	title = {Gene Selection for Cancer Classification using Support Vector Machines},
	volume = {46},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1012487302797},
	doi = {10.1023/A:1012487302797},
	pages = {389--422},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
	urldate = {2023-01-27},
	date = {2002-01-01},
	langid = {english},
	keywords = {cancer classification, diagnosis, diagnostic tests, {DNA} micro-array, drug discovery, feature selection, gene selection, genomics, proteomics, recursive feature elimination, {RNA} expression, support vector machines}
}

@article{kuhn_building_2008,
	title = {Building Predictive Models in R Using the caret Package},
	volume = {28},
	rights = {Copyright (c) 2008 Max Kuhn},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v028.i05},
	doi = {10.18637/jss.v028.i05},
	abstract = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
	pages = {1--26},
	journaltitle = {Journal of Statistical Software},
	author = {Kuhn, Max},
	urldate = {2023-02-17},
	date = {2008-11-10},
	langid = {english},
	file = {Submitted Version:/home/marc/Zotero/storage/NQHLZVFE/Kuhn - 2008 - Building Predictive Models in R Using the caret Pa.pdf:application/pdf},
}


@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	shorttitle = {Scikit-learn},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and {API} consistency. It has minimal dependencies and is distributed under the simplified {BSD} license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	pages = {2825--2830},
	number = {85},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
	urldate = {2023-02-17},
	date = {2011},
	file = {Full Text PDF:/home/marc/Zotero/storage/LV5YYWUX/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf},
}

@article{ambroise_selection_2002,
	title = {Selection bias in gene extraction on the basis of microarray gene-expression data},
	volume = {99},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.102102699},
	doi = {10.1073/pnas.102102699},
	abstract = {In the context of cancer diagnosis and treatment, we consider the problem of constructing an accurate prediction rule on the basis of a relatively small number of tumor tissue samples of known type containing the expression data on very many (possibly thousands) genes. Recently, results have been presented in the literature suggesting that it is possible to construct a prediction rule from only a few genes such that it has a negligible prediction error rate. However, in these results the test error or the leave-one-out cross-validated error is calculated without allowance for the selection bias. There is no allowance because the rule is either tested on tissue samples that were used in the first instance to select the genes being used in the rule or because the cross-validation of the rule is not external to the selection process; that is, gene selection is not performed in training the rule at each stage of the cross-validation process. We describe how in practice the selection bias can be assessed and corrected for by either performing a cross-validation or applying the bootstrap external to the selection process. We recommend using 10-fold rather than leave-one-out cross-validation, and concerning the bootstrap, we suggest using the so-called .632+ bootstrap error estimate designed to handle overfitted prediction rules. Using two published data sets, we demonstrate that when correction is made for the selection bias, the cross-validated error is no longer zero for a subset of only a few genes.},
	pages = {6562--6566},
	number = {10},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Ambroise, Christophe and {McLachlan}, Geoffrey J.},
	urldate = {2023-02-17},
	date = {2002-05-14},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	file = {Full Text PDF:/home/marc/Zotero/storage/EZ8RNDFJ/Ambroise and McLachlan - 2002 - Selection bias in gene extraction on the basis of .pdf:application/pdf},
}

@article{Chipman2010,
  annote = {BART paper},
  author = {Chipman, Hugh A and George, Edward I and McCulloch, Robert E},
  issn = {19326157, 19417330},
  journal = {The Annals of Applied Statistics},
  mendeley-groups = {AI/Models},
  number = {1},
  pages = {266--298},
  publisher = {Institute of Mathematical Statistics},
  title = {{BART: BAYESIAN ADDITIVE REGRESSION TREES}},
  url = {http://www.jstor.org/stable/27801587},
  volume = {4},
  year = {2010}
}

@article{Bonato2011,
  author = {Bonato, Vinicius and Baladandayuthapani, Veerabhadran and Broom, Bradley M. and Sulman, Erik P. and Aldape, Kenneth D. and Do, Kim Anh},
  doi = {10.1093/BIOINFORMATICS/BTQ660},
  issn = {1367-4803},
  journal = {Bioinformatics},
  month = {feb},
  number = {3},
  pages = {359--367},
  pmid = {21148161},
  publisher = {Oxford Academic},
  title = {{Bayesian ensemble methods for survival prediction in gene expression data}},
  url = {https://dx.doi.org/10.1093/bioinformatics/btq660},
  volume = {27},
  year = {2011}
}

@article{Sparapani2016,
  author = {Sparapani, Rodney A. and Logan, Brent R. and McCulloch, Robert E. and Laud, Purushottam W.},
  doi = {10.1002/SIM.6893},
  issn = {1097-0258},
  journal = {Statistics in Medicine},
  month = {jul},
  number = {16},
  pages = {2741--2753},
  pmid = {26854022},
  publisher = {John Wiley & Sons, Ltd},
  title = {{Nonparametric survival analysis using Bayesian Additive Regression Trees (BART)}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.6893},
  volume = {35},
  year = {2016}
}

@article{Sparapani2021,
  author = {Sparapani, Rodney and Spanbauer, Charles and McCulloch, Robert},
  doi = {10.18637/JSS.V097.I01},
  issn = {1548-7660},
  journal = {Journal of Statistical Software},
  month = {jan},
  number = {1},
  pages = {1--66},
  publisher = {American Statistical Association},
  title = {{Nonparametric Machine Learning and Efficient Computation with Bayesian Additive Regression Trees: The BART R Package}},
  url = {https://www.jstatsoft.org/index.php/jss/article/view/v097i01},
  volume = {97},
  year = {2021}
}

@article{Sonabend2022,
  author = {Sonabend, Raphael and Bender, Andreas and Vollmer, Sebastian},
  doi = {10.1093/BIOINFORMATICS/BTAC451},
  editor = {Lu, Zhiyong},
  isbn = {451/6640155},
  issn = {1367-4803},
  journal = {Bioinformatics},
  month = {jul},
  title = {{Avoiding C-hacking when evaluating survival distribution predictions with discrimination measures}},
  url = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btac451/6640155},
  year = {2022}
}

@article{Friedman2001,
  author = {Friedman, Jerome H},
  journal = {Annals of statistics},
  pages = {1189--1232},
  publisher = {JSTOR},
  title = {{Greedy function approximation: a gradient boosting machine}},
  year = {2001},
  doi = {10.1214/aos/1013203451}
}

@article{Sonabend2022a,
  archivePrefix = {arXiv},
  arxivId = {2206.03256},
  author = {Sonabend, Raphael and Pfisterer, Florian and Mishler, Alan and Schauer, Moritz and Burk, Lukas and Mukherjee, Sumantrak and Vollmer, Sebastian},
  doi = {10.48550/arxiv.2206.03256},
  eprint = {2206.03256},
  month = {may},
  title = {{Flexible Group Fairness Metrics for Survival Analysis}},
  url = {https://arxiv.org/abs/2206.03256v3},
  year = {2022}
}

@article{Meinshausen2010,
  author = {Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
  doi = {10.1111/J.1467-9868.2010.00740.X},
  eprint = {0809.2932},
  issn = {1369-7412},
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  month = {sep},
  number = {4},
  pages = {417--473},
  publisher = {Oxford Academic},
  title = {{Stability Selection}},
  url = {https://dx.doi.org/10.1111/j.1467-9868.2010.00740.x},
  volume = {72},
  year = {2010}
}

@article{Saeys2008,
  author = {Saeys, Yvan and Abeel, Thomas and {Van De Peer}, Yves},
  doi = {10.1007/978-3-540-87481-2_21},
  isbn = {3540874801},
  issn = {03029743},
  journal = {Machine Learning and Knowledge Discovery in Databases},
  pages = {313--325},
  publisher = {Springer, Berlin, Heidelberg},
  title = {{Robust feature selection using ensemble feature selection techniques}},
  url = {https://link.springer.com/chapter/10.1007/978-3-540-87481-2_21},
  volume = {5212 LNAI},
  year = {2008}
}


