---
title: Imabalanced ROC-Analysis threshold Tuning
group: Imbalanced
categories:
  - imputation
  - mlr3benchmarking
author:
  - name: Fiona Ewald
  - name: Essential Data Science Training GmbH
    url: https://www.essentialds.de
description: |
  Train a classifier on German Credit set and tune the output of a probabilistic model with ROC threshold analysis.
date: ""
params:
  showsolution: true
  base64encode: true
listing: false
search: false
format:
  html:
    filters:
      - ../../b64_solution.lua
---

{{< include ../../_setup.qmd >}}
{{< include ../../_setup_encrypt_lua.qmd >}}

```{r, include=FALSE}
set.seed(123)
```


# Goal

In this exercise, we will create a machine learning model that predicts the
credit risk of an individual (e.g., the probability of being a `good` or `bad` credit applicant for the bank). Our goal is not to
obtain an optimal classifier for this task, but to learn how to get a better
understanding of the  predictions made by this model. This means looking at its
sensitivity (ability to correctly identify positives) and specificity (ability
to correctly identify negatives). The sensitivity is also known as the true positive rate (TPR) and the
specificity is equal to (1 - FPR) where FPR is the false positive rate.

We will also cover how to obtain different response predictions from a probabilistic
model by modifying the threshold. We will inspect this relationship via the ROC curve and tune
the threshold for a given classifier to optimize our response predictions.


# 1 Training a classification tree on the german credit task

First load the pre-defined German credit task and set the positive class to `"good"`.
Train a random forest on 2/3 of the data (training data) and make probabilistic predictions on the remaining 1/3 (test data).

<details>
<summary>**Hint 1:**</summary>
- Create the German credit task using `tsk()` and set the positive class by modifying e.g. `task$positive`.
- Create a learner using `lrn()` and make sure to specify the `predict_type` so that the learner will predict probabilities instead of classes.
- When calling the methods `$train()` and `$predict()` of the learner, you can pass an argument `row_ids` to specify which observations should be used for the train and test data. 
- You can generate random train-test splits using, e.g., the `partition()` function.
</details>

<details>
<summary>**Hint 2:**</summary>
```{r, eval = FALSE}
library(mlr3verse)

task = tsk(...)
task$positive = ...
learner = lrn(..., predict_type = ...)
ids = partition(...)
learner$train(..., row_ids = ...)
pred = learner$predict(..., row_ids = ...)
```
</details>

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, eval = params$showsolution}
library(mlr3verse)
task = tsk("german_credit")
task$positive = "good"
learner = lrn("classif.ranger", predict_type = "prob")
ids = partition(task)
str(ids)
learner$train(task, row_ids = ids$train)
pred = learner$predict(task, row_ids = ids$test)
pred
```

:::

:::

# 2 Confusion matrices and classification thresholds

Inspect and save the confusion matrix of the predictions made in the previous exercise.
Manually calculate the FPR and TPR using the values from the confusion matrix.
Can you think of another way to compute the TPR and FPR using `mlr3` instead of manually computing them using the confusion matrix?

<details>
  <summary>**Recap**</summary>
A confusion matrix is a special kind of contingency table with two
dimensions "actual" and "predicted" to summarize the ground truth classes (truth) vs. the predicted classes of a classifier (response).

Binary classifiers can be understood as first predicting a score (possibly a probability) and then classifying all instances with a score greater than a certain threshold $t$ as positive and all others as negative. This means that one can obtain different class predictions using different threshold values $t$.
</details>

<details>
  <summary>**Hint 1:**</summary>
A prediction object has a field `$confusion`.
Since `good` was used as the positive class here, the TPR is $P(\hat{Y} = good | Y = good)$ and the FPR is $P(\hat{Y} = good | Y = bad)$ (where $\hat{Y}$ refers to the predicted response of the classifier and $Y$ to the ground truth class labels). Instead of manually computing the TPR and FPR, there are appropriate performance measures implemented in `mlr3` that you could use.
</details>

<details>
  <summary>**Hint 2:**</summary>
You need to replace `...` in the code below to access the appropriate columns and rows, e.g., `confusion1[1, 1]` is the element in the first row and first column of the confusion matrix and tells you how many observations with ground truth $Y = good$ were classified into the class $\hat{Y} = good$ by the learner.
```{r, eval = FALSE}
confusion1 = pred$confusion
TPR1 =  confusion1[...] / sum(confusion1[...])
TPR1
FPR1 = confusion1[...] / sum(confusion1[...])
FPR1
```

The names of the TPR and FPR performance measures implemented in `mlr3` can be found by looking at `as.data.table(mlr_measures)`. You can use the code below and pass the names of the `mlr3` measures in a vector to compute both the TPR and FPR: 
```{r, eval = FALSE}
pred$score(msrs(...))
```
</details>

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, eval = params$showsolution}
confusion1 = pred$confusion
TPR1 = confusion1[1, 1] / sum(confusion1[, 1])
TPR1
FPR1 = confusion1[1, 2] / sum(confusion1[, 2])
FPR1
```

Instead of manually computing the TPR and FPR, you could also just use 
```{r, eval = params$showsolution}
pred$score(msrs(c("classif.tpr", "classif.fpr")))
```

:::

:::

# 3 Asymmetric costs

Think about which type of error is worse for the given task and obtain new predictions
(without retraining the model) that takes this into account.

Then calculate the FPR and TPR and compare it with the results from the previous exercise.

<details>
<summary>**Hint 1:**</summary>
A prediction object has the method `$set_threshold()` that can be used to set a custom threshold and which will update the predicted classes according to the selected threshold value.
</details>

<details>
<summary>**Hint 2:**</summary>
```{r, eval = FALSE}
pred$set_threshold(...)
confusion2 = pred$confusion
TPR2 =  confusion2[...] / sum(...)
FPR2 = confusion2[...] / sum(...)

TPR2 - TPR1
FPR2 - FPR1
```
</details>

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

The error of making a false positive - in this case classifying someone who is not creditworthy ($Y = bad$)
as creditworthy ($\hat Y = good$) - is likely considerably higher than classifying someone who is creditworthy as not creditworthy. 
In the first scenario, the company may lose all the money that was not paid back duly. In the letter case, it only misses out on the profit.

We can take this fact into account by using a higher threshold to predict the positive class (`good`), i.e., being more conservative in classifying a good credit risk.
For illustration purposes, we will use the threshold $0.7$ which is higher than the default threshold $0.5$.

```{r, eval = params$showsolution}
pred$set_threshold(0.7)
pred
```
We can then access the updated confusion matrix and calculate the new FPR and TPR as before.

```{r, eval = params$showsolution}
confusion2 = pred$confusion
TPR2 =  confusion2[1, 1] / sum(confusion2[, 1])
FPR2 = confusion2[1, 2] / sum(confusion2[, 2])
```

When comparing it with the previous values, we observe a lower TPR and FPR.

```{r, eval = params$showsolution}
TPR2 - TPR1
FPR2 - FPR1
```

:::

:::

# 4 ROC curve

In the previous two exercises, we have calculated the FPR and TPR for two thresholds.
Now visualize the FPR and TPR for all possible thresholds, i.e. the ROC curve.

<details>
<summary>**Recap**</summary>
The receiver operating characteristic (ROC) displays the sensitivity and specificity for all
possible thresholds.
</details>


<details>
<summary>**Hint 1:**</summary>
You can use `autoplot()` on the prediction object and set the `type` argument to produce a ROC curve. 
You can open the help page of `autoplot` for a prediction object using `?mlr3viz::autoplot.PredictionClassif`.
</details>

<details>
<summary>**Hint 2:**</summary>
```{r, eval = FALSE}
autoplot(pred, type = ...)
```
</details>

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, eval = params$showsolution}
autoplot(pred, type = "roc", show_cb = FALSE)
```

:::

:::

# 5 Threshold tuning

In this exercise, we assume that predicting a false positive is 4 times worse than a false negative.
Use a measure that considers classification costs (e.g., misclassification costs `msr("classif.costs")$help()`) and tune the threshold of our classifier to systematically optimize the asymmetric cost function. 

## 5.1 Cost Matrix

First, define the cost matrix. Here, this is a 2x2 matrix with rows corresponding to the predicted class and columns corresponding to the true class. The first row/column implies `"good"`, the second `"bad"` credit rating.

<details>
<summary>**Hint 1:**</summary>

The order of the classes in the rows and columns of the matrix must correspond to the order of classes in `task$class_names`.

</details>

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, eval = params$showsolution}
costs = matrix(c(0, 1, 4, 0), nrow = 2, dimnames =
  list("Predicted Credit" = c("good", "bad"),
    Truth = c("good", "bad")))
costs
```

:::

:::

## 5.2 Cost-Sensitive Measure

Next, define a cost-sensitive measure. This measure takes one argument, which is a matrix with row and column names corresponding to the class labels in the task of interest. 

<details>
<summary>**Hint 1:**</summary>
You can use `as.data.table(mlr_measures)` to find the relevant measure.
</details>

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, eval = params$showsolution}
msr_costs = msr("classif.costs", costs = costs)
msr_costs
```

:::

:::

## 5.3 Thresholding

In default settings, a model will classify a customer as good credit if the predicted probability is greater than 0.5. Here, this might not be a sensible approach as we would likely act more conservatively and reject more credit applications with a higher threshold due to the non-uniform costs. Use the `autplot()` function to plot the costs associated with predicting at various thresholds between 0 and 1 for the random forest predictions stored in the `pred` object from before.

<details>
<summary>**Hint 1:**</summary>
You need to specify `type = "threshold` within autoplot as well as the previously defined measure.
</details>

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, eval = params$showsolution}
autoplot(pred, type = "threshold", measure = msr_costs)
```

As expected, the optimal threshold is greater than 0.5 which means the optimal model should predict ‘bad’ credit more often than not.

# Alternative without autoplot

```{r, eval = params$showsolution}
library(ggplot2)
thresholds = seq(0, 1, by = 0.01)
eval_threshold = function(pred, threshold, measure, task) {
  pred$set_threshold(threshold)
  score = pred$score(measure, task = task)
  return(score)
}
costs = matrix(c(0, 1, 4, 0), nrow = 2)
dimnames(costs) = list(truth = task$class_names, response = task$class_names)
measure = msr("classif.costs", costs = costs)
scores = sapply(thresholds, function(threshold) eval_threshold(pred, threshold, measure, task))
thresholds[which.min(scores)]
min(scores)
qplot(x = thresholds, y = scores) + geom_vline(xintercept = thresholds[which.min(scores)])

```

:::

:::

## 5.4 Tuning the Threshold

The optimal threshold can be automated via `po("tunethreshold")`. Create a graph that consists of a logistic regression learner and this threshold tuning pipeline object. Then, turn the graph into a learner as in previous tutorials. Finally, benchmark the pipeline against a standard logistic regression learner using 3-fold CV.

<details>
<summary>**Hint 1:**</summary>
You can use this code skeleton for the pipeline:
```{r, eval = FALSE}
logreg = po("learner_cv", lrn(...)) # base learner
graph =  logreg %>>% po(...) # graph with threshold tuning
```
</details>

<details>
<summary>**Hint 2:**</summary>
You can use this code skeleton for the benchmark:
```{r, eval = FALSE}
learners = list(..., lrn("classif.log_reg"))
bmr = benchmark(benchmark_grid(task, learners,
  rsmp("cv", folds = 3)))
```
</details>

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, eval = params$showsolution}
# Pipeline:
po_cv = po("learner_cv", lrn("classif.log_reg", predict_type = "prob"))
graph =  po_cv %>>% po("tunethreshold", measure = msr_costs)

# Benchmark:
learners = list(as_learner(graph), lrn("classif.log_reg"))
bmr = benchmark(benchmark_grid(task, learners,
  rsmp("cv", folds = 3)))

# Evaluate:
bmr$aggregate(msr_costs)[, c(4, 7)]
```

:::

:::

# 6 ROC Comparison

In this exercise, we will explore how to compare to learners by looking at their ROC curve.

The basis for this exercise will be a benchmark experiment that
compares a classification tree with a random forest on the german credit task.

Because we are now not only focused on the analysis of a given prediction, but on the
comparison of two learners, we selected a 10-fold cross-validation to reduce the
uncertainty of this comparison.

Conduct the benchmark experiment and show both ROC curves in one plot.
Which learner learner performs better in this case?

<details>
<summary>**Hint 1:**</summary>
Use `benchmark_grid()` to create the experiment design and execute it using `benchmark()`.
You can also apply the function `autoplot()` to benchmark results.
</details>

<details>
<summary>**Hint 2:**</summary>
```{r, eval = FALSE}
resampling = rsmp(..)
learners = lrns(...)
design = benchmark_grid(...)
bmr = benchmark(...)
autoplot(...)
```

</details>


:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

We create and execute the benchmark experiment in the usual fashion.

```{r, eval = params$showsolution}
resampling = rsmp("cv", folds = 10)
learners = list(
  lrn("classif.rpart", predict_type = "prob"),
  lrn("classif.ranger", predict_type = "prob")
)
design = benchmark_grid(task, learners, resampling)
bmr = benchmark(design)
```

Now we proceed with showing the ROC curve. 

```{r, eval = params$showsolution}
autoplot(bmr, type = "roc", show_cb = FALSE)
```

The random forest is clearly better, as for virtually every specificity it has a higher
sensitivity.

:::

:::

# 7 Area under the curve

In the previous exercise we have learned how to compare to learners using the ROC curve.
Although the random forest was dominating the classification tree in this specific case,
the more common case is that the ROC curves of two models are crossing, making a comparison
in the sense of $>$ / $<$ impossible.

The area under the curve tries to solve this problem by summarizing the ROC curve by its area
under the curve (normalized to 1), which allows for a scalar comparison.

Compare the AUC for the benchmark result.

<details>
<summary>**Hint 1:**</summary>
You can use the `autoplot()` function and use the AUC as performance measure in the `measure` argument.
</details>

<details>
<summary>**Hint 2:**</summary>
```{r, eval = FALSE}
autoplot(bmr, measure = msr(...))
```
</details>

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, eval = params$showsolution}
autoplot(bmr, measure = msr("classif.auc"))
```

As expected, the AUC for the random forest is higher than the AUC for the classification tree.

:::

:::

# Bonus exercise: Unbiased performance estimation

Revisit the exercise where we tuned the threshold.
Is the performance estimate for the best threshold unbiased?
If no, does this mean that our tuning strategy was invalid?

<details>
<summary>**Hint 1:**</summary>
Did we use an independent test set?
</details>

<details>
<summary>**Hint 2:**</summary>
Think of the uncertainty when estimating the ROC curve.
</details>

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

No, although our method is valid with respect to determining our "best guess"
for the optimal threshold, the obtained performance estimate suffers from
optimization bias. We would need an independent test set to evaluate our
method, i.e. nested holdout or cross-validation.

:::

:::


# Summary

In this exercise we improved our understanding of the performance of binary classifiers
by the means of the confusion matrix and a focus on different error types.
We have seen how we can analyze and compare classifiers using the ROC and
how to improve our response predictions using threshold tuning.
