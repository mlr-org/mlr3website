---
title: Filters 
group: Advanced Feature Preprocessing
categories:
  - advanced feature preprossesing
  - pipelines
author:
  - name: Giuseppe Casalicchio
  - name: Essential Data Science Training GmbH
    url: https://www.essentialds.de
description: |
  Use pipelines for efficient pre-processing and model training on a the kc_housing task.
date: ""
params:
  showsolution: true
  base64encode: true
listing: false
search: false
format:
  html:
    filters:
      - ../../b64_solution.lua
---

{{< include ../../_setup.qmd >}}
{{< include ../../_setup_encrypt_lua.qmd >}}

```{r, include=FALSE}
set.seed(123)
```

# Goal

Apply what you have learned about using pipelines for efficient pre-processing and model training on a regression problem.

# House Prices in King county

In this exercise, we want to model house sale prices in King county in the state of Washington, USA. 

```{r}
set.seed(124)
library(mlr3verse)
library("mlr3tuningspaces")
data("kc_housing", package = "mlr3data")
```

We do some simple feature pre-processing first:

```{r}
# Transform time to numeric variable:
library(anytime)
dates = anytime(kc_housing$date)
kc_housing$date = as.numeric(difftime(dates, min(dates), units = "days"))
# Scale prices:
kc_housing$price = kc_housing$price / 1000
# For this task, delete columns containing NAs:
yr_renovated = kc_housing$yr_renovated
sqft_basement = kc_housing$sqft_basement
kc_housing[,c(13, 15)] = NULL
# Create factor columns:
kc_housing[,c(8, 14)] = lapply(c(8, 14), function(x) {as.factor(kc_housing[,x])})
# Get an overview:
str(kc_housing)
```

# Add uncorrelated features to data

To test different strategies for feature selection in this exercise, we create two artificial features that are (mostly) uncorrelated with the outcome `price`:

```{r}
# Uncorrelated feature x1:
kc_housing$x1 <- runif(n = nrow(kc_housing))
cor(kc_housing$x1, kc_housing$price)

# Uncorrelated feature x2:
kc_housing$x2 <- sin(0.01*kc_housing$price*kc_housing$grade)
cor(kc_housing$x2, kc_housing$price)
```

# Train-test Split

Before we train a model, let's reserve some data for evaluating our model later on:

```{r}
task = as_task_regr(kc_housing, target = "price")
split = partition(task, ratio = 0.6)

tasktrain = task$clone()
tasktrain$filter(split$train)

tasktest = task$clone()
tasktest$filter(split$test)
```

# Conditional Encoding

In the King county data, there are two categorial features encoded as `factor`:

```{r}
lengths(task$levels())
```

Obviously, `waterfront` is a low-cardinality feature suitable for one-hot encoding and `zipcode` is a very high-cardinality feature. Therefore, it would make sense to create a pipeline that first pre-processes each factor variable with either impact or one-hot encoding, depending on the feature cardinality.

# Filters

Filter algorithms select features by assigning numeric scores to each feature, e.g. correlation between features and target variable, use these to rank the features and select a feature subset based on the ranking. Features that are assigned lower scores are then omitted in subsequent modeling steps. All filters are implemented via the package `mlr3filters`. A very simple filter approach could look like this:

1. Calculate the correlation coefficient between each feature and a numeric target variable
2. Select the 10 features with the highest correlation for further modeling steps.

A different strategy could entail selecting only features above a certain threshold of correlation with the outcome. For a full list of all implemented filter methods, take a look at https://mlr3filters.mlr-org.com.

# Exercises

## Exercise 1: Create a complex pipeline

Create a pipeline with the following sequence of elements:

1. Each factor variable gets pre-processed with either one-hot or impact encoding, depending on the cardinality of the feature.
2. A filter selector is applied to the features, sorting them by their Pearson correlation coefficient and selecting the 3 features with the highest correlation.
3. A random forest (`regr.ranger`) is trained.

The pipeline should be tuned within an `autotuner` with random search, two-fold CV and MSE as performance measure, and a search space from `mlr3tuningspaces`. Train the `autotuner` on the training data, and evaluate the performance on the holdout test data.

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, include = params$showsolution}
# Create rf learner 
rf = lrn("regr.ranger")

# Set search space from mlr3tuningspaces
rf_ts = lts(rf)
as.data.table(rf_ts$param_set$values)

# Create "correlation" filter pipeline element
cor_filter = po("filter", filter = flt("correlation"))
as.data.table(cor_filter$param_set)$id
# Use the 3 features with the highest filter value
cor_filter$param_set$values$filter.nfeat = 3

# Create conditional encoding pipeline element
factor_po = po("encode", method = "one-hot",
               affect_columns = selector_invert(selector_cardinality_greater_than(10)),
               id = "low_card_enc") %>>%
            po("encodeimpact",
               affect_columns = selector_cardinality_greater_than(10),
               id = "high_card_enc") 


# Combine rf_ts with impact encoding and the filter
rf_ts_cor = as_learner(factor_po %>>% cor_filter %>>% rf_ts)

# Use random search
tuner = tnr("random_search")

# Create autotuner
auto_rf_cor = auto_tuner(
  tuner = tuner,
  learner = rf_ts_cor,
  resampling = rsmp("cv", folds = 2),
  measure = msr("regr.mse"),
  term_evals = 20
)

lgr::get_logger("bbotk")$set_threshold("warn")
future::plan("multisession")
auto_rf_cor$train(tasktrain)
score_rf_cor = auto_rf_cor$predict(tasktest)$score()
score_rf_cor
```


:::

:::

## Exercise 2: Information gain

An alternative filter method is information gain (https://mlr3filters.mlr-org.com/reference/mlr_filters_information_gain.html). Recreate the pipeline from exercise 1, but use information gain as filter. Again, select the three features with the highest information gain. Train the `autotuner` on the training data, and evaluate the performance on the holdout test data.

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, include = params$showsolution}
# Create "information_gain" filter pipeline element
info_filter = po("filter", filter = flt("information_gain"))
# Use the 8 features with the highest filter value
info_filter$param_set$values$filter.nfeat = 3

# Combine the pipeline elements
rf_ts_info = as_learner(factor_po %>>% info_filter %>>% rf_ts)

# Create autotuner
auto_rf_info = auto_tuner(
  tuner = tuner,
  learner = rf_ts_info,
  resampling = rsmp("cv", folds = 2),
  measure = msr("regr.mse"),
  term_evals = 20
)

auto_rf_info$train(tasktrain)
score_rf_info = auto_rf_info$predict(tasktest)$score()
score_rf_info
```


:::

:::

## Exercise 3: Pearson correlation vs. Information gain

We receive the following performance scores for the two filter methods: 

```{r}
score_rf_cor
score_rf_info
```

As you can see, the Pearson correlation filter seems to select features that result in a better model. To investigate why that may have happened, inspect the trained autotuners. Which features have been selected? Given the selected features, reason to what extent which filter methods may be more helpful than others in determining features to select for the model training process.

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, eval = params$showsolution}
auto_rf_cor$model$learner$model$correlation$features
auto_rf_info$model$learner$model$information_gain$features
```

The correlation filter does not select `x2`, a feature that we artificially created and is uncorrelated with the outcome `price`. However, the information gain filter selects for `x2`. This can be seen from how it was computed in the first place:

```{r, eval = params$showsolution}
kc_housing$x2 <- sin(0.01*kc_housing$price*kc_housing$grade)
```

Indeed, it is a sin-transformed function of the `price` and `grade`, something that is not necessarily obvious from a simple visual inspection:

```{r, eval = params$showsolution}
plot(kc_housing$x2, kc_housing$price)
```

:::

:::

## Exercise 4: Imputation

In the three exercises before, we excluded two variables from the `kc_housing` data with missing values. Let's add them back to the data set and try to impute missing values automatically with a pipeline.

```{r}
kc_housing$yr_renovated = yr_renovated
kc_housing$sqft_basement = sqft_basement
task = as_task_regr(kc_housing, target = "price")
# Check again which features in the task have NAs:
names(which(task$missings() > 0))
```

Further, we use a simple train-test split as before:

```{r}
split = partition(task, ratio = 0.6)

tasktrain = task$clone()
tasktrain$filter(split$train)

tasktest = task$clone()
tasktest$filter(split$test)
```

As the two features with missing values are both numeric, we can compare two simple strategies for imputation: mean imputation and histogram imputation. While the former replaces `NAs` with the mean value of a feature, the latter samples from the empirical distribution of the non-`NA` values of the feature, ensuring that the marginal distribution is preserved. For each imputation method, construct a simple pipeline that trains a random forest without any HPO on the `tasktrain`, and evaluate the model on `tasktest` using MSE.

:::{.callout-note collapse="true"}

### Solution

:::{.b64-solution}

```{r, eval = params$showsolution}
# Mean imputation:
imp_mean_ranger = as_learner(po("imputemean") %>>% lrn("regr.ranger"))
imp_mean_ranger$train(tasktrain)

# Histogram imputation:
imp_hist_ranger = as_learner(po("imputehist") %>>% lrn("regr.ranger"))
imp_hist_ranger$train(tasktrain)

# Evaluate performance:
imp_mean_ranger$predict(tasktest)$score()
imp_hist_ranger$predict(tasktest)$score()
```


:::

:::

# Summary

We learned about more complex pipelines, including pre-processing methods for imputation, variable encoding and feature filtering.
