---
title: "Why is mlr3 Eating my Disk / RAM?"
description: |
    Possible explanations for exuberant memory usage of mlr3 objects.
categories:
  - tuning
  - classification
author:
  - name: Marc Becker
    url: https://github.com/be-marc
  - name: Sebastian Fischer
    url: https://github.com/sebffischer
date: 2023-11-09
knitr:
  opts_chunk:
    R.options:
      datatable.print.nrows: 6
      datatable.print.trunc.cols: TRUE
---

{{< include ../../_setup.qmd >}}

```{r}
#| include: false
lgr::get_logger("mlr3")$set_threshold("warn")
```


When serializing `mlr3` objects, it can happen that their size is much larger than expected.
While we are trying our best to keep such cases from happening, there are things that are beyond our control.
This gallery post serves as a technical trouble-shooting guide that covers various issues and offers solutions where possible.
We will update this post as new problems come to our attention.
Note that while some of these issues might seem neglibile, they can cause serious problems when running large benchmrk experiments, e.g. using `mlr3batchmark`.

## Avoid Installating Packages With Source References

Some objects in `mlr3` have parameters that can be functions.
One example for that is `po("colapply")`'s `applicator` parameter.

```{r, output = FALSE}
library("mlr3verse")
library("pryr")
po_center = po("colapply", applicator = function(x) x - mean(x))
```

Because `Learner`s store the hyperparameters that were used for training in their `$state`, it is important to ensure that their size is small.
One cause for large sizes of parameter values is the presence of source references in the function's attributes.
Source references are kept when installing packages with the `--with-keep.source` option.
Note that this option is enabled by default when installing packages with `renv`.
You can disble it by setting the following option before installing packages, e.g. by adding it to your `.Rprofile`.

```{r, eval = FALSE}
options("install.opts" = "--without-keep.source")
```

## Duplication of Data When Serializing

Another cause for increased object size is how R duplicates data when serializing objects.
Consider the simple example below:

```{r}
x = rnorm(1000000)
y = x
lx = list(x)
lxy = list(x, y)
object_size(lx)
object_size(lxy)
```

Because of R's copy-on-write semantics, data is only copied when it is modified, i.e. the list `lx` has the same size as `lxy` because `x` and `y` all point to the same underlying data.
However, when serializing `lxy`, both `x` and `y` are serialized independently and its memory footprint is doubled.

```{r}
object_size(serialize(lxy, NULL))
```

Because data is serialized not only when manually saving objects, but also when parallelizing execution via `future` or when using encapsulation, this can cause the same information to be duplicated many times and blow up both RAM and disk usage.
While we have some mechanisms (like `mlr3misc::leanify` and  database normalization) in place to counteract this to some extent, it is impossible to get rid of the problem completely.

## Setting the Correct Flags

Another -- easily amendable -- source for large object sizes is forgetting to set the right flags.
The list below contains some important configuration options that can be used to reduce the size of important `mlr3` objects:

* `benchmark()` and `resample()` have the flags `store_backends` and `store_models`
* `auto_tuner` has flags `store_tuning_instance` and `store_benchmark_result`
* `tune()` has flags `store_benchmark_result` and `store_models`
