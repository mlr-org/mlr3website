---
title: "Analyzing the Runtime Performance of tidymodels and mlr3"
description: |
  Compare the runtime performance of tidymodels and mlr3.
author:
  - name: Marc Becker
    orcid: 0000-0002-8115-0400
    url: https://github.com/be-marc
date: 2023-10-30
bibliography: ../../bibliography.bib
image: cover.png
---

{{< include ../../_setup.qmd >}}

```{r 2022-12-22-mlr3viz-001}
#| include: false
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
set.seed(0)

options("datatable.print.class" = FALSE)

library("mlr3verse")
library("tidymodels")
library("microbenchmark")
library("ggplot2")
library("cowplot")
library("mlr3misc")
library("stringr")

bm_rpart = readRDS("rpart_sonar_resample.rds")
bm_ranger = readRDS("ranger_sonar_resample.rds")
```

# Scope

In the realm of data science, machine learning frameworks play an important role in streamlining and accelerating the development of analytical workflows.
Among these, `r ref_pkg("tidymodels")` and `r ref_pkg("mlr3")` stand out as prominent tools within the R community.
They provide a unified interface for data preprocessing, model training, resampling and tuning.
The streamlined and accelerated development process, while efficient, typically results in a trade-off concerning runtime performance.
This article undertakes a detailed comparison of the runtime efficiency of `tidymodels` and `mlr3`, focusing on their performance in training, resampling, and tuning machine learning models.
Specifically, we assess the time efficiency of these frameworks in running the `r ref("rpart::rpart()")` and `r ref("ranger::ranger()")` models, using the `r ref("mlr_tasks_sonar", "Sonar")` dataset as a test case.
Additionally, the study delves into analyzing the runtime overhead of these frameworks by comparing their performance against training the models without a framework.
Through this comparative analysis, the article aims to provide valuable insights into the operational trade-offs of using these advanced machine learning frameworks in practical data science applications.

# Setup

We employ the `r ref_pkg("microbenchmark")` package to measure the time required for training, resampling, and tuning models.
This benchmarking process is applied to the `Sonar` dataset using the `rpart` and `ranger` algorithms.

```{r}
#| eval: false
library("mlr3verse")
library("tidymodels")
library("microbenchmark")

task = tsk("sonar")
data = task$data()
formula = Class ~ .
```

To ensure the robustness of our results, each function call within the benchmark is executed 100 times in a randomized sequence.
The microbenchmark package then provides us with detailed insights, including the median, lower quartile, and upper quartile of the runtimes.
To further enhance the reliability of our findings, we execute the benchmark on a cluster.
Each run of `microbenchmark` is repeated 100 times, with different seeds applied for each iteration.
Resulting in a total of 10,000 function calls of each command.
The computing environment for each worker in the cluster consists of 3 cores and 12 GB of RAM.
For transparency and reproducibility, the examples of the code used for this experiment are provided as snippets in the article.
The complete code, along with all details of the experiment, is available in our public repository, [mlr-org/mlr-benchmark](https://github.com/mlr-org/mlr-benchmark/tree/main/tidymodels).

It's important to note that our cluster setup is not specifically optimized for single-core performance.
Consequently, executing the same benchmark on a local machine with might yield faster results.

# Benchmark

## Train the Models

Our benchmark starts with the fundamental task of model training.
To facilitate a direct comparison, we have structured our presentation into two distinct segments.
On the left, we demonstrate the initialization of the `rpart` model, employing both `mlr3` and `tidymodels` frameworks.
The `rpart` model is a decision tree classifier, which is a simple and fast-fitting algorithm for classification tasks.
Simultaneously, on the right, we turn our attention to the initialization of the `ranger` model, known for its efficient implementation of the random forest algorithm.
Our aim is to mirror the configuration as closely as possible across both frameworks, maintaining consistency in parameters and settings.

:::{layout-ncol="2"}

```{r}
#| eval: false

# tidymodels
tm_mod = decision_tree() %>%
  set_engine("rpart",
    xval = 0L) %>%
  set_mode("classification")

# mlr3
learner = lrn("classif.rpart",
  xval = 0L)
```

```{r}
#| eval: false

# tidymodels
tm_mod = rand_forest(trees = 1000L) %>%
  set_engine("ranger",
    num.threads = 1L,
    seed = 1) %>%
  set_mode("classification")

# mlr3
learner = lrn("classif.ranger",
  num.trees = 1000L,
  num.threads = 1L,
  seed = 1,
  verbose = FALSE,
  predict_type = "prob")
```

:::

We measure the runtime for the train functions within each framework.
The result of the train function is a trained model in both frameworks.
In addition, we invoke the `rpart()` and `ranger()` functions to establish a baseline for the minimum achievable runtime.
This allows us to not only assess the efficiency of the train functions in each framework but also to understand how they perform relative to the base packages.

```{r}
#| eval: false

# tidymodels train
fit(tm_mod, formula, data = data)

# mlr3 train
learner$train(task)
```

When training an `rpart` model, `tidymodels` demonstrates superior speed, outperforming `mlr3` (@tbl-train-rpart).
Notably, the `mlr3` package requires approximately twice the time compared to the baseline.

A key observation from our results is the significant relative overhead when using a framework for `rpart` model training.
Given that `rpart` inherently requires a shorter training time, the additional processing time introduced by the frameworks becomes more pronounced.
This aspect highlights the trade-off between the convenience offered by these frameworks and their impact on runtime for quicker tasks.

Conversely, when we shift our focus to training a `ranger` model, the scenario changes (@tbl-train-ranger).
Here, the runtime performance of `ranger` is strikingly similar across both `tidymodels` and `mlr3`.
This equality in execution time can be attributed to the inherently longer training duration required by `ranger` models.
As a result, the relative overhead introduced by either framework becomes minimal, effectively diminishing in the face of the more time-intensive training process.
This pattern suggests that for more complex or time-consuming tasks, the choice of framework may have a less significant impact on overall runtime performance.

:::{layout-ncol="2"}

```{r}
#| echo: false
#| label: tbl-train-rpart
#| tbl-cap: "Average runtime in milliseconds of training `rpart` depending on the framework."
table = setDT(summary(bm_rpart$bm_1_1))[1:3]
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]

knitr::kable(
  x = table[, list(framework, lq, median, uq)],
  digits = 0,
  col.names = c("Framework", "LQ", "Median", "UQ"))
```

```{r}
#| echo: false
#| label: tbl-train-ranger
#| tbl-cap: "Average runtime in milliseconds of training `ranger` depending on the framework."
table = setDT(summary(bm_ranger$bm_1_1))[1:3]
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]

knitr::kable(
  x = table[, list(framework, lq, median, uq)],
  digits = 0,
  col.names = c("Framework", "LQ", "Median", "UQ"))
```

:::

## Resample Sequential

We proceed to evaluate the runtime performance of the resampling functions within both frameworks, specifically under conditions without parallelization.
This step involves the generation of resampling splits, including 3-fold, 6-fold, and 9-fold cross-validation.
Additionally, we run a 100 times repeated 3-fold cross-validation.

We generate the same resampling splits for both frameworks.
This consistency is key to ensuring that any observed differences in runtime are attributable to the frameworks themselves, rather than variations in the resampling process.

In our pursuit of a fair and balanced comparison, we address certain inherent differences between the two frameworks.
Notably, `tidymodels` inherently includes scoring of the resampling results as part of its process.
To align the comparison, we replicate this scoring step in `mlr3`, thus maintaining a level field for evaluation.
Furthermore, `mlr3` inherently saves predictions during the resampling process.
To match this, we activate the saving of the predictions in `tidymodels`.

```{r}
#| eval: false

# tidymodels resample
control = control_grid(save_pred = TRUE)
metrics = metric_set(accuracy)

tm_wf =
  workflow() %>%
  add_model(tm_mod) %>%
  add_formula(formula)

fit_resamples(tm_wf, folds, metrics = metrics, control = control)

# mlr3 resample
measure = msr("classif.acc")

rr = resample(task, learner, resampling)
rr$score(measure)
```

When resampling the fast-fitting `rpart` model, `mlr3` demonstrates a notable edge in speed, as detailed in @tbl-resample-sequential-rpart.
In contrast, when it comes to resampling the more computationally intensive `ranger` models, the performance of `tidymodels` and `mlr3` converges closely (@tbl-resample-sequential-ranger).
This parity in performance is particularly noteworthy, considering the differing internal mechanisms and optimizations of `tidymodels` and `mlr3`.
A consistent trend observed across both frameworks is a linear increase in runtime proportional to the number of folds in cross-validation (@fig-resample-sequential).

:::{layout-ncol="2"}

```{r}
#| echo: false
#| label: tbl-resample-sequential-rpart
#| tbl-cap: "Average runtime in milliseconds of `rpart` depending on the framework and resampling strategy."
table = setDT(summary(bm_rpart$bm_1_1))[4:11]
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]
table[, resampling := str_split_fixed(expr, "_", 2)[, 2]]

knitr::kable(
  x = table[, list(framework, resampling, lq, median, uq)],
  digits = 0,
  col.names = c("Framework", "Resampling", "LQ", "Median", "UQ"))
```

```{r}
#| echo: false
#| label: tbl-resample-sequential-ranger
#| tbl-cap: "Average runtime in milliseconds of `ranger` depending on the framework and resampling strategy."
table = setDT(summary(bm_ranger$bm_1_1))[4:11]
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]
table[, resampling := str_split_fixed(expr, "_", 2)[, 2]]

knitr::kable(
  x = table[, list(framework, resampling, lq, median, uq)],
  digits = 0,
  col.names = c("Framework", "Resampling", "LQ", "Median", "UQ"))
```

:::

```{r}
#| echo: false
#| label: fig-resample-sequential
#| fig-cap: "Average runtime, measured in milliseconds, for cross-validations using `rpart` (displayed on the left) and `ranger` (on the right). The comparison encompasses variations across different frameworks and the number of folds in the cross-validation."
res_1_rpart = as.data.table(bm_rpart$bm_1_1)
res_1_rpart[c("mlr3_cv3", "tidymodels_cv3"), folds := 3, on = "expr"]
res_1_rpart[c("mlr3_cv6", "tidymodels_cv6"), folds := 6, on = "expr"]
res_1_rpart[c("mlr3_cv9", "tidymodels_cv9"), folds := 9, on = "expr"]
res_1_rpart[c("mlr3_rcv100", "tidymodels_rcv100"), folds := 100, on = "expr"]
res_1_rpart[c("mlr3_cv3", "mlr3_cv6",  "mlr3_cv9", "mlr3_rcv100"), framework := "mlr3", on = "expr"]
res_1_rpart[c("tidymodels_cv3", "tidymodels_cv6",  "tidymodels_cv9", "tidymodels_rcv100"), framework := "tidymodels", on = "expr"]
res_1_rpart[, folds := factor(folds)]
res_1_rpart[, framework := factor(framework)]
res_1_rpart = res_1_rpart[, list(time = median(time)), by = c("expr", "framework", "folds")]
res_1_rpart[, time := time / 1e+6]

p1 = ggplot(res_1_rpart[c("mlr3_cv3", "tidymodels_cv3", "mlr3_cv6", "tidymodels_cv6", "mlr3_cv9", "tidymodels_cv9"), , on = "expr"],
  aes(x = folds, y = time, color = framework, group = framework)) +
  geom_point() +
  geom_line() +
  xlab("Number of Folds") +
  ylab("Runtime (ms)") +
  labs(color = "Framework") +
  ggtitle("rpart") +
  theme_minimal()

res_1_ranger = as.data.table(bm_ranger$bm_1_1)
res_1_ranger[c("mlr3_cv3", "tidymodels_cv3"), folds := 3, on = "expr"]
res_1_ranger[c("mlr3_cv6", "tidymodels_cv6"), folds := 6, on = "expr"]
res_1_ranger[c("mlr3_cv9", "tidymodels_cv9"), folds := 9, on = "expr"]
res_1_ranger[c("mlr3_rcv100", "tidymodels_rcv100"), folds := 100, on = "expr"]
res_1_ranger[c("mlr3_cv3", "mlr3_cv6",  "mlr3_cv9", "mlr3_rcv100"), framework := "mlr3", on = "expr"]
res_1_ranger[c("tidymodels_cv3", "tidymodels_cv6",  "tidymodels_cv9", "tidymodels_rcv100"), framework := "tidymodels", on = "expr"]
res_1_ranger[, folds := factor(folds)]
res_1_ranger[, framework := factor(framework)]
res_1_ranger = res_1_ranger[, list(time = median(time)), by = c("expr", "framework", "folds")]
res_1_ranger[, time := time / 1e+6]

p2 = ggplot(res_1_ranger[c("mlr3_cv3", "tidymodels_cv3", "mlr3_cv6", "tidymodels_cv6", "mlr3_cv9", "tidymodels_cv9"), , on = "expr"],
  aes(x = folds, y = time, color = framework, group = framework)) +
  geom_point() +
  geom_line() +
  xlab("Number of Folds") +
  ylab("Runtime (ms)") +
  labs(color = "Framework") +
  ggtitle("ranger") +
  theme_minimal()

legend = get_legend(
  p1 +
  guides(color = guide_legend(nrow = 1)) +
  theme(legend.position = "bottom")
)

prow = plot_grid(
  p1 + theme(legend.position="none"),
  p2 + theme(
    legend.position="none",
    axis.title.y = element_blank())
)

plot_grid(prow, legend, ncol = 1, rel_heights = c(1, .1))
```

## Resample Parallel

We conducted a second set of resampling function tests, this time incorporating parallelization to explore its impact on runtime efficiency.
In this phase, we utilized `doFuture` and `doParallel`  as the primary parallelization packages for tidymodels, recognizing their robust support and compatibility.
Meanwhile, for `mlr3`, the `future`  package was employed to facilitate parallel processing.

Our findings, as presented in the respective tables (@tbl-resample-parallel-mlr3-future-rpart and @tbl-resample-parallel-mlr3-future-ranger), reveal interesting dynamics about parallelization within the frameworks.
When the number of folds in the resampling process is doubled, we observe only a marginal increase in the average runtime.
This pattern suggests a significant overhead associated with initializing the parallel workers, a factor that becomes particularly influential in the overall efficiency of the parallelization process.

In the case of the `rpart` model, the parallelization overhead appears to outweigh the potential speedup benefits, as illustrated in the left section of @fig-resample-parallel.
This result indicates that for less complex models like `rpart`, where individual training times are relatively short, the initialization cost of parallel workers may not be sufficiently offset by the reduced processing time per fold.

Conversely, for the `ranger` model, the utilization of parallelization demonstrates a clear advantage over the sequential version, as evidenced in the right section of @fig-resample-parallel.
This finding underscores that for more computationally intensive models like `ranger`, which have longer individual training times, the benefits of parallel processing significantly overcome the initial overhead of worker setup.
This differentiation highlights the importance of considering the complexity and inherent processing time of models when deciding to implement parallelization strategies in these frameworks.

:::{layout-ncol="2"}

```{r}
#| echo: false
#| label: tbl-resample-parallel-mlr3-future-rpart
#| tbl-cap: "Average runtime in milliseconds of `mlr3` with `future` and `rpart` depending on the resampling strategy."
table = setDT(summary(bm_rpart$bm_2_1))
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]
table[, resampling := str_split_fixed(expr, "_", 2)[, 2]]

knitr::kable(
  x = table[, list(resampling, lq, median, uq)],
  digits = 0,
  col.names = c("Resampling", "LQ", "Median", "UQ"))
```

```{r}
#| echo: false
#| label: tbl-resample-parallel-mlr3-future-ranger
#| tbl-cap: "Average runtime in milliseconds of `mlr3` with `future` and `ranger` depending on the resampling strategy."
table = setDT(summary(bm_ranger$bm_2_1))
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]
table[, resampling := str_split_fixed(expr, "_", 2)[, 2]]

knitr::kable(
  x = table[, list(resampling, lq, median, uq)],
  digits = 0,
  col.names = c("Resampling", "LQ", "Median", "UQ"))
```

:::

When paired with doFuture, `tidymodels` exhibits significantly slower runtime compared to the `mlr3` package utilizing `future` (@tbl-resample-parallel-tidymodels-future-rpart and @tbl-resample-parallel-tidymodels-future-ranger).
We observed that `tidymodels` exports more data to the parallel workers, which notably exceeds that of `mlr3`.
This substantial difference in data export could plausibly account for the observed slower runtime when using `tidymodels` on small tasks.

:::{layout-ncol="2"}

```{r}
#| echo: false
#| label: tbl-resample-parallel-tidymodels-future-rpart
#| tbl-cap: "Average runtime in milliseconds of `tidymodels` with `doFuture` and `rpart` depending on the resampling strategy."
table = setDT(summary(bm_rpart$bm_2_2))
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]
table[, resampling := str_split_fixed(expr, "_", 2)[, 2]]

knitr::kable(
  x = table[, list(resampling, lq, median, uq)],
  digits = 0,
  col.names = c("Resampling", "LQ", "Median", "UQ"))
```

```{r}
#| echo: false
#| label: tbl-resample-parallel-tidymodels-future-ranger
#| tbl-cap: "Average runtime in milliseconds of `tidymodels` with `doFuture` and `ranger` depending on the resampling strategy."
table = setDT(summary(bm_ranger$bm_2_2))
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]
table[, resampling := str_split_fixed(expr, "_", 2)[, 2]]

knitr::kable(
  x = table[, list(resampling, lq, median, uq)],
  digits = 0,
  col.names = c("Resampling", "LQ", "Median", "UQ"))
```

:::

The utilization of the `doParallel` package demonstrates a notable improvement in handling smaller resampling tasks.
In these scenarios, the resampling process consistently outperforms the `mlr3` framework in terms of speed.
However, it's important to note that even with this enhanced performance, the `doParallel` package does not always surpass the efficiency of the sequential version, especially when working with the `rpart` model.
This specific observation is illustrated in the left section of @fig-resample-parallel.

:::{layout-ncol="2"}

```{r}
#| echo: false
#| label: tbl-resample-parallel-tidymodels-parallel-rpart
#| tbl-cap: "Average runtime in milliseconds of `tidymodels` with `doParallel`  and `rpart` depending on the resampling strategy."
table = setDT(summary(bm_rpart$bm_2_3))
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]
table[, resampling := str_split_fixed(expr, "_", 2)[, 2]]

knitr::kable(
  x = table[, list(resampling, lq, median, uq)],
  digits = 0,
  col.names = c("Resampling", "LQ", "Median", "UQ"))
```

```{r}
#| echo: false
#| label: tbl-resample-parallel-tidymodels-parallel-ranger
#| tbl-cap: "Average runtime in milliseconds of `tidymodels` with `doParallel`  and `ranger` depending on the resampling strategy."
table = setDT(summary(bm_ranger$bm_2_3))
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]
table[, resampling := str_split_fixed(expr, "_", 2)[, 2]]

knitr::kable(
  x = table[, list(resampling, lq, median, uq)],
  digits = 0,
  col.names = c("Resampling", "LQ", "Median", "UQ"))
```

:::


```{r}
#| echo: false
#| label: fig-resample-parallel
#| fig-cap: "Average runtime, measured in milliseconds, for cross-validations using `rpart` (displayed on the left) and `ranger` (on the right). The comparison encompasses variations across different frameworks, the number of folds in the cross-validation, and the implementation of parallelization."
res_2 = setNames(map(list(bm_rpart$bm_2_1, bm_rpart$bm_2_2, bm_rpart$bm_2_3), as.data.table), c("mlr3_future", "tidymodels_future", "tidymodels_parallel"))
res_2 = rbindlist(res_2, idcol = "framework")
res_2[c("mlr3_cv3", "tidymodels_cv3"), folds := 3, on = "expr"]
res_2[c("mlr3_cv6", "tidymodels_cv6"), folds := 6, on = "expr"]
res_2[c("mlr3_cv9", "tidymodels_cv9"), folds := 9, on = "expr"]
res_2[c("mlr3_rcv100", "tidymodels_rcv100"), folds := 100, on = "expr"]
res_2[, folds := factor(folds)]
res_2[, framework := factor(framework)]
res_2 = res_2[, list(time = median(time)), by = c("expr", "framework", "folds")]
res_2[, time := time / 1e+6]

res_rpart_1_2 = rbindlist(list(
  sequential = res_1_rpart,
  parallel = res_2),
  use.names = TRUE, idcol = "mode")

p1 = ggplot(res_rpart_1_2[c("mlr3_cv3", "tidymodels_cv3", "mlr3_cv6", "tidymodels_cv6", "mlr3_cv9", "tidymodels_cv9"), , on = "expr"],
  aes(x = folds, y = time, color = framework, group = framework)) +
  geom_point() +
  geom_line(aes(linetype = mode)) +
  xlab("Number of Folds") +
  ylab("Runtime (ms)") +
  labs(color = "Framework", linetype = "Mode") +
  ggtitle("rpart") +
  theme_minimal()

res_2 = setNames(map(list(bm_ranger$bm_2_1, bm_ranger$bm_2_2, bm_ranger$bm_2_3), as.data.table), c("mlr3_future", "tidymodels_future", "tidymodels_parallel"))
res_2 = rbindlist(res_2, idcol = "framework")
res_2[c("mlr3_cv3", "tidymodels_cv3"), folds := 3, on = "expr"]
res_2[c("mlr3_cv6", "tidymodels_cv6"), folds := 6, on = "expr"]
res_2[c("mlr3_cv9", "tidymodels_cv9"), folds := 9, on = "expr"]
res_2[c("mlr3_rcv100", "tidymodels_rcv100"), folds := 100, on = "expr"]
res_2[, folds := factor(folds)]
res_2[, framework := factor(framework)]
res_2 = res_2[, list(time = median(time)), by = c("expr", "framework", "folds")]
res_2[, time := time / 1e+6]

res_ranger_1_2 = rbindlist(list(
  sequential = res_1_ranger,
  parallel = res_2),
  use.names = TRUE, idcol = "mode")

p2 = ggplot(res_ranger_1_2[c("mlr3_cv3", "tidymodels_cv3", "mlr3_cv6", "tidymodels_cv6", "mlr3_cv9", "tidymodels_cv9"), , on = "expr"],
  aes(x = folds, y = time, color = framework, group = framework)) +
  geom_point() +
  geom_line(aes(linetype = mode)) +
  xlab("Number of Folds") +
  ylab("Runtime (ms)") +
  labs(color = "Framework", linetype = "Mode") +
  ggtitle("ranger") +
  theme_minimal()

legend = get_legend(
  p1 +
  guides(color = guide_legend(nrow = 2)) +
  theme(legend.position = "bottom")
)

prow = plot_grid(
  p1 + theme(legend.position="none"),
  p2 + theme(
    legend.position="none",
    axis.title.y = element_blank())
)

plot_grid(prow, legend, ncol = 1, rel_heights = c(1, .1))
```

In the context of repeated cross-validation, our findings underscore the efficacy of parallelization (@fig-resample-parallel-2). Across all frameworks tested, the adoption of parallel processing techniques yields a significant increase in speed.
This enhancement is particularly noticeable in larger resampling tasks, where the demands on computational resources are more substantial.

Interestingly, within these more extensive resampling scenarios, the `doFuture` package emerges as a more efficient option compared to `doParallel`.
This distinction is important, as it highlights the relative strengths of different parallelization packages under varying workload conditions.
While `doParallel` shows proficiency in smaller tasks, `doFuture` demonstrates its capability to handle larger, more complex resampling processes with greater speed and efficiency.

```{r}
#| echo: false
#| label: fig-resample-parallel-2
#| fig-cap: "Average runtime, measured in seconds, of a 100 times repeated 3-fold cross-validation using `rpart` (displayed on the left) and `ranger` (on the right). The comparison encompasses variations across different frameworks and the implementation of parallelization."
p1 = ggplot(res_rpart_1_2[c("tidymodels_rcv100", "mlr3_rcv100"), on = "expr"][, time := time / 1000],
  aes(x = framework, y = time, fill = mode)) +
  geom_col() +
  xlab("Framework") +
  ylab("Runtime (s)") +
  labs(fill = "Mode") +
  theme_minimal() +
  ggtitle("rpart") +
  theme(axis.text.x = element_text(angle = 90))

p2 = ggplot(res_ranger_1_2[c("tidymodels_rcv100", "mlr3_rcv100"), on = "expr"][, time := time / 1000],
  aes(x = framework, y = time, fill = mode)) +
  geom_col() +
  xlab("Framework") +
  ylab("Runtime (s)") +
  labs(fill = "Mode") +
  theme_minimal() +
  ggtitle("ranger") +
  theme(axis.text.x = element_text(angle = 90))

legend = get_legend(
  p1 +
  guides(color = guide_legend(nrow = 1)) +
  theme(legend.position = "bottom")
)

prow = plot_grid(
  p1 + theme(legend.position="none"),
  p2 + theme(
    legend.position="none",
    axis.title.y = element_blank())
)

plot_grid(prow, legend, ncol = 1, rel_heights = c(1, .1))
```

## Tune Sequential

We then shift our focus to assessing the runtime performance of the tuning functions.
In this phase, the `tidymodels` package is utilized to evaluate a predefined grid, comprising a specific set of hyperparameter configurations.
To ensure a balanced and comparable analysis, we employ the `"design_points"` tuner from the `mlr3tuning` package.
This approach allows us to evaluate the same grid within the `mlr3` framework, maintaining consistency across both platforms.
The grid used for this comparison contains 200 hyperparameter configurations each, for both the `rpart` and `ranger` models.
This approach helps us to understand how each framework handles the optimization of model hyperparameters, a key aspect of building effective and efficient machine learning models.

:::{layout-ncol="2"}

```{r}
#| eval: false

# tidymodels
tm_mod = decision_tree(
  cost_complexity = tune()) %>%
  set_engine("rpart",
    xval = 0) %>%
  set_mode("classification")

tm_design = data.table(
  cost_complexity = seq(0.1, 0.2, length.out = 200))

# mlr3
learner = lrn("classif.rpart",
  xval = 0,
  cp = to_tune())

mlr3_design = data.table(
  cp = seq(0.1, 0.2, length.out = 200))
```

```{r}
#| eval: false

# tidymodels
tm_mod = rand_forest(
  trees = tune()) %>%
  set_engine("ranger",
    num.threads = 1L,
    seed = 1) %>%
  set_mode("classification")

tm_design = data.table(
  trees = seq(1000, 1199))

# mlr3
learner = lrn("classif.ranger",
  num.trees = to_tune(1, 10000),
  num.threads = 1L,
  seed = 1,
  verbose = FALSE,
  predict_type = "prob")

mlr3_design = data.table(
  num.trees = seq(1000, 1199))
```

:::

We measure the runtime of the tune functions within each framework.
Both the `tidymodels` and `mlr3` frameworks are tasked with identifying the optimal hyperparameter configuration.

```{r}
#| eval: false

# tidymodels tune
tune::tune_grid(
  tm_wf,
  resamples = resamples,
  grid = design,
  metrics = metrics)

# mlr3 tune
tuner = tnr("design_points", design = design, batch_size = nrow(design))
mlr3tuning::tune(
  tuner = tuner,
  task = task,
  learner = learner,
  resampling = resampling,
  measures = measure,
  store_benchmark_result = FALSE)
```

In our sequential tuning tests, `mlr3` demonstrates a notable advantage in terms of speed.
This finding is clearly evidenced in our results, as shown in Table @tbl-tune-sequential-rpart for the `rpart` model and Table @tbl-tune-sequential-ranger for the `ranger` model.
The faster performance of `mlr3` in these sequential runs highlights its efficiency in handling the tuning process without parallelization.

:::{layout-ncol="2"}

```{r}
#| echo: false
#| label: tbl-tune-sequential-rpart
#| tbl-cap: "Average runtime in seconds of tuning 200 points of `rpart` depending on the framework."
table = setDT(summary(bm_rpart$bm_3_1))[, list(expr, sapply(.SD, function(x) x / 1000)), .SDcols = is.numeric]
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]

knitr::kable(
  x = table[, list(framework, lq, median, uq)],
  digits = 0,
  col.names = c("Framework", "LQ", "Median", "UQ"))
```

```{r}
#| echo: false
#| label: tbl-tune-sequential-ranger
#| tbl-cap: "Average runtime in seconds of tuning 200 points of `ranger` depending on the framework."
table = setDT(summary(bm_ranger$bm_3_1))[, list(expr, sapply(.SD, function(x) x / 1000)), .SDcols = is.numeric]
table[, framework := str_split_fixed(expr, "_", 2)[, 1]]

knitr::kable(
  x = table[, list(framework, lq, median, uq)],
  digits = 0,
  col.names = c("Framework", "LQ", "Median", "UQ"))
```

:::

```{r}
#| echo: false
#| label: fig-tune-sequential
#| fig-cap: "Average runtime in seconds of tuning 200 points of `rpart` (left) and `ranger` (right) depending on the framework."
res_3_rpart = as.data.table(bm_rpart$bm_3_1)
res_3_rpart[c("mlr3_200_point"), framework := "mlr3", on = "expr"]
res_3_rpart[c("tidymodels_200_point"), framework := "tidymodels", on = "expr"]
res_3_rpart[, framework := factor(framework)]
res_3_rpart = res_3_rpart[, list(time = median(time)), by = c("expr", "framework")]
res_3_rpart[, time := time / 1e+9]

# p1 = ggplot(res_3_rpart,
#   aes(x = framework, y = time)) +
#   geom_col(fill = "#F8766D") +
#   xlab("Framework") +
#   ylab("Runtime (s)") +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 90))

res_3_ranger = as.data.table(bm_ranger$bm_3_1)
res_3_ranger[c("mlr3_200_point"), framework := "mlr3", on = "expr"]
res_3_ranger[c("tidymodels_200_point"), framework := "tidymodels", on = "expr"]
res_3_ranger[, framework := factor(framework)]
res_3_ranger = res_3_ranger[, list(time = median(time)), by = c("expr", "framework")]
res_3_ranger[, time := time / 1e+9]

# p2 = ggplot(res_3_ranger,
#   aes(x = framework, y = time)) +
#   geom_col(fill = "#F8766D") +
#   xlab("Framework") +
#   ylab("Runtime (s)") +
#   theme_minimal() +
#   theme(
#     axis.text.x = element_text(angle = 90),
#     axis.title.y = element_blank())

# plot_grid(p1, p2)
```

## Tune Parallel

Concluding our analysis, we proceed to evaluate the runtime performance of the tune functions, this time implementing parallelization to enhance efficiency.
For these runs, parallelization is executed on 3 cores.

In the case of `mlr3`, we opt for the largest possible chunk size.
This strategic choice means that all points within the tuning grid are sent to the workers in a single batch, effectively minimizing the overhead typically associated with parallelization.
This approach is crucial in reducing the time spent in distributing tasks across multiple cores, thereby streamlining the tuning process.
On the other hand, the `tidymodels` package also operates with the same chunk size, but this setting is determined and managed internally within the framework.

By conducting these parallelization tests, we aim to provide a deeper understanding of how each framework handles the distribution and management of computational tasks during the tuning process, particularly in a parallel computing environment.
This final set of measurements is important in painting a complete picture of the runtime performance of the tune functions across both `tidymodels` and `mlr3` under different operational settings.

```{r}
options("mlr3.exec_chunk_size" = 200)
```

Our analysis of the parallelized tuning functions reveals that the runtimes for `mlr3` and `tidymodels` are remarkably similar.
However, subtle differences emerge upon closer inspection.
For instance, the `mlr3` package exhibits a slightly faster performance when tuning the `rpart` model, as indicated in @tbl-tune-parallel-mlr3-future-rpart.
In contrast, it falls marginally behind `tidymodels` in tuning the `ranger` model, as shown in @tbl-tune-parallel-mlr3-future-ranger.

Interestingly, when considering the specific context of a 3-fold cross-validation, the `doParallel` package outperforms `doFuture` in terms of speed, as demonstrated in @fig-tune-parallel.
This outcome suggests that the choice of parallelization package can have a significant impact on tuning efficiency, particularly in scenarios with a smaller number of folds.

A key takeaway from our study is the clear benefit of enabling parallelization, regardless of the chosen framework-backend combination.
Activating parallelization consistently enhances performance, making it a highly recommended strategy for tuning machine learning models, especially in tasks involving extensive hyperparameter exploration or larger datasets.
This conclusion underscores the value of parallel processing in modern machine learning workflows, offering a practical solution for accelerating model tuning across various computational settings.

:::{layout-ncol="2"}

```{r}
#| echo: false
#| label: tbl-tune-parallel-mlr3-future-rpart
#| tbl-cap: "Average runtime in seconds of tuning 200 points of `rpart` depending on the framework."

tab_rpart = rbindlist(list(summary(bm_rpart$bm_4_1), summary(bm_rpart$bm_4_2), summary(bm_rpart$bm_4_3)))
tab_rpart[, framework := str_split_fixed(expr, "_", 2)[, 1]]
tab_rpart[, parallelization := c("future", "doFuture", "doParallel")]

knitr::kable(
  x = tab_rpart[, list(framework, parallelization, lq, median, uq)][, list(framework, parallelization, .SD / 1000), .SDcols = is.numeric],
  digits = 0,
  col.names = c("Framework", "Backend", "LQ", "Median", "UQ"))
```

```{r}
#| echo: false
#| label: tbl-tune-parallel-mlr3-future-ranger
#| tbl-cap: "Average runtime in seconds of tuning 200 points of `ranger` depending on the framework."

tab_ranger = rbindlist(list(summary(bm_ranger$bm_4_1), summary(bm_ranger$bm_4_2), summary(bm_ranger$bm_4_3)))
tab_ranger[, framework := str_split_fixed(expr, "_", 2)[, 1]]
tab_ranger[, parallelization := c("future", "doFuture", "doParallel")]

knitr::kable(
  x = tab_ranger[, list(framework, parallelization, lq, median, uq)][, list(framework, parallelization, .SD / 1000), .SDcols = is.numeric],
  digits = 0,
  col.names = c("Framework", "Backend", "LQ", "Median", "UQ"))
```

:::

```{r}
#| echo: false
#| label: fig-tune-parallel
#| fig-cap: "Average runtime, measured in seconds, of a tuning 200 hyperparameter configurations of `rpart` (displayed on the left) and `ranger` (on the right). The comparison encompasses variations across different frameworks and the implementation of parallelization."
res_4 = setNames(map(list(bm_rpart$bm_4_1, bm_rpart$bm_4_2, bm_rpart$bm_4_3), as.data.table), c("mlr3_future", "tidymodels_future", "tidymodels_parallel"))
res_4 = rbindlist(res_4, idcol = "framework")
res_4[, framework := factor(framework)]
res_4 = res_4[, list(time = median(time)), by = c("expr", "framework")]
res_4[, time := time / 1e+9]

res_3_4 = rbindlist(list(
  sequential = res_3_rpart,
  parallel = res_4),
  use.names = TRUE, idcol = "mode")

p1 = ggplot(res_3_4,
  aes(x = framework, y = time, fill = mode)) +
  geom_col() +
  xlab("Framework") +
  ylab("Runtime (s)") +
  labs(fill = "Mode") +
  theme_minimal() +
  ggtitle("rpart") +
  theme(axis.text.x = element_text(angle = 90))

res_4 = setNames(map(list(bm_ranger$bm_4_1, bm_ranger$bm_4_2, bm_ranger$bm_4_3), as.data.table), c("mlr3_future", "tidymodels_future", "tidymodels_parallel"))
res_4 = rbindlist(res_4, idcol = "framework")
res_4[, framework := factor(framework)]
res_4 = res_4[, list(time = median(time)), by = c("expr", "framework")]
res_4[, time := time / 1e+9]

res_3_4 = rbindlist(list(
  sequential = res_3_ranger,
  parallel = res_4),
  use.names = TRUE, idcol = "mode")

p2 = ggplot(res_3_4,
  aes(x = framework, y = time, fill = mode)) +
  geom_col() +
  xlab("Framework") +
  ylab("Runtime (s)") +
  labs(fill = "Mode") +
  theme_minimal() +
  ggtitle("ranger") +
  theme(axis.text.x = element_text(angle = 90))

legend = get_legend(
  p1 +
  guides(color = guide_legend(nrow = 1)) +
  theme(legend.position = "bottom")
)

prow = plot_grid(
  p1 + theme(legend.position="none"),
  p2 + theme(
    legend.position="none",
    axis.title.y = element_blank())
)

plot_grid(prow, legend, ncol = 1, rel_heights = c(1, .1))
```

# Conclusion

Our analysis reveals that both `tidymodels` and `mlr3` exhibit comparable runtimes across key processes such as training, resampling, and tuning, each displaying its own set of strengths and efficiencies.

A notable observation is the relative overhead associated with using either framework, particularly when working with fast-fitting models like `rpart`.
In these cases, the additional processing time introduced by the frameworks is more pronounced due to the inherently short training time of `rpart` models.
This results in a higher relative overhead, reflecting the trade-offs between the convenience of a comprehensive framework and the directness of more basic approaches.

Conversely, when dealing with slower-fitting models such as `ranger`, the scenario shifts.
For these more time-intensive models, the relative overhead introduced by the frameworks diminishes significantly.
In such instances, the extended training times of the models absorb much of the frameworks' inherent overhead, rendering it relatively negligible.

In summary, while there is no outright winner in terms of overall performance, the decision to use `tidymodels` or `mlr3` should be informed by the specific requirements of the task at hand.
