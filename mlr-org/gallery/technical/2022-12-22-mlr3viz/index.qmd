---
title: "Visualization in mlr3"
description: |
  Quickly plot the mlr3 ecosystem.
categories:
  - visualization
author:
  - name: Marc Becker
    url: https://github.com/be-marc
date: 2022-12-22
---

{{< include ../_setup.qmd >}}

```{r}
#| include: false
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
set.seed(0)
future::plan("multisession")
```

# Scope

In this post, we showcase the visualization functions of the mlr3 ecosystem.
First, we introduce main functions of the package and explain the style.
Next, we start with plots of the base package mlr3.
Finally, we show highly specialized plots e.g. for Filter oder Tuning.

# Package

The `autoplot()` function uses `r ref_pkg("ggplot2")` to draw a plot for almost any mlr3 object.
The plots are styled with the `r ref("ggpplot2::theme_minimal()", "minimal theme")` and use the `r ref_pkg("viridis")` color scales.

The goal of mlr3viz are pretty plots
You can control the appearance to some extent using the `theme` argument.
However, if you need to highly customize the plot e.g. for a publication, we encourage you to check our code on on [GitHub](https://github.com/mlr-org/mlr3viz).
It is often easier to control the appearance to the last detail this way.
The `foritfy()` are handy to transform the mlr3 objects to `data.frames` that are readable by `r ref_pkg("ggplot2")`.



If you need to customize the plots, we encourage you to check out the code
Sometimes it is easier if you want a highly customized plot.


The `r ref_pkg("mlr3viz")` package includes plots for various mlr3 objects.

# Tasks

## Classification Task

We begin with plots of the classification task `r ref("mlr_tasks", "Palmer Penguins")`.
We plot the class frequency of the target variable.

```{r}
#| message: false
library(mlr3verse)
library(mlr3viz)

task = tsk("penguins")
task$select(c("body_mass", "bill_length"))

autoplot(task, type = "target")
```

The `"duo"` plot shows the distribution of multiple features.

```{r}
#| warning: false
autoplot(task, type = "duo")
```

The `"pairs"` plot shows the pairwise comparison of multiple features.
The classes of the target variable are shown in different colors.

```{r}
#| warning: false
autoplot(task, type = "pairs")
```

## Regression Task

Next, we plot the regression task `r ref("mlr_tasks", "mtcars")`.
We create a boxplot of the target variable.

```{r}
task = tsk("mtcars")
task$select(c("am", "carb"))

autoplot(task, type = "target")
```

The `"pairs"` plot shows the pairwise comparison of mutiple features and the target variable.

```{r}
autoplot(task, type = "pairs")
```

## Cluster Task

Finally, we plot the cluster task `r ref("mlr_tasks", "US Arrests")`.
The `"pairs"` plot shows the pairwise comparison of mutiple features.

```{r}
library(mlr3cluster)

task = mlr_tasks$get("usarrests")

autoplot(task, type = "pairs")
```

# Learner

## Glmnet

```{r}
library(mlr3data)

task = tsk("ilpd")
task$select(setdiff(task$feature_names, "gender"))
learner = lrn("classif.glmnet")
learner$train(task)

autoplot(learner)
```

```{r}
task = tsk("mtcars")
learner = lrn("regr.glmnet")
learner$train(task)

autoplot(learner)
```

## Rpart

The classifcation tree of an rpart learner can be plotted.
We have to fit the learner with `keep_model = TRUE` to keep the model object.

```{r}
task = tsk("penguins")
learner = lrn("classif.rpart", keep_model = TRUE)
learner$train(task)

autoplot(learner)
```

We can also plot regression trees.

```{r}
task = tsk("mtcars")
learner = lrn("regr.rpart", keep_model = TRUE)
learner$train(task)

autoplot(learner)
```

## ClustHierachical

The `"dend"` plot shows the result of hierarchical clustering of the data.

```{r}
#| warning: false

library(mlr3cluster)

task = tsk("usarrests")
learner = lrn("clust.hclust")
learner$train(task)

autoplot(learner, type = "dend", task = task)
```

The `"scree"` type plots the number of clusters and the height.

```{r}
autoplot(learner, type = "scree")
```

# Prediction

## Classification

We plot the predictions of a classification learner.
The `"stacked"` plot shows the predicted and true class labels.

```{r}
task = tsk("spam")
learner = lrn("classif.rpart", predict_type = "prob")
pred = learner$train(task)$predict(task)

autoplot(pred, type = "stacked")
```

The ROC curve plots the true positive rate against the false positive rate at different thresholds.

```{r}
autoplot(pred, type = "roc")
```


The precision-recall curve plots the precision against the recall at different thresholds.

```{r}
autoplot(pred, type = "prc")
```

The `"threshold"` plot varies the threshold of a binary classification and plots against the resulting performance.

```{r}
autoplot(pred, type = "threshold")
```

## Regression

The predictions of a regression learner are often presented as a scatterplot of truth and predicted response.

```{r}
task = tsk("boston_housing")
learner = lrn("regr.rpart")
pred = learner$train(task)$predict(task)

autoplot(pred, type = "xy")
```

Additionally, plot the response with the residuals.

```{r}
autoplot(pred, type = "residual")
```

We can also plot the distribution of the residuals.

```{r}
autoplot(pred, type = "histogram", binwidth = 1)
```

## Cluster

The predictions of a cluster learner are often presented as a scatterplot of the data points colored by the cluster.

```{r}
#| warning: false

library(mlr3cluster)

task = tsk("usarrests")
learner = lrn("clust.kmeans", centers = 3)
pred = learner$train(task)$predict(task)

autoplot(pred, task, type = "scatter")
```

The `"sil"` plot shows the silhouette width of the clusters.
The dashed line is the mean silhouette width.

```{r}
autoplot(pred, task, type = "sil")
```

The `"pca"` plot shows the first two principal components of the data colored by the cluster.

```{r}
autoplot(pred, task, type = "pca")
```

# Resample Result

## Classification

The `"boxplot"` shows the distribution of the performance measures.

```{r}
task = tsk("sonar")
learner = lrn("classif.rpart", predict_type = "prob")
resampling = rsmp("cv")
rr = resample(task, learner, resampling)

autoplot(rr, type = "boxplot")
```

We can also plot the distribution of the performance measures as a "`histogram`".

```{r}
autoplot(rr, type = "histogram", bins = 30)
```

The ROC curve plots the true positive rate against the false positive rate at different thresholds.

```{r}
autoplot(rr, type = "roc")
```

The precision-recall curve plots the precision against the recall at different thresholds.

```{r}
autoplot(rr, type = "prc")
```

The `"prediction"` plot shows two features and the predicted class in the background.
Points mark the observations of the test set and the color presents the truth.

```{r}
#| warning: false

task = tsk("pima")
task$filter(seq(100))
task$select(c("age", "glucose"))
learner = lrn("classif.rpart")
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)

autoplot(rr, type = "prediction")
```

Alternatively, we can plot class probabilities.

```{r}
#| warning: false

task = tsk("pima")
task$filter(seq(100))
task$select(c("age", "glucose"))
learner = lrn("classif.rpart", predict_type = "prob")
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)

autoplot(rr, type = "prediction")
```

In addition to the test set, we can also plot the train set.

```{r}
#| warning: false

task = tsk("pima")
task$filter(seq(100))
task$select(c("age", "glucose"))
learner = lrn("classif.rpart", predict_type = "prob", predict_sets = c("train", "test"))
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)

autoplot(rr, type = "prediction", predict_sets = c("train", "test"))
```

The `"prediction"` plot can also show categorical features.

```{r}
#| warning: false

task = tsk("german_credit")
task$filter(seq(100))
task$select(c("housing", "employment_duration"))
learner = lrn("classif.rpart")
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)

autoplot(rr, type = "prediction")
```

## Regression

The "`prediction`" plot shows one feature and the response.
Points mark the observations of the test set.

```{r}
task = tsk("boston_housing")
task$select("age")
task$filter(seq(100))
learner = lrn("regr.rpart")
resampling = rsmp("cv", folds  = 3)
rr = resample(task, learner, resampling, store_models = TRUE)

autoplot(rr, type = "prediction")
```

Additionally, we can add confidence bounds.

```{r}
task = tsk("boston_housing")
task$select("age")
task$filter(seq(100))
learner = lrn("regr.lm", predict_type = "se")
resampling = rsmp("cv", folds  = 3)
rr = resample(task, learner, resampling, store_models = TRUE)

autoplot(rr, type = "prediction")
```

And add the train set.

```{r}
task = tsk("boston_housing")
task$select("age")
task$filter(seq(100))
learner = lrn("regr.lm", predict_type = "se", predict_sets = c("train", "test"))
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)

autoplot(rr, type = "prediction", predict_sets = c("train", "test"))
```

We can also add the prediction surface to the background.

```{r}
task = tsk("boston_housing")
task$select(c("age", "rm"))
task$filter(seq(100))
learner = lrn("regr.rpart")
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)

autoplot(rr, type = "prediction")
```

# Benchmark Result

We show the performance distribution of a benchmark result with boxplots.

```{r}
tasks = tsks(c("pima", "sonar"))
learner = lrns(c("classif.featureless", "classif.rpart", "classif.xgboost"), predict_type = "prob")
resampling = rsmps("cv")
bmr = benchmark(benchmark_grid(tasks, learner, resampling))

autoplot(bmr, type = "boxplot")
```


```{r}
tasks = tsk("pima")
learner = lrns(c("classif.featureless", "classif.rpart", "classif.xgboost"), predict_type = "prob")
resampling = rsmps("cv")
bmr = benchmark(benchmark_grid(tasks, learner, resampling))
```

We plot an roc curve for each learner.

```{r}
autoplot(bmr, type = "roc")
```

Alternatively, we can plot precision-recall curves.

```{r}
autoplot(bmr, type = "prc")
```

# Tuning Instance

We tune the hyperparameters of a decision tree on the sonar task.
The "`performance`" plot shows the performance over batches.

```{r}
#| message: false

library(mlr3tuning)
library(mlr3tuningspaces)
library(mlr3learners)

instance = tune(
  method = tnr("gensa"),
  task = tsk("sonar"),
  learner = lts(lrn("classif.rpart")),
  resampling = rsmp("holdout"),
  measures = msr("classif.ce"),
  term_evals = 100
)

autoplot(instance, type = "performance")
```


The `"parameter"` plot shows the performance for each hyperparameter setting.

```{r}
```{r}
autoplot(instance, type = "parameter", cols_x = c("cp", "minsplit"))
```

The `"marginal"` plot shows the performance of different hyperparameter values.
The color indicates the batch.

```{r}
autoplot(instance, type = "marginal", cols_x = "cp")
```

The `"parallel"` plot visualizes the relationship of hyperparameters.

```{r}
autoplot(instance, type = "parallel")
```

We plot `cp` against `minsplit` and color the points by the performance.

```{r}
autoplot(instance, type = "points", cols_x = c("cp", "minsplit"))
```

Next, we plot all hyperparameters against each other.

```{r}
autoplot(instance, type = "pairs")
```

We plot the performance surface of two hyperparameters.
The surface is interpolated with a learner.

```{r}
autoplot(instance, type = "surface", cols_x = c("cp", "minsplit"), learner = mlr3::lrn("regr.ranger"))
```

# Filter

We plot filter scores for the mtcars task.

```{r}
library(mlr3filters)

task = tsk("mtcars")
f = flt("correlation")
f$calculate(task)

autoplot(f, n = 5)
```

# Conclusion

The `r ref_pkg("mlr3viz")` package brings together the visualization functions of the mlr3 ecosystem.


We are looking forward to new visualizations.
You can suggest new plots on [GitHub](https://github.com/mlr-org/mlr3viz).
