---
title: "Wrapper-based Ensemble Feature Selection"
description: |
  Find the most stable and predictive features using multiple learners and resampling techniques.
author:
  - name: John Zobolas
    orcid: 0000-0002-3609-8674
    url: https://github.com/bblodfon
date: 2025-01-12
bibliography: ../../bibliography.bib
---

{{< include ../../_setup.qmd >}}

```{r efs-001}
#| include: false
efs = readRDS(file = "efs.rds")
```

## Intro

Some papers from which we draw the ideas for this tutorial:

- **Stability selection**, i.e. drawing multiple subsamples from a dataset and performing feature selection on each [@Meinshausen2010].
- The **ensemble idea** for feature selection, i.e. using multiple methods or models to perform feature selection on a dataset [@Saeys2008].

In this post we will show how we can use the `mlr3fselect` R package to perform *wrapped*-based ensemble feature selection on a given dataset.

:::{.callout-note}
We also support *embedded-based* ensemble feature selection, see the function [`mlr3fselect::embedded_ensemble_fselect()`](https://mlr3fselect.mlr-org.com/reference/embedded_ensemble_fselect.html) for more details.
:::

## Libraries

```{r efs-002}
#| output: false
library(mlr3)
library(mlr3fselect)
library(fastVoteR) # for feature ranking
library(mlr3learners) # fpr xgboost, svm learners
library(paradox)
library(future)
library(progressr)
library(mlr3viz)
library(ggplot2)
```

## Dataset

We will use the `sonar` dataset, which is a binary classification task:
```{r efs-003}
task = tsk("sonar")
task
```

## EFS Workflow

The **ensemble feature selection (EFS)** workflow is the following (in parentheses we provide the arguments for the [`mlr3fselect::ensemble_fselect()`](https://mlr3fselect.mlr-org.com/reference/ensemble_fselect.html) function that implements this process):

1. Repeatedly split a dataset to **train/test sets** (`init_resampling`), e.g. by subsampling $B$ times.
2. Choose $M$ **learners** (`learners`).  
3. Perform **wrapped-based feature selection** on each train set from (1) using each of the models from (2).
This process results in a 'best' feature (sub)set and a final trained model using these best features, for each combination of train set and learner ($B \times M$ combinations in total).
4. Score the final models on the respective test sets.

To guide the feature selection process (3) we need to choose:

- An optimization algorithm (`fselector`), e.g. Recursive Feature Elimination (RFE)
- An inner resampling technique (`inner_resampling`), e.g. 5-fold cross-validation (CV)
- An inner measure (`inner_measure`), e.g. classification error
- A stopping criterion for the feature selection (`terminator`), i.e. how many iterations should the optimization algorithm run

:::{.callout-note}
- Choosing RFE as the feature selection optimization algorithm means that all `learners` need to have the `"importance"` property.
- The `inner_measure` (used for finding the best feature subset in each train set) and `measure` (assesses performance on each test set) can be different.
:::

### Parallelization

Internally, `ensemble_fselect()` performs a full `mlr3::benchmark()`, the results of which can be stored with the argument `store_benchmark_result`.
The process is fully parallelizable, where **every job is a (train set, learner) combination**.
So it's better to make sure that each RFE optimization (done via [mlr3fselect::auto_fselector](https://mlr3fselect.mlr-org.com/reference/auto_fselector.html)) is single-threaded.

Below we show the code that setups the configuration for the parallelization:

```{r efs-004}
#| eval: false

# Logging
lgr::get_logger("bbotk")$set_threshold("warn")
lgr::get_logger("mlr3" )$set_threshold("warn")

# Parallelization of XGBoost (use 1 core when training on a dataset)
Sys.setenv(OMP_NUM_THREADS = 1)
Sys.setenv(OMP_THREAD_LIMIT = 1)
Sys.setenv(MKL_NUM_THREADS = 1) # MKL is an Intel-specific thing
# Package-specific settings
try(data.table::setDTthreads(1))
try(RhpcBLASctl::blas_set_num_threads(1))
try(RhpcBLASctl::omp_set_num_threads(1))

# Progress bars
options(progressr.enable = TRUE)
handlers(global = TRUE)
handlers("progress")

# Parallelization for EFS: use 10 cores
plan("multisession", workers = 10)
```

### RFE

For each (train set, learner) combination we will run a Recursive Feature Elimination (RFE) optimization algorithm.
We configure the algorithm to start with all features of the `task`, remove the 80% less important features in each iteration, and stop when 2 features are reached.
In each RFE iteration, a 5-fold CV resampling of the given `task` takes place and a `learner` is trained and used for prediction on the test folds.
The outcome of each RFE iteration is the average CV error (performance estimate) and the feature importances (by default the average of the feature ranks from each fold).
Practically, for the `sonar` dataset, we will have **15 RFE iterations**, with the following feature subset sizes:

`60 48 38 30 24 19 15 12 10 8  6  5  4  3  2`

The best feature set will be chosen as the one with the **lowest 5-fold CV error**. i.e. the best performance estimate in the inner resampling.

In `mlr3` code, we specify the RFE `fselector` as:
```{r efs-005}
rfe = fs("rfe", n_features = 2, feature_fraction = 0.8)
```

See [this gallery post](https://mlr-org.com/gallery/optimization/2023-02-07-recursive-feature-elimination/) for more details on RFE optimization.

### Learners

We define a `list()` with the following classification `learners` (parameters are set at default values): 

1. XGBoost with early stopping
2. A tree
3. A random forest (RF)
4. A Support Vector Machine (SVM)

```{r efs-006}
max_nrounds = 500

learners = list(
  lrn("classif.xgboost", id = "xgb", nrounds = max_nrounds,
      early_stopping_rounds = 20, validate = "test"),
  lrn("classif.rpart", id = "tree"),
  lrn("classif.ranger", id = "rf", importance = "permutation"),
  lrn("classif.svm", id = "svm", type = "C-classification", kernel = "linear")
)
```

:::{.callout-note}
It is possible to perform tuning while also performing wrapper-based feature selection. This practically means that we would use an `AutoTuner` learner with its own inner resampling scheme and tuning space in the above list.
The whole process would then be a double (nested) cross-validation with outer loop the $B$ subsample iterations, which is computationally taxing.
Models that need minimum to no tuning (e.g. like Random Forests) are therefore ideal candidates for wrapper-based ensemble feature selection.
:::

### Callbacks

Since SVM doesn't support `importance` scores by itself, we convert the coefficients of the trained linear SVM model to importance scores via a callback:
```{r efs-007}
svm_rfe = clbk("mlr3fselect.svm_rfe")
svm_rfe
```

---

Also, since the XGBoost learner performs *simultaneously* feature selection (via RFE) and internal tuning (via early stopping - using as validation sets the test folds in the inner cross-validation resampling scheme), we need to define the following callback:
```{r efs-008}
internal_ss = ps(
  nrounds = p_int(upper = max_nrounds, aggr = function(x) as.integer(mean(unlist(x))))
)

xgb_clbk = clbk("mlr3fselect.internal_tuning", internal_search_space = internal_ss)
xgb_clbk
```

This practically sets the boosting rounds of the final XGBoost model as the average boosting rounds from each subsequent training fold (corresponding to the model trained with the 'best' feature subset).
For example, since we're performing a 5-fold inner CV, we would have 5 different early-stopped boosting `nrounds`, from which we will use the average value to train the final XGBoost model using the whole train set.

---

For all learners we will prefer **sparser models during the RFE optimization process**.
This means that across all RFE iterations, we will choose as 'best' feature subset the one that has the minimum number of features and its performance is **within one standard error** of the feature set with the best performance (e.g. the lowest classification error).
This can be achieved with the following callback:
```{r efs-009}
one_se_clbk = clbk("mlr3fselect.one_se_rule")
one_se_clbk
```

## Execute EFS

Using the [`mlr3fselect::ensemble_fselect()`](https://mlr3fselect.mlr-org.com/reference/ensemble_fselect.html) function, we split the `sonar` task to $B = 50$ subsamples (each corresponding to a 80%/20% train/test set split) and perform RFE in each train set using each of the $M = 4$ learners.

For a particular (train set, learner) combination, the RFE process will evaluate the $15$ feature subsets mentioned above.
Using the inner 5-fold CV resampling scheme, the average CV classification error will be used to find the best feature subset.
Using only features from this best feature set, a final model will be trained using all the observations from each trained set.
Lastly, the performance of this final model will be assessed on the corresponding test set using the classification accuracy metric.

```{r efs-010}
#| eval: false

set.seed(42)
efs = ensemble_fselect(
  fselector = rfe,
  task = task,
  learners = learners,
  init_resampling = rsmp("subsampling", repeats = 50, ratio = 0.8),
  inner_resampling = rsmp("cv", folds = 5),
  inner_measure = msr("classif.ce"),
  measure = msr("classif.acc"),
  terminator = trm("none"),
  # following list must be named with the learners' ids
  callbacks = list(
    xgb  = list(one_se_clbk, xgb_clbk),
    tree = list(one_se_clbk),
    rf   = list(one_se_clbk),
    svm  = list(one_se_clbk, svm_rfe)
  ),
  store_benchmark_result = FALSE
)
```

The result is stored in an [`EnsembleFSResult`](https://mlr3fselect.mlr-org.com/reference/ensemble_fs_result.html) object, which can use to visualize the results, rank the features and assess the stability of the ensemble feature selection process, among others.

# Analyze EFS Results

## Result Object 

Printing the result object provides some initial information:
```{r efs-011}
print(efs)
```

As we can see, we have $M \times B = 4 \times 50 = 200$ (train set, learner) combinations.
We can inspect the actual `data.table` result:

```{r efs-012}
efs$result
```

For each learner (`"learner_id"`) and dataset subsample (`"resampling_iteration"`) we get:

- The 'best' feature subsets (`"features"`)
- The number of 'best' features (`"nfeatures"`)
- The importances for these 'best' features (`"importance"`) - this output column we get only because RFE optimization was used
- The inner optimization performance scores on the train sets (`"classif.ce_inner"`)
- The performance scores on the test sets (`"classif.acc"`)

Since there are two ways in this process to evaluate performance, we can always check which is the **active measure**:
```{r efs-013}
efs$active_measure
```

By default the active measure is the `"outer"`, i.e. the measure used to evaluate each learner's performance in the test sets.
In our case that was the classification accuracy:

```{r efs-014}
efs$measure
```

:::{.callout-note}
In the following sections we can use the inner optimization scores (i.e. `"classif.ce_inner"`) by executing `efs$set_active_measure("inner")`.
This affects all methods and plots that use performance scores.
:::

## Performance

We can view the **performance scores of the different learners** used in the ensemble feature selection process.
Each box represents the distribution of scores across different resampling iterations for a particular learner.

```{r efs-015}
#| warning: false

autoplot(efs, type = "performance") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal(base_size = 14)
```

We observe that RF has better classification accuracy on the test sets of the $50$ subsamples, followed by XGBoost, then the SVM and last the tree model.

## Number of Selected Features

Continuing, we can plot **the number of features selected by each learner** in the different resampling iterations:

```{r efs-016}
#| warning: false

autoplot(efs, type = "n_features") +
  scale_fill_brewer(palette = "Set1") +
  scale_y_continuous(breaks = seq(0, 60, 10)) +
  theme_minimal(base_size = 14)
```

We observe that RF needed more features to achieve the best average performance, followed by SVM, then XGBoost and the tree model was the model using the least features (but with worst performance).

## Pareto Plot

Both performance scores and number of features selected by the RFE optimization process can be visualized jointly in the Pareto plot.
Here we also draw the **Pareto front**, i.e. the set of points that represent the trade-off between the number of features and performance (classification accuracy).
As we see below, these points are derived from multiple learners and resamplings:
```{r efs-017}
#| warning: false

autoplot(efs, type = "pareto") +
  scale_color_brewer(palette = "Set1") +
  theme_minimal(base_size = 14) +
  labs(title = "Empirical Pareto front")
```

We can also draw an **estimated Pareto front curve** by fitting a linear model with the inverse of the number of selected features ($1/x$) of the empirical Pareto front as input, and the associated performance scores as output:

```{r efs-018}
#| warning: false

autoplot(efs, type = "pareto", pareto_front = "estimated") +
  scale_color_brewer(palette = "Set1") +
  theme_minimal(base_size = 14) +
  labs(title = "Estimated Pareto front")
```

## Knee Point Identification

No matter the type of Pareto front that we chose, specialized methods are available to identify **knee points**, i.e. points of the Pareto front with an **optimal trade-off between performance and number of selected features**.

By default, we use the geometry-based *Normal-Boundary Intersection* (NBI) method.
This approach calculates the perpendicular distance of each point from the line connecting the first (worst performance, minimum number of features) and last (best performance, maximum number of features) point of the Pareto front.
The knee point is then identified as the point with the maximum distance from this line [@Das1999].

Using the empirical and estimated Pareto fronts, we observe that the optimal knee points correspond to different numbers of features:
```{r efs-019}
efs$knee_points()
efs$knee_points(type = "estimated")
```

:::{.callout-tip title="Number of features cutoff"}
The number of features at the identified knee point provides a cutoff for prioritizing features when working with a ranked feature list (see "Feature Ranking" section).
:::

## Stability

The `stabm` R package [@Bommert2021] implements many measures for the assessment of the **stability of feature selection**, i.e. the similarity between the selected feature sets (`"features"` column in the `EnsembleFSResult` object).
We can use these measures to assess and visualize the stability across all resampling iterations and learners (`global = "TRUE"`) or per each learner separately (`global = "FALSE"`).

The default stability measure is the **Jaccard Index**:
```{r efs-020}
efs$stability(stability_measure = "jaccard", global = TRUE)
```

Stability per learner:
```{r efs-021}
efs$stability(stability_measure = "jaccard", global = FALSE)
```

We observe that the RF model was the most stable in identifying similar predictive features across the different subsamples of the dataset, while the SVM model the least stable.

To visualize stability, the following code generates a stability barplot:
```{r efs-022}
#| warning: false

autoplot(efs, type = "stability") +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal(base_size = 14)
```

Alternatively, the **Nogueira** stability measure can be used, which unlike the Jaccard Index, it's a chance-corrected similarity measure [@Nogueira2018]:
```{r efs-023}
#| warning: false
autoplot(efs, type = "stability", stability_measure = "nogueira", 
         stability_args = list(p = task$n_features)) +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal(base_size = 14)
```

## Feature Ranking

Using the Pareto method, we demonstrated how we can identify a reasonable cutoff for the number of selected features.
Now we will focus on how to create a consensus ranked feature list based on the results of the ensemble feature selection.

The most straightforward ranking is obtained by counting how often each feature appears in the 'best' feature subsets (`"features"`).
Below we show the top 8 features, i.e. up to the cutoff derived from the knee point of the estimated Pareto front.
The column `"score"` represents these counts, while the column `"norm_score"` is the **feature selection frequency** or also known as **selection probability** [@Meinshausen2010]:
```{r efs-024}
efs$feature_ranking(method = "av", use_weights = FALSE, committee_size = 8)
```

In the language of Voting Theory, we call the method that generates these counts *approval voting* (`method = "av"`) [@Lackner2023].
Using this framework, learners act as *voters*, features act as *candidates* and voters select certain candidates (features).
The primary objective is to compile these selections into a consensus ranked list of features (a committee).
The `committee_size` specifies how many (top-ranked) features to return.

Internally, `$feature_ranking()` uses the [`fastVoteR`](https://bblodfon.github.io/fastVoteR/) R package, which supports more advanced ranking methods.
For example, we can perform **weighted ranking**, by considering the varying performance (accuracy) of each learner.
This results in the same top 8 features but with slightly different ordering:
```{r efs-025}
efs$feature_ranking(method = "av", use_weights = TRUE, committee_size = 8)
```

Additionally, alternative ranking methods are supported.
Below, we use *satisfaction approval voting* (SAV), which results in a different feature ranking (e.g. `"V10"` feature is now included in the top 8 features but not `"V4"`):
```{r efs-026}
efs$feature_ranking(method = "sav", committee_size = 8)
```

# EFS-based Feature Selection

The ultimate goal of the ensemble feature selection process is to identify predictive and stable features.
By combining the ranked feature list with the Pareto-derived cutoff, we can select the final set of features and subset the original dataset for further modeling:
```{r efs-027}
n_features = efs$knee_points(type = "estimated")$n_features
res = efs$feature_ranking(method = "sav", committee_size = n_features)
task$select(cols = res$feature)
task
```

{{< include ../../_session_info.qmd >}}

## References
