---
title: "Benchmarking mlr3"
description: |

author:
  - name: Marc Becker
    url: https://github.com/be-marc
date: 2023-02-11
bibliography: ../../bibliography.bib
knitr:
  opts_chunk:
    R.options:
      datatable.print.nrows: 6
---

{{< include ../_setup.qmd >}}

# Scope

Frameworks significantly speed up and simplify the development process of machine learning models.

Estimating the performance of models with resampling is a fundamental task in machine learning.
In this article, we will benchmark the `resample()` function of `r ref_pkg("mlr3")` against an implementation in base R.

```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")

library(microbenchmark)
library(mlr3verse)
```

# Cross-Validation

We run the benchmark on the Palmer Penguins data set.
It is a medium-sized data set.
For the base R implemtation, we extract the data from the task.

```{r}
task = tsk("penguins")
data = task$data()
```

The cross-validation in base R splits the data set into folds and then iterates the resampling splits.
Instead of fitting a model on the train set and predicting on the test set, we suspend the execution for a specific time interval.
We can simulate different training times this way.

```{r}
base_cv = function(data, folds, sleep) {
  ids = seq(nrow(data))
  splits = split(ids, sample(0:(length(ids) -1) %% folds + 1L))

  lapply(seq(folds), function(i) {
    train_data = data[-i, ]
    Sys.sleep(sleep / 2)
    test_data = data[i, ]
    Sys.sleep(sleep / 2)
  })
}
```

Running a 3-fold cross-validation looks like this.
We simulate a train and predict time of 1 second.

```{r}
base_cv(data, folds = 3, sleep = 1)
```

The counterpart to the base R version looks like this in mlr3.
We use the `r ref("mlr_learners_classif.debug", "debug learner")` that can be also stopped for a specific time interval.

```{r}
learner = lrn("classif.debug", sleep_train = function() 0.5, sleep_predict = function() 0.5)
resampling = rsmp("cv", folds = 3)

resample(task, learner, resampling)
```

# Benchmark

We run a benchmark with the base R and mlr3 implementation on different training times.
The process is repeated 10 times to get reliable timings.

```{r}
bmr = microbenchmark(
  mlr3_cv3_1000 = resample(task, lrn("classif.debug", sleep_train = function() 0.5, sleep_predict = function() 0.5), rsmp("cv", folds = 3)),
  base_cv3_1000 = base_cv(data, 3, 1),
  mlr3_cv3_100 = resample(task, lrn("classif.debug", sleep_train = function() 0.05, sleep_predict = function() 0.05), rsmp("cv", folds = 3)),
  base_cv3_100 = base_cv(data, 3, 0.1),
  mlr3_cv3_10 = resample(task, lrn("classif.debug", sleep_train = function() 0.005, sleep_predict = function() 0.005), rsmp("cv", folds = 3)),
  base_cv3_10 = base_cv(data, 3, 0.01),
  mlr3_cv3_1 = resample(task, lrn("classif.debug", sleep_train = function() 0.0005, sleep_predict = function() 0.0005), rsmp("cv", folds = 3)),
  base_cv3_1 = base_cv(data, 3, 0.001),
  unit = "ms",
  times = 10
)
```
