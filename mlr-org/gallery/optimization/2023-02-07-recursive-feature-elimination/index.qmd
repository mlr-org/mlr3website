---
title: "Recursive Feature Elimination on the Sonar Data Set"
description: |
  Utilize the built-in feature importance of models.
categories:
  - feature selection
  - classification
author:
  - name: Marc Becker
    url: https://github.com/be-marc
date: 2023-02-07
bibliography: ../../bibliography.bib
knitr:
  opts_chunk:
    R.options:
      datatable.print.nrows: 12
---

{{< include ../../_setup.qmd >}}

```{r 2023-02-07-recursive-feature-elimination-001}
#| include: false
requireNamespace("gbm")

future::plan("sequential")

lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
set.seed(1)
```

# Scope

Feature selection is the process of finding an optimal subset of features in order to improve the performance, interpretability and robustness of machine learning algorithms.
In this article, we introduce the wrapper feature selection method *Recursive Feature Elimination*.
[Wrapper methods](https://mlr3book.mlr-org.com/feature-selection.html#fs-wrapper) iteratively select features that optimize a performance measure.
As an example, we will search for the optimal set of features for a `r ref("mlr_learners_classif.gbm", "gradient boosting machine")` and `r ref("mlr_learners_classif.svm", "support vector machine")` on the `r ref("mlr_tasks_pima", text = "Sonar")` data set.
We assume that you are already familiar with the basic building blocks of the [mlr3 ecosystem](https://mlr-org.com/ecosystem.html).
If you are new to feature selection, we recommend reading the [feature selection chapter](https://mlr3book.mlr-org.com/feature-selection.html) of the mlr3book first.

# Recursive Feature Elimination

Recursive Feature Elimination (RFE) is a widely used feature selection method for high-dimensional data sets.
The idea is to iteratively remove the least predictive feature from a model until the desired number of features is reached.
This feature is determined by the built-in feature importance method of the model.
Currently, RFE works with support vector machines (SVM), decision tree algorithms and gradient boosting machines (GBM).
Supported learners are tagged with the `"importance"` property.
For a full list of supported learners, see the learner page on the [mlr-org website](https://mlr-org.com/learners.html) and search for `"importance"`.

@guyon_gene_2002 developed the RFE algorithm for SVMs (SVM-RFE) to select informative genes in cancer classification.
The importance of the features is given by the weight vector of a linear support vector machine.
This method was later extended to other machine learning algorithms.
The only requirement is that the models can internally measure the feature importance.
The random forest algorithm offers multiple options for measuring feature importance.
The commonly used methods are the mean decrease in accuracy (MDA) and the mean decrease in impurity (MDI).
The MDA measures the decrease in accuracy for a feature if it was randomly permuted in the out-of-bag sample.
The MDI is the total reduction in node impurity when the feature is used for splitting.
Gradient boosting algorithms like `r ref("mlr_learners_classif.xgboost", "XGBoost")`, `r ref("mlr_learners_classif.lightgbm", "LightGBM")` and `r ref("mlr_learners_classif.gbm", "GBM")` use similar methods to measure the importance of the features.

Resampling strategies can be combined with the algorithm in different ways.
The frameworks scikit-learn [@pedregosa_scikit-learn_2011] and caret [@kuhn_building_2008] implement a variant called recursive feature elimination with cross-validation (RFE-CV) that estimates the optimal number of features with cross-validation first.
Then one more RFE is carried out on the complete dataset with the optimal number of features as the final feature set size.
The RFE implementation in mlr3 can rank and aggregate importance scores across resampling iterations.
We will explore both variants in more detail below.

`r ref_pkg("mlr3fselect")` is the feature selection package of the [mlr3 ecosystem](https://mlr-org.com/ecosystem.html).
It implements the `r ref("mlr_fselectors_rfe", "RFE")` and `r ref("mlr_fselectors_rfecv", "RFE-CV")` algorithm.
We load all packages of the ecosystem with the `r ref("mlr3verse")` package.

```{r 2023-02-07-recursive-feature-elimination-002}
#| message: false

library(mlr3verse)
```

We retrieve the `r ref("mlr_fselectors_rfe", "RFE")` optimizer with the `r ref("fs()")` function.

```{r 2023-02-07-recursive-feature-elimination-003}
optimizer = fs("rfe",
  n_features = 1,
  feature_number = 1,
  aggregation = "rank")
```

The algorithm has multiple control parameters.
The optimizer stops when the number of features equals `n_features`.
The parameters `feature_number`, `feature_fraction` and `subset_size` determine the number of features that are removed in each iteration.
The `feature_number` option removes a fixed number of features in each iteration, whereas `feature_fraction` removes a fraction of the features.
The `subset_size` argument is a vector that specifies exactly how many features are removed in each iteration.
The parameters are mutually exclusive and the default is `feature_fraction = 0.5`.
Usually, RFE fits a new model in each resampling iteration and calculates the feature importance again.
We can deactivate this behavior by setting `recursive = FALSE`.
The selection of feature subsets in all iterations is then based solely on the importance scores of the first model trained with all features.
When running an RFE with a resampling strategy like cross-validation, multiple models and importance scores are generated.
The `aggregation` parameter determines how the importance scores are aggregated.
The option `"rank"` ranks the importance scores in each iteration and then averages the ranks of the features.
The feature with the lowest average rank is removed.
The option `"mean"` averages the importance scores of the features across the iterations.
The `"mean"` should only be used if the learner's importance scores can be reasonably averaged.

# Task

The objective of the `r ref("mlr_tasks_sonar", text = "Sonar")` data set is to predict whether a sonar signal bounced off a metal cylinder or a rock.
The data set includes 60 numerical features (see @fig-features).

```{r 2023-02-07-recursive-feature-elimination-004}
task = tsk("sonar")
```

```{r 2023-02-07-recursive-feature-elimination-005}
#| code-fold: true
#| column: page
#| fig-width: 12
#| fig-height: 3
#| label: fig-features
#| fig-cap: "Distribution of the first 5 features in the Sonar dataset."
#| warning: false

library(ggplot2)
library(data.table)

data = melt(as.data.table(task), id.vars = task$target_names, measure.vars = task$feature_names)
data = data[c("V1", "V10", "V11", "V12", "V13", "V14"), , on = "variable"]

ggplot(data, aes(x = value, fill = Class)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ variable, ncol = 6, scales = "free") +
  scale_fill_viridis_d(end = 0.8) +
  theme_minimal() +
  theme(axis.title.x = element_blank())
```

# Gradient Boosting Machine

We start with the `r ref("mlr_learners_classif.gbm", "GBM learner")` and set the predict type to `"prob"` to obtain class probabilities.

```{r 2023-02-07-recursive-feature-elimination-006}
learner = lrn("classif.gbm",
  distribution = "bernoulli",
  predict_type = "prob")
```

Now we define the feature selection problem by using the `r ref("fsi()")` function that constructs an `r ref("FSelectInstanceSingleCrit")`.
In addition to the task and learner, we have to select a `r ref("Resampling", text = "resampling strategy")` and `r ref("Measure", text = "performance measure")` to determine how the performance of a feature subset is evaluated.
We pass the `"none"` terminator because the `n_features` parameter of the optimizer determines when the feature selection stops.

```{r 2023-02-07-recursive-feature-elimination-007}
instance = fsi(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 6),
  measures = msr("classif.auc"),
  terminator = trm("none"))
```

We are now ready to start the RFE.
To do this, we simply pass the instance to the `$optimize()` method of the optimizer.

```{r 2023-02-07-recursive-feature-elimination-008}
#| output: false

optimizer$optimize(instance)
```

The optimizer saves the best feature set and the corresponding estimated performance in `instance$result`.

@fig-gbm shows the optimization path of the feature selection.
We observe that the performance increases first as the number of features decreases.
As soon as informative features are removed, the performance drops.

```{r 2023-02-07-recursive-feature-elimination-009}
#| code-fold: true
#| label: fig-gbm
#| fig-cap: "Performance of the gradient-boosting models depending on the number of features."

library(viridisLite)
library(mlr3misc)

data = as.data.table(instance$archive)
data[, n:= map_int(importance, length)]

ggplot(data, aes(x = n, y = classif.auc)) +
  geom_line(
    color = viridis(1, begin = 0.5),
    linewidth = 1) +
  geom_point(
    fill = viridis(1, begin = 0.5),
    shape = 21,
    size = 3,
    stroke = 0.5,
    alpha = 0.8) +
  xlab("Number of Features") +
  scale_x_reverse() +
  theme_minimal()
```

The importance scores of the feature sets are recorded in the archive.

```{r 2023-02-07-recursive-feature-elimination-010}
#| column: body-outset

as.data.table(instance$archive)[, list(features, classif.auc, importance)]
```

# Support Vector Machine

Now we will select the optimal feature set for an SVM with a linear kernel.
The importance scores are the weights of the model.

```{r 2023-02-07-recursive-feature-elimination-011}
learner = lrn("classif.svm",
  type = "C-classification",
  kernel = "linear",
  predict_type = "prob")
```


The `r ref("mlr_learners_classif.svm", "SVM learner")` does not support the calculation of importance scores at first.
The reason is that importance scores cannot be determined with all kernels.
This can be seen by the missing `"importance"` property.

```{r 2023-02-07-recursive-feature-elimination-012}
learner$properties
```

Using the `"mlr3fselect.svm_rfe"` callback however makes it possible to use a linear SVM with the `r ref("mlr_fselectors_rfe", "RFE")` optimizer.
The callback adds the `$importance()` method internally to the learner.
We load the callback with the `r ref("clbk()")` function and pass it as the `"callback"` argument to `r ref("fsi()")`.

```{r 2023-02-07-recursive-feature-elimination-013}
instance = fsi(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 6),
  measures = msr("classif.auc"),
  terminator = trm("none"),
  callback = clbk("mlr3fselect.svm_rfe"))
```

We start the feature selection.

```{r 2023-02-07-recursive-feature-elimination-014}
#| output: false

optimizer$optimize(instance)
```

@fig-svm shows the average performance of the SVMs depending on the number of features.
We can see that the performance increases significantly with a reduced feature set.

```{r 2023-02-07-recursive-feature-elimination-015}
#| code-fold: true
#| label: fig-svm
#| fig-cap: "Performance of the support vector machines depending on the number of features."

library(viridisLite)
library(mlr3misc)

data = as.data.table(instance$archive)
data[, n:= map_int(importance, length)]

ggplot(data, aes(x = n, y = classif.auc)) +
  geom_line(
    color = viridis(1, begin = 0.5),
    linewidth = 1) +
  geom_point(
    fill = viridis(1, begin = 0.5),
    shape = 21,
    size = 3,
    stroke = 0.5,
    alpha = 0.8) +
  xlab("Number of Features") +
  scale_x_reverse() +
  theme_minimal()
```

For datasets with a lot of features, it is more efficient to remove several features per iteration.
We show an example where 25% of the features are removed in each iteration.

```{r 2023-02-07-recursive-feature-elimination-016}
#| output: false

optimizer = fs("rfe", n_features = 1, feature_fraction = 0.75)

instance = fsi(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 6),
  measures = msr("classif.auc"),
  terminator = trm("none"),
  callback = clbk("mlr3fselect.svm_rfe"))

optimizer$optimize(instance)
```

@fig-svm-2 shows a similar optimization curve as @fig-svm but with fewer evaluated feature sets.

```{r 2023-02-07-recursive-feature-elimination-017}
#| code-fold: true
#| label: fig-svm-2
#| fig-cap: "Optimization path of the feature selection."

library(viridisLite)
library(mlr3misc)

data = as.data.table(instance$archive)
data[, n:= map_int(importance, length)]

ggplot(data, aes(x = n, y = classif.auc)) +
  geom_line(
    color = viridis(1, begin = 0.5),
    linewidth = 1) +
  geom_point(
    fill = viridis(1, begin = 0.5),
    shape = 21,
    size = 3,
    stroke = 0.5,
    alpha = 0.8) +
  xlab("Number of Features") +
  scale_x_reverse() +
  theme_minimal()
```

# Recursive Feature Elimination with Cross Validation

RFE-CV estimates the optimal number of features before selecting a feature set.
For this, an RFE is run in each resampling iteration and the number of features with the best mean performance is selected (see @fig-flowchart).
Then one more RFE is carried out on the complete dataset with the optimal number of features as the final feature set size.

```{mermaid}
%%| label: fig-flowchart
%%| fig-cap: Example of an RFE-CV. The optimal number of features is estimated with a 3-fold cross-validation. One RFE is executed with each train-test split (RFE 1 to RFE 3). The number of features with the best mean performance (green rectangles) is used as the size of the final feature set. A final RFE is performed on all observations. The algorithm stops when the optimal feature set size is reached (purple rectangle) and the optimized feature set is returned.

%%{ init: { 'flowchart': { 'curve': 'bump' } } }%%
flowchart TB
    cross-validation[3-Fold Cross-Validation]
    cross-validation-->rfe-1
    cross-validation-->rfe-2
    cross-validation-->rfe-3
    subgraph rfe-1[RFE 1]
    direction TB
    f14[4 Features]
    f13[3 Features]
    f12[2 Features]
    f11[1 Features]
    f14-->f13-->f12-->f11
    style f13 fill:#ccea84
    end
    subgraph rfe-2[RFE 2]
    direction TB
    f24[4 Features]
    f23[3 Features]
    f22[2 Features]
    f21[1 Features]
    f24-->f23-->f22-->f21
    style f23 fill:#ccea84
    end
    subgraph rfe-3[RFE 3]
    direction TB
    f34[4 Features]
    f33[3 Features]
    f32[2 Features]
    f31[1 Features]
    f34-->f33-->f32-->f31
    style f33 fill:#ccea84
    end
    all_obs[All Observations]
    rfe-1-->all_obs
    rfe-2-->all_obs
    rfe-3-->all_obs
    all_obs --> rfe
    subgraph rfe[RFE]
    direction TB
    f54[4 Features]
    f53[3 Features]
    f54-->f53
    style f53 fill:#8e6698
    end
```

We retrieve the `r ref("mlr_fselectors_rfecv", "RFE-CV")` optimizer.
RFE-CV has almost the same control parameters as the RFE optimizer.
The only difference is that no aggregation is needed.

```{r 2023-02-07-recursive-feature-elimination-018}
optimizer = fs("rfecv",
  n_features = 1,
  feature_number = 1)
```

The chosen resampling strategy is used to estimate the optimal number of features.
The 6-fold cross-validation results in 6 RFE runs.
You can choose any other resampling strategy with multiple iterations.
Let's start the feature selection.

```{r 2023-02-07-recursive-feature-elimination-019}
#| output: false

learner = lrn("classif.svm",
  type = "C-classification",
  kernel = "linear",
  predict_type = "prob")

instance = fsi(
  task = task,
  learner = learner,
  resampling = rsmp("cv", folds = 6),
  measures = msr("classif.auc"),
  terminator = trm("none"),
  callback = clbk("mlr3fselect.svm_rfe"))

optimizer$optimize(instance)
```

::: {.callout-warning}
The performance of the optimal feature set is calculated on the complete data set and should not be reported as the performance of the final model.
Estimate the performance of the final model with [nested resampling](https://mlr3book.mlr-org.com/optimization.html#sec-nested-resampling).
:::

We visualize the selection of the optimal number of features.
Each point is the mean performance of the number of features.
We achieved the best performance with 19 features.

```{r 2023-02-07-recursive-feature-elimination-020}
#| code-fold: true
#| label: fig-rfecv
#| fig-cap: "Estimation of the optimal number of features. The best mean performance is achieved with 19 features (blue line)."
library(ggplot2)
library(viridisLite)
library(mlr3misc)

data = as.data.table(instance$archive)[!is.na(iteration), ]
aggr = data[, list("y" = mean(unlist(.SD))), by = "batch_nr", .SDcols = "classif.auc"]
aggr[, batch_nr := 61 - batch_nr]

data[, n:= map_int(importance, length)]

ggplot(aggr, aes(x = batch_nr, y = y)) +
  geom_line(
    color = viridis(1, begin = 0.5),
    linewidth = 1) +
  geom_point(
    fill = viridis(1, begin = 0.5),
    shape = 21,
    size = 3,
    stroke = 0.5,
    alpha = 0.8) +
  geom_vline(
    xintercept = aggr[y == max(y)]$batch_nr,
    colour = viridis(1, begin = 0.33),
    linetype = 3
  ) +
  xlab("Number of Features") +
  ylab("Mean AUC") +
  scale_x_reverse() +
  theme_minimal()
```

The archive contains the extra column `"iteration"` that indicates in which resampling iteration the feature set was evaluated.
The feature subsets of the final RFE run have no value in the `"iteration"` column because they were evaluated on the complete data set.

```{r 2023-02-07-recursive-feature-elimination-021}
#| column: body-outset

as.data.table(instance$archive)[, list(features, classif.auc, iteration, importance)]
```

# Final Model

The learner we use to make predictions on new data is called the final model.
The final model is trained with the optimal feature set on the full data set.
The optimal set consists of 19 features and is stored in `instance$result_feature_set`.
We subset the task to the optimal feature set and train the learner.

```{r 2023-02-07-recursive-feature-elimination-022}
#| output: false

task$select(instance$result_feature_set)
learner$train(task)
```

The trained model can now be used to predict new, external data.

# Conclusion

The RFE algorithm is a valuable feature selection method, especially for high-dimensional datasets with only a few observations.
The numerous settings of the algorithm in mlr3 make it possible to apply it to many datasets and learners.
If you want to know more about feature selection in general, we recommend having a look at our [book](https://mlr3book.mlr-org.com/feature-selection.html).

