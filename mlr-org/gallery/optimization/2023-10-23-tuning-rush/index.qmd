---
title: "Tuning with the Rush Package"
description: |
  Efficient tuning with the parallel computing package rush.
categories:
  - feature selection
  - classification
author:
  - name: Marc Becker
    url: https://github.com/be-marc
date: 2023-10-23
bibliography: ../../bibliography.bib
knitr:
  opts_chunk:
    R.options:
      datatable.print.nrows: 12
---

{{< include ../../_setup.qmd >}}


```{r}
#| include: false

lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
set.seed(1)
```


# Scope

Parallel processing is a powerful tool to speed up the tuning of machine learning algorithms.
So far, `r ref("mlr3tuning")` has taken the approach to evaluating a batch of hyperparameter configurations in parallel.


that the tuner proposes a batch of hyperparameter configurations, which are then evaluated in parallel with `r ref("benchmark()")`.
The benchmark function uses the `r ref_pkg("future")` package to evaluate the configurations in parallel on different workers.
However, this kind of parallelization excludes some novel optimization algorithms and does not improve the runtime performance in all situations as expected.
If the runtime of the configurations is heterogeneous, the system may wait for the last configuration to finish, while the other workers are idle.
This is referred to as synchronization overhead and can reduce the runtime performance of the tuner drastically.
This blocking behavior also excludes newer algorithms that want to react immediately to a single evaluated configuration.
We address these issues with the `r ref_pkg("rush")` package.
The package implements a non-blocking parallelization strategy that is based on a [Redis](https://redis.io/) server.

```{r}
#| eval: false
remotes::install_github(c(
  "mlr-org/rush",
  "mlr-org/bbotk@rush",
  "mlr-org/mlr3tuning@rush"))
```


# Batch Parallelization

The current strategy of `r ref_pkg("mlr3tuning")` parallelizes the evaluation of hyperparameter configurations.
The tuner proposes a batch of configurations, which are then evaluated in parallel with `r ref("benchmark()")`.
The process waits until all configurations are evaluated and then continues with the next batch.
This approach has several disadvantages.
The `r ref("benchmark()")` function uses the `r ref_pkg("future")` package to evaluate the configurations in parallel on different workers.
The `r ref_pkg("future")` package has to serialize the learner, task and resampling in each tuning iteration.
This adds a significant overhead to each tuning iteration especially if the runtime of the configurations is short.

Another problem is that the tuning process is blocked until the last configuration of a batch is evaluated.
This is especially problematic if the runtime of the configurations is heterogeneous.
While the system waits for the last configuration to finish, the other workers are idle.
This is referred to as synchronization overhead and can reduce the runtime performance of the tuner drastically.

The chunk size determines how many configurations are evaluated in a single future.
Deciding the appropriate chunk size is difficult.
If the chunk size is too small, the overhead of creating the futures is too high.
A chunk size of 1 creates a new future for each configuration.
If the chunk size is too large and the runtimes are heterogeneous, the risk that many workers idle increases.

Blocking the process also prevents the implementation of some novel optimization algorithms.
For example, the asynchronous successive halving algorithm reacts immediately to the evaluation of a single configuration.
Waiting for the last configuration to finish would make the algorithm inefficient.

We have also observed that the main process is quickly overloaded when a large number of fast-fitting models are evaluated.
This makes it necessary to subdivide batches even further.

# Benchmark

```{r}
#| echo: false
#| label: fig-sleep-benchmark
#| fig-cap: "Runtime of a random search in seconds depending on batch size and chunk size for different training times. The first three runs use a constant training time of 10 ms, 100 ms and 1 second. The last three runs use a homogenous training time ranging from 10 ms to 10 seconds. Rush does not have a batch size and chunk size parameter."
#| fig-subcap:
#|   - "Random Search on 100 hyperparameter configuration with a 3-fold cross-validation on 3 workers."
#|   - "Random Search on 1000 hyperparameter configuration with a 3-fold cross-validation on 30 workers."
#| layout-ncol: 1
library(data.table)
library(mlr3misc)
library(ggplot2)
library(patchwork)

res = readRDS("/home/marc/repositories/mlr3website/mlr-org/gallery/optimization/2023-10-23-tuning-rush/sleep_benchmark.rds")

# plot 3 workers
plots = map(unique(res$sleep_train), function(sleep) {
  data = res[sleep_train == sleep & workers == 3, ]

  ggplot(data, aes(x = sleep_train, y = experiment, fill = result)) +
    geom_tile(alpha = 0.3) +
    geom_text(aes(label = result)) +
    scale_fill_distiller(palette = "BuGn", trans = "log") +
    theme_minimal() +
    theme(
      legend.position = "none",
      axis.title.y = element_blank(),
      axis.text.y = element_blank(),
      axis.title.x = element_blank(),
      axis.text.x = element_text(angle=45),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank())
})

plots[[1]] = plots[[1]] +
  ylab("Tuner Configuration") +
  scale_y_discrete(labels = c(
    "Batch 1 Chunk 1",
    "Batch 10 Chunk 10",
    "Batch 100 Chunk 1",
    "Batch 100 Chunk 10",
    "Batch 100 Chunk 100",
    "Rush")) +
  theme(
    axis.title.y = element_text(size = 10, angle = 90, vjust = 3),
    axis.text.y = element_text())


plots[[3]] = plots[[3]] +
  xlab("Sleep Time") +
  theme(
    axis.title.x = element_text(size = 10, hjust = 2.5)
    )

wrap_plots(plots, ncol = 6)

# plot 30 workers
plots = map(unique(res$sleep_train), function(sleep) {
  data = res[sleep_train == sleep & workers == 30, ]

  ggplot(data, aes(x = sleep_train, y = experiment, fill = result)) +
    geom_tile(alpha = 0.3) +
    geom_text(aes(label = result)) +
    scale_fill_distiller(palette = "BuGn", trans = "log") +
    theme_minimal() +
    theme(
      legend.position = "none",
      axis.title.y = element_blank(),
      axis.text.y = element_blank(),
      axis.title.x = element_blank(),
      axis.text.x = element_text(angle=45),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank())
})

plots[[1]] = plots[[1]] +
  ylab("Tuner Configuration") +
  scale_y_discrete(labels = c(
    "Batch 10 Chunk 1",
    "Batch 100 Chunk 10",
    "Batch 1000 Chunk 1",
    "Batch 1000 Chunk 10",
    "Batch 1000 Chunk 100",
    "Rush")) +
  theme(
    axis.title.y = element_text(size = 10, angle = 90),
    axis.text.y = element_text())


plots[[3]] = plots[[3]] +
  xlab("Sleep Time") +
  theme(
    axis.title.x = element_text(size = 10, hjust = 2.5)
    )

wrap_plots(plots, ncol = 6)
```

# Rush Parallelization

The parallelization with rush takes a different approach.
At the beginning of the tuning process, a future is started on each available worker.
The future contains a loop that evaluates configurations from a queue.
The future only stops when the tuning is finished.
This minimizes the overhead of serializing the learner, task and resampling.

The tuner proposes one or more configurations and sends them to a queue.
This process is non-blocking and the tuner can react immediately to the evaluation of a single configuration.
The overhead per configuration is in the range of 1 to 2 milliseconds.


![](flow.png)


# Rush Examples

```{r}
#| eval: false
library(rush)
library(mlr3tuning)

rush_plan(config = redux::redis_config())


#rush = rsh(network_id = "mlr3tuning")
#rush
```

The `ti()` function returns a `TuningInstanceRushSingleCrit` instead of a `TuningInstanceSingleCrit` when a `rush` controller is passed.

```{r}
#| eval: false
instance = ti(
  task = tsk("pima"),
  learner = lrn("classif.rpart", cp = to_tune(0.01, 0.1), minsplit = to_tune(1, 100)),
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 100),
  #rush = rush
)
```

The new instance has a new method `$start_workers()` to start the workers.
The `await_workers` argument determines if the main process waits until all workers are ready.

```{r}
#| eval: false
future::plan("multisession", workers = 2L)

#instance$start_workers(await_workers = TRUE)
instance$rush
```

The tuner classes of `mlr3tuning` are extended to support rush.
Just use the tuners as normal.
Internally a slightly different algorithm is used when parallelizing with rush.

```{r}
#| eval: false
tuner = tnr("random_search")
```

The optimization is started as always with the `$optimize()` method.

```{r}
#| eval: false
tuner$optimize(instance)
```


How does rush work?

* The overhead of creating futures in tuning iteration is eliminated. The workers run permanently and wait for new configurations.
* The tuning process is not blocked by the slowest configuration. The workers can immediately start with the next configuration.

## Supported Tuners

Currently, the tuners random search, grid search and design points from `mlr3tuning` are supported.
Depending on the scenario, small runtime performance improvements can be seen here.
The `mlr3hyperband` package adds asynchronous successive halving and hyperband to the list of supported tuners.
These tuners especially benefit from parallelization with rush.
The batch parallelization was always a major bottleneck for tuners of the hyperband family.

## Terminators

If the tuning was running on a system with time constraints e.g. a high-performance cluster, it was difficult to finish the tuning in time with long-running models.
Since the process was blocked until the last configuration was evaluated, the tuning could not be stopped immediately.
To prevent long-running models from threatening the whole tuning, a time limit had to be set on the training.
Too large batches also endangered the correct completion of the tuning.
With rush, these problems no longer exist.
The terminators are much more precise with rush since the termination criterion is constantly checked.
Learners that train far beyond the time limit and thus do not deliver a result are simply ignored when evaluating the tuning.

## Local Worker

A local worker runs on the same machine as the main tuning process.
We recommend using the `future` package with the `multisession` backend to spawn local workers.

## Remote Worker

A remote worker runs on a different machine than the main tuning process.
Remote workers can be started with `future` or manually with a bash script.
There is no restriction for the system on which the manually started worker runs. It must only be possible to establish a connection to the Redis server in the main process.

```{r}
#| eval: false
instance$create_worker_script()
```

## Reproducibility

Reproducibility can't be guaranteed with rush.
If multiple workers are used, the order in which the configurations are evaluated and finished is non-deterministic.
We recommend running the tuning on a single worker if reproducibility is important.

## Logging

The logging on the workers is not synchronized with the main process.
This means that the logging messages of the workers are not displayed in the console.
The logging messages are stored in the Redis database and can be retrieved with the `read_log()` function.
This is useful for debugging purposes.
To activate the logging on the workers, the `lgr_thresholds`  must be set when the workers are started.

```{r}
#| eval: false
instance$set_worker_options(
  lgr_thresholds = c(mlr3 = "info", bbotk = "warn", rush = "warn"))
```

This example sets the logging threshold of the `mlr3` package to `info` and the logging threshold of the `bbotk` and `rush` package to `warn`.
See the chapter on [logging](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-logging) for details on logging in `mlr3`.
To retrieve the logging messages, the `read_log()` function of the rush controller must be called.

```{r}
#| eval: false
rush$read_log()
```

## Heartbeat

The [encapsulation](https://mlr3book.mlr-org.com/chapters/chapter10/advanced_technical_aspects_of_mlr3.html#sec-encapsulation) mechanism of `mlr3` guards the tuning against segmentation faults and simple errors.
A heartbeat process gives additional safety when the tuning runs on remote workers.
The heartbeat is a mechanism to detect if a remote worker has crashed, was killed or lost the connection to the main process.
If a worker is lost, the evaluation of a hyperparameter configuration seems to run forever.
The heartbeat process periodically signals that the worker is still alive.
If the heartbeat is not received for a certain time, the worker is considered lost.
The option `detect_lost_tasks` of the `$start_workers()` function activates the detection of lost evaluations via the heartbeat.
The heartbeat is enabled by passing `heartbeat_period` and  `heartbeat_expire` when starting the workers.
The `heartbeat_period` is the time in seconds between two heartbeats.
The `heartbeat_expire` defines the time in seconds after which a worker is considered lost.

```{r}
#| eval: false
instance$set_worker_options(
  heartbeat_period = 1,
  heartbeat_expire = 3,
  detect_lost_tasks = TRUE)
```

In this example, the heartbeat process sends a heartbeat every second.
If the heartbeat is not received for three seconds, the worker is considered lost.
The tuning process marks the corresponding evaluations as failed.
The heartbeat is a separate process that runs on each worker.
Running the process adds a small overhead to the worker.

# Hotstarting

The hotstart mechanism of mlr3 can now finally be used effectively in tuning.


# Questions

How to implement seeds?
How to implement auto tuner? When call `start_workers()`? Set start options before?
Check hotstart stacks.



